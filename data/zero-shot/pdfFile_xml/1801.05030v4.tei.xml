<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting abnormal events in video using Narrowed Normality Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting abnormal events in video using Narrowed Normality Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate the abnormal event detection problem as an outlier detection task and we propose a two-stage algorithm based on k-means clustering and one-class Support Vector Machines (SVM) to eliminate outliers. In the feature extraction stage, we propose to augment spatio-temporal cubes with deep appearance features extracted from the last convolutional layer of a pre-trained neural network. After extracting motion and appearance features from the training video containing only normal events, we apply k-means clustering to find clusters representing different types of normal motion and appearance features. In the first stage, we consider that clusters with fewer samples (with respect to a given threshold) contain mostly outliers, and we eliminate these clusters altogether. In the second stage, we shrink the borders of the remaining clusters by training a one-class SVM model on each cluster. To detected abnormal events in the test video, we analyze each test sample and consider its maximum normality score provided by the trained one-class SVM models, based on the intuition that a test sample can belong to only one cluster of normality. If the test sample does not fit well in any narrowed normality cluster, then it is labeled as abnormal. We compare our method with several state-of-the-art methods on three benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve better results in most cases, while processing the test video in real-time at 24 frames per second on a single CPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Abnormal event detection in video is a challenging task in computer vision, since it is extremely hard, if not impossible, to define abnormal events independent of context. For example, a truck driving by on the street is considered a perfectly normal event, but if the truck drives through a pedestrian area, then it is regarded as an abnormal event. Another example that illustrates the importance of context is represented by two people fighting in a boxing ring (normal event) versus fighting on the street (abnormal event). In addition to the reliance on context, we can generally <ref type="bibr">Figure 1</ref>. Our anomaly detection framework based on Narrowed Normality Clusters. In the training phase, we apply a two-stage outlier detection algorithm based on k-means and one-class SVM. In the testing phase, we label a test sample as abnormal if its maximum normality score among the scores provided by the trained one-class SVM models is negative. Best viewed in color. agree that abnormal behavior should rather be represented by rare (less expected) events <ref type="bibr" target="#b19">[20]</ref> that do not occur as often as usual (more familiar) events. Due to the scarcity and variability of abnormal events, it is generally hard to obtain a representative set of anomalies at training time. Hence, traditional supervised learning methods are usually ruled out. Therefore, most abnormal event detection approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref> are based on learning a model of normality from training videos con-taining only familiar events. At test time, events are labeled as abnormal if they deviate from the normality model. We approach abnormal event detection in a similar manner, and propose to build a model of familiarity using the twostage outlier detection algorithm illustrated in <ref type="figure">Figure 1</ref>. We first extract spatio-temporal cubes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, which we augment with additional information about location, motion direction and object appearance. After extracting augmented spatio-temporal cubes from the training video containing only normal events, we apply k-means clustering to find clusters representing different types of normal motion and appearance. In the first stage, we eliminate the clusters with fewer samples (with respect to a pre-defined threshold), based on the hypothesis that these smaller clusters contain predominantly outliers. Different from other outlier detection approaches based on k-means <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21]</ref>, we do not modify the clustering algorithm. Instead, we propose a simple and straightforward approach that aims to coarsely remove some of the outliers, leaving the outliers that are harder to pinpoint for the second stage. In the second stage, we narrow down the borders of the remaining clusters by training a one-class Support Vector Machines (SVM) classifier on each individual cluster. In the end, the learned oneclass SVM models represent narrowed clusters of different types of normality. We therefore coin the term Narrowed Normality Clusters (NNC) for our two-stage outlier detection algorithm. To detected abnormal events in a test video, we analyze each augmented spatio-temporal cube and consider its maximum normality score among the scores provided by the trained one-class SVM models, based on the natural intuition that a test sample (spatio-temporal cube) should belong to a single narrowed cluster of normal motion and appearance. If the test sample does not fit well in any normality cluster, its corresponding maximum normality score will be negative. Consequently, the respective test sample is labeled as abnormal.</p><p>In summary, the novelty of our paper consists of (i) augmenting spatio-temporal cubes <ref type="bibr" target="#b26">[27]</ref> with deep appearance features, (ii) assembling together two popular methods for outlier detection (k-means and one-class SVM) into a simple and fast framework and (iii) narrowing down normality clusters by learning a tight border around each cluster.</p><p>We conduct experiments on the Avenue <ref type="bibr" target="#b26">[27]</ref>, the Subway [2] and the UMN <ref type="bibr" target="#b29">[30]</ref> data sets in order to compare our NNC approach with several state-of-the-art abnormal event detection methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. The empirical results indicate that, on two of the test sets (Avenue and Subway), we obtain better results than all these approaches. We also report the secondbest score on the third data set (UMN). It is important to mention that our approach yields impressive results, while running in real-time at 24 frames per second on a CPU.</p><p>We organize the paper as described next. We present re-lated work on abnormal event detection in Section 2. We describe our outlier detection framework in Section 3. We present the abnormal event detection experiments in Section 4. We draw our final conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Abnormal event detection is commonly formalized as an outlier detection task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, in which the general approach is to learn a model of normality from training data and label the detected outliers as abnormal events. Some abnormal event detection approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> are based on learning a dictionary of atoms representing normal events, and on labeling the events not represented in the dictionary as abnormal. At a conceptual level, we can find some resemblance between our approach and dictionary learning. However, going down to the implementation level, there are some important differences. We can interpret the use of k-means to group the training samples into clusters as an unconventional way of building a dictionary of atoms. To the best of our knowledge, there are no dictionary learning approaches that try to remove a part of the atoms as outliers. Unlike dictionary learning approaches, we eliminate the smaller clusters in our framework. Another difference is that we consider that a test sample can belong to a single cluster, or in other words, it can be reconstructed by a single atom. Hence, instead of using the reconstruction error given by a set of basis vectors as the abnormality score, we consider the maximum normality score among the scores given by a set of one-class SVM models, each trained on a different cluster.</p><p>Recent abnormal event detection approaches have employed locality sensitive hashing filters <ref type="bibr" target="#b41">[42]</ref> or deep features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> to achieve better results. Hasan et al. <ref type="bibr" target="#b14">[15]</ref> proposed two autoencoders, one that is learned on conventional handcrafted spatio-temporal local features, and another one that is learned end-to-end using a fully convolutional feed-forward architecture. Hinami et al. <ref type="bibr" target="#b16">[17]</ref> proposed to train convolutional neural networks (CNN) on multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events, while Smeureanu et al. <ref type="bibr" target="#b36">[37]</ref> simply applied convolutional neural networks pre-trained on the ILSVRC benchmark <ref type="bibr" target="#b32">[33]</ref>. Luo et al. <ref type="bibr" target="#b27">[28]</ref> proposed a Temporally-coherent Sparse Coding approach, which can be mapped to a stacked Recurrent Neural Network which facilitates parameter optimization and accelerates anomaly prediction. The approach presented in <ref type="bibr" target="#b30">[31]</ref> is based on training Generative Adversarial Nets (GAN) using normal frames and corresponding optical-flow images in order to learn an internal representation of the scene normality. The test data is compared with both the appearance and the motion representations reconstructed by the GAN and abnormal areas are detected by computing local differences. Liu et al. <ref type="bibr" target="#b25">[26]</ref> proposed a method for abnormal event detection based on a deep future frame prediction framework. The approach uses the difference between a predicted future frame and the ground-truth frame to detect abnormal events. For a better detection rate, the authors add a temporal constraint based on optical flow along with the spatial constraints.</p><p>There have been some approaches that employ unsupervised steps for abnormal event detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, the authors presented a method that constructs a model of familiar events from training data. The model is incrementally updated in an unsupervised manner as new patterns are observed in the test data. In a similar manner, Sun et al. <ref type="bibr" target="#b37">[38]</ref> proposed to train a Growing Neural Gas model starting with the training videos and continuing with the test videos, as the test videos are analyzed for anomaly detection. Ren et al. <ref type="bibr" target="#b31">[32]</ref> used an unsupervised approach, spectral clustering, to construct a dictionary of atoms, each representing a single type of normal behavior. However, the approach of Ren et al. <ref type="bibr" target="#b31">[32]</ref> requires training videos of normal events to build the dictionary. In order to learn deep feature representations in an unsupervised manner, Xu et al. <ref type="bibr" target="#b40">[41]</ref> employed Stacked Denoising Auto-Encoders. In the end, they used one-class SVM classifiers to detect the abnormal events. There are some works that do not require any kind of training data in order to detect abnormal events <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. The approach proposed by Del Giorno et al. <ref type="bibr" target="#b11">[12]</ref> detects changes in a short sequence of frames from the video by deciding which frames are distinguishable from all the previous frames. Since Del Giorno et al. <ref type="bibr" target="#b11">[12]</ref> aimed to obtain an approach independent of temporal ordering, they created shuffles of the test data by permuting the frames before running each instance of the change detection. Ionescu et al. <ref type="bibr" target="#b18">[19]</ref> applied unmasking, a technique based on training a binary classifier to distinguish between two consecutive short video sequences, while gradually removing the most discriminant features. Their hypothesis is that the higher training accuracy rates of the intermediately obtained classifiers represent abnormal events.</p><p>Regarding the feature representation, we use spatiotemporal cubes to represent motion, as other recent approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. Unlike all these approaches, we propose to augment each cube with its location within a spatial pyramid applied over the video frames, with the mean direction given by the 3D motion gradients inside the cube, and with deep appearance features. Our experiments show that the augmentation is useful. Regarding the outlier detection approach, there are a few works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> that applied k-means clustering for abnormal event detection. Abuolaim et al. <ref type="bibr" target="#b0">[1]</ref> used k-means at a coarse level to divide the data points into precisely three clusters: normal, abnormal and ambiguous. On the other hand, we apply k-means with a completely different purpose, namely to obtain many clusters representing different types of normality. Moreover, their approach does not allow to set an abnormality threshold, and thus, it cannot be optimized for better precision or recall. More closely to our approach, Auslander et al. <ref type="bibr" target="#b4">[5]</ref> defined three possible assumptions (see Section 4.1 in <ref type="bibr" target="#b4">[5]</ref>) for using clustering to detect anomalies. Interestingly, our approach is based on similar assumptions. However, their approach adopts only the first two assumptions defined in <ref type="bibr" target="#b4">[5]</ref>, while we satisfy the second assumption by eliminating smaller clusters (in the first stage), and the first and third assumptions by training a one-class SVM on each cluster (in the second stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose an abnormal event detection framework based on a two-stage algorithm for outlier detection. Our anomaly detection framework is divided into a training phase and a testing phase, as illustrated in <ref type="figure">Figure 1</ref>. We next provide an high-level summary of our approach, leaving the additional details about the more important steps for later. From both training and testing videos, we extract spatio-temporal cubes. In the training phase, we cluster the extracted spatio-temporal cubes using k-means and we eliminate the smaller clusters as outliers. On each remaining cluster, we train a one-class SVM model to remove outlier cubes. During inference, each spatio-temporal cube is tested against each one-class SVM model to obtain a set of normality scores. The maximum score is used (with a change of sign) as the abnormality score for the respective test cube. By putting together the cubes from an entire frame, we obtain an anomaly prediction map for each frame. To obtain pixel-level anomaly predictions, the prediction map can be simply resized to match the size of the input video frame. To obtain frame-level predictions, we consider the highest score in the prediction map as the anomaly score of the respective frame. We then apply a Gaussian filter to temporally smooth the final frame-level anomaly scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>Unlike other approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref>, we apply the same steps in order to extract motion and appearance features from video, irrespective of the data set. Encoding motion. Given the input video, we resize all frames to 120 ? 160 pixels and uniformly partition each frame to a set of non-overlapping 10 ? 10 patches. Corresponding patches in 5 consecutive frames are stacked together to form a spatio-temporal cube. The dimension of each spatio-temporal cube is 10 ? 10 ? 5. We next derive 3D gradient features from each spatio-temporal cube and normalize the resulted feature vectors using the L 2 -norm. Until this point, our approach of representing motion is essentially the same as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, we eliminate cubes in a region, if the video is static in the respective region. Different from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>, we do not em-ploy Principal Component Analysis to reduce the feature vector dimension from 500 to 100 components, as it has no impact on the performance. Moreover, we diverge from the standard spatio-temporal cube representation by augmenting the cubes with additional information about location, motion direction and object appearance, as described next. Encoding location. We divide each frame into a spatial pyramid <ref type="bibr" target="#b23">[24]</ref> with two levels, the first level containing 2 ? 2 bins and the second one containing 4 ? 4 bins. We encode the location of each spatio-temporal cube as a one-hot vector for each level of the pyramid. This gives 20 additional features (2 ? 2 + 4 ? 4) for each cube. The purpose of recording spatial information into the cube representation is to accurately detect situations in which abnormal events can appear in only some region of the video. For instance in a traffic surveillance video, people crossing the street on a crosswalk is a normal event, but if they cross it outside the designated area this should be labeled as abnormal. Encoding mean direction. To extract the mean motion direction from each spatio-temporal cube, we first consider the individual patches of the cube. In each patch, we compute the center of mass of the 3D gradients. We then encode the displacement of the center of mass in consecutive patches as vectors representing motion direction. For a better estimation of the mean motion direction, we also compute motion direction vectors after dividing each patch into 2 ? 2 bins. Finally, the motion direction vectors are quantized into an orientation-based histogram with 8 bins. The histogram bins are evenly spread over 0 to 360 degrees. Our histogram is produced in a similar way to the histogram corresponding to a cell in the Histograms of Oriented Gradients (HOG) descriptor <ref type="bibr" target="#b10">[11]</ref>. Along with the histogram, we add another feature given by the sum of all vector magnitudes. In total, there are 9 additional features for augmenting the cube. The purpose of recording the mean direction into the cube representation is to enable the accurate detection of abnormal events triggered by objects moving in a certain direction. For example in a traffic surveillance video, a car driving the wrong way should be labeled as abnormal. Encoding object appearance. In many computer vision tasks, for instance image difficulty prediction <ref type="bibr" target="#b17">[18]</ref>, higher level features, such as the ones learned with convolutional neural networks (CNN) <ref type="bibr" target="#b22">[23]</ref> are the most effective. To build our appearance features, we consider a shallow pre-trained CNN architecture, namely VGG-f <ref type="bibr" target="#b6">[7]</ref>, which is able to process the video frames in real-time on a CPU. Considering that we want our detection framework to work in real-time on a standard desktop computer, not equipped with expensive GPU, the VGG-f architecture is an excellent choice. We hereby note that better anomaly detection performance can probably be achieved by employing deeper CNN architectures, such as VGG-verydeep <ref type="bibr" target="#b35">[36]</ref> or ResNet <ref type="bibr" target="#b15">[16]</ref>. We use a VGG-f model pre-trained on the ILSVRC bench- mark <ref type="bibr" target="#b32">[33]</ref> to extract deep features as follows. Given the input video, we resize the frames to 224?224 pixels. We then subtract the mean imagine from each frame and provide it as input to the VGG-f model. We remove the fully-connected layers (identified as fc6, fc7 and softmax) and consider the activation maps of the last convolutional layer (conv5) as appearance features. While the fully-connected layers are adapted for object recognition, the last convolutional layer contains valuable appearance and pose information which is more useful for our anomaly detection task. Ideally, we would like to have at least slightly different representations for a person walking versus a person running, hence conv5 is more suitable than fc6 or fc7. From the conv5 layer, we obtain 256 activation maps, each of 13 ? 13 units. We then resize each activation map to match the number of spatiotemporal cubes in a frame, i.e. from 13?13 to 12?16, using bicubic interpolation. We then concatenate each set of 256 filter activations to the corresponding spatio-temporal cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-Stage Outlier Detection</head><p>First stage detection based on k-means. We cluster the augmented spatio-temporal cubes extracted from the training video to find clusters representing different types of normality. Next, we eliminate the clusters with fewer samples, based on the assumption that these smaller clusters contain mostly outlier samples. We note that the same assumption also sits at the basis of the method proposed in <ref type="bibr" target="#b4">[5]</ref>. Nonetheless, we motivate the assumption through the following toy example. We generate 400 data points sampled from two normal distributions of different means. We group the points into k = 30 clusters using k-means and we illustrate the result in <ref type="figure" target="#fig_0">Figure 2</ref>. We then count the number of points in each cluster and obtain the histogram depicted in <ref type="figure">Figure 3</ref>. In this example, we consider that the clusters with less than 10 data points contain mostly outliers. The centroids of these smaller clusters are marked with a large blue <ref type="figure">Figure 3</ref>. A histogram representing the number of data points in each cluster. The histogram corresponds to the k-means clustering applied over the 400 data points illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. A threshold of 10 is used to detect clusters of outliers. Best viewed in color. square in <ref type="figure" target="#fig_0">Figure 2</ref>. We can clearly see that the marked clusters are farthest from both normal distribution means, indicating that the containing points are indeed outliers. Nevertheless, our aim is to test out the assumption on real data, in the context of abnormal event detection in video. Although the training does not contain abnormal events, we believe that k-means helps to remove noisy or weak patterns that can occur in the normal video.</p><p>Second stage detection based on one-class SVM. After removing the smaller k-means clusters, we are left with a set of clusters C = {c 1 , c 2 , ..., c r | r ? k} that accurately model the stronger patterns of normality. However, k-means does not provide a tight boundary around the remaining clusters, and, in some cases, it leaves a lot of room to accommodate outliers. For example, the borders of some remaining clusters represented in <ref type="figure" target="#fig_0">Figure 2</ref> span to infinity. To alleviate this problem, we propose to narrow down the borders of the remaining clusters by training a one-class SVM <ref type="bibr" target="#b34">[35]</ref> classifier on each cluster. We note that the border learned by SVM is tighter (or narrower) than the border of the original cluster (which includes all cluster's samples), since the oneclass SVM model is forced to single out a small percentage of samples within the cluster as outliers. In this regard, we can state that one-class SVM narrows (or tightens) the border around the cluster's centroid. Hence, the learned oneclass SVM models can be interpreted as a set of narrowed clusters representing different types of normal motion and appearance. To train our set of classifiers, we consider each spatio-temporal cube as an independent and individual sample, disregarding the temporal relations among cubes. Let X = {x 1 , x 2 , ..., x n | x i ? R m } denote the set of training cubes in a given cluster c j . The one-class SVM model corresponding to a cluster c j will learn to separate a small region capturing the normal cubes inside cluster c j from the rest of the feature space, by maximizing the distance from the separating hyperplane to the origin. This associates to each cluster a binary classification function g that singles out a region in the input space where the probability density of a particular type of normality lives:</p><formula xml:id="formula_0">g(x) = sign n i=1 ? i k(x, x i ) ? ? ,<label>(1)</label></formula><p>where x is a test cube that must be classified either as normal or abnormal, x i ? X is a training cube, ? i are the weights assigned to the support vectors x i , ? is the distance from the hyperplane to the origin, and k is a kernel function, in our case, the linear kernel. If we just need a score reflecting the normality level of a spatio-temporal cube, we can simply remove the sign transfer function from Equation <ref type="formula" target="#formula_0">(1)</ref> and obtain a scoring function. It is important to note that for each cluster c j ? C, we have a different scoring function g cj . Then, for a given test cube, we will have a set of r normality scores. However, since the narrowed clusters are independent (they reside in different areas of the feature space), we can naturally assume that a spatio-temporal cube belongs to a single cluster. Therefore, we consider the maximum normality score, the one that corresponds to the narrowed cluster that better fits (is closer to) the test cube.</p><p>If the test spatio-temporal cube does not fit well in any normality cluster, its corresponding maximum normality score will be negative (the cube is outside the nearest cluster).</p><p>Consequently, the respective test sample is labeled as abnormal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>We consider three data sets for the abnormal event detection experiments. Avenue. The Avenue data set <ref type="bibr" target="#b26">[27]</ref> is composed of 16 training videos and 21 test videos. In total, the Avenue data set contains 15328 frames for training and 15324 frames for testing. The resolution of each frame is 360 ? 640 pixels. The locations of anomalies are annotated in groundtruth pixel-level masks for each frame in the test videos. Hinami et al. <ref type="bibr" target="#b16">[17]</ref> argued that the Avenue test set contains five videos <ref type="bibr">(1, 2, 8, 9 and 10)</ref> with static abnormal objects that are not properly annotated. Hence, they evaluated their approach on a subset (Avenue17) that excludes these five videos. When we compare our results with those reported in <ref type="bibr" target="#b16">[17]</ref>, we also remove the same five videos for a fair comparison. Subway. The Subway surveillance data set <ref type="bibr" target="#b1">[2]</ref> is one of the largest data sets for anomaly detection in video. The Subway data set is formed of two videos, one of 96 minutes (Entrance gate) and another one of 43 minutes (Exit gate). The Entrance gate video contains 144251 frames, while the Exit gate video contains 64903 frames. The resolution of each video frame is 384 ? 512 pixels. Abnormal events are labeled at the frame level. UMN. The UMN Unusual Crowd Activity data set <ref type="bibr" target="#b29">[30]</ref> is composed of three different crowded scenes of various lengths. The first scene contains 1453 frames, the second scene contains 4144 frames, and the third scene contains 2144 frames. The resolution of each frame is 240 ? 320 pixels. In the normal scenario, people walk around in the scene, and the abnormal behavior is defined as people running in all directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>As evaluation metrics, we opt for ROC curves and the corresponding area under the curve (AUC) computed with respect to ground-truth frame-level annotations, and, when available (for the Avenue data set), pixel-level annotations. We use the same frame-level and pixel-level AUC definitions as in previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. At the frame-level, a frame is considered a correct detection if it contains at least one abnormal pixel. At the pixel-level, the corresponding frame is considered as being correctly detected if more than 40% of truly anomalous pixels are detected. Before the evaluation, we smooth the pixel-level detection maps with the same filter used by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> in order to obtain the final pixel-level detections. We exclude the recent approach (based on k-means) of Abuolaim et al. <ref type="bibr" target="#b0">[1]</ref> from our evaluation, because their approach is constrained to provide a single point on the ROC curve and the framelevel or pixel-level AUC metrics cannot be determined in their case. Many works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> include the Equal Error Rate (EER) as evaluation metric, but some recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> argue that metrics such as the EER can be misleading in a realistic anomaly detection setting, in which abnormal events are expected to be very rare. As we agree with perspective of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, we refrain from employing the EER in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter and Implementation Details</head><p>We extract spatio-temporal cubes from the training and the test video sequences using the code available online at https://alliedel.github.io/anomalydetection/. We use our own implementation to augment the cubes with location and mean direction. For the appearance features, we consider the pre-trained VGG-f <ref type="bibr" target="#b6">[7]</ref> model provided in MatCon-vNet <ref type="bibr" target="#b39">[40]</ref>, and extract the features from the conv5 layer. For a faster processing time, we extract features for one in every two frames in the test video, without observing any drop in performance. To cluster the augmented cubes, we employ the k-means implementation from VLFeat <ref type="bibr" target="#b38">[39]</ref> based on the original Lloyd algorithm <ref type="bibr" target="#b12">[13]</ref>. We use k-means++ <ref type="bibr" target="#b3">[4]</ref> initialization. We repeat the clustering 10 times and choose the partitioning with the minimum energy. We choose the number of clusters k such that we have on average 1000 cubes per cluster, hence k is always proportional to the number of training cubes. For instance, there are almost 280 thousand cubes extracted from the Avenue training videos, hence we set k = 280 for the Avenue data set. We then eliminate the clusters with less than 500 cubes, irrespective of the data Method AUC Frame Pixel Lu et al. <ref type="bibr" target="#b26">[27]</ref> 80.9 92.9 Hasan et al. <ref type="bibr" target="#b14">[15]</ref> 70.2 -Del Giorno et al. <ref type="bibr" target="#b11">[12]</ref> 78.3 91.0 Smeureanu et al. <ref type="bibr" target="#b36">[37]</ref> 84.6 93.5 Ionescu et al. <ref type="bibr" target="#b18">[19]</ref> 80.6 93.0 Luo et al. <ref type="bibr" target="#b27">[28]</ref> 81.7 -Liu et al. <ref type="bibr" target="#b25">[26]</ref> 85.  <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, which are listed in temporal order.</p><p>set. To remove the outliers from each cluster, we employ the one-class SVM implementation from LibSVM <ref type="bibr" target="#b5">[6]</ref>. In all the experiments, we set the regularization parameter of one-class SVM to 0.01, which means that the model will have to single out 99% of the training cubes as normal (the other 1% are outliers). At test time, we are able to process the test videos at nearly 24 FPS on a computer with Intel Core i7 2.3 GHz processor and 8 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on the Avenue Data Set</head><p>We first compare our approach with several state-of-theart approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref> that reported results on the Avenue data set. The corresponding framelevel and pixel-level AUC metrics are presented in <ref type="table">Table 1</ref>. The table also includes ablation results for stripped-down versions of our approach, to show the performance gain brought by each component. A basic approach based on spatio-temporal cubes and one-class SVM yields a framelevel AUC of 81.3%. When we augment the cubes, the frame-level AUC grows by 2.9% (up to 84.2%). Another 2.2% (up to 86.4%) are gained when we employ k-means and train one-class SVM models on all k clusters. By removing the smaller clusters, we obtain an improvement of 2.5% and reach a frame-level AUC of 88.9%. We also tested an approach that removes the smaller k-means clusters in the first stage, but replaces the one-class SVM in the second stage with a one nearest neighbor (1-NN) model based the Euclidean distance to the nearest cluster centroid. The obtained frame-level AUC is 78.8%, which is nearly 10% lower than the result of NNC. This ablation result shows the importance of using one-class SVM after kmeans in order to learn a tight border around each cluster.</p><p>Using Narrowed Normality Clusters of augmented spatio-temporal cubes, we are able to surpass the results re-Method Frame AUC Pixel AUC Hinami et al. <ref type="bibr" target="#b16">[17]</ref> 89.8 -NNC (ours) 91.1 94.3 <ref type="table">Table 2</ref>. Abnormal event detection results (in %) in terms of framelevel and pixel-level AUC on the Avenue17 data set. Our framework is compared with <ref type="bibr" target="#b16">[17]</ref>.  ported in previous works in terms of frame-level and pixellevel AUC. Compared to the most recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, our framework brings an improvement of more than 3.8% in terms of frame-level AUC. Since our framework is able to process the video online on a single CPU, we consider that our results on the Avenue data set are noteworthy.</p><p>We also compare our approach with <ref type="bibr" target="#b16">[17]</ref> on the Av-enue17 data set, a subset of the Avenue data set. Our framelevel AUC scores presented in <ref type="table">Table 2</ref> are better than those reported by Hinami et al. <ref type="bibr" target="#b16">[17]</ref>. It is worth nothing that our approach yields better performance on the Avenue17 subset than on the full Avenue data set, indicating that the five removed test videos are actually more difficult than those left in Avenue17. As Hinami et al. <ref type="bibr" target="#b16">[17]</ref> observed, the removed videos contain abnormal objects that are not properly annotated, hence methods are prone to reach higher false positive rates on these five test videos. <ref type="figure" target="#fig_1">Figure 4</ref> depicts the frame-level anomaly scores produced by our approach against the ground-truth labels on test video 4 of the Avenue data set. We notice that our scores correlate well with the ground-truth labels. There are two abnormal events in this video and we can easily identify</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Frame AUC Entrance gate Exit gate Average Cong et al. <ref type="bibr" target="#b9">[10]</ref> 80.0 83.0 81.5 Saligrama et al. <ref type="bibr" target="#b33">[34]</ref> 89.1 --Cheng et al. <ref type="bibr" target="#b8">[9]</ref> 92.7 --Hasan et al. <ref type="bibr" target="#b14">[15]</ref> 94.3 80.7 87.5 Del Giorno et al. <ref type="bibr" target="#b11">[12]</ref> 69  <ref type="table">Table 3</ref>. Abnormal event detection results (in %) in terms of framelevel AUC on the Subway data set. Our framework is compared with several state-of-the-art approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>, which are listed in temporal order. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on the Subway Data Set</head><p>Although there are many works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> that report results on the Subway data set, some of these works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref> did not use the frame-level AUC as evaluation metric. Therefore, we only compare our approach with those methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34</ref>] that reported the frame-level AUC. The results of our comparative study are reported in <ref type="table">Table 3</ref>. On the Entrance gate video, we obtain the second-best score, after Hasan et al. <ref type="bibr" target="#b14">[15]</ref>. They report a frame-level AUC of 94.3%, which is 0.8% higher than our score (93.5%). Things look differently on the Exit gate video, as we obtain the best score (95.1%) among all methods, surpassing the approach of Hasan et al. <ref type="bibr" target="#b14">[15]</ref> by 14.4% and the second-best method <ref type="bibr" target="#b18">[19]</ref> by 8.8%. On average (last column in <ref type="table">Table 3</ref>), we obtain the best results on the Subway data set.</p><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we present some interesting qualitative re-  <ref type="table">Table 4</ref>. Abnormal event detection results (in %) in terms of framelevel AUC on the UMN data set. Our framework is compared with several state-of-the-art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, which are listed in temporal order. sults obtained by our framework on the Entrance gate video. The true positive abnormal events are a person jumping over the fence, two people walking in the wrong direction and a person jumping over the gate, while false positive detections are a person running and two people walking synchronously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on the UMN Data Set</head><p>On the UMN data set, we compare our approach with several methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. In Table 4, we report the frame-level AUC score for each individual scene, as well as the average score for all the three scenes. It is worth noting that UMN seems to be the easiest abnormal event detection data set, since most works report frame-level AUC scores above 95.0%. We reach the highest performance (99.9%) among all methods on the first scene. On the last scene, we obtain the second best score (99.8%). Remarkably, our approach is able to correctly identify the three abnormal events in the third scene without any false positives, by applying a threshold of 0.5, as illustrated in <ref type="figure" target="#fig_4">Figure 7</ref>. Our lowest performance (98.2%) is on the second scene. Over all scenes, we reach the second highest framelevel AUC (99.3%), which is 0.4% lower than the best score (99.7%) obtained by Sun et al. <ref type="bibr" target="#b37">[38]</ref>.</p><p>In <ref type="figure" target="#fig_5">Figure 8</ref>, we present some interesting qualitative results obtained by our framework on the second scene, as it was almost impossible to find false positive detections in the other scenes. The true positive examples represent people running around in all directions, while the false detections are triggered by people opening the doors to enter or exit the room. In the first (left-most) false positive example, it seems that our method detects the significant amount of light that enters the room as the doors open. Perhaps the first impression is that our approach is not robust to illumination variations. However, we noticed that our training video does not contain examples of people walking through the doors. Therefore, it is impossible to learn a complete model of normality that includes this kind of event (people walking through the doors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this work, we proposed Narrowed Normality Clusters, a novel framework for abnormal event detection in video that is based on a two-stage outlier elimination algorithm. The algorithm works by removing outlier clusters obtained with k-means and by learning a tight border around each remaining cluster using one-class SVM. Our secondary contribution was to augment the spatio-temporal cubes with location, motion direction, and deep appearance features. We conducted abnormal event detection experiments on three data sets to compare our approach with a series of state-ofthe-art approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. The empirical results indicate that our approach yields better performance than all other methods on the Avenue and the Subway data sets. Furthermore, our approach is second best on the UMN data set. At the same time, we can process the test video in real-time at 24 frames per second on a single CPU. In future work, we aim to develop an approach to train deep features on a closely related task, such as action recognition, and transfer the learned features to our task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A set of 400 data points sampled from two normal distributions of different means. The points are clustered into 30 clusters using k-means. The centroids of clusters with less than 10 samples are represented with a large blue square. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Frame-level anomaly detection scores (between 0 and 1) provided by our approach for test video 4 in the Avenue data set. The video has 947 frames. Ground-truth abnormal events are represented in pink and our scores are illustrated in blue. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our framework. Examples are selected from the Avenue data set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our framework. Examples are selected from the Subway Entrance gate. Best viewed in color. both of them by setting a threshold of 0.5, without including any false positive detections. We also show some examples of true positive and false positive detections inFigure 5. The true positive abnormal events are (from left to right) a person running, a child running and a person throwing an object. The first (left-most) false positive detection represents two people walking synchronously. The last two false positive examples indicate that our method detects a child running even if the child is partially occluded, or a person throwing an object before the object is in the air.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Frame-level anomaly detection scores (between 0 and 1) provided by our framework for the third scene in the UMN data set. The video has 1744 test frames. Ground-truth abnormal events are represented in pink and our scores are illustrated in blue. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our framework. Examples are selected from the UMN data set. Best viewed in color.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The work of Radu Tudor Ionescu was supported by a grant of Ministery of Research and Innovation, CNCS -UEFISCDI, project number PN-III-P1-1.1-PD-2016-0787, within PNCDI III.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Essence of Unsupervised Detection of Anomalous Motion in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Leow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CAIP</title>
		<meeting>CAIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">k-means++: The Advantages of Careful Seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparative evaluation of anomaly detection algorithms for local maritime video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Auslander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LibSVM: A Library for Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>2:27:1-27:27</idno>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm.6" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">k-means--: A unified approach to clustering and outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Centroidal Voronoi Tessellations: Applications and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gunzburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="676" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3755" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How hard can it be? Estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2157" to="2166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Principled Approach to Detecting Surprising Events in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="631" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-phase clustering process for outliers detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="691" to="700" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos using Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating the support of a highdimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10485</biblScope>
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">VLFeat: An Open and Portable Library of Computer Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACMMM</title>
		<meeting>eeding of ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>pages 8.1-8.12, 2015. 1, 2, 3</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
