<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ARTEMIS: ATTENTION-BASED RETRIEVAL WITH TEXT-EXPLICIT MATCHING &amp; IMPLICIT SIMILARITY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ginger</forename><surname>Delmas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ARTEMIS: ATTENTION-BASED RETRIEVAL WITH TEXT-EXPLICIT MATCHING &amp; IMPLICIT SIMILARITY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the first provides rich and implicit context for the search, the latter explicitly calls for new traits, or specifies how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the features of each of the two elements of the query into a single representation, which can then be compared to the ones of the potential target images. Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval. Taking inspiration from them, we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modifiers. Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works. Our code is available at https://github.com/naver/artemis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When using an image search engine, should the user provide a detailed textual description or a visual example? These two image retrieval tasks, respectively studied as cross-modal search <ref type="bibr" target="#b49">(Wang et al., 2016)</ref> and visual search <ref type="bibr" target="#b38">(Philbin et al., 2007)</ref> in the computer vision community, have been the topic of extensive studies.</p><p>However, limiting search queries to a single modality is restrictive. On the one hand, text proposes an accurate but only partial depiction of the desired result, as it is impossible to provide an exhaustive textual description. On the other hand, visual queries are richer but a lot more ambiguous: how should the system guess which characteristics the user wants to maintain for the target image? The task of image search with free-form text modifiers reduces this ambiguity by allowing a dual query composed of an example (called reference) image, and a textual description (called text modifier) which explains how the reference image should be modified to obtain the desired result (the target image). This task is illustrated in <ref type="figure">Figure 1</ref>. From this formulation, a challenging research question arises: How should the two facets of this dual query be leveraged and combined?</p><p>The first methods to answer this question directly are borrowed from the Visual Question Answering literature <ref type="bibr" target="#b22">(Kim et al., 2016;</ref><ref type="bibr" target="#b42">Santoro et al., 2017)</ref>. The most standard approach for this task consists in fusing the features of the two components of the query into a single representation, so it can be compared to the representation of any potential target image <ref type="bibr" target="#b48">(Vo et al., 2019;</ref>. Among the current strategies, some resort to rich external information <ref type="bibr" target="#b28">Liu et al., 2021)</ref> while others rely on multi-level visual representations  or heavy cross-attention architectures <ref type="bibr" target="#b19">(Hosseinzadeh &amp; Wang, 2020;</ref><ref type="bibr" target="#b5">Chawla et al., 2021;</ref><ref type="bibr" target="#b28">Liu et al., 2021)</ref>.</p><p>Departing from these strategies, we draw inspiration from two fields related to the task, cross-modal and visual search, and we advocate for a combination of the two components of the query taking into account their specific relationship with the target image. Additionally, we note that the text modifier provides some insights about what should change (as an explicit request from the user), and what should be preserved in the target image with respect to the reference image (i.e. what is <ref type="figure">Figure 1</ref>: Image search with free-form text modifiers (shown in yellow) enriches the visual query with text in natural language. The task differs from visual search (using only the reference image, shown in red), and cross-modal search (using only the textual cue, shown in green). Yet, these can be seen as two complementary aspects of the studied task. Our method leverages both modalities to look for target images that i) explicitly match (EM) the characteristics mentioned in the text, and ii) bear some resemblance with the reference image using text-guided implicit similarity (IS). not mentioned in the text modifier). All these observations lead to the design of two independent modules, one for each modality, that are eventually combined.</p><p>Our Explicit Matching (EM) module measures the compatibility of potential target images with the properties mentioned in the textual part of the query, as a form of cross-modal retrieval. Simultaneously, our Implicit Similarity (IS) module considers the relevance of the target images with respect to the properties of the reference image implied by the textual modifier.</p><p>Additionally, each of our modules uses a light-weight attention mechanism, guided by the text modifier which helps selecting the characteristics of the target image to focus on for retrieval. This results in our proposed model, named Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity (ARTEMIS), which jointly learns these two modules, their respective attention mechanisms, and the way they should be combined, in a unified manner.</p><p>In summary, our contribution is twofold. First, we suggest a new way to look at the task of image search with free-form text modifiers, where we draw inspiration from -and combine -the two fields of cross-modal and visual retrieval. Second, we propose a computationally efficient model based on two complementary modules and their respective text-guided light-weight attention layers, where each handles one modality of the query. These modules are trained jointly.</p><p>We experimentally validate ARTEMIS on several datasets and show that, despite its simplicity in terms of architecture and training strategy, it consistently outperforms the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image retrieval with purely visual queries has been extensively studied and is typically referred to as instance-level image retrieval <ref type="bibr" target="#b38">(Philbin et al., 2007)</ref>. Most approaches either learn global image representations for efficient search <ref type="bibr" target="#b37">(Perronnin et al., 2010;</ref><ref type="bibr" target="#b39">Radenovi? et al., 2016;</ref><ref type="bibr" target="#b14">Gordo et al., 2016)</ref> or extract local descriptors for a costly yet robust matching <ref type="bibr" target="#b1">(Arandjelovi? &amp; Zisserman, 2012;</ref><ref type="bibr" target="#b33">Noh et al., 2017)</ref>. Current state-of-the-art models use both global and local descriptors sequentially, using the latter to refine the initial global ranking <ref type="bibr" target="#b4">(Cao et al., 2020;</ref><ref type="bibr" target="#b46">Tolias et al., 2020)</ref>.</p><p>The most standard retrieval scenario involving text, cross-modal retrieval, uses a textual description of the image of interest as a query. A common approach is to learn a joint embedding space and modality-specific encoders producing global representations in that shared space. Then, the task boils down to computing distances between those representations <ref type="bibr" target="#b12">(Frome et al., 2013;</ref><ref type="bibr" target="#b49">Wang et al., 2016;</ref><ref type="bibr" target="#b11">Faghri et al., 2018)</ref>. The most recent works focus on cross-attention between text segments and image region features <ref type="bibr" target="#b23">(Lee et al., 2018;</ref>. However, these models do not scale, and they require to score each potential text-query target-image pair independently. <ref type="bibr" target="#b31">Miech et al. (2021)</ref> address this issue by distilling a cross-attention architecture into a dual encoder one.</p><p>Despite similitudes between these two tasks and the one of image search with text modifiers, the first baselines for the latter initially adapted visual question answering (VQA) approaches <ref type="bibr" target="#b22">(Kim et al., 2016)</ref>. To reason with features from different modalities, <ref type="bibr" target="#b42">Santoro et al. (2017)</ref> use a sequence of MLPs, whereas FiLM <ref type="bibr" target="#b36">(Perez et al., 2018)</ref> injects text features into the image encoder at multiple layers, altering its behavior through complex modifications. Current approaches address this task by composing the two query elements into a single joint representation that is compared to any potential target image feature. As such, in TIRG <ref type="bibr" target="#b48">(Vo et al., 2019)</ref>, the reference image and the "relative captions" (or text modifiers) are fused through a gating-residual mechanism, and the text feature acts as a bridge between the two images in the visual representation space. <ref type="bibr" target="#b0">Anwaar et al. (2021)</ref> use an autoencoder-based model to map the reference and the target images into the same complex space and learn the text modifier representation as a transformation in this space. <ref type="bibr" target="#b24">Lee et al. (2021)</ref> and <ref type="bibr" target="#b5">Chawla et al. (2021)</ref> both propose to disentangle the multi-modal information into content and style.  resort to image's descriptive texts as side information to train a joint visual-semantic space, training a TIRG model on top. The alignment of visual and textual features for regularisation is reused in VAL  which also inserts a composite transformer at many levels of the visual encoder to preserve and transform the visual content depending on the text modifier; the model is then optimized with a hierarchical matching objective. <ref type="bibr" target="#b19">Hosseinzadeh &amp; Wang (2020)</ref> also align image and text through a cross-modal module, but they use region proposals instead of CNN activations. <ref type="bibr" target="#b28">Liu et al. (2021)</ref> propose a transformerbased model that leverages rich pre-trained vision-and-language knowledge for modifying the visual features of the query conditioned by the text. We note that these last methods are reminiscent of the heavy cross-attention models and share the same limitations with respect to scaling.</p><p>In contrast to most methods described above, ARTEMIS does not compose modalities into a joint global feature for the query <ref type="bibr" target="#b48">(Vo et al., 2019;</ref><ref type="bibr" target="#b24">Lee et al., 2021)</ref>, does not compute costly cross-attention involving the target image <ref type="bibr" target="#b19">(Hosseinzadeh &amp; Wang, 2020;</ref><ref type="bibr" target="#b5">Chawla et al., 2021)</ref> and does not extract multi-level visual representations . Instead it leverages the textual modifier in simple attention mechanisms to weight the dimensions of the visual representation, emphasizing the characteristics on which the matching should focus. This results in a model with a manageable amount of parameters to learn, efficient inference on the query side, and that leads to state-of-the-art results.</p><p>The term attention has been heavily used in the past, referring to light-weighting mechanisms <ref type="bibr" target="#b52">(Xu et al., 2015;</ref><ref type="bibr" target="#b21">Kang et al., 2018;</ref><ref type="bibr" target="#b50">Wang et al., 2019</ref>) similarly to us, or to more complex weighting mechanisms <ref type="bibr" target="#b30">(Lu et al., 2016)</ref>. This differs from the more recent definition of attention introduced by <ref type="bibr" target="#b47">Vaswani et al. (2017)</ref>, which became standard in many approaches tackling the crossmodal retrieval <ref type="bibr" target="#b23">(Lee et al., 2018;</ref> and VQA <ref type="bibr" target="#b2">(Bai et al., 2018)</ref> tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>This section describes ARTEMIS, our proposed approach for the task of image search with freeform text modifiers. In this setting, queries are bimodal pairs, composed of a reference image I r and a text modifier T m . Such queries are used to retrieve any relevant image I t from a gallery of images.</p><p>Let ?(.) and ?(.) be the visual and textual encoders respectively. We denote the resulting L2normalized features of the reference image r, of the text modifier m, and of the target image t,</p><formula xml:id="formula_0">i.e. r = ?(I r ) ? R H I , m = ?(T m ) ? R H T , and t = ?(I t ) ? R H I .</formula><p>Two complementary views of the task. Both the textual and the visual cues are important to find relevant target images. Consider for example the query in <ref type="figure">Figure 1</ref>. The reference image provides strong visual context and displays some semantic details -e.g. . the dress' length, among other style components-that should be shared with the target image, to the extent that the text does not specify their alteration. In other words, by the absence of an explicit modification request of some properties, the modifying text implicitly "validates" the visual cues from the reference image associated to those properties, and these cues can be used to directly query for the target image. Hence, part of the task boils down to image-to-image retrieval (visual search), implicitly conditioned on the text modifier.</p><p>On the other hand, the textual cue explicitly calls for modifications of the reference image. It was even observed that, in some cases, users tend to directly describe characteristics of the target image, without explicitly stating the discrepancy between the reference and the target image 1 (c.f. the analysis presented for FashionIQ in <ref type="bibr" target="#b51">(Wu et al., 2021)</ref> and for CIRR in <ref type="bibr" target="#b28">(Liu et al., 2021)</ref>) Therefore the text, which describes mandatory details for the target, can be directly used to query for the target image, as is common in text-to-image retrieval. Yet, due to the specificities of the task, we go beyond the usual text-to-image retrieval.</p><p>Proposed approach. We propose to revisit text-to-image and image-to-image retrieval to tackle the task at stake, which is at the intersection of both. Standard strategies, for both, boil down to directly comparing the global feature of the query (image or text) to those of the potential targets, producing two independent, modality-specific, and not necessarily immediately compatible scoring strategies. In particular, directly comparing the representations of the reference and the target images is inaccurate as the two images should differ, based on the modifying text. Likewise, directly comparing the representations of the modifying text and the target image is insufficient and potentially detrimental since part of the target image should stay similar to the reference image.</p><p>Here, we reconcile the two tasks and enhance them by designing a model which is trained to match i) the characteristics of the reference image to the ones of the target image and ii) the requirements provided by the text with properties of the target image. Based on the intuitions above, in both cases we use the text to design an attention mechanism that selects the visual cues which should be emphasized during matching.</p><p>The first objective is carried out by the Implicit Similarity module (Section 3.1), which draws from visual search, and the second one by the Explicit Matching module (Section 3.2), inherited from cross-modal retrieval. These two modules, jointly trained, respectively output the IS and the EM scores, which are simply summed into one, as a form of late fusion (Section 3.3). The full architecture of the model is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IMPLICIT SIMILARITY</head><p>In the first module, the textual cue m guides the comparison between the target and the reference images; it is used to define an attention mechanism A IS (.) : R H T ? [0, 1] H I , taking m as input, which predicts the importance of the visual representation dimensions when comparing the target to the reference image. The IS score is then computed as:</p><formula xml:id="formula_1">s IS (r, m, t) = A IS (m) r | A IS (m) t ,<label>(1)</label></formula><p>where is the pointwise product and .|. represents the cosine similarity between the two L2normalized terms. A IS (.) is a two-layer MLP separated by a ReLU, and followed by a softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPLICIT MATCHING</head><p>This second module captures how well the target image matches the textual cue. As the target image t and the textual modifier m belong to different modalities, we first project m into the visual space with a learned linear transformation T (.) : R H T ? R H I . In addition, since the textual modifier should not act on all the characteristics of the image ( the target image should remain partially similar to the reference image), we rely on a second attention mechanism, A EM (.) : R H T ? [0, 1] H I to emphasize the dimensions of the image feature to focus on for matching. Hence, the EM score is computed as:</p><formula xml:id="formula_2">s EM (m, t) = T (m) | A EM (m) t .</formula><p>( <ref type="formula">2)</ref> where T (.) consists in a fully connected layer. Note that A IS (.) and A EM (.) share the same architecture but not their weights, as the role they give to m is different. Indeed, intuitively A EM (.) focuses on what is mentioned by the textual modifier while A IS (.) looks for what is not. Put differently, A EM (.) "up-weights" the dimensions in the image feature that correspond to the semantic information provided by the textual cue, whereas A IS (.) "down-weights" these dimensions to rather focus on the shared characteristics between the reference and target images. text modifier T m ) as input and feed each element to its respective encoder. The text feature is then used in multiple ways (including two attention mechanisms) to compute the implicit similarity (IS) and the explicit matching (EM) scores. These two complementary retrieval scores are summed to produce the final score which is used to rank potential target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING PIPELINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a minibatch of B training samples</head><formula xml:id="formula_3">B = {(I ri , T mi , I ti )} B i=1</formula><p>where I ti corresponds to an image that is annotated as a correct target for the query (I ri , T mi ), we train our model using the batch-based classification (BBC) loss:</p><formula xml:id="formula_4">L BBC (B) = ? 1 B B i=1 log exp{? s(r i , m i , t i )} j exp{? s(r i , m i , t j )} ,<label>(3)</label></formula><p>where ? is a learnable temperature parameter and s(r i , m j , t k ) is the "compatibility" score of the triplet (i, j, k) in [1, B] 3 . For ARTEMIS , this compatibility score is the sum of the EM and IS scores, respectively obtained by each of the modules presented above:</p><formula xml:id="formula_5">s(r i , m j , t k ) = s EM (m j , t k ) + s IS (r i , m j , t k ).<label>(4)</label></formula><p>The BBC loss, proposed as InfoNCE in the self-supervised learning literature <ref type="bibr" target="#b34">(Oord et al., 2018)</ref>, was introduced for image retrieval with text modifiers by <ref type="bibr" target="#b48">Vo et al. (2019)</ref> and its efficacy for the task was confirmed by <ref type="bibr" target="#b24">Lee et al. (2021)</ref>. In contrast to the triplet and the soft triplet losses which rely on negative mining strategies to obtain optimal results, the BBC loss considers all the negative samples in a mini-batch, and therefore simultaneously learns from easy and hard negatives.</p><p>Encoders. Our approach is generic and can accommodate for diverse visual encoders ? and textual encoders ?. We present results with several combinations, for the sake of comparison with prior work. We typically use GloVe word embeddings <ref type="bibr" target="#b35">(Pennington et al., 2014)</ref> followed by either a LSTM <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref> or a BiGRU <ref type="bibr" target="#b8">(Cho et al., 2014)</ref> as text encoder and an ImageNet pretrained ResNet18 or ResNet50 model <ref type="bibr" target="#b17">(He et al., 2016)</ref> as image encoder. Further implementation details can be found in Appendix ? A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETTING</head><p>Datasets. We consider two datasets focusing on the fashion domain and one on open-domain images, all three using human-written textual modifiers in natural language. The Fashion IQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref> is composed of 46.6k training images and around 15.5k images for both the validation and test sets. There are 18k training queries, and 12k queries per evaluation split, covering three fashion categories: women's tops (toptee), women's dresses (dress) and men's shirts Evaluation. We use the standard evaluation protocol for each dataset. We report Recall@K (R@K), which is the percentage of queries for which at least one of the correct ground truth item is ranked among the top K retrieved items. All reported numbers are obtained as an average of 3 runs. In Section 4.3, we additionally report standard deviations.</p><p>For FashionIQ, the standard evaluation metric is the one used for the companion challenge <ref type="bibr" target="#b13">(Gao &amp; Guo, 2020)</ref>. It computes the R@10 and R@50 for each of the 3 fashion categories and averages these six results. For Shoes, we report the R@1, R@10 and R@50 as well as their average. For CIRR, following <ref type="bibr" target="#b28">Liu et al. (2021)</ref>, we also report Recall subset @K, in which the set of candidate target images is restricted to images semantically similar to the correct target image. According to <ref type="bibr" target="#b28">Liu et al. (2021)</ref>, this metric is less sensitive to false-negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation.</head><p>Since the test split annotations of FashionIQ became available only very recently, previous works only report results on the validation set, and resort to ad hoc strategies to set their hyper-parameters and early-stop criteria. In order to compare our method to previously published approaches, we provide results on both the test and validation sets by performing a bidirectional cross-validation between them: we run our experiments for a fixed number of epochs and, at the end of each epoch, we evaluate our model on both sets. We select the best performing model on the validation set to report results on the test set and vice-versa.When no validation split is available, as for Shoes, we report the results on the last checkpoint. For CIRR, we select the best checkpoint on the validation split to evaluate on their test server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY</head><p>We first conduct an ablative study on the validation set of all datasets to evaluate the influence of several design choices in our architecture. For FashionIQ and Shoes, we additionally report median rank of the correct target. For CIRR, we report the R@5 and R s @1 in addition of R@50, since they best capture the model capabilities according to <ref type="bibr" target="#b28">Liu et al. (2021)</ref>. Results are presented in <ref type="table" target="#tab_0">Table 1</ref>. All experiments in this section use ResNet50 and BiGRU as visual and textual encoders, trained as described in the Appendix ? A. We separate our ablation studies into two parts; first we look at several baselines which do not use any attention mechanism, then we challenge the different module choices by training them independently.</p><p>Baselines. Image-only and text-only baselines use only one of the queries elements to retrieve the target. Their compatibility scores can be written as s img (r, m, t) = r | t and s txt (r, m, t) = m | t , respectively. We also report the late fusion of the two query embeddings, which can be seen as an attention-free version of ARTEMIS. In this case, the compatibility score becomes s lf (r, m, t) = r + m | t .  The performance of these baselines changes according to the nature of each dataset. For FashionIQ, where 68% of the modifiers make direct reference to the target image <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>, the textonly baseline outperforms the image-only one. For Shoes, based on visual inspection, we conjecture that the reference-target image pairs are, on average, more similar than for FashionIQ, making the reference image more crucial for many queries. For CIRR, while querying only with the reference image offers a strong baseline, the text modifier is more discriminative when ranking on the restricted set of very similar images. This is measured by the R s @1 metric. Note that the late fusion baseline produces strong results for both FashionIQ and Shoes, but barely improves upon the image-only baseline for CIRR, which highlights the importance of the attention mechanisms.</p><p>Independent modules. We evaluate the IS and EM independently as separate retrieval models. Their corresponding compatibility scores are given by Equations 1 and 2, respectively. We observe that IS and EM perform differently for different datasets, and as expected, follow the previous observations made for the image-only and text-only baselines. We see that both our modules slightly outperform their corresponding baseline. The full ARTEMIS performs best for all three datasets. This highlights the synergy between IS and EM, supporting the motivations extensively discussed in Sections 3.1 and 3.2, but also the fact that the method is robust to different types of datasets/ queries: those where the visual part is more important as well as those where the textual part is preponderant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARISON WITH THE STATE OF THE ART</head><p>We now compare ARTEMIS to state-of-the-art approaches on each benchmark. We also report results for our re-implementation of TIRG <ref type="bibr" target="#b48">(Vo et al., 2019)</ref>, a well-established composition network baseline, with the same training pipeline as ARTEMIS, as the original paper does not provide results on these more recent datasets.   <ref type="bibr" target="#b28">Liu et al. (2021)</ref>, Recall subset @1 best assess fine-grained reasoning ability). Gray background is not directly comparable to our results, since it relies on a model pre-trained on a very large set of image-caption pairs. Overall 1 st /2 nd in black/blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Recall@K Recallsubset@K (R@5+R sub @1) 2 Results on Fashion IQ are reported in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> for the validation and the test sets, respectively. Since the test split annotations were unavailable until very recently (only <ref type="bibr" target="#b51">Wu et al. (2021)</ref> published results on that split), most published approaches report results on the validation set. In both cases our best ARTEMIS (RN50 + BiGRU) outperforms the state of the art.</p><formula xml:id="formula_6">K = 1 K = 5 K = 10 K = 50 K = 1 K = 2 K = 3 CIRPLANT (init.</formula><p>As most published works <ref type="bibr" target="#b24">Lee et al., 2021)</ref> use LSTM as text backbone and ResNet50 as image backbone, we also report results for ARTEMIS in this setting and show that we remain competitive, outperforming VAL by an average of 1.1% over the challenge metric. Furthermore, we also report results with ResNet18 to compare to several recent works <ref type="bibr" target="#b0">(Anwaar et al., 2021;</ref>, who prioritize a faster and smaller backbone. We observe that, for smaller image backbone, LSTM seems to perform better then BiGRU. This is also true for our re-implementation of TIRG.</p><p>Results on Shoes are presented in <ref type="table" target="#tab_3">Table 4</ref>. As the Shoes dataset does not contain a dedicated validation set, we reuse the exact same training procedure and hyperparameters as for FashionIQ. ARTEMIS obtains state-of-the-art results with both LSTM and BiGRU encoders, confirming our approach's ability to generalize.</p><p>CIRR results are reported in <ref type="table" target="#tab_4">Table 5</ref>. As recommended by <ref type="bibr" target="#b28">Liu et al. (2021)</ref>, we use pre-extracted ResNet152 features for the images. This affects our training pipeline as the image backbone cannot be finetuned and we cannot apply any image transformation. CIRPLANT obtains competitive results only when its multi-layer cross-modal transformer is initialized with the weights from the Oscar model <ref type="bibr" target="#b27">(Li et al., 2020</ref>) trained on 6.5 million image-caption pairs. When compared to methods trained on the same data, ARTEMIS outperforms CIRPLANT on all metrics, in spite of a simpler architecture. In particular, our R s @1 result suggests, according to <ref type="bibr" target="#b28">Liu et al. (2021)</ref>, that ARTEMIS has a great fine-grained reasoning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">QUALITATIVE RESULTS</head><p>In <ref type="figure">Figure 3</ref>  Hide part of the body of the white dog behind a tree trunk.</p><p>More dogs and they are more huddled on the white cloth. visual cues that are shared between the reference and the target images, such as the shirt's collar and bottom, the sole shape &amp; the heel, or the color and the shoe opening. In a complementary way, the EM module addresses the image parts that are the most related to the caption: the color, the shirt's fit, the slits on the side or the shoe's closure.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we present some query examples from the CIRR dataset along with the Top-6 retrieved images.</p><p>In both examples, we see that the reference image contributes a strong semantic context, and that the text modifier provides important information to distinguish the correct target image (framed in green) from other very similar images. Additional qualitative examples and further analysis are available in the Appendix ? D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed ARTEMIS, a new method for image search with free-form text modifiers. It combines two modules, each focusing on one of the modalities of the query. The Explicit Matching module assesses how potential targets fit the textual modifier while the Implicit Similarity module compares potential target images to the reference image, assisted by the text. Both modules contain a lightweight attention mechanism which uses the text to guide the retrieval process in a way that is specific to each modality of the query. Extensive experiments on several benchmarks validate our approach and show that we consistently reach state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>This document provides additional results that complement the main paper. Its content includes some implementation details ( ? A), our experiments on an additional dataset ( ? B), an efficiency study of our method in comparison to other approaches ( ? C), and some extra qualitative results for ARTEMIS ( ? D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>Texts are pre-processed to replace special characters by spaces and to remove all other characters than letters. We observe that using spelling correction and lemmatization does not improve the results. The text modifier is first tokenized into words, and encoded using 300-dimensional GloVe embeddings <ref type="bibr" target="#b35">(Pennington et al., 2014)</ref>. Either a BiGRU <ref type="bibr" target="#b8">(Cho et al., 2014)</ref>, or a LSTM <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref> (followed by an average pooling and a fully-connected layer, as in ) are learned from scratch on top of the word embeddings to produce a sentence-level feature. For Fashion IQ <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>, the only dataset for which each reference/target image pair is provided with two relative captions (used as text modifiers) from two different annotators, we follow previous works <ref type="bibr" target="#b24">(Lee et al., 2021)</ref> and use the concatenation of the two captions (in both orders) for training and evaluation. This means that we use about 12k queries fr the validation split.</p><p>To maintain the aspect ratio and avoid losing information about the top and the bottom of the clothing items, images are first padded with white pixels to obtain a square. They are then resized to 256?256 pixels and augmented using random horizontal flip and a random crop of size of 224 ? 224 pixels during training. For inference, images are resized to 256 ? 256 and center cropped to 224 ? 224. The image features are obtained using a CNN with a GeM pooling <ref type="bibr" target="#b40">(Radenovi? et al., 2018)</ref>, which is widely used in the instance-level image retrieval community instead of the usual average pooling. The pooling layer is followed by a newly learned fully-connected layer. Our experiments employ either a ResNet18 or ResNet50 architecture <ref type="bibr" target="#b17">(He et al., 2016)</ref>, initialized on ImageNet <ref type="bibr" target="#b41">(Russakovsky et al., 2015)</ref>. The only exceptions are the experiments on CIRR, for which not all images are available to download. <ref type="bibr" target="#b28">Liu et al. (2021)</ref> provide pre-extracted ResNet152 features (outputs of the average pooling layer) that we use as a replacement of the image backbone. As a consequence, for all experiments on CIRR, the image backbone is not fine-tuned and we do not apply any image transformation.</p><p>Following <ref type="bibr" target="#b44">Song &amp; Soleymani (2019)</ref>, we freeze the base encoders during the first 8 epochs to pretrain the sentence encoder, as well as the EM and IS modules. Then, we train our model end-to-end for 50 epochs. Our training pipeline uses AdamW optimizer <ref type="bibr" target="#b29">(Loshchilov &amp; Hutter, 2017)</ref>, a batch size of 32 and an initial learning rate of 5?10 ?4 with a decay of 0.5 every 10 epochs. The dimension of both the image and the textual embeddings is set to H T = H I = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTS ON FASHION200K</head><p>In the literature <ref type="bibr" target="#b48">(Vo et al., 2019;</ref><ref type="bibr" target="#b24">Lee et al., 2021)</ref>, the Fashion200K dataset  is used as a dataset for image search with text modifiers. The dataset is composed of 201k in-shop fashion article images, split into 172k train images and 29k test images. From these test images, <ref type="bibr" target="#b48">Vo et al. (2019)</ref> provides a list of 31k image pairing which is used to produce candidate queries for evaluation. The text modifiers, in contrast to the other presented datasets, are not in natural language: they are automatically generated from the set of image attributes following the "replace att. X by att. Y" template. In our work, we focus on free-form human annotations, which is the common point between the three datasets presented in the main paper. However, we here compare ARTEMIS to the other published works on this dataset, for reference.</p><p>In order to better handle the large train set of Fashion200K, we follow VAL  and CoSMo <ref type="bibr" target="#b24">(Lee et al., 2021)</ref>, and we train our model with a larger mini-batch of 128 for 100 epochs end-to-end.  Interestingly, while ARTEMIS performed better with a BiGRU as text encoder for all other datasets, our best results for Fashion200K are obtained with a LSTM. We observe the same for our implementation of TIRG, and more generally for all our experiments that use a ResNet18.</p><p>We observe on <ref type="table" target="#tab_6">Table 6</ref> that VAL obtains the best performance, thanks to the extra captions used to pre-train their auxiliary regularizer. The same side information was also leveraged by . Excluding models trained with this extra data, both ARTEMIS and CoSMo outperform VAL, and our approach obtain the best results in 2 out of the 4 evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPLEXITY AND EFFICIENCY STUDY</head><p>In the main paper, we extensively discuss our method's performance in terms of accuracy, on several benchmarks. This section provides a complementary study that compares ARTEMIS' complexity with the complexity of several approaches. Since no single measure is enough to assess the complexity of a model <ref type="bibr" target="#b10">(Dehghani et al., 2021)</ref>, we compare the different models on three metrics:</p><p>? Number of parameters. We measure the number of trainable parameters of the full models. Many works use this metric as a proxy to model complexity and memory footprint.</p><p>? Number of multiply-add operations. The number of multiply-accumulate operations (MAC) is a theoretical measure that allows to compare the number of operations needed by the model to perform a forward pass, independently of the hardware <ref type="bibr" target="#b20">(Johnson, 2018)</ref>. We report the number of GMACs for the forward pass of a triplet (I r , T m , I t ), where reference and target images are both of size 224?224, and the modifier text is composed of 20 tokens (in average, CIRR sentences contain 11.3 words and FashionIQ 5.3, according to <ref type="table" target="#tab_0">Table 1</ref> in <ref type="bibr" target="#b28">Liu et al. (2021)</ref>).</p><p>? Latency time. Latency usually refers to the required time to perform an inference forward pass, either for a batch of images or the full dataset. It is an informative speed metric for real-time systems that require user input, so it is well suited to the retrieval task. We measure the time each method needs to process all queries, all target images, and all querytarget scores of the FashionIQ validation split. Note that 12k queries are involved, since we do the inference for the concatenation of the two captions in both orders: 'cap1 and cap2', 'cap2 and cap1', following <ref type="bibr" target="#b24">Lee et al. (2021)</ref> (see details in Appendix ?A). All latency times are measured on the same GPU NVIDIA T4.</p><p>We report results for ARTEMIS, TIRG, VAL and CoSMo. For a fair comparison, all reported numbers are obtained for ResNet50 and LSTM as image and text encoder respectively. Since these encoders are an important source of complexity in the models, we also evaluate the encoders alone,  <ref type="bibr" target="#b48">(Vo et al., 2019)</ref> 8.35 (+0.002) 33.75 (+2.10) 118.20 (+4.11) VAL  8.39 (+0.040) 59.48 (+27.83) -CoSMo <ref type="bibr" target="#b24">(Lee et al., 2021)</ref> 10.45 <ref type="bibr">(+2.110)</ref>  <ref type="bibr">79.32 (+47.73)</ref> 133.84 <ref type="bibr">(+19.75)</ref> to provide a reference point. For this reference, we estimate the corresponding latency by evaluating the late-fusion baseline described in Section 4.2. We report our results in <ref type="table" target="#tab_7">Table 7</ref>. We make the following observations:</p><p>ARTEMIS and TIRG both add very little complexity compared to the reference. Similarly to TIRG <ref type="bibr" target="#b48">(Vo et al., 2019)</ref>, our approach adds only a few operations and parameters to the base encoders used to compute r, m and t. Both methods' parameters mostly come from their fullyconnected layers. TIRG uses more parameters than ARTEMIS because its gating mechanisms takes the concatenated vector [r, m] as input instead of just the text embeddings m, doubling the number of required connections with respect to ARTEMIS .</p><p>ARTEMIS and TIRG add some reasonable extra-latency compared to the reference. Both ARTEMIS and TIRG add little latency time to the encoder reference point. TIRG is more efficient due to its simple pass on the target representation: pre-computed t can be directly compared to the composite query representation by a simple cosine similarity. ARTEMIS also leverages precomputed t, but re-weights its dimensions using A IS (m) and A EM (m) then L2-normalizes the result before comparing to the query features, with cosine similarity as well. This double use of the target explains ARTEMIS ' slightly longer latency time.</p><p>VAL has a higher computational complexity. We were unable to produce the same experiments on the publicly available code of VAL to compute the latency. For the complexity measurements, we can attest to an increase in the number of GMACs and parameters that is more significant than for both ARTEMIS and TIRG. The latency is likely to increase similarly, as VAL inserted composite transformers at different levels of the visual encoder, and compares representations coming from the concatenation of average-pooled features at low-, mid-and high-level (i.e. features of size 2048 + 1024 + 512).</p><p>CoSMo adds both extra-complexity and extra-latency. CoSMo, similarly to TIRG, uses a composition module to fuse r and m, and does a single pass on each target. However, its composition module, combining a disentangled non-local block for content modulation and text-guided affine transformations for style modulation, is more complex than ARTEMIS . Furthermore, while ARTEMIS and TIRG introduce little complexity, CoSMo adds around 25% multiply-accumulation operations, and more than doubles the number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D QUALITATIVE RESULTS</head><p>Additional ranking results. We provide qualitative results for the FashionIQ <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>, the Shoes <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b3">Berg et al., 2010)</ref> and the CIRR <ref type="bibr">(Liu et al., 2021) datasets in Figures 5, 6, 7, 8 and 9</ref>. They illustrate the model's ability to properly encode and reason on general concepts. In the case of FashionIQ, it includes the length of sleeves and dresses, the patterns (floral, stripes, dots, plaid) or the logo color (different from the background color) and the looseness of clothing items. As for Shoes it captures shininess, wool/suede material, and the presence of laces, of fur, of Velcro closures, or of a buckle, for instance. The CIRR dataset is open-domain, and we notice that the model is able to reason on a wide variety of semantic concepts (from trees to the people's youth).</p><p>The model is also able to deal with more complex information, such as dolphins ( <ref type="figure">Figure 6</ref>, second row) or different renderings of the same logo ( <ref type="figure">Figure 6, third row)</ref>. Eventually, we observe that the model gathers pieces of information from the reference image when needed (style and cut, length or size, collar form, sleeve length, pattern or logo, sole shape, animal breed, sculpture etc.). While the model relies on the text modifier to propose coherent images, it also resorts to the reference image to fill in the gaps.</p><p>Limitations. Extensive studies of the qualitative results ( <ref type="figure">Figure 10</ref>) reveal that the model has trouble to understand the negative formulations: in the third row of <ref type="figure">Figure 10</ref>, the proposed toptees remain colorful; in the second row, the ranked clothing items are still loose and sheer; the last row shows a mix of shoes with laces and other with straps. For FashionIQ, it is likely due to the fact that only 3.5% of the queries use negation, according to a semantic analysis carried out in the companion paper <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>. Supposedly, there are not enough examples at training time for the model to learn the associated semantic.</p><p>Besides, the captions were written by different human annotators. They sometimes present a subjective judgment (e.g. "is cuter", "more manly") or lack essential details to discriminate better the target among other images in the gallery. We give a few illustrative examples of such cases in <ref type="figure">Figure  10</ref>. For the top query, the specific required color is not mentioned, hence the various guesses taken by the model.</p><p>One limitation inherent to the task is well illustrated by the blue dress example in <ref type="figure">Figure 5</ref>, the boot example in <ref type="figure">Figure 8</ref> or the bird example in <ref type="figure" target="#fig_4">Figure 9</ref>: very often, many candidate target images would be acceptable answers to a given query, beyond the one annotated as ground-truth. Hence we believe R@K is a more reliable metric when K is bigger as it's more lenient to plausible wrong predictions. Other works <ref type="bibr" target="#b32">(Musgrave et al., 2020;</ref><ref type="bibr" target="#b9">Chun et al., 2021)</ref> have pointed out the limitations of relying on R@K for retrieval tasks when K is too small, and suggested other metrics. This is also a limitation that <ref type="bibr" target="#b28">Liu et al. (2021)</ref> tries to address with their R s @K metric, by annotating images while aware of possible false negatives.</p><p>Additional heatmaps on examples extracted from the FashionIQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref> are presented in <ref type="figure">Figure 11</ref>. Similarly, <ref type="figure" target="#fig_0">Figure 12</ref> presents results on the Shoes <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b3">Berg et al., 2010)</ref> dataset.</p><p>The most important parts of the image with regard to the computation of each score, Explicit Matching (EM) and Implicit Similarity (IS), are highlighted in red. The coefficients of EM and IS intermediary results (i.e. before completing the scalar product with the sum) are referred as "components". The heatmaps are obtained by backpropagating to the last convolutional layer of the CNN some of the components that contribute the most to the EM and IS scores. We show the influence of a same IS component both on the reference and the target images, since both images are involved in the computation of the IS score. Conversely, EM is based only on the target image, with regard to the text modifier.</p><p>It emerges from the IS visualizations that the model is able to draw the parallel between the two images on several concepts. For clothing items, it can be the shape of the collar, the sleeve length or the cut of the dress. For shoe items, it includes the presence of heels or laces, the shoe opening or the sole shape. In a complementary way, the EM module addresses the image parts that are the most related to the caption: for example, it focuses on the pattern, the color or decoration requirements.  <ref type="figure">Figure 5</ref>: Qualitative results for queries from the "dress" category of the FashionIQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>. We show the six top ranked images for each query. The expected targets are indicated with a green border. It appears that the model is able to reason on several concepts (color, pattern, ruffles, sleeve length, form of the neck...), and to use the reference image to infer unspecified properties in the text modifier (style, length, tightness...).</p><p>is navy blue with yellow lettering [and] Is navy in color Has dolphins on a light blue tee shirt <ref type="bibr">[and]</ref> is lighter has more realistic picture <ref type="bibr">[and]</ref> has a 3d printed logo plaid long sleeve turquoise shirt [and] has a plaid pattern and longer sleeves <ref type="figure">Figure 6</ref>: Qualitative results for queries from the "shirt" category of the FashionIQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>. We show the six top ranked images for each query. The expected targets are indicated with a green border. It appears that the model is able to reason on several concepts (different color for the clothing background and the logo, graphic, style, sleeve length...), and to use the reference image to infer unspecified properties in the text modifier (kind, style, form of the collar...).</p><p>The shirt is white with flowers. <ref type="bibr">[and]</ref> is lighter has a longer sleeve with polka dots <ref type="bibr">[and]</ref> is orange and has buttons and longer sleeves</p><p>The shirt is short sleeved and light green in color. <ref type="bibr">[and]</ref> is soft green colored Is brighter in color and has horizontal stripes <ref type="bibr">[and]</ref> is red striped instead of blue. <ref type="figure">Figure 7</ref>: Qualitative results for queries from the "toptee" category of the FashionIQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref>. We show the six top ranked images for each query. The expected targets are indicated with a green border. It appears that the model is able to reason on several concepts (color, pattern, presence of buttons, sleeve length, form of the neck...), and to use the reference image to infer unspecified properties in the text modifier (kind, style, length, form of the collar...).  <ref type="figure">Figure 8</ref>: Qualitative results for queries from the Shoes dataset <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b3">Berg et al., 2010)</ref>.</p><p>It appears that the model is able to reason on several concepts (kind, color, style, shininess, fur, buckle, laces, Velcro closures...), and to use the reference image to infer unspecified properties in the text modifier (kind, style, presence of heels or laces, shapes of the sole and of the opening, etc.   <ref type="bibr" target="#b28">(Liu et al., 2021)</ref>. We show the six top ranked images for each query. The expected targets are indicated with a green border. It appears that the model is able to reason on several concepts (plurality, background/foreground, natural elements, viewpoint...), and to use the reference image to infer unspecified properties in the text modifier (e.g. the animal breed).</p><p>is lighter <ref type="bibr">[and]</ref> is more colorful has more of a print and is less sheer <ref type="bibr">[and]</ref> is not as loose and more floral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is not colorful</head><p>[and] is more revealing have laces, not Velcro straps <ref type="figure">Figure 10</ref>: Qualitative results showing some limitations, for queries from FashionIQ <ref type="bibr" target="#b51">(Wu et al., 2021)</ref> (the first 3 rows) and Shoes <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b3">Berg et al., 2010</ref>) (last row). For any of these examples, the ground truth is not included by the model in the top ranking of potential targets. This is either due to a lack of information (first row) or the non-understanding of negation (last 3 rows).  <ref type="figure">Figure 11</ref>: Visualisation of image parts contributing the most to the EM and IS scores, for queries from the FashionIQ dataset <ref type="bibr" target="#b51">(Wu et al., 2021)</ref> where the correct target is ranked first. For each of the six blocks, we provide the heatmaps of an EM component on the target image, and of two relevant IS components, applied to both the reference and the target images. The EM heatmaps highlight some of the connections made by the model between the text modifier and the target image. The IS heatmaps display some detected similarities between the two images. Both EM and IS help to evaluate the relevance of the target image with regard to the query.  <ref type="figure" target="#fig_0">Figure 12</ref>: Visualisation of image parts contributing the most to the EM and IS scores, for queries from the Shoes dataset <ref type="bibr" target="#b15">(Guo et al., 2018;</ref><ref type="bibr" target="#b3">Berg et al., 2010)</ref> where the correct target is ranked first. For each of the six blocks, we provide the heatmaps of an EM component on the target image, and of two relevant IS components, applied to both the reference and the target images. The EM heatmaps highlight some of the connections made by the model between the text modifier and the target image. The IS heatmaps display some detected similarities between the two images. Both EM and IS help to evaluate the relevance of the target image with regard to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EM</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed architecture. Our model takes a triplet (reference image I r , target image I t ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>we present a few heatmap examples obtained with the Grad-CAM<ref type="bibr" target="#b43">(Selvaraju et al., 2017)</ref> algorithm. Specifically, the image activations at the last convolutional layer of the CNN are reweighed with the pooled gradients of the same layer, after back-propagation of the coefficients contributing the most to the EM and IS scores. It appears clearly that the IS module attends to theFigure 3: Visualisation of image parts contributing the most to the EM and IS scores, for queries from the Fashion IQ and Shoes datasets where the correct target is ranked first. An EM component is presented on the target image. The bottom two rows show two relevant components for IS, applied to both the source and the target images (see Appendix ? D for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative examples of image-text queries of CIRR and its Top-6 retrieved results. A green frame denotes the ground-truth target (see Appendix ? D for more examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>While Chen et al. (2020); Chen &amp; Bazzani (2020) use MobileNet-v1 as image backbone, we follow Vo et al. (2019); Lee et al. (2021) and use ResNet18. As Lee et al. (2021) pointed out, though the difference in model architecture may play a role in overall results, both models have almost identical scores on both ImageNet Top1 and Top5 error rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results for queries from the CIRR dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablative study of the different components of our method. Overall 1 st /2 nd in black/blue. R@10 R@50 m. rank R@10 R@50 m. rank R@5 R@50 Rs@1 The text modifier is composed of two relative captions produced by two different human annotators, exposed to the same reference-target image pair. The Shoes dataset<ref type="bibr" target="#b15">(Guo et al., 2018)</ref> is extracted from the Attribute Discovery Dataset<ref type="bibr" target="#b3">(Berg et al., 2010)</ref>. It consists of 10k training images structured in 9k training triplets, and 4.7k test images including 1.7k test queries. The recently released CIRR dataset<ref type="bibr" target="#b28">(Liu et al., 2021)</ref> is composed of 36k pairs of open-domain images, arranged in a 80%-10%-10% split between the train/validation/test. The annotation process is such that the modifying text should only be relevant to one image pair, and irrelevant to any other image pairs containing the same reference image.</figDesc><table><row><cell>Flavors of ARTEMIS</cell><cell></cell><cell>FIQ val split</cell><cell></cell><cell></cell><cell>Shoes</cell><cell cols="2">CIRR val split</cell><cell></cell></row><row><cell>Image only r | t</cell><cell>4.86</cell><cell>12.01</cell><cell>917.11</cell><cell>28.47</cell><cell>53.13</cell><cell>41.67 30.10</cell><cell>75.75</cell><cell>20.84</cell></row><row><cell>Text only m | t</cell><cell>15.55</cell><cell>36.11</cell><cell>121.44</cell><cell>13.50</cell><cell>31.46</cell><cell>154.00 21.93</cell><cell>65.71</cell><cell>38.28</cell></row><row><cell>Late fusion r + m | t</cell><cell>25.69</cell><cell>50.14</cell><cell>51.78</cell><cell>48.95</cell><cell>75.62</cell><cell>11.33 30.94</cell><cell>78.04</cell><cell>21.65</cell></row><row><cell>IS module only</cell><cell>6.18</cell><cell>16.42</cell><cell>449.56</cell><cell>32.63</cell><cell>57.41</cell><cell>31.33 32.28</cell><cell>77.50</cell><cell>21.16</cell></row><row><cell>EM module only</cell><cell>15.61</cell><cell>36.43</cell><cell>113.89</cell><cell>13.72</cell><cell>32.39</cell><cell>146.67 29.71</cell><cell>72.24</cell><cell>43.48</cell></row><row><cell>Full ARTEMIS</cell><cell>26.05</cell><cell>50.29</cell><cell>52.89</cell><cell>53.11</cell><cell>79.31</cell><cell>8.67 48.95</cell><cell>89.19</cell><cell>41.42</cell></row><row><cell>(shirt).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fashion IQ, official validation set. We report the challenge metric (CM) and individual R@K scores. ? means our re-implementation. denotes the use of additional side information (e.g. extra captions from other datasets) at train time. Unless mentioned otherwise, each method uses Resnet50 and LSTM as visual and textual backbones, respectively. Overall 1 st /2 nd in black/blue . 16?0.22 24.19?0.04 20.13?0.54 26.05?0.95 23.46?0.31 50.42?1.19 43.56?0.47 52.57?0.33 48.85?0.27 TIRG ? (RN50 + BiGRU) 35.32?0.74 23.80?1.55 19.90?0.62 25.82?0.73 23.17?0.70 48.64?1.22 42.14?1.65 51.64?0.30 47.70?0.10 25.23?0.36 20.35?0.56 23.36?0.22 22.98?0.07 48.64?0.61 43.67?0.94 46.97?0.47 46.43?0.13 ARTEMIS (RN50 + LSTM) (ours) 36.51?0.46 27.34?0.44 21.05?1.89 24.91?0.57 24.43?0.61 51.71?0.78 44.18?0.39 49.87?0.56 48.59?0.40 ARTEMIS (RN18 + BiGRU) (ours) 34.75?0.03 24.84?0.06 20.40?0.11 23.63?0.05 22.95?0.02 49.00?0.15 43.22?0.04 47.39?0.58 46.54?0.04 ARTEMIS (RN50 + BiGRU) (ours) 38.17?0.35 27.16?0.52 21.78?0.26 29.20?0.69 26.05?0.33 52.40?0.20 43.64?1.01 54.83?0.30 50.29?0.40</figDesc><table><row><cell>Method</cell><cell>CM</cell><cell></cell><cell></cell><cell>R@10</cell><cell></cell><cell></cell><cell></cell><cell>R@50</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Dress</cell><cell>Shirt</cell><cell>Toptee</cell><cell>Mean</cell><cell>Dress</cell><cell>Shirt</cell><cell>Toptee</cell><cell>Mean</cell></row><row><cell>JVSM (Chen &amp; Bazzani, 2020)</cell><cell>19.27</cell><cell>10.70</cell><cell>12.00</cell><cell>13.00</cell><cell>11.90</cell><cell>25.90</cell><cell>27.10</cell><cell>26.90</cell><cell>26.63</cell></row><row><cell>ComposeAE (Anwaar et al., 2021)</cell><cell>20.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.40</cell></row><row><cell>TCIR (Chawla et al., 2021)</cell><cell>29.51</cell><cell>19.33</cell><cell>14.47</cell><cell>19.73</cell><cell>17.84</cell><cell>43.52</cell><cell>35.47</cell><cell>44.56</cell><cell>41.18</cell></row><row><cell>CIRPLANT (Liu et al., 2021)</cell><cell>25.17</cell><cell>14.38</cell><cell>13.64</cell><cell>16.44</cell><cell>14.82</cell><cell>34.66</cell><cell>33.56</cell><cell>38.34</cell><cell>35.52</cell></row><row><cell>CIRPLANT (Liu et al., 2021)</cell><cell>30.20</cell><cell>17.45</cell><cell>17.53</cell><cell>21.64</cell><cell>18.87</cell><cell>40.41</cell><cell>38.81</cell><cell>45.38</cell><cell>41.53</cell></row><row><cell>TIRG  ? (RN50 + LSTM)</cell><cell cols="9">36.48?0.83</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>33.82</cell><cell>21.12</cell><cell>21.03</cell><cell>25.64</cell><cell>22.60</cell><cell>42.19</cell><cell>43.44</cell><cell>49.49</cell><cell>45.04</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>35.38</cell><cell>22.53</cell><cell>22.38</cell><cell>27.53</cell><cell>24.15</cell><cell>44.00</cell><cell>44.15</cell><cell>51.68</cell><cell>46.61</cell></row><row><cell>CoSMo (Lee et al., 2021)</cell><cell>31.26</cell><cell>21.39</cell><cell>16.90</cell><cell>21.32</cell><cell>19.87</cell><cell>44.45</cell><cell>37.49</cell><cell>46.02</cell><cell>42.65</cell></row><row><cell>ARTEMIS (RN18 + LSTM) (ours)</cell><cell>34.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>52?0.02 25.66?0.14 20.35?0.05 24.64?0.08 23.55?0.05 49.81?0.06 43.36?0.19 49.31?0.10 47.49?0.00 TIRG ? (RN50 + BiGRU) 34.86?0.23 25.01?0.72 19.48?1.13 24.43?0.02 22.97?0.32 49.17?0.10 42.48?0.09 48.61?0.73 46.75?0.17 ARTEMIS (RN18 + LSTM) (ours) 35.05?0.11 23.57?0.08 20.09?0.15 25.26?0.16 22.97?0.04 48.15?0.04 42.67?0.14 50.57?0.86 47.13?0.24 ARTEMIS (RN50 + LSTM) (ours) 36.96?0.34 26.60?0.35 20.69?0.74 27.11?0.00 24.80?0.21 51.45?0.77 43.13?0.80 52.76?0.16 49.11?0.50 ARTEMIS (RN18 + BiGRU) (ours) 35.36?0.08 23.92?0.06 20.19?0.13 25.80?0.20 23.30?0.04 48.71?0.18 42.01?0.28 51.54?0.50 47.42?0.15 ARTEMIS (RN50 + BiGRU) (ours) 37.13?0.03 28.13?0.36 21.43?0.22 25.91?0.02 25.16?0.01 51.66?0.24 45.22?0.02 50.41?0.86 49.10?0.19</figDesc><table><row><cell>Method</cell><cell>CM</cell><cell></cell><cell></cell><cell>R@10</cell><cell></cell><cell></cell><cell></cell><cell>R@50</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Dress</cell><cell>Shirt</cell><cell>Toptee</cell><cell>Mean</cell><cell>Dress</cell><cell>Shirt</cell><cell>Toptee</cell><cell>Mean</cell></row><row><cell>Transf. Dialog (Wu et al., 2021)</cell><cell>21.57</cell><cell>12.45</cell><cell>11.05</cell><cell>11.24</cell><cell>11.58</cell><cell>35.21</cell><cell>28.99</cell><cell>30.45</cell><cell>31.55</cell></row><row><cell>Transf. Dialog (Wu et al., 2021)</cell><cell>22.05</cell><cell>13.39</cell><cell>11.03</cell><cell>11.74</cell><cell>12.05</cell><cell>35.56</cell><cell>29.03</cell><cell>31.52</cell><cell>32.04</cell></row><row><cell>TIRG  ? (RN50 + LSTM)</cell><cell>35.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fashion IQ, test set. We report the challenge metric (CM) and individual R@K scores.? means our re-implementation. denotes the use of additional side information (e.g. attributes) at training time. Overall 1 st /2 nd in black/blue.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Shoes dataset. ? means our re-implementation. denotes the use of additional information (e.g. extra-captions, attributes) at training time. All reported methods use a ResNet50 as backbone. Overall 1 st /2 nd in black/blue.</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>R@10</cell><cell>R@50</cell><cell>( R@K) /3</cell></row><row><cell>TIRG  ? (RN50-LSTM)</cell><cell cols="4">15.52?0.52 48.65?1.03 76.49?1.42 46.89?0.99</cell></row><row><cell>TIRG  ? (RN50-BiGRU)</cell><cell cols="4">14.46?0.96 47.51?0.68 75.17?0.32 45.71?0.50</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>16.49</cell><cell>49.12</cell><cell>73.53</cell><cell>46.38</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>17.18</cell><cell>51.52</cell><cell>75.83</cell><cell>48.18</cell></row><row><cell>CoSMo (Lee et al., 2021)</cell><cell>16.72</cell><cell>48.36</cell><cell>75.64</cell><cell>46.91</cell></row><row><cell>ARTEMIS (RN50-LSTM) (ours)</cell><cell cols="4">17.60?0.57 51.05?0.21 76.85?0.31 48.50?0.25</cell></row><row><cell cols="5">ARTEMIS (RN50-BiGRU) (ours) 18.72?0.23 53.11?0.77 79.31?0.19 50.38?0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>CIRR dataset, test set. Recall@K and Recall subset @K (according to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>01?0.34 38.31?0.55 54.59?0.59 84.69?0.78 37.36?0.06 59.31?0.53 72.51?0.80 37.84?0.27 ARTEMIS (BIGRU) (ours) 16.96?0.43 46.10?0.36 61.31?1.30 87.73?0.62 39.99?0.65 62.20?0.78 75.67?0.75 43.05?0.51</figDesc><table><row><cell></cell><cell>OSCAR) 19.55</cell><cell>52.55</cell><cell>68.39</cell><cell>92.38</cell><cell>39.20</cell><cell>63.03</cell><cell>79.49</cell><cell>45.88</cell></row><row><cell>CIRPLANT</cell><cell>15.18</cell><cell>43.36</cell><cell>60.48</cell><cell>87.64</cell><cell>33.81</cell><cell>56.99</cell><cell>75.40</cell><cell>38.59</cell></row><row><cell>TIRG  ? (BiGRU)</cell><cell>10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Fashion200K dataset results. ? means results of our re-implementation. ? denotes results published by<ref type="bibr" target="#b48">Vo et al. (2019)</ref>. denotes the use of additional information (e.g. extra captions) at training time. For our results, we report the average of 3 runs. Overall 1 st /2 nd /3 rd in black/blue/red.</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>R@10</cell><cell>R@50</cell><cell>( R@K) /3</cell></row><row><cell>FiLM  ? (Perez et al., 2018)</cell><cell>12.9</cell><cell>39.5</cell><cell>61.9</cell><cell>38.1</cell></row><row><cell>MRN  ? (Kim et al., 2016)</cell><cell>13.4</cell><cell>40.0</cell><cell>61.9</cell><cell>38.4</cell></row><row><cell>Relationship  ? (Santoro et al., 2017)</cell><cell>13.0</cell><cell>40.5</cell><cell>62.4</cell><cell>38.6</cell></row><row><cell>TIRG  ? (Vo et al., 2019)</cell><cell>14.1</cell><cell>42.5</cell><cell>63.8</cell><cell>40.1</cell></row><row><cell>TIRG  ? (RN18+LSTM)</cell><cell cols="4">20.8?2.5 50.7?0.4 69.9?0.8 47.1?1.0</cell></row><row><cell>TIRG  ? (RN18+BiGRU)</cell><cell cols="4">20.2?0.4 49.4?1.2 69.8?1.2 46.5?0.6</cell></row><row><cell cols="2">LBF (Hosseinzadeh &amp; Wang, 2020) 17.8</cell><cell>48.4</cell><cell>68.5</cell><cell>44.9</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>21.2</cell><cell>49.0</cell><cell>68.8</cell><cell>46.3</cell></row><row><cell>VAL (Chen et al., 2020)</cell><cell>22.9</cell><cell>50.8</cell><cell>72.7</cell><cell>48.8</cell></row><row><cell>CoSMo (Lee et al., 2021)</cell><cell>23.3</cell><cell>50.4</cell><cell>69.3</cell><cell>47.7</cell></row><row><cell>JVSM (Chen &amp; Bazzani, 2020)</cell><cell>19.0</cell><cell>52.1</cell><cell>70.0</cell><cell>47.0</cell></row><row><cell>ARTEMIS (ours) (RN18+LSTM)</cell><cell cols="4">21.5?0.9 51.1?1.0 70.5?0.8 47.7?0.8</cell></row><row><cell>ARTEMIS (ours) (RN18+BiGRU)</cell><cell cols="4">20.2?0.9 49.3?0.4 69.3?0.9 46.2?0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Efficiency comparison. All models use Resnet50 as image encoder and LSTM as text encoder. In red, we give the added values compared to our reference point which only considers the encoders. Latency is computed over the 12k queries of the FashionIQ validation set.</figDesc><table><row><cell>Method</cell><cell>MACs (G)</cell><cell cols="2">Parameters (M) Latency FIQ (s)</cell></row><row><cell>Reference (only RN50 + LSTM encoders)</cell><cell>8.35</cell><cell>31.65</cell><cell>114.09</cell></row><row><cell>ARTEMIS</cell><cell cols="2">8.35 (+0.001) 32.96 (+1.31)</cell><cell>122.87 (+8.78)</cell></row><row><cell>TIRG</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Eventually, these visualizations help to understand what semantic parts of the image are taken into account by the model to evaluate the relevance of a candidate target image for a given query.</figDesc><table><row><cell>has longer</cell></row><row><cell>sleeves and</cell></row><row><cell>has stripes</cell></row><row><cell>[and] has a</cell></row><row><cell>higher neckline</cell></row><row><cell>with sleeves</cell></row><row><cell>Is a short blue v</cell></row><row><cell>neck sleeveless</cell></row><row><cell>dress [and] has</cell></row><row><cell>sleeves and is a</cell></row><row><cell>solid brighter</cell></row><row><cell>color blue</cell></row><row><cell>is orange</cell></row><row><cell>and less</cell></row><row><cell>revealing</cell></row><row><cell>[and] is</cell></row><row><cell>orange and</cell></row><row><cell>ruffled.</cell></row><row><cell>is brown</cell></row><row><cell>with long</cell></row><row><cell>sleeves and</cell></row><row><cell>a u neck</cell></row><row><cell>[and] has</cell></row><row><cell>more red</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>).</figDesc><table><row><cell>two bottles are</cell></row><row><cell>identical, and</cell></row><row><cell>in the middle</cell></row><row><cell>one pepsi</cell></row><row><cell>bottle</cell></row><row><cell>Dog sits on</cell></row><row><cell>a rock in</cell></row><row><cell>the water.</cell></row><row><cell>Younger</cell></row><row><cell>girls and</cell></row><row><cell>different</cell></row><row><cell>pajamas</cell></row><row><cell>Dog runs</cell></row><row><cell>through the</cell></row><row><cell>snow.</cell></row><row><cell>Focus on two</cell></row><row><cell>birds with orange</cell></row><row><cell>and green</cell></row><row><cell>bodies perched</cell></row><row><cell>on a branch.</cell></row><row><cell>Take a far</cell></row><row><cell>away shot</cell></row><row><cell>showing trees</cell></row><row><cell>behind scene</cell></row><row><cell>Another</cell></row><row><cell>angle and</cell></row><row><cell>gorilla's</cell></row><row><cell>mouth</cell></row><row><cell>open</cell></row><row><cell>More distant</cell></row><row><cell>pandas and</cell></row><row><cell>one with his</cell></row><row><cell>face turned</cell></row><row><cell>to the left.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Not having the reference image explicitly referred to in the text does not mean it is useless (c.f. first view).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional learning of image-text query for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Muhammad Umer Anwaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Labintcev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinsteuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1140" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep attention neural tensor network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unifying deep local and global features for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="726" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Leveraging style and content features for text conditioned image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranit</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surgan</forename><surname>Jandial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinkesh</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausoom</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Worshops</title>
		<meeting>CVPR Worshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="3978" to="3982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning joint visual semantic matching embeddings for languageguided retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image search with text feedback by visiolinguistic attention learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3001" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic embeddings for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12894</idno>
		<title level="m">The efficiency misnomer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VSE++: Improving visualsemantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fashion iq challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://sites.google.com/view/cvcreative2020/fashion-iq" />
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Worshops</title>
		<meeting>CVPR Worshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dialog-based interactive image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="676" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic spatially-aware fashion concept discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Phoenix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1463" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Composed query image retrieval using locally bounded features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3596" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking floating point for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation: The Benefit of Target Expectation Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cosmo: Content-style modulation for image retrieval with text feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="802" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person Search with Natural Language Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image retrieval on real-life images with pre-trained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9826" to="9836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3456" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-tuning CNN image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image search with selective match kernels: aggregation across single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning and aggregating deep local descriptors for instance-level recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="460" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval-an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6439" to="6448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transferable Attention for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fashion IQ: A new dataset towards retrieving images by natural language feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11307" to="11317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
