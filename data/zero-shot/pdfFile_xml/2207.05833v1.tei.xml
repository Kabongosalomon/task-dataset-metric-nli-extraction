<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Earthformer: Exploring Space-Time Transformers for Earth System Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
							<email>zhihan.gao@connect.ust.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
							<email>xjshi@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
							<email>yuyawang@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Earthformer: Exploring Space-Time Transformers for Earth System Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventionally, Earth system (e.g., weather and climate) forecasting relies on numerical simulation with complex physical models and are hence both expensive in computation and demanding on domain expertise. With the explosive growth of the spatiotemporal Earth observation data in the past decade, data-driven models that apply Deep Learning (DL) are demonstrating impressive potential for various Earth system forecasting tasks. The Transformer as an emerging DL architecture, despite its broad success in other domains, has limited adoption in this area. In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. Earthformer is based on a generic, flexible and efficient space-time attention block, named Cuboid Attention. The idea is to decompose the data into cuboids and apply cuboid-level self-attention in parallel. These cuboids are further connected with a collection of global vectors. We conduct experiments on the MovingMNIST dataset and a newly proposed chaotic N -body MNIST dataset to verify the effectiveness of cuboid attention and figure out the best design of Earthformer. Experiments on two real-world benchmarks about precipitation nowcasting and El Ni?o/Southern Oscillation (ENSO) forecasting show Earthformer achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Earth is a complex system. Variabilities of the Earth system, ranging from regular events like temperature fluctuation to extreme events like drought, hail storm, and El Ni?o/Southern Oscillation (ENSO), impact our daily life. Among all the consequences, Earth system variabilities can influence crop yields, delay airlines, cause floods and forest fires. Precise and timely forecasting of these variabilities can help people take necessary precautions to avoid crisis, or better utilize natural resources such as wind and solar energy. Thus, improving forecasting models for Earth variabilities (e.g. weather and climate) has a huge socioeconomic impact. Despite its importance, the operational weather and climate forecasting systems have not fundamentally changed for almost 50 years <ref type="bibr" target="#b31">[32]</ref>. These operational models, including the state-of-the-art High Resolution Ensemble Forecast (HREF) rainfall 0 Min.  nowcasting model used in National Oceanic and Atmospheric Administration (NOAA) <ref type="bibr" target="#b29">[30]</ref>, rely on meticulous numerical simulation of physical models. Such simulation-based systems inevitably fall short in the ability to incorporate signals from newly emerging geophysical observation systems <ref type="bibr" target="#b11">[12]</ref>, or take advantage of the Petabytes-scale Earth observation data <ref type="bibr" target="#b40">[41]</ref>.</p><p>As an appealing alternative, deep learning (DL) is offering a new approach for Earth system forecasting <ref type="bibr" target="#b31">[32]</ref>. Instead of explicitly incorporating physical rules, DL-based forecasting models are trained on the Earth observation data <ref type="bibr" target="#b33">[34]</ref>. By learning from a large amount of observations, the DL models are able to figure out the system's intrinsic physical rules and generate predictions that outperform simulation-based models <ref type="bibr" target="#b8">[9]</ref>. Such technique has demonstrated success in several applications, including precipitation nowcasting <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6]</ref> and ENSO forecasting <ref type="bibr" target="#b14">[15]</ref>. Because the Earth system is chaotic <ref type="bibr" target="#b19">[20]</ref>, high dimensional, and spatiotemporal, designing appropriate DL architecture for modeling the system is particularly challenging. Previous works relied on the combination of Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>. These two architectures impose temporal and spatial inductive biases that help capturing spatiotemporal patterns. However, as a chaotic system, variabilities of the Earth system, such as rainfall and ENSO, are highly sensitive to the system's initial condition and can respond abruptly to internal changes. It is unclear whether the inductive biases in RNN and CNN still hold for such complex system.</p><p>On the other hand, recent years have witnessed major breakthroughs in DL led by the wide adoption of Transformer. The model was originally proposed for natural language processing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7]</ref>, and has been later extended to computer vision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, multimodal text-image generation <ref type="bibr" target="#b28">[29]</ref>, graph learning <ref type="bibr" target="#b49">[50]</ref>, etc. Transformer relies on the attention mechanism to capture data correlations and is powerful at modeling complex and long-range dependencies, both of which appear in Earth systems (See <ref type="figure" target="#fig_0">Fig. 1</ref> for an example of Earth observation data). Despite being suitable for the problem, Transformer sees limited adoption in Earth system forecasting. Naively applying the Transformer architecture is infeasible because the O(N 2 ) attention mechanism is too computationally expensive for the high dimensional Earth observation data. How to design a space-time Transformer that is good at predicting the future of the Earth systems is largely an open problem to the community.</p><p>In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. To better explore the design of space-time attention, we propose Cuboid Attention, which is a generic building block for efficient space-time attention. The idea is to decompose the input tensor to non-overlapping cuboids and apply cuboid-level self-attention in parallel. Since we limit the O(N 2 ) self-attention inside the local cuboids, the overall complexity is greatly reduced. Different types of correlations can be captured via different cuboid decompositions. By stacking multiple cuboid attention layers with different hyperparameters, we are able to subsume several previously proposed video Transformers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> as special cases, and also come up with new attention patterns that were not studied before. A limitation of this design is the lack of a mechanism for the local cuboids to communicate with each other. Thus, we introduce a collection of global vectors that attend to all the local cuboids, thereby gathering the overall status of the system. By attending to the global vectors, the local cuboids can grasp the general dynamics of the system and share information with each other.</p><p>To verify the effectiveness of cuboid attention and figure out the best design under the Earth system forecasting scenario, we conducted extensive experiments on two synthetic datasets: the MovingM-NIST <ref type="bibr" target="#b33">[34]</ref> dataset and a newly proposed N -body MNIST dataset. Digits in the N -body MNIST follow the chaotic 3-body motion pattern <ref type="bibr" target="#b23">[24]</ref>, which makes the dataset not only more challenging than MovingMNIST but more relevant to Earth system forecasting. The synthetic experiments reveal the following findings: 1) stacking cuboid attention layers with the Axial attention pattern is both efficient and effective, achieving the best overall performance, 2) adding global vectors provides consistent performance gain without increasing the computational cost, 3) adding hierarchy in the encoder-decoder architecture can improve performance. Based on these findings, we figured out the optimal design for Earthformer and made comparisons with other baselines on the SEVIR <ref type="bibr" target="#b40">[41]</ref> benchmark for precipitation nowcasting and the ICAR-ENSO dataset <ref type="bibr" target="#b14">[15]</ref> for ENSO forecasting. Experiments show that Earthformer achieves the state-of-the-art (SOTA) performance on both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning architectures for Earth system forecasting. Conventional DL models for Earth system forecasting are based on CNN and RNN. U-Net with either 2D CNN or 3D CNN have been used for precipitation nowcasting <ref type="bibr" target="#b40">[41]</ref>, Seasonal Arctic Sea ice prediction <ref type="bibr" target="#b0">[1]</ref>, and ENSO forecasting <ref type="bibr" target="#b14">[15]</ref>. Shi et al. <ref type="bibr" target="#b33">[34]</ref> proposed the ConvLSTM network that combines CNN and LSTM for precipitation nowcasting. Wang et al. <ref type="bibr" target="#b42">[43]</ref> proposed PredRNN which adds the spatiotemporal memory flow structure to ConvLSTM. To better learn long-term high-level relations, Wang et al. <ref type="bibr" target="#b41">[42]</ref> proposed E3D-LSTM that integrates 3D CNN to LSTM. To disentangle PDE dynamics from unknown complementary information, PhyDNet <ref type="bibr" target="#b12">[13]</ref> incorporates a new recurrent physical cell to perform PDE-constrained prediction in latent space. Espeholt et al. <ref type="bibr" target="#b8">[9]</ref> proposed MetNet-2 that outperforms HREF for forecasting precipitation. The architecture is based on ConvLSTM and dilated CNN. Very recently, there are works that tried to apply Transformer for solving Earth system forecasting problems. Pathak et al. <ref type="bibr" target="#b25">[26]</ref> proposed the FourCastNet for global weather forecasting, which is based on Adaptive Fourier Neural Operators (AFNO) <ref type="bibr" target="#b13">[14]</ref>. Bai et al. <ref type="bibr" target="#b2">[3]</ref> proposed Rainformer for precipitation nowcasting, which is based on an architecture that combines CNN and Swin-Transformer <ref type="bibr" target="#b20">[21]</ref>. In the experiments, we can see that Earthformer outperforms Rainformer.</p><p>Space-time Transformers for video modeling. Inspired by the success of ViT <ref type="bibr" target="#b7">[8]</ref> for image classification, space-time Transformer is adopted for improved video understanding. In order to bypass the huge memory consumption brought by joint spatiotemporal attention, several pioneering work propose efficient alternatives, such as divided attention <ref type="bibr" target="#b3">[4]</ref>, axial attention <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>, factorized encoder <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref> and separable attention <ref type="bibr" target="#b51">[52]</ref>. Beyond minimal adaptation from ViT, some recent work introduce more vision prior to the design of space-time transformers, including trajectory <ref type="bibr" target="#b26">[27]</ref>, multi-scale <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref> and multi-view <ref type="bibr" target="#b46">[47]</ref>. However, no prior work focuses on exploring the design of space-time Transformers for Earth system forecasting.</p><p>Global and local attention in vision Transformers. To make self-attention more efficient in terms of both memory consumption and speed, recent works have adapted the essence of CNN to perform local attention in transformers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref>. HaloNets <ref type="bibr" target="#b38">[39]</ref> develops a new self-attention model family that are simple local self-attention and convolutional hybrids, which outperform both CNN and vanilla ViT on a range of downstream vision tasks. GLiT <ref type="bibr" target="#b4">[5]</ref> introduces a locality module and use neural architecture search to find an efficient backbone. Focal transformer <ref type="bibr" target="#b48">[49]</ref> proposes focal self-attention that can incorporate both fine-grained local and coarse-grained global interactions. However, these architectures are not directly applicable to spatiotemporal forecasting. Besides, they are also different from our design because we keep K global vectors to summarize the statistics of the dynamic system and connect the local cuboids; experiments show that such a global vector design is crucial for successful spatiotemporal forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Similar to previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>, we formulate Earth system forecasting as a spatiotemporal sequence forecasting problem. The Earth observation data, such as radar echo maps from NEXRAD <ref type="bibr" target="#b16">[17]</ref> and climate data from CIMP6 <ref type="bibr" target="#b9">[10]</ref>, are represented as a spatiotemporal sequence</p><formula xml:id="formula_0">[X i ] T i=1 , X i ? R H?W ?Cin .</formula><p>Based on these observations, the model predicts the K-step-ahead future</p><formula xml:id="formula_1">[Y T +i ] K i=1 , Y T +i ? R H?W ?Cout .</formula><p>Here, H, W denote the spatial resolution, and C in , C out denote the number of measurements available at each space-time coordinate from the input and the target sequence, respectively. As illustrated in <ref type="figure" target="#fig_4">Fig. 2</ref>, our proposed Earthformer is a hierarchical Transformer encoder-decoder based on Cuboid Attention. The input observations are encoded as a hierarchy of hidden states and then decoded to the prediction target. In the following, we present the detailed design of cuboid attention and the hierarchical encoder-decoder architecture adopted in Earthformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cuboid Attention</head><p>Compared with images and text, spatiotemporal data in Earth systems usually have higher dimensionality. As a consequence, applying Transformers to this task is challenging. For example, for a 3D  tensor with shape (T, H, W ), the complexity of the vanilla self-attention is O(T 2 H 2 W 2 ) and can be computationally infeasible. Previous literature proposed various structure-aware space-time attention mechanisms to reduce the complexity <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>These space-time attention mechanisms share the common design of stacking multiple elementary attention layers that focus on different types of data correlations (e.g., temporal correlation and spatial correlation). Steming from this observation, we propose the generic cuboid attention layer that involves three steps: "decompose", "attend", and "merge".</p><p>Decompose. We first decompose the input spatiotemporal tensor X ? R T ?H?W ?C into a sequence of cuboids {x (n) }.</p><formula xml:id="formula_2">{x (n) } = Decompose(X , cuboid_size, strategy, shift),<label>(1)</label></formula><formula xml:id="formula_3">where cuboid_size = (b T , b H , b W )</formula><p>is the size of the local cuboid, strategy ? {"local", "dilated"} controls whether to adopt the local decomposition strategy or the dilated decomposition strategy <ref type="bibr" target="#b3">[4]</ref>, shift = (s T , s H , s W ) is the window shift offset <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure" target="#fig_2">Fig. 4</ref> provides three examples showing how an input tensor will be decomposed following different hyperparameters of Decompose(?). There are a total number of T</p><formula xml:id="formula_4">b T H b H W b W cuboids in {x (n) }.</formula><p>To simplify the notation, we assume that T, H, W are divisible by b T , b H , b W . In the implementation, we pad the input tensor if it is not divisible. Assume x (n) is the (n T , n H , n W )-th cuboid in {x (n) }. The (i, j, k)-th element of x (n) can be mapped to the (i , j , k )-th element of X via Eqn. 2 if the strategy is "local" or Eqn. 3 if the strategy is "dilated".</p><formula xml:id="formula_5">i ? s T + b T (n T ? 1) + i mod T j ? s H + b H (n H ? 1) + j mod H k ? s W + b W (n W ? 1) + k mod W (2) i ? s T + b T (i ? 1) + n T mod T j ? s H + b H (j ? 1) + n H mod H k ? s W + b W (k ? 1) + n W mod W (3)</formula><p>Since the mapping is bijective, one can then map the elements from X to {x (n) } via the inverse operation.</p><p>Attend. After decomposing the input tensor into a sequence of non-overlapping cuboids {x (n) }, we apply self-attention within each cuboid in parallel.</p><formula xml:id="formula_6">x (n) out = Attention ? (x (n) , x (n) , x (n) ), 1 ? n ? N.<label>(4)</label></formula><p>The query, key, and value matrices Q, K, and V of Attention ? (Q,</p><formula xml:id="formula_7">K, V ) = Softmax (W Q Q)(W K K) T / ? C (W V V )</formula><p>are all flattened versions of x (n) , and we unravel the resulting matrix back to a 3D tensor. W Q , W K and W V are linear projection weights and are abbreviated together as ?. The self-attention parameter ? are shared across all cuboids. The computational complexity of the "attend" step is O</p><formula xml:id="formula_8">T b T H b H W b W (b T b H b W ) 2 ? O(T HW ? b T b H b W ),</formula><p>which scales linearly with the cuboid size. Since the cuboid size can be much smaller than the size of the input tensor, the layer is more efficient than full attention.</p><p>Merge. Merge(?) is the inverse operation of Decompose(?). The sequence of cuboids obtained after the attention step {x (n) out } are merged back to the original input shape to produce the final output of cuboid attention, as shown in Eqn. 5. The mapping follows the same bijections in Eqn. 2 and Eqn. 3.</p><formula xml:id="formula_9">X out = Merge({x (n) out } n , cuboid_size, strategy, shift).<label>(5)</label></formula><p>We combine the "decompose", "attend" and "merge" steps described in Eqn. 1,4,5 to construct the generic cuboid attention as in Eqn. 6.</p><p>X out = CubAttn ? (X , cuboid_size, strategy, shift).</p><p>Explore cuboid attention patterns. By stacking multiple cuboid attention layers with different choices of "cuboid_size", "strategy" and "shift", we are able to efficiently explore existing and potentially more effective space-time attention. In this paper, we explore the cuboid attention patterns as listed in <ref type="table">Table 1</ref>. From the table, we can see that cuboid attention subsumes previously proposed space-time attention methods like axial attention, video swin-Transformer, and divided space-time attention. Also, we manually picked the patterns that are reasonable and not computationally expensive as our search space. The flexibility of cuboid attention allows us to conduct Neural Architecture Search (NAS) to automatically search for a pattern but we will leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Vectors</head><p>One limitation of the previous formulation is that the cuboids do not communicate with each other. This is sub-optimal because each cuboid is not capable of understanding the global dynamics of the system. Thus, inspired by the [CLS] token adopted in BERT <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51]</ref>, we propose to introduce a collection of P global vectors G ? R P ?C to help cuboids scatter and gather crucial global information. When each cuboid is performing the self-attention, the elements will not only attend to the other elements within the same cuboid but attend to the global vectors G. We revise Eqn. 4 to Eqn. 7 to enable local-global information exchange. We also use Eqn. 8 to update the global vectors G by aggregating the information from all elements of the input tensor X .</p><p>x (n)</p><formula xml:id="formula_11">out = Attention ? x (n) , Cat(x (n) , G), Cat(x (n) , G) , 1 ? n ? N.<label>(7)</label></formula><p>G out = Attention ? (G, Cat(G, X ), Cat(G, X )) . <ref type="table">Table 1</ref>: Configurations of the cuboid attention patterns explored in the paper. The input tensor has shape (T, H, W ). If "shift" or "strategy" is not given, we use shift = (0, 0, 0) and strategy = "local" by default. When stacking multiple cuboid attention layers, each layer will be coupled with layer normalization layers and feed-forward network as in the Pre-LN Transformer <ref type="bibr" target="#b45">[46]</ref>. The first row shows the configuration of the generic cuboid attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Configurations Values</head><p>Generic Cuboid Attention</p><formula xml:id="formula_13">cuboid_size (T 1 , H 1 , W 1 ) ? (T 2 , H 2 , W 2 ) ? ? ? ? ? (T L , H L , W L ) shift (P 1 , M 1 , M 1 ) ? (P 2 , M 2 , M 2 ) ? ? ? ? ? (P L , M L , M L ) strategy "loc./dil." ? "loc./dil." ? ? ? ? ? "loc./dil." Axial cuboid_size (T, 1, 1) ? (1, H, 1) ? (1, 1, W) Divided Space-Time cuboid_size (T, 1, 1) ? (1, H, W) Video-Swin P ? M cuboid_size (P, M, M) ? ( P , M , M ) shift (0, 0, 0) ? (P/2, M/2, M/2) Spatial Local-Dilate-M cuboid_size (T, 1, 1) ? (1, M, M) ? (1, M, M) strategy "local" ? "local" ? "dilated" Axial Space Dilate-M cuboid_size (T, 1, 1) ? (1, H/M, 1) ? (1, H/M, 1) ? (1, 1, W/M) ? (1, 1, W/M) strategy "local" ? "dilated" ? "local" ? "dilated" ? "local"</formula><p>Here, Cat(?) flattens and concatenates its input tensors. By combining Eqn. 1,7,8,5, we abbreviate the overall computation of the cuboid attention layer with global vectors as in Eqn. 9.</p><p>X out = CubAttn ? (X , G, cuboid_size, strategy, shift),</p><formula xml:id="formula_14">G out = Attn global ? (G, X ).<label>(9)</label></formula><p>The additional complexity caused by the global vectors is approximately O T HW ? P + P 2 . Given that P is usually small (in our experiments, P is at most 8), computational overhead induced by the global structure is negligible. The architecture of the cuboid attention layer is illustrated in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Encoder-Decoder Architecture</head><p>Earthformer adopts a hierarchical encoder-decoder architecture illustrated in <ref type="figure" target="#fig_4">Fig. 2</ref>. The hierarchical architecture gradually encodes the input sequence to multiple levels of representations and generates the prediction via a coarse-to-fine procedure. Each hierarchy stacks D cuboid attention blocks. The cuboid attention block in the encoder uses one of the patterns described in <ref type="table">Table 1</ref>, and each cuboid block in the decoder adopts the "Axial" pattern. To reduce the spatial resolution of the input to cuboid attention layers, we include a pair of initial downsampling and upsampling modules that consist of stacked 2D-CNN and Nearest Neighbor Interpolation (NNI) layers. Different from other papers that adopt Transformer for video prediction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, we generate the predictions in a non-auto-regressive fashion rather than an auto-regressive patch-by-patch fashion. This means that our decoder directly generates the predictions from the initial learned positional embeddings. We also conducted experiments with an auto-regressive decoder based on visual codebook <ref type="bibr" target="#b30">[31]</ref>. However, the auto-regressive decoder underperforms the non-auto-regressive decoder in terms of forecasting skill scores. The comparison between non-auto-regressive decoder and auto-regressive decoder are shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first conducted experiments on two synthetic datasets, MovingMNIST and a newly proposed N -body MNIST, to verify the effectiveness of Earthformer and conduct ablation study on our design choices. Results on these two datasets lead to the following findings: 1) Among all patterns listed in <ref type="table">Table 1</ref>, "Axial" achieves the best overall performance; 2) Global vectors bring consistent performance gain with negligible increase in computational cost; 3) Using a hierarchical coarseto-fine structure can boost the performance. Based on these findings, we figured out the optimal design of Earthformer and compared it with other state-of-the-art models on two real-world datasets: SEVIR <ref type="bibr" target="#b40">[41]</ref> and ICAR-ENSO 2 . On both datasets, Earthformer achieved the best overall performance. The statistics of all the datasets used in the experiments are shown in <ref type="table" target="#tab_1">Table 2</ref>. We normalize the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Synthetic Datasets</head><p>MovingMNIST. We follow <ref type="bibr" target="#b35">[36]</ref> to use the public MovingMNIST dataset <ref type="bibr" target="#b2">3</ref> . The dataset contains 10,000 sequences. Each sequence shows 2 digits moving inside a 64 ? 64 frame. We split the dataset to use 8,100 samples for training, 900 samples for validation and 1,000 samples for testing. The task is to predict the future 10 frames for each sequence conditioned on the first 10 frames.</p><p>N -body MNIST. Earth is a complex system where an extremely large number of variables interact with each other. Compared with the Earth system, the dynamics of the synthetic MovingMNIST dataset, in which the digits move independently with constant speed, is over-simplified. Thus, achieving good performance on MovingMNIST does not imply that the model is capable of modeling complex interactions in Earth system. On the other hand, the real-world Earth observation data, though have experienced rapid development, are still noisy and may not provide useful insights for model development. Therefore, we extend MovingMNIST to N -body MNIST, where N digits are moving with the N -body motion pattern inside a 64 ? 64 frame. Each digit has its mass and is subjected to the gravity from other digits. We choose N = 3 in the experiments so that the digits will follow the chaotic 3-body motion <ref type="bibr" target="#b23">[24]</ref>. The highly non-linear interactions in N -body MNIST makes it much more challenging than the original MovingMNIST. We generate 20,000 sequences for training, 1,000 for validation and 1,000 for test. Perceptual examples of the dataset can be found at the first two rows of <ref type="figure" target="#fig_3">Fig. 5</ref>. In Appendix D, we demonstrate the chaotic behavior of N -body MNIST.</p><p>Hierarchical v.s. non-hierarchical. We choose "Axial" without global vectors as our cuboid attention pattern and compare the performance of non-hierarchical and hierarchical architectures on MovingMNIST. Ablation study on the importance of adopting a hierarchical encoder-decoder are shown in <ref type="table" target="#tab_2">Table 3</ref>. We can see that the hierarchical architecture has similar FLOPS with the nonhierarchical architectures while being better in MSE. This observation is consistent as we increase the depth until performance saturates.</p><p>Cuboid pattern search. The design of cuboid attention greatly facilitates the search for optimal space-time attention. We compare the patterns listed in <ref type="table">Table 1</ref> on both MovingMNIST and Nbody MNIST to investigate the effectiveness and efficiency of different space-time attention on spatiotemporal forecasting tasks. Besides the previously proposed space-time attention methods, we also include new configurations that are reasonable and not computationally expensive in our search  space. For each pattern, we also compare the variant that uses global vectors. Results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. We find that the "Axial" pattern is both effective and efficient and adding global vectors improves performance for all patterns while having similar FLOPS. We thus pick "Axial + global" as the pattern in Earthformer when conducting experiments on real-world datasets.</p><p>Comparison to the state of the art. We evaluate six spatiotemporal forecasting algorithms: UNet <ref type="bibr" target="#b40">[41]</ref>, ConvLSTM <ref type="bibr" target="#b33">[34]</ref>, PredRNN <ref type="bibr" target="#b42">[43]</ref>, PhyDNet <ref type="bibr" target="#b12">[13]</ref>, E3D-LSTM <ref type="bibr" target="#b41">[42]</ref> and Rainformer <ref type="bibr" target="#b2">[3]</ref>. The results are in <ref type="table" target="#tab_4">Table 5</ref>. Note that the MovingMNIST performance on several papers <ref type="bibr" target="#b12">[13]</ref> are obtained by training the model with on-the-fly generated digits while we pre-generate the digits and train all models on a fixed dataset. Comparing the numbers in the table with numbers shown in these papers are not fair. We train all baselines from scratch on both MovingMNIST and N -body MNIST using the default hyperparameters and configurations in their officially released code 4 .</p><p>Qualitative results on N -body MNIST. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the generation results of different methods on a sample sequence from N -body MNIST test set. The qualitative example demonstrates that our Earthformer is capable of learning the long-range interactions among digits and correctly predicting their future motion trajectories. Also, we can see that Earthformer is able to more accurately predict the position of the digits with the help of global vectors. On the contrary, none of the baseline algorithms that achieved solid performance on MovingMNIST gives the correct and precise position of the digit "0" in the last frame. They either predict incorrect motion trajectories (PredRNN and E3D-LSTM), or generate highly blurry predictions (Rainformer, UNet and PhyDNet) to accommodate the uncertainty about the future.   <ref type="bibr" target="#b2">[3]</ref>, UNet <ref type="bibr" target="#b40">[41]</ref>, ConvLSTM <ref type="bibr" target="#b33">[34]</ref>, PhyDNet <ref type="bibr" target="#b12">[13]</ref>, E3D-LSTM <ref type="bibr" target="#b41">[42]</ref>, PredRNN <ref type="bibr" target="#b42">[43]</ref>, Earthformer without using global vectors, Earthformer. The results are sorted according to the MSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEVIR Precipitation Nowcasting</head><p>Storm EVent ImageRy (SEVIR) <ref type="bibr" target="#b40">[41]</ref> is a spatiotemporally aligned dataset containing over 10,000 weather events. Each event consists of 384 km ? 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD Vertically Integrated Liquid (VIL) mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. SEVIR benchmark supports scientific research on multiple meteorological applications including precipitation nowcasting, synthetic radar generation, front detection, etc. We adopt SEVIR for benchmarking precipitation nowcasting, i.e., to predict the future VIL up to 60 minutes (12 frames) given 65 minutes context VIL (13 frames). <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of VIL observation sequences in SEVIR.</p><p>Besides MSE, we also include the Critical Success Index (CSI), which is commonly used in precipitation nowcasting and is defined as CSI = #Hits #Hits+#Misses+#F.Alarms . To count the #Hits (truth=1, pred=1), #Misses (truth=1, pred=0) and #F.Alarms (truth=0, pred=1), the prediction and the ground-truth are rescaled back to the range 0-255 and binarized at thresholds <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">74,</ref><ref type="bibr">133,</ref><ref type="bibr">160,</ref><ref type="bibr">181,</ref><ref type="bibr">219]</ref>. We report CSI at different thresholds and also their mean CSI-M.</p><p>SEVIR is much larger than MovingMNIST and N -body MNIST and has higher resolution. We thus slightly adjust the configurations of baselines based on those for MovingMNIST for fair comparison. Detailed configurations are shown in the Appendix A. The experiment results are listed in <ref type="table" target="#tab_6">Table 6</ref>. Earthformer consistently outperforms baselines on almost all metrics and brings significant performance gain especially at high thresholds like CSI-219, which are more valued by the communities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ICAR-ENSO Sea Surface Temperature Anomalies Forecasting</head><p>El Ni?o/Southern Oscillation (ENSO) has a wide range of associations with regional climate extremes and ecosystem impacts. ICAR-ENSO consists of historical climate observation and stimulation data provided by Institute for Climate and Application Research (ICAR). We forecast the SST anomalies up to 14 steps (2 steps more than one year for calculating three-month-moving-average) given context 12 steps SST anomalies observations. <ref type="table" target="#tab_7">Table 7</ref> compares the performance of our Earthformer with baselines on ICAR-ENSO dataset. We report the mean correlation skill C-Nino3.4-M = 1 K k C Nino3.4 k and the weighted mean correlation skill C-Nino3.4-WM = 1 K k a k ?C Nino3.4 k over K = 12 forecasting steps 5 , as well as the MSE between the spatiotemporal SST anomalies prediction and the corresponding ground-truth. We can find that Earthformer consistently outperforms baselines in all concerned evaluation metrics and that using global vectors further improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. Earthformer is based on a generic and efficient building block called Cuboid Attention. It achieves SOTA on MovingMNIST, our newly proposed N -body MNIST, SEVIR, and ICAR-ENSO. For future works, we plan to extend Earthformer with NAS and GAN to further improve the performance and apply it on more Earth system forecasting problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>All experiments are conducted on machines with NVIDIA V100 GPUs. All models including Earthformer and baselines can fit in a single GPU (with gradient checkpointing) and get trained without model parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Earthformer</head><p>Cuboid attention block. A cuboid attention block consists of L cuboid attention layers with different hyperparameters. as shown in 1. For example, as illustrated in the "Configuration Values" of <ref type="table">Table 1</ref>, "Axial" has L = 3 since it has 3 consecutive patterns (T, 1, 1) ? <ref type="figure" target="#fig_0">(1, H, 1) ? (1, 1, W)</ref>, "Divided Space-Time" and "Video Swin P ? M " have L = 2. We also add Layer Normalization (LN) and Feed-Forward Networks (FFNs) in the same way as Pre-LN Transformer <ref type="bibr" target="#b45">[46]</ref>. Given the input tensor X d?1 and global vectors G d?1 , the output (X d , G d ) of the d-th cuboid attention block (there are D cuboid attention blocks in total for each hierarchy as illustrated in <ref type="figure" target="#fig_4">Fig. 2)</ref>, is computed as Eqn. 10: is the combination of "decompose", "attend" and "merge" pipeline described in Eqn. 1,7,5. We omit cuboid_size, strategy and shift for brevity. Attn global d,l is equivalent to Eqn. 8. For the l-th cuboid pattern in a cuboid attention block, the input X d,l?1 and G d,l?1 go through LN layer, cuboid attention layer and FFN layer with residual connections sequentially, produce the output X d,l and G d,l . The output of the final cuboid pattern X d,L and G d,L serve as the output of the whole cuboid attention block X d and G d .</p><formula xml:id="formula_15">X d,1 = FFN local d,1 X d?1 + CubAttn local d,1 LN local d,1 (X d?1 ), LN global d,1 (G d?1 ) , G d,1 = FFN global d,1 G d?1 + Attn global d,1 LN global d,1 (G d?1 ), LN local d,1 (X d?1 ) , . . . X d,l = FFN local d,l X d,l?1 + CubAttn local d,l LN local d,l (X d,l?1 ), LN global d,l (G d,l?1 ) , G d,l = FFN global d,l G d,l?1 + Attn global d,l LN global d,l (G d,l?1 ), LN local d,l (X d,l?1 ) , . . . X d = X d,L = FFN local d,L X d,L?1 + CubAttn local d,L LN local d,L (X d,L?1 ), LN global d,L (G d,L?1 ) , G d = G d,L = FFN global d,L G d,L?1 + Attn global d,L LN global d,L (G d,L?1 ), LN local d,L (X d,L?1 ) .<label>(10)</label></formula><p>Hyperparameters of Earthformer architecture. The detailed architecture configurations of Earthformer are described in <ref type="table" target="#tab_10">Table 8 and Table 9</ref>. We adopt the same configurations for MovingMNIST, N -body MNIST and ICAR-ENSO datasets, as shown in <ref type="table">Table 8</ref>. We slightly adjust the configurations on SEVIR as shown in <ref type="table" target="#tab_10">Table 9</ref>, due to its high resolution and large dataset size.</p><p>Optimization. We train all Earthformer variants using AdamW optimizer. Detailed configurations are shown in <ref type="table" target="#tab_12">Table 10</ref>. We train for 100 epochs on all datasets and early stop the model training according to validation score with tolerance = 20. We adopt 20% linear warm-up and Cosine learning rate scheduler that decays the lr from its maximum to zero after warm-up. We adopt data parallel and gradient accumulation to use total batch size 64 while the 16GB GPU can only afford smaller batch size like 2 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baselines</head><p>We train baseline algorithms following their officially released configurations and tune the learning rate, learning rate scheduler, working resolution, etc. to optimize their performance on each dataset. We list the modifications we applied to baselines in each dataset in <ref type="table">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Ablation Analysis on Synthetic Datasets</head><p>As mentioned in Sec. 4.1, the design of cuboid attention greatly facilitates the search for optimal space-time attention. <ref type="table" target="#tab_3">Table 4</ref> summarizes the cuboid attention pattern search results with the model depth equals to 4 on both MovingMNIST and N -body MNIST. We further investigate if it is feasible to accelerate the pattern search using shallower models. The results with model depth equals to 2 are shown in <ref type="table" target="#tab_1">Table 12</ref>. We find that the "Axial" pattern is still the optimal among the patterns listed in <ref type="table">Table 1</ref>, and adding global vectors improves performance for all patterns while having similar FLOPS. This observation implies that in the future work, it might be feasible do NAS using smaller and shallower models with affordable cost and transfer the results to larger and deeper models. <ref type="table">Table 8</ref>: The details of the Earthformer model on MovingMNIST, N -body MNIST and ICAR-ENSO datasets. Conv3 ? 3 is the 2D convolutional layer with 3 ? 3 kernel. GroupNorm16 is the Group Normalization (GN) layer <ref type="bibr" target="#b44">[45]</ref> with 16 groups. The negative slope in LeakyReLU is 0.1. The FFN consists of two Linear layers separated by a GeLU activation layer <ref type="bibr" target="#b17">[18]</ref>. PatchMerge splits a 2D input tensor with C channels into N non-overlapping p ? p patches and merges the spatial dimensions into channels, gets N 1 ? 1 patches with p 2 ? C channels and concatenates them back along spatial dimensions.   <ref type="bibr" target="#b17">[18]</ref>. PatchMerge splits a 2D input tensor with C channels into N non-overlapping p ? p patches and merges the spatial dimensions into channels, gets N 1 ? 1 patches with p 2 ? C channels and concatenates them back along spatial dimensions.   <ref type="table">Table 11</ref>: Implementation details of baseline algorithms. Modifications based on the officially released implementations are listed according to different datasets. "-" means no modification is applied. "reverse enc-dec" means adopting the reversed encoder-decoder architecture proposed in <ref type="bibr" target="#b34">[35]</ref>. "2D CNN downsampler/upsampler (8?) wrapper" means we wrap the model with a pair of 2D CNN downsampler and upsampler to downsample the spatial resolution 8 times in order to reduce the GPU memory cost as well as the FLOPS. The 2D CNN downsampler and upsampler are of the same designs as those used in Earthformer. Other terms listed are the hyperparameters in their officially released implementations.</p><formula xml:id="formula_16">Model MovingMNIST N -body MNIST SEVIR ICAR-ENSO UNet [41] - - - num_layer=3</formula><p>ConvLSTM <ref type="bibr" target="#b33">[34]</ref> reverse enc-dec <ref type="bibr" target="#b34">[35]</ref> reverse enc-dec <ref type="bibr" target="#b34">[35]</ref> reverse enc-dec <ref type="bibr" target="#b34">[35]</ref> reverse enc-dec [35] conv_kernels = [(7,7), <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>] conv_kernels = [(7,7), <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>] conv_kernels = [(7,7), <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>] conv_kernels = [(7,7), <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>,(3,3)] deconv_kernels = [ <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>] deconv_kernels = [ <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr">(</ref>   <ref type="figure" target="#fig_9">Figure 6</ref>: Illustration of the Earthformer variant in auto-regressive fashion (Earthformer AR). It is a hierarchical Transformer encoder-decoder based on cuboid attention. The input is a length-3 sequence of frames containing discrete visual codes and the target sequence has length 2. "?D" means stacking D (causal) cuboid attention blocks with residual connection. "M ?" means M layers of hierarchies. A causal cuboid attention block is a cuboid attention block with causal mask. The "Downsample" and "Upsample" layers in the decoder follow the designs described in <ref type="bibr" target="#b32">[33]</ref> and are hence also causal. Causal cuboid attention blocks and causal downsampling and upsampling layers prevent each element attending to the elements with larger indices in raster-scan ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Non-Auto-Regressive v.s. Auto-Regressive</head><p>As mentioned in Sec. 3.3, besides generating the future predictions directly in a non-auto-regressive way, previous works generate the predictions patch-by-patch in raster-scan ordering <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b27">28]</ref>. In fact, the pros and cons of auto-regressive approach and non-auto-regressive approach are not clear under the setting of Earth system forecasting. Therefore, we replace the decoder of the non-autoregressive Earthformer with the auto-regressive decoder shown in <ref type="figure" target="#fig_9">Fig. 6</ref> and propose an Earthformer variant called Earthformer AR.</p><p>The auto-regressive approach uses discrete visual tokens as the input and target. Here, we pretrain a VQ-VAE <ref type="bibr" target="#b37">[38]</ref> with a codebook e ? R Q?C code , where Q is the codebook size. The discrete latent space is hence Q-way categorical. The VQ-VAE encoder downsamples an input frame X i ? R H?W ?C to Enc(X i ) ? R H ?W ?C code , then maps each element to its nearest code e i learned in the codebook, and produces the encoding Enc code (X i ) ? [Q] H ?W ?1 . The decoder does it reversely: it takes the latent visual codes Z code i ? [Q] H ?W ?1 , indexes the codes in the codebook to convert the discrete codes to real-valued vectors Z i ? R H ?W ?C code , and then upsamples the frame back to pixel space Y i ? R H?W ?C . <ref type="figure" target="#fig_9">Fig. 6</ref> illustrates the architecture of the Earthformer AR. The input is a sequence of frames containing discrete visual codes [Enc code (X i )] i , which are the output of the VQ-VAE encoder. The target sequence is [Enc code (Y i )] i . Earthformer AR generates the target code one-by-one in raster-scan ordering. The generation at current step Z code (a,b,c) is conditioned on the context X i and all the previously generated codes [Z code &lt;(a,b,c) ]:</p><formula xml:id="formula_17">p(Z code ) = (a,b,c) p Z code (a,b,c) |Z code &lt;(a,b,c) , X i .<label>(11)</label></formula><p>The VQ-VAE decoder decodes the generated visual codes Z code to the final output in pixel space.</p><p>We conduct preliminary experiments of Earthformer AR on SEVIR dataset and found that Earthformer AR is able to give more perceptually satisfying predictions than Earthformer, but is inferior in terms of skill scores. <ref type="figure" target="#fig_5">Fig. 7</ref> to <ref type="figure" target="#fig_0">Fig. 10</ref> show the qualitative results of Earthformer and Earthformer AR on several challenging test cases. The outputs of Earthformer AR look more like "real" VIL images and does not suffer from producing blurry predictions. However, the performance of Earthformer AR is much worse than Earthformer (even worse than some simple baselines including UNet <ref type="bibr" target="#b40">[41]</ref>) in  the concerned evaluation metrics. We further investigate the effect of the sampling algorithm for Earthformer AR. We compared 1) generating the codes with argmax step-by-step and 2) randomly draw samples from p Z code (a,b,c) |Z code &lt;(a,b,c) , X i specified in Eqn. 11. We denote the argmax-variant as Earthformer AR argmax. We find that argmax sampling can give better skill scores but the generated results have less perceptual similarity than the "real" VIL images.</p><p>Based on these results, we pick the non-auto-regressive decoder for experiments in the main paper. The fact that Earthformer AR gives more perceptually satisfying predictions than Earthformer while having worse skill scores triggers future work. So far, there is no well-established metric for evaluating the preceptual quality of the predictions generated by Earth system forecasting models. The Fr?chet Inception Distance (FID) and Inception Score (IS) adopted in evaluating image GANs <ref type="bibr" target="#b22">[23]</ref> rely on a pretrained network. We are able to pretrain Earthformer on Earth observations and adopt the pretrained network for both calculating the FID and initializing the GAN discriminator.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Chaos in N-Body MNIST Dataset</head><p>Earth system is chaotic <ref type="bibr" target="#b36">[37]</ref>, meaning that the future is very sensitive to the initial conditions. In <ref type="figure" target="#fig_0">Fig. 11</ref>, we illustrate the chaotic effect of N -body MNIST. By only slightly changing the initial velocities of the digits, the positions of the digits after 20 steps change significantly. N -body MNIST is thus much more challenging than MovingMNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Evaluation Metrics in SEVIR</head><p>In Sec. 4.2 and <ref type="table" target="#tab_6">Table 6</ref>, we follow <ref type="bibr" target="#b40">[41]</ref> to use the Critical Success Index (CSI) for prediction quality evaluation <ref type="bibr" target="#b5">6</ref> . Besides <ref type="bibr" target="#b40">[41]</ref>, the SEVIR team holds SEVIR Dataset Challenge 7 , where the data involved are exactly the same while the evaluation metrics are slightly different 8 .</p><p>Specifically, <ref type="bibr" target="#b40">[41]</ref> used six precipitation thresholds which correspond to pixel values <ref type="bibr">[219,</ref><ref type="bibr">181,</ref><ref type="bibr">160,</ref><ref type="bibr">133,</ref><ref type="bibr">74,</ref><ref type="bibr" target="#b15">16]</ref>. The prediction and the ground-truth are rescaled back to the range 0-255. The #Hits(? ) (truth? ? , pred? ? ), #Misses(? ) (truth? ? , pred&lt; ? ) and #F.Alarms(? ) (truth&lt; ? , pred? ? ) at certain threshold ? are counted over all test pixels as shown in Eqn. 12. CSI-?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Hits(? ) =</head><p>? ? [219, 181, 160, 133, 74, 16] is one of the thresholds. Hits(? ), Misses(? ), F.Alarms(? ) ? [0, 1] N ?T ?H?W , where N is the test dataset size, T is the forecasting horizon, H and W are the height and width respectively. We denote the average CSI-? over the thresholds <ref type="bibr">[219,</ref><ref type="bibr">181,</ref><ref type="bibr">160,</ref><ref type="bibr">133,</ref><ref type="bibr">74,</ref><ref type="bibr" target="#b15">16]</ref> as CSI-M. The results using the above evaluation metrics are demonstrated in Sec. 4.2 and <ref type="table" target="#tab_6">Table 6</ref>.</p><p>The calculation of CSI is slightly different in SEVIR Dataset Challenge as shown in Eqn. 13. <ref type="bibr" target="#b5">6</ref> The implementation details of CSI used in Sec. 4.2 and <ref type="table" target="#tab_6">Table 6</ref> follow https://github.com/ MIT-AI-Accelerator/neurips-2020-sevir <ref type="bibr" target="#b6">7</ref> Challenge website at https://sevir.mit.edu/nowcasting <ref type="bibr" target="#b7">8</ref> The implementation details of CSI used in SEVIR Dataset Challenge is available at https://github. com/MIT-AI-Accelerator/sevir_challenges.  CSI-?</p><p>The CSI-? , at certain threshold, is the mean of CSI-? (t) over forecasting horizon T . We denote the average CSI-? over the three thresholds [133, 74, 16] used in SEVIR Dataset Challenge as CSI-M3, and the average CSI-? over the six thresholds <ref type="bibr">[219,</ref><ref type="bibr">181,</ref><ref type="bibr">160,</ref><ref type="bibr">133,</ref><ref type="bibr">74,</ref><ref type="bibr" target="#b15">16]</ref> used in <ref type="bibr" target="#b40">[41]</ref> as CSI-M6. The experiment results evaluated using the metrics in SEVIR Dataset Challenge are listed in <ref type="table" target="#tab_2">Table 13</ref>. Earthformer still consistently outperforms baselines on almost all metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example Vertically Integrated Liquid (VIL) observation sequence from the Storm EVent ImageRy (SEVIR) dataset. The observation intensity is mapped to pixel value of the range 0-255. The larger value indicates the higher precipitation intensity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Illustration of the Earthformer architecture. It is a hierarchical Transformer encoder-decoder based on cuboid attention. The input sequence has length T and the target sequence has length K. "?D" means to stack D cuboid attention blocks with residual connection. "M ?" means to have M layers of hierarchies. Illustration of the cuboid attention layer with global vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of cuboid decomposition strategies when the input shape is (T, H, W ) =<ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4)</ref>, and cuboid size (b T , b H , b W ) = (3, 2, 2). Cells that have the same color belong to the same cuboid and will attend to each other. shift = (0, 1, 1) shifts the cuboid decomposition by 1 pixel along height and width dimensions. strategy = "local" means to aggregate contiguous (b T , b H , b W ) pixels as a cuboid. strategy = "dilated" means to aggregate pixels everyT b T ( H b H ,W b W ) steps along time (height, width) dimension. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>A set of examples showing the perceptual quality of the predictions on the N -body MNIST test set. From top to bottom: input frames, target frames, predictions by Rainformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 N</head><label>2</label><figDesc>ENSO sea surface temperature (SST) anomalies forecasting for lead times up to one year (12 steps) is a valuable and challenging problem. Nino3.4 index, which is the areaaveraged SST anomalies across a certain area (170 ? -120 ? W, 5 ? S-5 ? N) of the Pacific, serves as a crucial indicator of this climate event. The forecast quality is evaluated by the correlation skill [15] of the three-month-moving-averaged Nino3.4 index C Nino3.4 = N (X?X)(Y ?? ) ? N (X?X) (Y ?? ) 2 ? R K calculated on the whole test set of size N , where Y ? R N ?K is the ground-truth of K-step Nino3.4 index, X ? R N ?K is the corresponding prediction of Nino3.4 index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Non-Auto-Regressive v.s. Auto-Regressive on SEVIR: qualitative example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Non-Auto-Regressive v.s. Auto-Regressive on SEVIR: qualitative example 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Non-Auto-Regressive v.s. Auto-Regressive on SEVIR: qualitative example 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Non-Auto-Regressive v.s. Auto-Regressive on SEVIR: qualitative example 4. Chaos in N -body MNIST: the effect of a slight disturbance on the initial velocities is much more significant on N -body MNIST than on MovingMNIST. The top half are two MovingMNIST sequences, where their initial conditions only slightly differ in the the initial velocities. The bottom half are two N -body MNIST sequences. N -body MNIST sequence 1 has exactly the same initial condition as MovingMNIST sequence 1. N -body MNIST sequence 2 has exactly the same initial condition as MovingMNIST sequence 2. The final positions of digits in MovingMNIST after 20 steps evolution only slightly differ from each other, while the differences are much more significant in the final frames of N -body MNIST sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F 6 ?</head><label>6</label><figDesc>)[n, t, h, w] .Alarms(? )[n, t, h, w] CSI-? = #Hits(? ) #Hits(? ) + #Misses(? ) + #F.Alarms(? ) CSI-M = 1 ?[219,181,160,133,74,16]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>F?</head><label></label><figDesc>)[n, t, h, w] #F.Alarms(?, t) = .Alarms(? )[n, t, h, w] CSI-? (t) = #Hits(?, t) #Hits(?, t) + #Misses(?, t) + #F.Alarms(?, t) ?[219,181,160,133,74,<ref type="bibr" target="#b15">16]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets used in the experiments.</figDesc><table><row><cell>Dataset</cell><cell>train</cell><cell>Size val</cell><cell>test</cell><cell cols="3">Seq. Len. Spatial Resolution in out H ? W</cell></row><row><cell>MovingMNIST</cell><cell>8,100</cell><cell>900</cell><cell cols="2">1,000 10</cell><cell>10</cell><cell>64 ? 64</cell></row><row><cell cols="3">N -body MNIST 20,000 1,000</cell><cell cols="2">1,000 10</cell><cell>10</cell><cell>64 ? 64</cell></row><row><cell>SEVIR</cell><cell cols="4">35,718 9,060 12,159 13</cell><cell>12</cell><cell>384 ? 384</cell></row><row><cell>ICAR-ENSO</cell><cell>5,205</cell><cell>334</cell><cell cols="2">1,667 12</cell><cell>14</cell><cell>24 ? 48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the importance of adopting a hierarchical encoder-decoder. We conducted experiments on MovingMNIST. "Depth D" means the model stacks D cuboid attention blocks and there is no hierarchical structure. "Depth D1, D2" means the model stacks D1 cuboid attention blocks, applies the pooling layer, and stacks another D2 cuboid attention blocks.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param. (M) GFLOPS</cell><cell cols="3">Metrics MSE ? MAE ? SSIM ?</cell></row><row><cell>Depth 2</cell><cell>1.4</cell><cell>17.9</cell><cell>63.80</cell><cell>140.6</cell><cell>0.8324</cell></row><row><cell>Depth 4</cell><cell>3.1</cell><cell>36.3</cell><cell>52.46</cell><cell>114.8</cell><cell>0.8685</cell></row><row><cell>Depth 6</cell><cell>4.9</cell><cell>54.6</cell><cell>50.49</cell><cell>110.0</cell><cell>0.8738</cell></row><row><cell>Depth 8</cell><cell>6.6</cell><cell>73.0</cell><cell>48.04</cell><cell>104.6</cell><cell>0.8797</cell></row><row><cell>Depth 1,1</cell><cell>1.4</cell><cell>11.5</cell><cell>60.99</cell><cell>135.7</cell><cell>0.8388</cell></row><row><cell>Depth 2,2</cell><cell>3.1</cell><cell>18.9</cell><cell>50.41</cell><cell>106.9</cell><cell>0.8805</cell></row><row><cell>Depth 3,3</cell><cell>4.9</cell><cell>26.3</cell><cell>47.69</cell><cell>100.1</cell><cell>0.8873</cell></row><row><cell>Depth 4,4</cell><cell>6.6</cell><cell>33.7</cell><cell>46.91</cell><cell>101.5</cell><cell>0.8825</cell></row></table><note>data to the range [0, 1] and trained all models with the Mean-Squared Error (MSE) loss. More implementation details are shown in the Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of different cuboid attention patterns and the effect of global vectors on MovingMNIST and N -body MNIST. The variant that achieved the best performance is in bold-case while the second best is underscored. We also compared the performance of the cuboid attention patterns with and without global vectors and highlight the better one with grey background. MAE ? SSIM ? MSE ? MAE ? SSIM ?</figDesc><table><row><cell cols="6">Model MSE ? Axial Metrics on MovingMNIST #Param. (M) GFLOPS 6.61 33.7 46.91 101.5 0.8825</cell><cell cols="3">Metrics on N -Body 15.89 41.38 0.9510</cell></row><row><cell>+ global</cell><cell>7.61</cell><cell>34.0</cell><cell>41.79</cell><cell>92.78</cell><cell>0.8961</cell><cell>14.82</cell><cell>39.93</cell><cell>0.9538</cell></row><row><cell>DST</cell><cell>5.70</cell><cell>35.2</cell><cell>57.43</cell><cell>118.6</cell><cell>0.8623</cell><cell>18.24</cell><cell>45.88</cell><cell>0.9435</cell></row><row><cell>+ global</cell><cell>6.37</cell><cell>35.5</cell><cell>52.92</cell><cell>108.3</cell><cell>0.8760</cell><cell>17.77</cell><cell>45.84</cell><cell>0.9433</cell></row><row><cell>Video Swin 2x8</cell><cell>5.66</cell><cell>31.1</cell><cell>54.45</cell><cell>111.7</cell><cell>0.8715</cell><cell>19.89</cell><cell>49.02</cell><cell>0.9374</cell></row><row><cell>+ global</cell><cell>6.33</cell><cell>31.4</cell><cell>52.70</cell><cell>108.5</cell><cell>0.8766</cell><cell>19.53</cell><cell>48.43</cell><cell>0.9389</cell></row><row><cell>Video Swin 10x8</cell><cell>5.89</cell><cell>39.2</cell><cell>63.34</cell><cell>125.3</cell><cell>0.8525</cell><cell>23.35</cell><cell>53.17</cell><cell>0.9274</cell></row><row><cell>+ global</cell><cell>6.56</cell><cell>39.4</cell><cell>62.15</cell><cell>123.4</cell><cell>0.8541</cell><cell>22.81</cell><cell>52.94</cell><cell>0.9293</cell></row><row><cell>Spatial Local-Global 2</cell><cell>6.61</cell><cell>33.3</cell><cell>59.88</cell><cell>122.1</cell><cell>0.8572</cell><cell>23.24</cell><cell>54.63</cell><cell>0.9263</cell></row><row><cell>+ global</cell><cell>7.61</cell><cell>33.7</cell><cell>59.42</cell><cell>122.9</cell><cell>0.8565</cell><cell>21.88</cell><cell>52.49</cell><cell>0.9305</cell></row><row><cell>Spatial Local-Global 4</cell><cell>6.61</cell><cell>33.5</cell><cell>58.72</cell><cell>118.5</cell><cell>0.8600</cell><cell>21.02</cell><cell>49.82</cell><cell>0.9344</cell></row><row><cell>+ global</cell><cell>7.61</cell><cell>33.9</cell><cell>54.84</cell><cell>115.5</cell><cell>0.8585</cell><cell>19.82</cell><cell>48.12</cell><cell>0.9371</cell></row><row><cell>Axial Space Dilate 2</cell><cell>8.59</cell><cell>41.8</cell><cell>50.11</cell><cell>104.4</cell><cell>0.8814</cell><cell>15.97</cell><cell>42.19</cell><cell>0.9494</cell></row><row><cell>+ global</cell><cell>10.30</cell><cell>42.4</cell><cell>46.86</cell><cell>98.95</cell><cell>0.8884</cell><cell>15.73</cell><cell>41.85</cell><cell>0.9510</cell></row><row><cell>Axial Space Dilate 4</cell><cell>8.59</cell><cell>41.6</cell><cell>47.40</cell><cell>99.31</cell><cell>0.8865</cell><cell>19.49</cell><cell>51.04</cell><cell>0.9352</cell></row><row><cell>+ global</cell><cell>10.30</cell><cell>42.2</cell><cell>45.11</cell><cell>95.98</cell><cell>0.8928</cell><cell>17.91</cell><cell>46.35</cell><cell>0.9440</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>UNet [41]</cell><cell>16.6</cell><cell>0.9</cell><cell>110.4 249.4 0.6170 38.90 94.29 0.8260</cell></row><row><cell>ConvLSTM [34]</cell><cell>14.0</cell><cell>30.1</cell><cell>62.04 126.9 0.8477 32.15 72.64 0.8886</cell></row><row><cell>PredRNN [43]</cell><cell>23.8</cell><cell>232.0</cell><cell>52.07 108.9 0.8831 21.76 54.32 0.9288</cell></row><row><cell>PhyDNet [13]</cell><cell>3.1</cell><cell>15.3</cell><cell>58.70 124.1 0.8350 28.97 78.66 0.8206</cell></row><row><cell>E3D-LSTM [42]</cell><cell>12.9</cell><cell>302.0</cell><cell>55.31 101.6 0.8821 22.98 62.52 0.9131</cell></row><row><cell>Rainformer [3]</cell><cell>19.2</cell><cell>1.2</cell><cell>85.83 189.2 0.7301 38.89 96.47 0.8036</cell></row><row><cell>Earthformer w/o global</cell><cell>6.6</cell><cell>33.7</cell><cell>46.91 101.5 0.8825 15.89 41.38 0.9510</cell></row><row><cell>Earthformer</cell><cell>7.6</cell><cell>34.0</cell><cell>41.79 92.78 0.8961 14.82 39.93 0.9538</cell></row></table><note>Comparison of Earthformer with baselines on MovingMNIST and N -body MNIST.Model #Param. (M) GFLOPS MovingMNIST N -body MNIST MSE ? MAE ? SSIM ? MSE ? MAE ? SSIM ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on SEVIR. We include Critical Success Index (CSI) besides MSE as evaluation metrics. The CSI, a.k.a intersection over union (IOU), is calculated at different precipitation thresholds and denoted as CSI-thresh. ? CSI-219 ? CSI-181 ? CSI-160 ? CSI-133 ? CSI-74 ? CSI-16 ? MSE (10 ?3 ) ?</figDesc><table><row><cell cols="4">Model CSI-M Persistence #Param. (M) GFLOPS --0.2613</cell><cell>0.0526</cell><cell>0.0969</cell><cell>0.1278</cell><cell>Metrics 0.2155</cell><cell>0.4705</cell><cell>0.6047</cell><cell>11.5338</cell></row><row><cell>UNet [41]</cell><cell>16.6</cell><cell>33</cell><cell>0.3593</cell><cell>0.0577</cell><cell>0.1580</cell><cell>0.2157</cell><cell>0.3274</cell><cell>0.6531</cell><cell>0.7441</cell><cell>4.1119</cell></row><row><cell>ConvLSTM [34]</cell><cell>14.0</cell><cell>527</cell><cell>0.4185</cell><cell>0.1288</cell><cell>0.2482</cell><cell>0.2928</cell><cell>0.4052</cell><cell>0.6793</cell><cell>0.7569</cell><cell>3.7532</cell></row><row><cell>PredRNN [43]</cell><cell>46.6</cell><cell>328</cell><cell>0.4080</cell><cell>0.1312</cell><cell>0.2324</cell><cell>0.2767</cell><cell>0.3858</cell><cell>0.6713</cell><cell>0.7507</cell><cell>3.9014</cell></row><row><cell>PhyDNet [13]</cell><cell>13.7</cell><cell>701</cell><cell>0.3940</cell><cell>0.1288</cell><cell>0.2309</cell><cell>0.2708</cell><cell>0.3720</cell><cell>0.6556</cell><cell>0.7059</cell><cell>4.8165</cell></row><row><cell>E3D-LSTM [42]</cell><cell>35.6</cell><cell>523</cell><cell>0.4038</cell><cell>0.1239</cell><cell>0.2270</cell><cell>0.2675</cell><cell>0.3825</cell><cell>0.6645</cell><cell>0.7573</cell><cell>4.1702</cell></row><row><cell>Rainformer [3]</cell><cell>184.0</cell><cell>170</cell><cell>0.3661</cell><cell>0.0831</cell><cell>0.1670</cell><cell>0.2167</cell><cell>0.3438</cell><cell>0.6585</cell><cell>0.7277</cell><cell>4.0272</cell></row><row><cell>Earthformer w/o global</cell><cell>13.1</cell><cell>257</cell><cell>0.4356</cell><cell>0.1572</cell><cell>0.2716</cell><cell>0.3138</cell><cell>0.4214</cell><cell>0.6859</cell><cell>0.7637</cell><cell>3.7002</cell></row><row><cell>Earthformer</cell><cell>15.1</cell><cell>257</cell><cell>0.4419</cell><cell>0.1791</cell><cell>0.2848</cell><cell>0.3232</cell><cell>0.4271</cell><cell>0.6860</cell><cell>0.7513</cell><cell>3.6957</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on ICAR-ENSO. C-Nino3.4-M and C-Nino3.4-WM are the mean and the weighted mean of the correlation skill C Nino3.4 over K = 12 forecasting steps. C-Nino3.4-WM assigns more weights to longer-term prediction scores. MSE is calculated between the spatiotemporal SST anomalies prediction and the corresponding ground-truth.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param. (M) GFLOPS</cell><cell cols="3">Metrics C-Nino3.4-M ? C-Nino3.4-WM ? MSE (10 ?4 ) ?</cell></row><row><cell>Persistence</cell><cell>-</cell><cell>-</cell><cell>0.3221</cell><cell>0.447</cell><cell>4.581</cell></row><row><cell>UNet [41]</cell><cell>12.1</cell><cell>0.4</cell><cell>0.6926</cell><cell>2.102</cell><cell>2.868</cell></row><row><cell>ConvLSTM [34]</cell><cell>14.0</cell><cell>11.1</cell><cell>0.6955</cell><cell>2.107</cell><cell>2.657</cell></row><row><cell>PredRNN [43]</cell><cell>23.8</cell><cell>85.8</cell><cell>0.6492</cell><cell>1.910</cell><cell>3.044</cell></row><row><cell>PhyDNet [13]</cell><cell>3.1</cell><cell>5.7</cell><cell>0.6646</cell><cell>1.965</cell><cell>2.708</cell></row><row><cell>E3D-LSTM [42]</cell><cell>12.9</cell><cell>99.8</cell><cell>0.7040</cell><cell>2.125</cell><cell>3.095</cell></row><row><cell>Rainformer [3]</cell><cell>19.2</cell><cell>1.3</cell><cell>0.7106</cell><cell>2.153</cell><cell>3.043</cell></row><row><cell>Earthformer w/o global</cell><cell>6.6</cell><cell>23.6</cell><cell>0.7239</cell><cell>2.214</cell><cell>2.550</cell></row><row><cell>Earthformer</cell><cell>7.6</cell><cell>23.9</cell><cell>0.7329</cell><cell>2.259</cell><cell>2.546</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>The details of the Earthformer model on SEVIR dataset. Conv3 ? 3 is the 2D convolutional layer with 3 ? 3 kernel. GroupNorm16 is the Group Normalization (GN) layer<ref type="bibr" target="#b44">[45]</ref> with 16 groups. The negative slope in LeakyReLU is 0.1. The FFN consists of two Linear layers separated by a GeLU activation layer</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters of the AdamW optimizer for training Earthformer on MovingMNIST, N -body MNIST, SEVIR and ICAR-ENSO datasets.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row><row><cell>? 1</cell><cell>0.9</cell></row><row><cell>? 2</cell><cell>0.999</cell></row><row><cell>Weight decay</cell><cell>0.00001</cell></row><row><cell>Batch size</cell><cell>64</cell></row><row><cell>Training epochs</cell><cell>100</cell></row><row><cell>Warm up percentage</cell><cell>20%</cell></row><row><cell>Learning rate decay</cell><cell>Cosine</cell></row><row><cell>Early stop</cell><cell>True</cell></row><row><cell>Early stop tolerance</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Ablation results with depth equals to 2. We can see that the findings still hold for shallower models. MAE ? SSIM ? MSE ? MAE ? SSIM ?</figDesc><table><row><cell>Model</cell><cell cols="3">Metrics on MovingMNIST</cell><cell cols="3">Metrics on N -body MNIST</cell></row><row><cell cols="2">MSE ? Axial 50.41</cell><cell>106.9</cell><cell>0.8805</cell><cell>18.98</cell><cell>48.49</cell><cell>0.9391</cell></row><row><cell>+ global</cell><cell>49.74</cell><cell>105.2</cell><cell>0.8838</cell><cell>18.49</cell><cell>47.31</cell><cell>0.9416</cell></row><row><cell>DST</cell><cell>56.43</cell><cell>119.5</cell><cell>0.8620</cell><cell>21.47</cell><cell>54.06</cell><cell>0.9290</cell></row><row><cell>+ global</cell><cell>55.87</cell><cell>118.2</cell><cell>0.8635</cell><cell>21.16</cell><cell>53.03</cell><cell>0.9303</cell></row><row><cell>Video-Swin 2 ? 8</cell><cell>59.31</cell><cell>124.4</cell><cell>0.8522</cell><cell>23.04</cell><cell>56.66</cell><cell>0.9226</cell></row><row><cell>+ global</cell><cell>60.05</cell><cell>125.9</cell><cell>0.8522</cell><cell>22.97</cell><cell>55.90</cell><cell>0.9241</cell></row><row><cell>Video-Swin 10 ? 8</cell><cell>66.68</cell><cell>133.2</cell><cell>0.8387</cell><cell>25.91</cell><cell>62.49</cell><cell>0.9100</cell></row><row><cell>+ global</cell><cell>65.58</cell><cell>132.8</cell><cell>0.8383</cell><cell>25.56</cell><cell>61.52</cell><cell>0.9118</cell></row><row><cell cols="2">Spatial Local-Dilated 2 62.85</cell><cell>131.3</cell><cell>0.8441</cell><cell>25.98</cell><cell>64.24</cell><cell>0.9075</cell></row><row><cell>+ global</cell><cell>60.03</cell><cell>127.2</cell><cell>0.8489</cell><cell>24.90</cell><cell>60.13</cell><cell>0.9162</cell></row><row><cell cols="2">Spatial Local-Dilated 4 59.87</cell><cell>125.2</cell><cell>0.8514</cell><cell>23.68</cell><cell>57.11</cell><cell>0.9207</cell></row><row><cell>+ global</cell><cell>58.08</cell><cell>120.4</cell><cell>0.8611</cell><cell>22.93</cell><cell>56.22</cell><cell>0.9236</cell></row><row><cell>Axial Space Dilate 2</cell><cell>53.12</cell><cell>110.4</cell><cell>0.8747</cell><cell>19.52</cell><cell>48.77</cell><cell>0.9378</cell></row><row><cell>+ global</cell><cell>51.03</cell><cell>107.4</cell><cell>0.8782</cell><cell>18.53</cell><cell>47.00</cell><cell>0.9419</cell></row><row><cell>Axial Space Dilate 4</cell><cell>55.85</cell><cell>116.0</cell><cell>0.8645</cell><cell>20.30</cell><cell>50.51</cell><cell>0.9350</cell></row><row><cell>+ global</cell><cell>49.84</cell><cell>104.9</cell><cell>0.8815</cell><cell>19.38</cell><cell>49.32</cell><cell>0.9380</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Performance comparison on SEVIR using metrics in SEVIR Dataset Challenge. The CSI is calculated at precipitation thresholds[219, 181, 160, 133, 74,<ref type="bibr" target="#b15">16]</ref>. Different fromTable 6,CSI-? = 1 T T t CSI-? (t)is the mean of CSI-? (t) over forecasting horizon T . CSI-M3 and CSI-M6 are the average of CSI-? over thresholds[133, 74,<ref type="bibr" target="#b15">16]</ref> and[219, 181, 160, 133, 74,<ref type="bibr" target="#b15">16]</ref>, respectively. CSI-181 ? CSI-160 ? CSI-133 ? CSI-74 ? CSI-16 ? CSI-M3 ? CSI-M6 ? MSE (10 ?3 ) ? MAE (10 ?2 ) ?</figDesc><table><row><cell cols="4">Model CSI-219 ? Persistence #Param. (M) GFLOPS --0.0575</cell><cell>0.1056</cell><cell>0.1374</cell><cell>0.2259</cell><cell>0.4796</cell><cell>Metrics 0.6109</cell><cell>0.4386</cell><cell>0.2695</cell><cell>11.5283</cell><cell>4.4349</cell></row><row><cell>UNet [41]</cell><cell>16.6</cell><cell>33</cell><cell>0.0521</cell><cell>0.1364</cell><cell>0.1887</cell><cell>0.3032</cell><cell>0.6542</cell><cell>0.7444</cell><cell>0.5673</cell><cell>0.3465</cell><cell>4.1119</cell><cell>2.7633</cell></row><row><cell>ConvLSTM [34]</cell><cell>14.0</cell><cell>527</cell><cell>0.1071</cell><cell>0.2275</cell><cell>0.2829</cell><cell>0.4155</cell><cell>0.6873</cell><cell>0.7555</cell><cell>0.6194</cell><cell>0.4126</cell><cell>3.7532</cell><cell>2.5898</cell></row><row><cell>PredRNN [43]</cell><cell>46.6</cell><cell>328</cell><cell>0.1164</cell><cell>0.2246</cell><cell>0.2718</cell><cell>0.3873</cell><cell>0.6744</cell><cell>0.7544</cell><cell>0.6054</cell><cell>0.4048</cell><cell>3.9014</cell><cell>2.6963</cell></row><row><cell>PhyDNet [13]</cell><cell>13.7</cell><cell>701</cell><cell>0.1041</cell><cell>0.2123</cell><cell>0.2583</cell><cell>0.3755</cell><cell>0.6627</cell><cell>0.7176</cell><cell>0.5853</cell><cell>0.3884</cell><cell>4.8165</cell><cell>3.1896</cell></row><row><cell>E3D-LSTM [42]</cell><cell>35.6</cell><cell>523</cell><cell>0.1125</cell><cell>0.2218</cell><cell>0.2637</cell><cell>0.3847</cell><cell>0.6674</cell><cell>0.7591</cell><cell>0.6037</cell><cell>0.4015</cell><cell>4.1702</cell><cell>2.5023</cell></row><row><cell>Rainformer [3]</cell><cell>184.0</cell><cell>170</cell><cell>0.0745</cell><cell>0.1580</cell><cell>0.2087</cell><cell>0.3397</cell><cell>0.6599</cell><cell>0.7308</cell><cell>0.5768</cell><cell>0.3619</cell><cell>4.0272</cell><cell>3.0711</cell></row><row><cell>Earthformer w/o global</cell><cell>13.1</cell><cell>257</cell><cell>0.1429</cell><cell>0.2643</cell><cell>0.3086</cell><cell>0.4215</cell><cell>0.6885</cell><cell>0.7671</cell><cell>0.6257</cell><cell>0.4321</cell><cell>3.7006</cell><cell>2.5306</cell></row><row><cell>Earthformer</cell><cell>15.1</cell><cell>257</cell><cell>0.1480</cell><cell>0.2748</cell><cell>0.3126</cell><cell>0.4231</cell><cell>0.6886</cell><cell>0.7682</cell><cell>0.6266</cell><cell>0.4359</cell><cell>3.6702</cell><cell>2.5112</cell></row><row><cell></cell><cell cols="3">#Hits(?, t) =</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Dataset available at https://tianchi.aliyun.com/dataset/dataDetail?dataId=98942</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">MovingMNIST: https://github.com/mansimov/unsupervised-videos</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Except for Rainformer which originally has 212M parameters and thus suffers overfitting severely.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">a k = b k ? ln k, where b k = 1.5, for k ? 4; b k = 2, for 4 &lt; k ? 11; b k = 3, for k &gt; 11.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seasonal Arctic sea ice forecasting with probabilistic deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Tom R Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>P?rez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Luvci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rainformer: Features extraction balanced network for radar-based precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GLiT: Neural architecture search for global and local image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RainBench: towards global precipitation forecasting from satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><forename type="middle">De</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddie</forename><surname>Kalaitzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Chantry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Watson-Parris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bilinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Bromberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cenk</forename><surname>Gazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07470</idno>
		<title level="m">Skillful twelve hour precipitation forecasts using large context neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the coupled model intercomparison project phase 6 (CMIP6) experimental design and organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Eyring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandrine</forename><surname>Bony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Meehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">E</forename><surname>Stouffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geoscientific Model Development</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1937" to="1958" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The GOES-R series: a new generation of geostationary environmental satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Schmit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert J</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redmon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vincent Le Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adaptive fourier neural operators: Efficient token mixers for transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13587</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning for multi-year ENSO forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo-Geun</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Hwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jia</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">573</biblScope>
			<biblScope unit="issue">7775</biblScope>
			<biblScope unit="page" from="568" to="572" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NEXRAD: next generation weather radar (wsr-88d)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sirmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microwave Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Chaos in nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Letellier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>World Scientific</publisher>
			<biblScope unit="volume">94</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are GANs created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The three-body problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Valtonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauri</forename><surname>Valtonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Karttunen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaideep</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11214</idno>
		<title level="m">A global data-driven high-resolution weather model using adaptive fourier neural operators</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10704</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Latent video transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skilful precipitation nowcasting using deep generative models of radar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Willson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kangin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Fitzsimons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Athanassiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheleem</forename><surname>Kashem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Madge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">597</biblScope>
			<biblScope unit="issue">7878</biblScope>
			<biblScope unit="page" from="672" to="677" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with VQ-VAE-2. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Aaron Van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning and process understanding for data-driven earth system science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Reichstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Carvalhais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="issue">7743</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">PixelCNN++: A PixelCNN implementation with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">GRACE measurements of mass variability in the earth system. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Byron D Tapley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bettadpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="503" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SEVIR: A storm event imagery dataset for deep learning applications in radar and satellite meteorology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Veillette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mattioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22009" to="22019" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Eidetic 3D LSTM: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PredRNN: A recurrent neural network for spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">VideoGPT: Video generation using vq-vae and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13577" to="13587" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
