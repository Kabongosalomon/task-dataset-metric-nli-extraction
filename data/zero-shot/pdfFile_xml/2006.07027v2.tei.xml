<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 SEQ2TENS: AN EFFICIENT REPRESENTATION OF SE- QUENCES BY LOW-RANK TENSOR PROJECTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Toth</surname></persName>
							<email>toth@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Bonnier</surname></persName>
							<email>bonnier@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Oberhauser</surname></persName>
							<email>oberhauser@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 SEQ2TENS: AN EFFICIENT REPRESENTATION OF SE- QUENCES BY LOW-RANK TENSOR PROJECTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequential data such as time series, video, or text can be challenging to analyse as the ordered structure gives rise to complex dependencies. At the heart of this is non-commutativity, in the sense that reordering the elements of a sequence can completely change its meaning. We use a classical mathematical object -the free algebra -to capture this non-commutativity. To address the innate computational complexity of this algebra, we use compositions of low-rank tensor projections. This yields modular and scalable building blocks that give state-of-the-art performance on standard benchmarks such as multivariate time series classification, mortality prediction and generative models for video. Code and benchmarks are publically available at https://github.com/tgcsaba/seq2tens.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A central task of learning is to find representations of the underlying data that efficiently and faithfully capture their structure. In the case of sequential data, one data point consists of a sequence of objects. This is a rich and non-homogeneous class of data and includes classical uni-or multi-variate time series (sequences of scalars or vectors), video (sequences of images), and text (sequences of letters). Particular challenges of sequential data are that each sequence entry can itself be a highly structured object and that data sets typically include sequences of different length which makes naive vectorization troublesome.</p><p>Contribution. Our main result is a generic method that takes a static feature map for a class of objects (e.g. a feature map for vectors, images, or letters) as input and turns this into a feature map for sequences of arbitrary length of such objects (e.g. a feature map for time series, video, or text). We call this feature map for sequences Seq2Tens for reasons that will become clear; among its attractive properties are that it (i) provides a structured, parsimonious description of sequences; generalizing classical methods for strings, (ii) comes with theoretical guarantees such as universality, (iii) can be turned into modular and flexible neural network (NN) layers for sequence data. The key ingredient to our approach is to embed the feature space of the static feature map into a larger linear space that forms an algebra (a vector space equipped with a multiplication). The product in this algebra is then used to "stitch together" the static features of the individual sequence entries in a structured way. The construction that allows to do all this is classical in mathematics, and known as the free algebra (over the static feature space).</p><p>Outline. Section 2 formalizes the main ideas of Seq2Tens and introduces the free algebra T(V ) over a space V as well as the associated product, the so-called convolution tensor product. Section 3 shows how low rank (LR) constructions combined with sequence-to-sequence transforms allows one to efficiently use this rich algebraic structure. Section 4 applies the results of Sections 2 and 3 to build modular and scalable NN layers for sequential data. Section 5 demonstrates the flexibility and modularity of this approach on both discriminative and generative benchmarks. Section 6 makes connections with previous work and summarizes this article. In the appendices we provide mathematical background, extensions, and detailed proofs for our theoretical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CAPTURING ORDER BY NON-COMMUTATIVE MULTIPLICATION</head><p>We denote the set of sequences of elements in a set X by Seq(X ) = {x = (x i ) i=1,...,L : x i ? X , L ? 1}</p><p>(1)</p><p>where L ? 1 is some arbitrary length. Even if X itself is a linear space, e.g. X = R, Seq(X ) is never a linear space since there is no natural addition of two sequences of different length.</p><p>Seq2Tens in a nutshell. Given any vector space V we may construct the so-called free algebra T(V ) over V . We describe the space T(V ) in detail below, but as for now the only thing that is important is that T(V ) is also a vector space that includes V , and that it carries a non-commutative product, which is, in a precise sense, "the most general product" on V .</p><p>The main idea of Seq2Tens is that any "static feature map" for elements in X ? : X ? V can be used to construct a new feature map ? : Seq(X ) ? T(V ) for sequences in X by using the algebraic structure of T(V ): the non-commutative product on T(V ) makes it possible to "stitch together" the individual features ?(x 1 ), . . . , ?(x L ) ? V ? T(V ) of the sequence x in the larger space T(V ) by multiplication in T(V ). With this we may define the feature map ?(x) for a sequences x = (x 1 , . . . , x L ) ? Seq(X ) as follows (i) lift the map ? : X ? V to a map ? : X ? T(V ), (ii) map Seq(X ) ? Seq(T(V )) by (x 1 , . . . , x L ) ? (?(x 1 ), . . . , ?(x L )), (iii) map Seq(T(V )) ? T(V ) by multiplication (?(x 1 ), . . . , ?(x L )) ? ?(x 1 ) ? ? ? ?(x L ).</p><p>In a more concise form, we define ? as</p><formula xml:id="formula_0">? : Seq(X ) ? T(V ), ?(x) = L i=1 ?(x i )<label>(2)</label></formula><p>where denotes multiplication in T(V ). We refer to the resulting map ? as the Seq2Tens map, which stands short for Sequences-2-Tensors. Why is this construction a good idea? First note, that step (i) is always possible since V ? T(V ) and we discuss the simplest such lift before Theorem 2.1 as well as other choices in Appendix B. Further, if ?, respectively ?, provides a faithful representation of objects in X , then there is no loss of information in step (ii). Finally, since step (iii) uses "the most general product" to multiply ?(x 1 ) ? ? ? ?(x L ) one expects that ?(x) ? T(V ) faithfully represents the sequence x as an element of T(V ).</p><p>Indeed in Theorem 2.1 below we show an even stronger statement, namely that if the static feature map ? : X ? V contains enough non-linearities so that non-linear functions from X to R can be approximated as linear functions of the static feature map ?, then the above construction extends this property to functions of sequences. Put differently, if ? is a universal feature map for X , then ? is a universal feature map for Seq(X ); that is, any non-linear function f (x) of a sequence x can be approximated as a linear functional of ?(x), f (x) ? , ?(x) . We also emphasize that the domain of ? is the space Seq(X ) of sequences of arbitrary (finite) length. The remainder of this Section gives more details about steps (i),(ii),(iii) for the construction of ?.</p><p>The free algebra T(V ) over a vector space V . Let V be a vector space. We denote by T(V ) the set of sequences of tensors indexed by their degree m,</p><formula xml:id="formula_1">T(V ) := {t = (t m ) m?0 | t m ? V ?m }<label>(3)</label></formula><p>where by convention V ?0 = R. For example, if V = R d and t = (t m ) m?0 is some element of T(R d ), then its degree m = 1 component is a d-dimensional vector t 1 , its degree m = 2 component is a d ? d matrix t 2 , and its degree m = 3 component is a degree 3 tensor t 3 . By defining addition and scalar multiplication as</p><formula xml:id="formula_2">s + t := (s m + t m ) m?0 , c ? t = (ct m ) m?0<label>(4)</label></formula><p>the set T(V ) becomes a linear space. By identifying v ? V as the element (0, v, 0, . . . , 0) ? T(V ) we see that V is a linear subspace of T(V ). Moreover, while V is only a linear space, T(V ) carries a product that turns T(V ) into an algebra. This product is the so-called tensor convolution product, and is defined for s, t ? T(V ) as</p><formula xml:id="formula_3">s ? t := m i=0 s i ? t m?i m?0 = 1, s 1 + t 1 , s 2 + s 1 ? t 1 + t 2 , . . . ? T(V )<label>(5)</label></formula><p>where ? denotes the usual outer tensor product; e.g. for vectors u = (u i ), v = (v i ) ? R d the outer tensor product u ? v is the d ? d matrix (u i v j ) i,j=1,...,d . We emphasize that like the outer tensor product ?, the tensor convolution product ? is non-commutative, i.e. s ? t = t ? s. In a mathematically precise sense, T(V ) is the most general algebra that contains V ; it is a "free construction". Since T(V ) is realized as series of tensors of increasing degree, the free algebra T(V ) is also known as the tensor algebra in the literature. Appendix A contains background on tensors and further examples.</p><p>Lifting static feature maps.</p><p>Step (i) in the construction of ? requires turning a given feature map ? : X ? V into a map ? : X ? T(V ). Throughout the rest of this article we use the lift</p><formula xml:id="formula_4">?(x) = (1, ?(x), 0, 0 . . .) ? T(V ).<label>(6)</label></formula><p>We discuss other choices in Appendix B, but attractive properties of the lift 6 are that (a) the evaluation of ? against low rank tensors becomes a simple recursive formula (Proposition 3.3, (b) it is a generalization of sequence sub-pattern matching as used in string kernels (Appendix B.3, (c) despite its simplicity it performs exceedingly well in practice (Section 4).</p><p>Extending to sequences of arbitrary length. Steps (i) and (ii) in the construction specify how the map ? : X ? T(V ) behaves on sequences of length-1, that is, single observations. Step (iii) amounts to the requirement that for any two sequences x = (x 1 , . . . , x K ), y = (y 1 , . . . , y L ) ? Seq(V ), their concatenation defined as z = (x 1 , . . . , x K , y 1 , . . . , y L ) ? Seq(V ) can be understood in the feature space as (non-commutative) multiplication of their corresponding features ?(z) = ?(x) ? ?(y).</p><p>In other words, we inductively extend the lift ? to sequences of arbitrary length by starting from sequences consisting of a single observation, which is given in equation 2. Repeatedly applying the definition of the tensor convolution product in equation 5 leads to the following explicit formula</p><formula xml:id="formula_6">? m (x) = 1?i1&lt;???&lt;im?L x i1 ? ? ? ? ? x im ? V ?m , ?(x) = (? m (x)) m?0 ,<label>(8)</label></formula><p>where x = (x 1 , . . . , x L ) ? Seq(V ) and the summation is over non-contiguous subsequences of x.</p><p>Some intuition: generalized pattern matching. Our derivation of the feature map ?(x) = (1, ? 1 (x), ? 2 (x), . . .) ? T(V ) was guided by general algebraic principles, but equation 8 provides an intuitive interpretation. It shows that for each m ? 1, the entry ? m (x) ? V ?m constructs a summary of a long sequence x = (x 1 , . . . , x L ) ? Seq(V ) based on subsequences (x i1 , . . . , x im ) of x of length-m. It does this by taking the usual outer tensor product x i1 ? ? ? ? ? x im ? V ?m and summing over all possible subsequences. This is completely analogous to how string kernels provide a structured description of text by looking at non-contiguous substrings of length-m (indeed, Appendix B.3 makes this rigorous). However, the main difference is that the above construction works for arbitrary sequences and not just sequences of discrete letters. Readers with less mathematical background might simply take this as motivation and regard equation 8 as definition. However, the algebraic background allows to prove that ? is universal, see Theorem 2.1 below.</p><p>Universality. A function ? : X ? V is said to be universal for X if all continuous functions on X can be approximated as linear functions on the image of ?. One of the most powerful features of neural nets is their universality <ref type="bibr">(Hornik, 1991)</ref>. A very attractive property of ? is that it preserves universality: if ? : X ? V is universal for X , then ? : Seq(X) ? T(V ) is universal for Seq(X ). To make this precise, note that V ?m is a linear space and therefore any = ( 0 , 1 , . . . , M , 0, 0, . . .) ? </p><p>Thus linear functionals of the feature map ?, are real-valued functions of sequences. Theorem 2.1 below shows that any continuous function f : Seq(X ) ? R can by arbitrary well approximated by a ? T(V ), f (x) ? , ?(x) . Theorem 2.1. Let ? : X ? V be a universal map with a lift that satisfies some mild constraints, then the following map is universal:</p><formula xml:id="formula_8">? : Seq(X ) ? T(V ), x ? ?(x).<label>(10)</label></formula><p>A detailed proof and the precise statement of Theorem 2.1 is given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROXIMATION BY LOW-RANK LINEAR FUNCTIONALS</head><p>The combinatorial explosion of tensor coordinates and what to do about it. The universality of ? suggests the following approach to represent a function f : Seq(X ) ? R of sequences: First compute ?(x) and then optimize over (and possibly also the hyperparameters of ?) such that</p><formula xml:id="formula_9">f (x) ? , ?(x) = M m=0 m , ? m (x)</formula><p>. Unfortunately, tensors suffer from a combinatorial explosion in complexity in the sense that even just storing</p><formula xml:id="formula_10">? m (x) ? V ?m ? T(V ) requires O(dim(V ) m )</formula><p>real numbers. Below we resolve this computational bottleneck as follows: in Proposition 3.3 we show that for a special class of low-rank elements ? T(V ), the functional x ? , ?(x) can be efficiently computed in both time and memory. This is somewhat analogous to a kernel trick since it shows that , ?(x) can be cheaply computed without explicitly computing the feature map ?(x). However, Theorem 2.1 guarantees universality under no restriction on , thus restriction to rank-1 functionals limits the class of functions f (x) that can be approximated. Nevertheless, by iterating these "low-rank functional" constructions in the form of sequence-to-sequence transformations this can be ameliorated. We give the details below but to gain intuition, we invite the reader to think of this iteration analogous to stacking layers in a neural network: each layer is a relatively simple non-linearity (e.g. a sigmoid composed with an affine function) but by composing such layers, complicated functions can be efficiently approximated.</p><p>Rank-1 functionals are computationally cheap. Degree m = 2 tensors are matrices and lowrank (LR) approximations of matrices are widely used in practice <ref type="bibr" target="#b41">(Udell &amp; Townsend, 2019)</ref> to address the quadratic complexity. The definition below generalizes the rank of matrices (tensors of degree m = 2) to tensors of any degree m. Definition 3.1. The rank (also called CP rank <ref type="bibr" target="#b13">(Carroll &amp; Chang, 1970)</ref>) of a degree-m tensor t m ? V ?m is the smallest number r ? 0 such that one may write</p><formula xml:id="formula_11">t m = r i=0 v 1 i ? ? ? ? ? v m i , v 1 i , . . . , v m i ? V.<label>(11)</label></formula><p>We say that t = (t m ) m?0 ? T(V ) has rank-1 (and degree-M ) if each t m ? V ?m is a rank-1 tensor and t i = 0 for i &gt; M . Remark 3.2. For x = (x 1 , . . . , x L ) ? Seq(V ), the rank r m ? N of ? m (x) satisfies r m ? L m , while the rank and degree r, d ? N of ?(x) satisfy r ? L K for K = L 2 and d ? L.</p><p>A direct calculation shows that if is of rank-1, then , ?(x) can be computed very efficiently by inner product evaluations in V . Proposition 3.3. Let = ( m ) m?0 ? T(V ) be of rank-1 and degree-M . If ? is lifted to ? as in equation 6, then</p><formula xml:id="formula_12">, ?(x) = M m=0 1?i1&lt;???&lt;im?L m k=1 v m k , ?(x i k )<label>(12)</label></formula><formula xml:id="formula_13">where m = v m 1 ? ? ? ? ? v m m ? V ?m , v m i ? V and m = 0, . . . , M .</formula><p>Note that the inner sum is taken over all non-contiguous subsequences of x of length-m, analogously to m-mers of strings and we make this connection precise in Appendix B.3; the proof of Proposition 3.3 is given in Appendix B.1.1. While equation 12 looks expensive, by casting it into a recursive formulation over time, it can be computed in O(M 2 ?L?d) time and O(M 2 ?(L+c)) memory, where d is the inner product evaluation time on V , while c is the memory footprint of a v ? V . This can further be reduced to O(M ? L ? d) time and O(M ? (L + c)) memory by an efficient parametrization of the rank-1 element ? T(V ). We give further details in Appendices D.2, D.3, D.4.</p><p>Low-rank Seq2Tens maps. The composition of a linear map L : T(V ) ? R N with ? can be computed cheaply in parallel using equation 12 when L is specified through a collection of N ? N rank-1 elements 1 , . . . , N ? T(V ) such that</p><formula xml:id="formula_14">??(x 1 , . . . , x L ) := L ? ?(x 1 , . . . , x L ) = ( j , ?(x 1 , . . . , x L )) N j=1 ? R N .<label>(13)</label></formula><p>We call the resulting map?? : Seq(X ) ? R N a Low-rank Seq2Tens map of width-N and order-M , where M ? N is the maximal degree of 1 , . . . , N such that j i = 0 for i &gt; M . The LS2T map is parametrized by (1) the component vectors v k j,m ? V of the rank-1 elements j m = v 1 j,m ?? ? ??v m j,m , (2) by any parameters ? that the static feature map ? ? : X ? V may depend on. We jointly denote these parameters by? = (?, 1 , . . . , N ). In addition, by the subsequent composition of?? with a linear functional R N ? R, we get the following function subspace as hypothesis class for the LS2T</p><formula xml:id="formula_15">H = N j=1 ? j j , ?(x 1 , . . . , x L ) | ? j ? R H = , ?(x 1 , . . . , x L ) | ? T(V )<label>(14)</label></formula><p>Hence, we acquire an intuitive explanation of the (hyper)parameters: the width of the LS2T, N ? N specifies the maximal rank of the low-rank linear functionals of ? that the LS2T can represent, while the span of the rank-1 elements, span( 1 , . . . , N ) determine an N -dimensional subspace of the dual space of T(V ) consisting of at most rank-N functionals.</p><p>Recall now that without rank restrictions on the linear functionals of Seq2Tens features, Theorem 2.1 would guarantee that any real-valued function f : Seq(X ) ? R could be approximated by f (x) ? , ?(x 1 , . . . , x L ) . As pointed out before, the restriction of the hypothesis class to lowrank linear functionals of ?(x 1 , . . . , x L ) would limit the class of functions of sequences that can be approximated. To ameliorate this, we use LS2T transforms in a sequence-to-sequence fashion that allows us to stack such low-rank functionals, significantly recovering expressiveness.</p><p>Sequence-to-sequence transforms. We can use LS2T to build sequence-to-sequence transformations in the following way: fix the static map ? ? : X ? V parametrized by ? and rank-1 elements such that? = (?, 1 , . . . , N ) and apply the resulting LS2T map?? over expanding windows of x:</p><formula xml:id="formula_16">Seq(X ) ? Seq(R N ), x ? ?? (x 1 ),??(x 1 , x 2 ), . . . ,??(x 1 , . . . , x L ) .<label>(15)</label></formula><p>Note that the cost of computing the expanding window sequence-to-sequence transform in equation 15 is no more expensive than computing??(x 1 , . . . , x L ) itself due to the recursive nature of our algorithms, for further details see Appendices D.2, D.3, D.4.</p><p>Deep sequence-to-sequence transforms. Inspired by the empirical successes of deep RNNs <ref type="bibr">(Graves et al., 2013b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b35">Sutskever et al., 2014)</ref>, we iterate the transformation 15 D-times:</p><formula xml:id="formula_17">Seq(X ) ? Seq(R N1 ) ? Seq(R N2 ) ? ? ? ? ? Seq(R N D ).<label>(16)</label></formula><p>Each of these mappings Seq(R Ni ) ? Seq(R Ni+1 ) is parametrized by the parameters? i of a static feature map ? ?i and a linear map L i specified by N i rank-1 elements of T(V ); these parameters are collectively denoted by? i = (? i , 1 i , . . . , Ni i ). Evaluating the final sequence in Seq(R N D ) at the last observation-time t = L, we get the deep LS2T map with depth-D ?? 1,...,?D : Seq(X ) ? R n D .</p><p>Making precise how the stacking of such low-rank sequence-to-sequence transformations approximates general functions requires more tools from algebra, and we provide a rigorous quantitative statement in Appendix C. Here, we just appeal to the analogy made with adding depth in neural networks mentioned earlier and empirically validate this in our experiments in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BUILDING NEURAL NETWORKS WITH LS2T LAYERS</head><p>The Seq2Tens map ? built from a static feature map ? is universal if ? is universal, Theorem 2.1. NNs form a flexible class of universal feature maps with strong empirical success for data in X = R d , and thus make a natural choice for ?. Combined with standard deep learning constructions, the framework of Sections 2 and 3 can build modular and expressive layers for sequence learning.</p><p>Neural LS2T layers. The simplest choice among many is to use as static feature map ? :</p><formula xml:id="formula_19">X = R d ? R h a feedforward network with depth-P , ? = ? P ? ? ? ? ? ? 1 where ? j (x) = ?(W j x + b j ) for W j ? R h?d , b j ? R h .</formula><p>We can then lift this to a map ? : R d ? T(R h ) as prescribed in equation 6. Hence, the resulting LS2T layer x ? (??(x 1 , . . . ,</p><formula xml:id="formula_20">x i )) i=1,...,L is a sequence-to-sequence transform Seq(R d ) ? Seq(R h ) that is parametrized by? = (W 1 , b 1 , . . . , W P , b P , 1 1 , . . . , N1 1 ).</formula><p>Bidirectional LS2T layers. The transformation in equation 15 is completely causal in the sense that each step of the output sequence depends only on past information. For generative models, it can behove us to make the output depend on both past and future information, see <ref type="bibr">Graves et al. (2013a)</ref>; <ref type="bibr" target="#b0">Baldi et al. (1999);</ref><ref type="bibr">Li &amp; Mandt (2018)</ref>. Similarly to bidirectional RNNs and LSTMs <ref type="bibr" target="#b32">(Schuster &amp; Paliwal, 1997;</ref><ref type="bibr">Graves &amp; Schmidhuber, 2005)</ref>, we may achieve this by defining a bidirectional layer,</p><formula xml:id="formula_21">? b (?1,?2) (x) : Seq(R d ) ? Seq(R N +N ), x ? (?? 1 (x 1 , . . . , x i ),?? 2 (x i , . . . , x L )) L i=1 . (18)</formula><p>The sequential nature is kept intact by making the distinction between what classifies as past (the first N coordinates) and future (the last N coordinates) information. This amounts to having a form of precognition in the model, and has been applied in e.g. dynamics generation <ref type="bibr">(Li &amp; Mandt, 2018)</ref>, machine translation <ref type="bibr" target="#b34">(Sundermeyer et al., 2014)</ref>, and speech processing <ref type="bibr">(Graves et al., 2013a)</ref>.</p><p>Convolutions and LS2T. We motivate to replace the time-distributed feedforward layers proposed in the paragraph above by temporal convolutions (CNN) instead. Although theory only requires the preprocessing layer of the LS2T to be a static feature map, we find that it is beneficial to capture some of the sequential information in the preprocessing layer as well, e.g. using CNNs or RNNs. From a mathematical point of view, CNNs are a straightforward extension since they can be interpreted as time-distributed feedforward layers applied to the input sequence augmented with a p ? N number of its lags for CNN kernel size p (see Appendix D.1 for further discussion).</p><p>In the following, we precede our deep LS2T blocks by one or more CNN layers. Intuitively, CNNs and LS2Ts are similar in that both transformations operate on subsequences of their input sequence. The main difference between the two lies in that CNNs operate on contiguous subsequences, and therefore, capture local, short-range nonlinear interactions between timesteps; while LS2Ts (equation 12) use all non-contiguous subsequences, and hence, learn global, long-range interactions in time. This observation motivates that the inductive biases of the two types of layers (local/global time-interactions) are highly complementary in nature, and we suggest that the improvement in the experiments on the models containing vanilla CNN blocks are due to this complementarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We demonstrate the modularity and flexibility of the above LS2T and its variants by applying it to (i) multivariate time series classification, (ii) mortality prediction in healthcare, (iii) generative modelling of sequential data. In all cases, we take a strong baseline model (FCN and GP-VAE, as detailed below) and upgrade it with LS2T layers. As Thm. 2.1 requires the Seq2Tens layers to be preceded by at least a static feature map, we expect these layers to perform best as an add-on on top of other models, which however can be quite simple, such as a CNN. The additional computation time is negligible (in fact, for FCN it allows to reduce the number of parameters significantly, while retaining performance), but it can yield substantial improvements. This is remarkable, since the original models are already state-of-the-art on well-established (frequentist and Bayesian) benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MULTIVARIATE TIME SERIES CLASSIFICATION</head><p>As the first task, we consider multivariate time series classification (TSC) on an archive of benchmark datasets collected by <ref type="bibr" target="#b2">Baydogan (2015)</ref>. Numerous previous publications report results on this The FCN is an interesting model to upgrade with LS2T layers, since the LS2T also employs parameter sharing across the sequence length, and as noted previously, convolutions are only able to learn local interactions in time, that in particular makes them ill-suited to picking up on longrange autocorrelations, which is exactly where the LS2T can provide improvements. As our models, we consider three simple architectures: (i) LS2T 3 64 stacks 3 LS2T layers of order-2 and width-64; (ii) FCN 64 -LS2T 3 64 precedes the LS2T 3 64 block by an FCN 64 block; a downsized version of FCN 128 ; (iii) FCN 128 -LS2T 3 64 uses the full FCN 128 and follows it by a LS2T 3 64 block as before. Also, both FCN-LS2T models employ skip-connections from the input to the LS2T block and from the FCN to the classification layer, allowing for the LS2T to directly see the input, and for the FCN to directly affect the final prediction. These hyperparameters were only subject to hand-tuning on a subset of the datasets, and the values we considered were H, N ? {32, 64, 128}, M ? {2, 3, 4} and D ? {1, 2, 3}, where H, N ? N is the FCN and LS2T width, resp., while M ? N is the LS2T order and D ? N is the LS2T depth. We also employ techniques such as time-embeddings <ref type="bibr">(Liu et al., 2018a)</ref>, sequence differencing and batch normalization, see Appendix D.1; Appendix E.1 for further details on the experiment and <ref type="figure" target="#fig_7">Figure 2</ref> in thereof for a visualization of the architectures.</p><p>Results. We trained the models, FCN 128 , ResNet, LS2T 3 64 , FCN 64 -LS2T 3 64 , FCN 128 -LS2T 3 64 on each of the 16 datasets 5 times while results for other methods were borrowed from the cited publications. In Appendix E.1, <ref type="figure" target="#fig_1">Figure 3</ref> depicts the box-plot of distributions of accuracies and a CD diagram using the Nemenyi test <ref type="bibr" target="#b24">(Nemenyi, 1963)</ref>, while <ref type="table" target="#tab_8">Table 7</ref> shows the full list of results. Since mean-ranks based tests raise some paradoxical issues <ref type="bibr" target="#b6">(Benavoli et al., 2016)</ref>, it is customary to conduct pairwise comparisons using frequentist <ref type="bibr">(Dem?ar, 2006)</ref> or Bayesian <ref type="bibr" target="#b7">(Benavoli et al., 2017)</ref> hypothesis tests. We adopted the Bayesian signed-rank test from <ref type="bibr" target="#b5">Benavoli et al. (2014)</ref>, the posterior probabilities of which are displayed in <ref type="table" target="#tab_1">Table 1</ref>, while the Bayesian posteriors are visualized on <ref type="figure" target="#fig_8">Figure 4</ref> in App. E.1. The results of the signed-rank test can be summarized as follows: (1) LS2T 3 64 already outperforms some classic TS classifiers with high probability (p ? 0.8), but it is not competitive with other DL classifiers. This observation is not surprising since even theory requires at least a static feature map to precede the LS2T. (2) FCN 64 -LS2T 3 64 outperforms almost all models with high probability (p ? 0.8), except for ResNet (which is stil outperformed by p ? 0.7), FCN 128 and FCN 128 -LS2T 3 64 . When compared with FCN 128 , the test is unable to decide between the two, which upon inspection of the individual results in <ref type="table" target="#tab_8">Table 7</ref> can be explained by that on some datasets the benefit of the added LS2T block is high enough that it outweighs the loss of flexibility incurred by reducing the width of the FCN -arguably these are the datasets where long-range autocorrelations are present in the input time series, and picking up on these improve the performance -however, on a few datasets the contrary is true. (3) Lastly, FCN 128 -LS2T 3 64 , outperforms all baseline methods with high probability (p ? 0.8), and hence successfully improves on the FCN 128 via its added ability to learn long-range time-interactions. We remark that FCN 64 -LS2T 3 64 has fewer parameters than FCN 128 by more than 50%, hence we managed to compress the FCN to a fraction of its original size, while on average still slightly improving its performance, a nontrivial feat by its own accord.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MORTALITY PREDICTION</head><p>We consider the PHYSIONET2012 challenge dataset <ref type="bibr">(Goldberger et al., 2000)</ref> for mortality prediction, which is a case of medical TSC as the task is to predict in-hospital mortality of patients after their admission to the ICU. This is a difficult ML task due to missingness in the data, low signalto-noise ratio (SNR), and imbalanced class distributions with a prevalence ratio of around 14%. We extend the experiments conducted in Horn et al. <ref type="bibr">(2020)</ref>, which we also use as very strong baselines. Under the same experimental setting, we train two models: FCN-LS2T as ours and the FCN as another baseline. For both models, we conduct a random search for all hyperparameters with 20 samples from a pre-specified search space, and the setting with best validation performance is used for model evaluation on the test set over 5 independent model trains, exactly the same way as it was done in <ref type="bibr">Horn et al. (2020)</ref>. We preprocess the data using the same method as in <ref type="bibr">Che et al. (2018, eq. (9)</ref>) and additionally handle static features by tiling them along the time axis and adding them as extra coordinates. We additionally introduce in both models a SpatialDropout1D layer after all CNN and LS2T layers with the same tunable dropout rate to mitigate the low SNR of the dataset. Results. <ref type="table" target="#tab_2">Table 2</ref> compares the performance of FCN-LS2T with that of FCN and the results from Horn et al. (2020) on 3 metrics: (1) ACCURACY, (2) area under the precision-recall curve (AUPRC), (3) area under the ROC curve (AUROC). We can observe that FCN-LS2T takes on average first place according to both ACCURACY and AUPRC, outperforming FCN and all SOTA methods, e.g. TRANS-FORMER <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>, GRU-D Che et al. <ref type="formula" target="#formula_0">(2018)</ref>, <ref type="bibr">SEFT (Horn et al., 2020)</ref>, and also being competitive in terms of AUROC. This is very promising, and it suggests that LS2T layers might be particularly well-suited to complex and heterogenous datasets, such as medical time series, since the FCN-LS2T models significantly improved accuracy on ECG as well, another medical dataset in the previous experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GENERATING SEQUENTIAL DATA</head><p>Finally, we demonstrate on sequential data imputation for time series and video that LS2Ts do not only provide good representations of sequences in discriminative, but also generative models.</p><p>The GP-VAE model. In this experiment, we take as base model the recent GP-VAE <ref type="bibr">(Fortuin et al., 2020)</ref>, that provides state-of-the-art results for probabilistic sequential data imputation. The GP-VAE is essentially based on the HI-VAE <ref type="bibr" target="#b22">(Nazabal et al., 2018)</ref> for handling missing data in variational autoencoders (VAEs) <ref type="bibr">(Kingma &amp; Welling, 2013)</ref> adapted to the handling of time series data by the use of a Gaussian process (GP) prior <ref type="bibr" target="#b44">(Williams &amp; Rasmussen, 2006)</ref> across time in the latent sequence space to capture temporal dynamics. Since the GP-VAE is a highly advanced model, its in-depth description is deferred to Appendix E.3.We extend the experiments conducted in Fortuin et al. <ref type="formula" target="#formula_0">(2020)</ref>, and we make one simple change to the GP-VAE architecture without changing any other hyperparameters or aspects: we introduce a single bidirectional LS2T layer (B-LS2T) into the encoder network that is used in the amortized representation of the means and covariances of the variational posterior. The B-LS2T layer is preceded by a time-embedding and differencing block, and succeeded by channel flattening and layer normalization as depicted in <ref type="figure" target="#fig_10">Figure 5</ref>. The idea behind this experiment is to see if we can improve the performance of a highly complicated model that is composed of many interacting submodels, by the naive introduction of LS2T layers. </p><formula xml:id="formula_22">FORWARD IMPUTATION - 0.177 ? 0.000 0.935 ? 0.000 0.028 ? 0.000 0.710 ? 0.000 VAE 0.599 ? 0.002 0.232 ? 0.000 0.922 ? 0.000 0.028 ? 0.000 0.677 ? 0.002 HI-VAE 0.372 ? 0.008 0.134 ? 0.003 0.962 ? 0.001 0.007 ? 0.000 0.686 ? 0.010 GP-VAE 0.350 ? 0.007 0.114 ? 0.002 0.960 ? 0.002 0.002 ? 0.000 0.730 ? 0.006 GP-VAE (B-LS2T) 0.251 ? 0.008 0.092 ? 0.003 0.962 ? 0.001 0.002 ? 0.000 0.743 ? 0.007 BRITS - - - - 0.742 ? 0.008</formula><p>Results. To make the comparison, we ceteris paribus re-ran all experiments the authors originally included in their paper <ref type="bibr">(Fortuin et al., 2020)</ref>, which are imputation of Healing MNIST, Sprites, and Physionet 2012. The results are in <ref type="table" target="#tab_3">Table 3</ref>, which report the same metrics as used in Fortuin et al. <ref type="formula" target="#formula_0">(2020)</ref>, i.e. negative log-likelihood (NLL, lower is better), mean squared error (MSE, lower is better) on test sets, and downstream classification performance of a linear classifier (AUROC, higher is better). For all other models beside our GP-VAE (B-LS2T), the results were borrowed from Fortuin et al. <ref type="bibr">(2020)</ref>. We observe that simply adding the B-LS2T layer improved the result in almost all cases, except for Sprites, where the GP-VAE already achieved a very low MSE score. Additionally, when comparing GP-VAE to BRITS on Physionet, the authors argue that although the BRITS achieves a higher AUROC score, the GP-VAE should not be disregarded as it fits a generative model to the data that enjoys the usual Bayesian benefits of predicting distributions instead of point predictions. The results display that by simply adding our layer into the architecture, we managed to elevate the performance of GP-VAE to the same level while retaining these same benefits. We believe the reason for the improvement is a tighter amortization gap in the variational approximation <ref type="bibr">(Cremer et al., 2018)</ref> achieved by increasing the expressiveness of the encoder by the LS2T allowing it to pick up on long-range interactions in time. We provide further discussion in Appendix E.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK AND SUMMARY</head><p>Related Work. The literature on tensor models in ML is vast. Related to our approach we mention pars-pro-toto Tensor Networks <ref type="bibr">(Cichocki et al., 2016)</ref>, that use classical LR decompositions, such as CP <ref type="bibr" target="#b13">(Carroll &amp; Chang, 1970)</ref>, Tucker <ref type="bibr" target="#b39">(Tucker, 1966)</ref>, tensor trains (Oseledets, 2011) and tensor rings <ref type="bibr" target="#b47">(Zhao et al., 2019)</ref>; further, CNNs have been combined with LR tensor techniques <ref type="bibr">(Cohen et al., 2016;</ref><ref type="bibr">Kossaifi et al., 2017)</ref> and extended to <ref type="bibr">RNNs (Khrulkov et al., 2019)</ref>; Tensor Fusion Networks <ref type="bibr" target="#b45">(Zadeh et al., 2017)</ref> and its LR variants <ref type="bibr">(Liu et al., 2018b;</ref><ref type="bibr">Liang et al., 2019;</ref><ref type="bibr">Hou et al., 2019)</ref>; tensor-based gait recognition <ref type="bibr" target="#b37">(Tao et al., 2007)</ref>. Our main contribution to this literature is the use of the free algebra T(V ) with its convolution product ?, instead of V ?m with the outer product ? that is used in the above papers. While counter-intuitive to work in a larger space T(V ), the additional algebra structure of (T(V ), ?) is the main reason for the nice properties of ? (universality, making sequences of arbitrary length comparable, convergence in the continuous time limit; see Appendix B) which we believe are in turn the main reason for the strong benchmark performance.</p><p>Stacked LR sequence transforms allow to exploit this rich algebraic structure with little computational overhead. Another related literature are path signatures in ML <ref type="bibr" target="#b18">(Lyons, 2014;</ref><ref type="bibr">Chevyrev &amp; Kormilitzin, 2016;</ref><ref type="bibr">Graham, 2013;</ref><ref type="bibr" target="#b11">Bonnier et al., 2019;</ref><ref type="bibr">Toth &amp; Oberhauser, 2020)</ref>. These arise as special case of Seq2Tens (Appendix B) and our main contribution to this literature is that Seq2Tens resolves a well-known computational bottleneck in this literature since it never needs to compute and store a signature, instead it directly and efficiently learns the functional of the signature.</p><p>Summary. We used a classical non-commutative structure to construct a feature map for sequences of arbitrary length. By stacking sequence transforms we turned this into scalable and modular NN layers for sequence data. The main novelty is the use of the free algebra T(V ) constructed from the static feature space V . While free algebras are classical in mathematics, their use in ML seems novel and underexplored. We would like to re-emphasize that (T(V ), ?) is not a mysterious abstract space: if you know the outer tensor product ? then you can easily switch to the tensor convolution product ? by taking sums of outer tensor products, as defined in equation 5. As our experiments show, the benefits of this algebraic structure are not just theoretical but can significantly elevate performance of already strong-performing models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOW TO USE THIS APPENDIX</head><p>For practitioners, we recommend a look at Section A for a refresher on tensor notation and an introduction to T(V ); further, the introduction of Section B contains a brief summary of the main theoretical properties of Seq2Tens that make it an attractive feature map for sequence data. Sections D and E contain details on algorithms and experiments.</p><p>For theoreticians, we recommend Section B for a proof that ? is universal (Theorem B.3), how the Seq2Tens map behaves in the high-frequency limit as one goes from discrete to continuous time (Proposition B.10), and to Section C for a quantitative statement of low-rank functionals can be turned into high-rank functionals with sequence-to-sequence transformations. We re-emphasize that these more algebra-heavy sections are not needed for practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TENSORS AND THE FREE ALGEBRA</head><p>This section recalls some basics on the tensor product ? and the convolution product that turns the linear space T(V ) into an algebra -the so-called free algebra or free algebra over V . We refer to <ref type="bibr">(Lang, 2002, Chapter 16</ref>) for more on tensors, to <ref type="bibr" target="#b26">Reutenauer (1993)</ref> for free algebras. Put briefly, for any linear space V there exists a linear space T(V ) that contains V but that also carries a noncommutative product.</p><p>Tensor products on R d . If x = (x 1 , . . . , x d ) ? R d and y = (y 1 , . . . , y e ) ? R e are two vectors, then their tensor product x ? y is defined as the (d ? e)-matrix, or degree 2 tensor, with entries (x ? y) i,j = x i y j . This is also commonly called the outer product of the two vectors. The space</p><formula xml:id="formula_23">R d ? R e is defined as the linear span of all degree 2 tensors x ? y for x ? R d , y ? R e . If z ? R f is another vector, then one may form a degree 3 tensor x ? y ? z with shape (d ? e ? f ) defined to have entries (x ? y ? z) i,j,k = x i y j z k . The space R d ? R e ? R f is analogously defined as the linear span of all degree 3 tensors x ? y ? z for x ? R d , y ? R e , z ? R f .</formula><p>The tensor product of two general vector spaces V and W can be defined even if they are infinite dimensional, see <ref type="bibr">(Lang, 2002, Chapter 16</ref>), but we invite readers unfamiliar with general tensor spaces to think of V as R d below.</p><p>The free algebra T(V ). Ultimately we are not only interested in tensors of some fixed degree m -that is an element of V ?m -but sequences of tensors of increasing degree. Given some linear space V , the linear space T(V ) is defined as set of all tensors of any degree over V . Formally</p><formula xml:id="formula_24">T(V ) := m?0 V ?m = {t = (t m ) m?0 | t ? V ?m }<label>(19)</label></formula><p>where we use the notation</p><formula xml:id="formula_25">V ?1 = V , V ?2 = V ?V, V ?3 = V ?V ?V and so on; by convention we let V ?0 = R. We normally write elements of T(V ) as t = (t 0 , t 1 , t 2 , t 3 , . . .) such that t m ? V ?m ,</formula><p>that is, t 0 is a scalar, t 1 is a vector, t 2 is a matrix, t 3 is a 3-tensor and so on. Note that T(V ) is again a linear space if we define addition and scalar multiplication as</p><formula xml:id="formula_26">s + t = (s m + t m ) m?0 ? T(V ) and c ? t = (ct m ) m?0 ? T(V ) (20) for s, t ? T(V ) and c ? R. Example A.1. Let V = R d . For v = (v i ) i=1,...,d ? R d consider t = (v ?m ) m?0 ? T(R d ) where we denote for brevity t m := v ?m := v ? ? ? ? ? v m many tensor products ? ? (R d ) ?m and by convention we set v ?0 := 1 ? (R d ) ?0 .</formula><p>That is,</p><formula xml:id="formula_27">t 1 = v ?1 = v is a d-dimensional vector, with the i coordinate equal to v i ; t 2 = v ?2 is d ? d-matrix with the (i, j)-coordinate equal to v i v j ; t 3 = v ?3 is degree 3-tensor with the (i, j, k)-coordinate equal to v i v j v k .</formula><p>In this special case, the element t ? T(R d ) consists of entries t m = v ?m ? (R d ) ?m that are symmetric tensors, that is the (i 1 , . . . , i m )-th coordinate is the same as the (i ?(1) , . . . , i ?(d) ) coordinate if ? is a permutation of {1, . . . , d}. However, we emphasize that in general an element of T(R d ) does not need to be made up of symmetric tensors.</p><p>A product on T(V ). Key to our approach is that T(V ) is not only a linear space, but what distinguishes it as a feature space for sequences is that it carries a non-commutative product. In other words, T(V ) is not just a vector space but a (non-commutative) algebra (an algebra is a vector space where one can multiply elements). This is the so-called tensor convolution product and defined as follows</p><formula xml:id="formula_28">s ? t := m i=0 s i ? t m?i m?0 = 1, s 1 + t 1 , s 2 + s 1 ? t 1 + t 2 , . . . .<label>(21)</label></formula><p>In a precise mathematical sense, T(V ) is the most general algebra containing V , namely T(V ) is the "free algebra" that contains V ; see <ref type="bibr">(Lang, 2002, Chapter 16</ref>) for the precise definition of free objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A UNIVERSAL FEATURE MAP FOR SEQUENCES OF ARBITRARY LENGTH</head><p>Recall from Section 2, that given a map defined on a set X ? : X ? V we lift ? to a map ? : X ? T(V ) and define the Seq2Tens feature map for sequences in X of arbitrary length as</p><formula xml:id="formula_29">? : Seq(X ) ? T(V ), x ? T i=1 ?(x i ).</formula><p>The remainder of Section B makes the following statements mathematically rigorous:</p><p>(i) ? is a universal feature map whenever ? is a universal (Section B.1 and B.2), (ii) ? makes sequences of different length comparable analogous to how m-mers make strings of different length comparable (Section B.3), (iii) ? converges to a well-defined object when we go from discrete to continuous time (sequences converge to paths) (Section B.4).</p><p>B.1 THE UNIVERSALITY OF ?.</p><p>Definition B.1. Let X be a topological space (the "data space") and W a linear space ("the feature space"). We say that a function f : <ref type="bibr" target="#b28">(Rudin, 1965)</ref>.</p><formula xml:id="formula_30">X ? W is universal (to C b (X )) if the the set of functions {x ? , f (x) : ? W } ? C b (X ) (22) is dense in C b (X ). Example B.2. Classic examples of this in ML are ? For X ? R d bounded and W = T(R d ), the polynomial map p : R d ? T(R d ), x ? (1, x, x ?2 , x ?3 , x ?4 , . . .) is universal</formula><formula xml:id="formula_31">? The 1-layer neural net map x ? ? N ? (x)</formula><p>where ? runs over all configurations of parameters is universal under some very mild conditions <ref type="bibr">(Hornik, 1991)</ref>.</p><p>We now prove the main result of this section</p><formula xml:id="formula_32">Theorem B.3. Let ? : X ? T(V ), x ? (? m (x)) m?0 , ? m (x) ? V ?m be such that:</formula><p>1. For any n ? 1 the support of (? 0 , ? 1 , . . . , ? m1 ) ?n and ? m2 are disjoint if 1 ? m 1 &lt; m 2 .</p><p>2. ? 0 = 1 and ? 1 : X ? V is a bounded universal map with at least one constant term.</p><p>Then</p><formula xml:id="formula_33">? : Seq(X ) ? T(V ), (x 1 , . . . , x L ) ? L i=1 ?(x i )<label>(23)</label></formula><p>is universal.</p><formula xml:id="formula_34">Remark B.4. (i) TheoremB.3 implies that x ? L i=1 (1, ?(x), 0, . . .) is universal whenever ? : X ? V is universal.</formula><p>This is the lift we use throughout the main text, see equation 6.</p><p>(ii) By taking ? :</p><formula xml:id="formula_35">R d ? T(R d ), x ? (1, x, x ?2 2! , x ?3 3! , .</formula><p>. .) one recovers Chen's signature <ref type="bibr" target="#b15">(Chen, 1954;</ref><ref type="bibr" target="#b16">1957;</ref><ref type="bibr" target="#b17">1958)</ref> as used in rough paths. (iv) By taking each ? m to be a trainable Neural Network one gets a trainable universal map ? for sequences that includes all of the above,</p><formula xml:id="formula_36">B.1.1 THE ALGEBRA OF LINEAR FUNCTIONALS ON ?.</formula><p>The proof of Theorem B.3 uses that if ? is universal, then the space of linear functionals on ?(x) forms a commutative algebra, that is for two linear functionals 1 , 2 there exists another linear functional such that</p><formula xml:id="formula_37">1 , ?(x) 2 , ?(x) = , ?(x) .<label>(24)</label></formula><p>This new functional is constructed in explicit way from 1 and 2 , with a so-called quasi-shuffle product. In the remainder of this section B.1, we prepare and give the proof of Theorem B.3: subsection B.1.1 introduces the quasi-shuffle product, and subsection B.1.1 uses this to prove Theorem B.3.</p><p>We spell out the proof for the case ? = (1, ?, 0, 0, . . . ) ? T(V ) since this is the form we use in the main text, Proposition 2.1, and the other cases follow similarly. In fact, without loss of generality we can take ? = id since this does not change the algebraic structure in any way. That is, we take</p><formula xml:id="formula_38">? : Seq(V ) ? T(V ), ?(x 1 , . . . , x L ) := L i=1 ?(x i )<label>(25)</label></formula><p>with ?(x) = (1, x, 0, 0, . . .). By using the definition of the product in T(V ) and expanding equation 25 we get</p><formula xml:id="formula_39">?(x 1 , . . . , x L ) = (1, L i=1 x i V , 1?i1&lt;i2?L x i1 ? x i2 V ?2 , 1?i1&lt;i2&lt;i3?L x i1 ? x i2 ? x i3 V ?3 , ? ? ? ) (26) In general, writing ? m (x) for the projection of ?(x) onto V ?m , we have ? m (x) = 1?i1&lt;???&lt;im?L x i1 ? ? ? ? ? x im .<label>(27)</label></formula><formula xml:id="formula_40">So if = (0, 0, . . . , v 1 ? ? ? ? ? v m , 0, . . .) with v 1 , . . . , v m ? V , then , ? m (x) = v 1 ? ? ? ? ? v m , 1?i1&lt;???&lt;im?L x i1 ? ? ? ? ? x im (28) = 1?i1&lt;???&lt;im?L v 1 ? ? ? ? ? v m , x i1 ? ? ? ? ? x im = 1?i1&lt;???&lt;im?L v 1 , x i1 ? ? ? v m , x im .<label>(29)</label></formula><p>Hence , ? m (x) can be computed efficiently without computing ?(x). Proposition 3.3 follows by linearity since by definition , ?(x) = m?0 m , ?(x) and for each of the terms we can use the above formula when = ( 0 , 1 , 2 , . . . , M , 0, . . .) is of rank-1 and of degree M (Definition 3.1).</p><p>Non-linear functionals acting on ?. We now investigate what happens when one applies nonlinear functions to ?(x). To do this, we first note that since T(V ) is a vector space, we may form the free algebra over T(V ), denoted by T(T(V )), or T 2 (V ). It may be decomposed as</p><formula xml:id="formula_41">T 2 (V ) = n1,...,n k ?0 V ?n1 ? ? ? V ?n k<label>(30)</label></formula><p>where we use the notation ? for the tensor product on V and the bar | for the tensor product on T(V ). See Ebrahimi-Fard &amp; Patras <ref type="formula" target="#formula_0">(2015)</ref> for more on T 2 (V ) and the bar notation.</p><p>Definition B.5. If x ? V is a vector, we denote by x its extension</p><formula xml:id="formula_42">x := (x ?m ) m?0 = (1, x, x ?2 , x ?3 , . . .)<label>(31)</label></formula><p>and if x = (x 1 , . . . , x L ) ? Seq(V ) is a sequence, then</p><formula xml:id="formula_43">x := (x 1 , . . . , x L ) ? Seq(T(V ))<label>(32)</label></formula><p>Since x is a sequence in T(V ), we may compute ?(x ) which takes values in T(T(V )) = T 2 (V ).</p><p>The reason for the above definition is that when products of linear functions in T(V ) act on ?(x), they may be described as linear functions in T 2 (V ) acting on ?(x ). That is, T(V ) is not big enough to capture all non-linear functions acting on ?(x), but T 2 (V ) is. Definition B.6. Assume that V has basis e 1 , . . . , e d . The quasi-shuffle product</p><formula xml:id="formula_44">: T 2 (V ) ? T 2 (V ) ? T 2 (V )<label>(33)</label></formula><p>is defined inductively on rank 1 elements 1 = e i1 | ? ? ? |e im , 2 = e j1 | ? ? ? |e jn by</p><formula xml:id="formula_45">( 1 |e i ) ( 2 |e j ) = ( 1 |e i 2 )|e j + ( 1 2 |e j )|e i + ( 1 2 )|(e i ? e j ).<label>(34)</label></formula><p>By linearity extends to a product on all of T(V ). Lemma B.7. The map ? satisfies the following</p><formula xml:id="formula_46">1 , ?(x) 2 , ?(x) = 1 2 , ?(x ) .<label>(35)</label></formula><p>Proof. By writing out equation 27 in coordinates we get</p><formula xml:id="formula_47">e i1 | ? ? ? |e im , ?(x) = 1?k1&lt;???&lt;km?L e i1 , x k1 ? ? ? e im , x km .<label>(36)</label></formula><p>which shows that ? satisfies a recurrence equation. The proof follows by induction.</p><p>The space T 2 (V ) might seem very large and difficult to work with at first. The power of this representation comes from the fact that one may leverage this in proving strong statements about the original map ? : Seq(V ) ? T(V ), and we will use this in the next subsection.</p><p>B.2 PROOF OF THEOREM B.3.</p><p>We prepare the proof of Theorem B.3 with the following lemma.</p><formula xml:id="formula_48">Lemma B.8. Let Seq 1 (V ) be the set of all x = (x 1 , . . . , x L ) ? Seq(V ) with the form x i = (1, x 1 i , . . . , x d i ).</formula><p>That is, all sequences where one of the terms is constant. Then the map</p><formula xml:id="formula_49">Seq 1 (V ) ? T(V ), (x 1 , . . . , x L ) ? L i=1 (1 + x i )<label>(37)</label></formula><p>is injective.</p><p>Proof. Follows from an induction argument over L. For L = 1 it is clear since</p><formula xml:id="formula_50">e i , ?(x) = x i .<label>(38)</label></formula><p>Assume that it is true for L, let x = (x 1 , . . . , x L+1 ), y = (y 1 , . . . , y L+1 ), where we may assume that both have length L + 1 by taking any number of components to be 0 if necessary. Let 1 be some linear function that separates ?(x 1 , . . . , x L ) and ?(y 1 , . . . , y L ) and 2 some linear function that separates ?(x 2 , . . . , x L+1 ) and ?(y 2 , . . . , y L+1 ), then by fixing some ? ? R:</p><formula xml:id="formula_51">1 ? e 0 + ?e 0 ? 2 , ?(x) ? ?(y) (39) = 1 , ?(x 1 , . . . , x L ) ? ?(y 1 , . . . , y L ) + ? 2 , ?(x 2 , . . . , x L+1 ) ? ?(y 2 , . . . , y L+1 ) .<label>(40)</label></formula><p>Since neither 1 , ?(x 1 , . . . , x L ) ? ?(y 1 , . . . , y L ) nor 2 , ?(x 2 , . . . , x L+1 ) ? ?(y 2 , . . . , y L+1 ) are 0 by assumption there exists some ? ? R such that 1 ? e 0 + ?e 0 ? 2 , ?(x) ? ?(y) = 0. This shows the assertion.</p><p>We now have everything to give a proof of Theorem B.3.</p><p>Proof of Theorem B.3. We will show that linear functionals on ? are dense in the strict topology <ref type="bibr">(Giles, 1971)</ref>. By Theorem (Giles, 1971, Theorem 3.1) it is enough to show that linear functions on ? form an algebra since by Lemma B.8 they separates the points of Seq(X ). Since they clearly form a vector space it is enough to show that they are closed under point-wise multiplication. Let 1 , 2 be two such, then by Lemma B.7</p><formula xml:id="formula_52">1 , ?(x) 2 , ?(x) = 1 2 , L i=1 ?(x i )<label>(41)</label></formula><p>so it is enough to show that 1 2 also is a linear function on ?(x). Note that inductively it is enough to show that if e i , e j are unit vectors, then e i ? e j is a linear function on ?(x). By assumption ? is bounded and universal, so the continuous bounded function x ? e i , ?(x k ) e j , ?(x k ) is approximately linear, and we may write</p><formula xml:id="formula_53">e i , ?(x k ) e j , ?(x k ) = h, ?(x k ) + ?(x k )<label>(42)</label></formula><p>where ?(x k ) can be made arbitrarily small in the strict topology. The assertion now follows since A classical way to produce a graded description of strings is by counting their non-contiguous substrings. These are the so-called m-mers; for example,</p><formula xml:id="formula_54">e i ? e j , L i=1 ?(x i ) ) = L k=1 e i , ?(x k ) e j , ?(x k ) = L k=1 h, ?(x k ) + ?(x k ) (43) = e h , ?(x) + n max 1?k?n ?(x k ).<label>(44)</label></formula><p>The string "aabc" has the 2-mers {aa, ab, ac, ab, ac, bc}.</p><p>Measuring similarity between strings by counting how many substrings they have in common is a sensible similarity measure, even if the strings have different length; we refer to <ref type="bibr">Leslie &amp; Kuang (2004)</ref> for applications and to (Cristianini &amp; Shawe-Taylor, 2000, Chapter 11) for detailed introduction to the use of substrings in ML.</p><p>Our Seq2Tens feature map can be regarded as a vast generalization of such subpattern matching: </p><formula xml:id="formula_56">if ?(x) = (? m (x)) m?0 ? T(V ) then the tensor ? m (x) ? V ?m represents non-contiguous sub- sequences of x = (x 1 , . . . ,<label>x</label></formula><p>) =(1, e 1 , 0, 0, . . . , ) ? (1, e 1 , 0, 0, . . .) ? (1, e 2 , 0, 0, . . .) ? (1, e 3 , 0, 0, . . .) (47) =(1, 2e 1 + e 2 + e 3 ?V , e 1 ? e 1 + 2e 1 ? e 2 + 2e 1 ? e 3 + e 2 ? e 3</p><formula xml:id="formula_58">?V ?2 ,<label>(48)</label></formula><p>e 1 ? e 1 ? e 2 + e 1 ? e 1 ? e 3 , e 1 ? e 1 ? e 3 ? e 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?V ?3</head><p>, e 1 ? e 1 ? e 2 ? e 3 ?V ?4</p><p>, 0, . . .).</p><p>We see that the tensor ?(x) ? V ?m of degree m contains the m-mers, that is the coordinates of ? m (x) count how often a subsequence of length m in x = (a, a, b, c) appears; e.g. the coordinate e 1 ? e 2 of ? 2 (x) equals 2 because the substring "a,b" appears twice but the e 1 ? e 1 coordinate of ? 2 (x) equals 1 since the substring "a,a" appears only once in (a, a, b, c), etc. Similarly, the only non-zero coordinate of ? 4 (x) is e 1 ? e 1 ? e 2 ? e 3 since "a,a,b,c" is the only substring of (a, a, b, c) and consequently all coordinate of ? m (x) are 0 for m ? 5.</p><p>An analogous calculation shows that for continuous domain such as X = R d the coordinates of ? m (x) ? (R d ) ?m measure movement patterns in these coordinates. Section B.4 below shows that this interpretation also holds when we go from discrete time (sequences) to continuous time (paths); Example B.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 CONVERGENCE FROM DISCRETE TO CONTINUOUS TIME</head><p>A common source of of sequence data is to measure a quantity (x(t)) t?[0,T ] that evolves in continuous time at fixed times t 1 , . . . , t L to produce a sequence x = (x(t 1 ), . . . , x(t L )) ? Seq(X ). Often the measurements are of high-frequency (|t i+1 ? t i | ? 0 and L ? ?) and is interesting to understand how our Seq2Tens approach behaves in this limiting case. As it turns out, when combined with taking finite differences 1 our feature map ?(x) converges to a classical object in analysis, the so-called signature of the path x in this limit, <ref type="bibr">(Chevyrev &amp; Kormilitzin, 2016)</ref> . For brevity, we spell it out here for smooth paths and with the lift ?(x) = (1, ?(x), 0, . . .), but readers familiar with rough paths will notice that the result generalizes even to non-smooth paths such as Brownian motion. Proposition B.10. Let x ? C 1 ([0, 1], R d ) and for every L ? 1 define</p><formula xml:id="formula_60">x L := (x(t L 0 ), x(t L 1 ) ? x(t L 0 ), . . . , x(t L L ) ? x(t L L?1 ))<label>(50)</label></formula><p>where t i := i L for i = 0, . . . , L. Then for every m ? 0 we have</p><formula xml:id="formula_61">? m (x L ) ? 0?t1?????tm?1 dx dt (t 1 ) ? ? ? ? ? dx dt (t m )dt 1 ? ? ? dt m as L ? ?.<label>(51)</label></formula><p>Proof. Using the recurrence relation from the proof of Lemma B.7 we may write</p><formula xml:id="formula_62">? m (x L ) = L i=1 ? m?1 (x L i ) ? x(t i+1 ) ? x(t i ) (52) where x L i denotes the sequence (x(t L 0 ), x(t L 1 )?x(t L 0 ), . . . , x(t L i )?x(t L i?1 )</formula><p>). By a Taylor expansion this is equal to</p><formula xml:id="formula_63">? m (x L ) = L i=1 ? m?1 (x L i ) ? dx dt (t i )(t i+1 ? t i ) + O(1/L),<label>(53)</label></formula><p>so by unravelling the recurrence relation we may write</p><formula xml:id="formula_64">? m (x L ) = 1?i1&lt;...&lt;im?L dx dt (t i1 ) ? ? ? ? ? dx dt (t im )(t i1+1 ? t i1 ) ? ? ? (t im+1 ? t im ) + O(1/L),<label>(54)</label></formula><p>which is a Riemann sum, and thus converges to the asserted limit.</p><p>The interpretation of ? m (x) as counting sub-patterns remains even in the continuous time case: Example B.11. Let x ? C 1 ([0, 1], R d ) and e 1 , . . . , e d the standard basis of V = R d . For m = 1 the e 1 coordinate equals</p><formula xml:id="formula_65">e 1 , 1 t=0 dx dt (t)dt = e 1 , x(1) ? x(0) x ? ? ? ?(x) ? ? ? ? 2 (x) ? ? ? . . . . . . . . . . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>? ? ? <ref type="figure">Figure 1</ref>: The sequence-to-sequence transformation. The bottom row is the original sequence x and subsequent ones apply the map ? in the sequence-to-sequence manner.</p><p>which measures the total movement of the path x in the direction e 1 . Analogously, for m = 2 the e 1 ? e 2 coordinate equals</p><formula xml:id="formula_66">e 1 ? e 2 , 0?t1?????tm?1 x(t 1 ) dt 1 ? x(t 2 ) dt 2 dt 1 dt 2 = 0?t1?t2?1 e 1 , x(t 1 ) dt 1 e 2 , x(t 2 ) dt 2 dt 1 dt 2 . (55)</formula><p>which measure the number of ordered tuples (t 1 , t 2 ), t 1 &lt; t 2 , such that x moves in direction e 1 at time t 1 and subsequently in direction e 2 at time t 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C STACKING SEQUENCE-TO-SEQUENCE TRANSFORMS</head><p>As ? applies to sequences of any length, we may use it to map the original sequence to another sequence in feature space,</p><formula xml:id="formula_67">Seq(V ) ? Seq(T(V )) (56) (x 1 , x 2 , x 3 , . . . , x L ) ? ?(x 1 ), ?(x 1 , x 2 ), ?(x 1 , x 2 , x 3 ), . . . , ?(x 1 , . . . , x L ) .<label>(57)</label></formula><p>Since T(V ) is again a linear space, we can repeat this procedure to map Seq(T(V )) to Seq(T(T(V ))). By repeating this D times, we have constructed sequence-to-sequence transforms Seq(V ) = Seq(T(V )) ? Seq(T(T(V ))) ? ? ? ? ? Seq(T (? ? ? T D times (V ) ? ? ? )).</p><p>See <ref type="figure">Figure 1</ref> for an illustration. We emphasize that in each step of the iteration, the newly created sequence evolves in a much richer space than in the previous step. To make this precise we now introduce the higher rank free algebras.</p><p>Higher rank free algebras. Just like in Appendix B we need to enlarge the ambient space T(V ).</p><p>Recall that we defined T 2 (V ) := T(T(V )). This construction can be iterated indefinitely and leads to the higher rank free algebras, recursively defined as follows: Definition C.1. Define the spaces</p><formula xml:id="formula_69">T 0 (V ) = V, T D (V ) = m?0 T D?1 (V ) ?m .<label>(59)</label></formula><p>We use the notation ? (D) for the tensor product on T D?1 (V ) which makes T D (V ), +, ? (D) into a multi-graded algebra over s. See <ref type="bibr" target="#b10">Bonnier et al. (2020)</ref> for more on this iterated construction.</p><p>Half-shuffles. By iterating the sequence-to-sequence D times, one gets a map</p><formula xml:id="formula_70">? D : Seq(V ) ? T D (V ).<label>(60)</label></formula><p>These are very large spaces, but as we will see in Proposition C.3 below, linear functionals on the full map Seq(V ) ? T D (V ) can be de-constructed into so called half-quasi-shuffle on the original map ? : Seq(V ) ? T(V ).</p><p>Just like in Appendix B we consider the sequence x as its extension x taking values in T(V ). Hence linear functionals can be written as linear combinations of elements of the form e i1 ? (2) ? ? ? ? (2) e in . Definition C.2. The half-quasi-shuffle product is defined on rank 1 tensors by</p><formula xml:id="formula_71">1 ? ( 2 ? (2) e i ) = ( 1 2 ) ? (2) e i<label>(61)</label></formula><p>and extends by bi-linearity to a map T 2 (V ) ? T 2 (V ) ? T 2 (V ).</p><p>Proposition C.3 shows that by composing ? with itself, low degree tensors on the second level can be rewritten as higher degree tensors on the first level. This indicates that iterated compositions of ? can be much more efficient than computing everything on the first level. We show this for the first level, but by iterating the statement it can be applied for any number D ? 2. Proposition C.3. Let ? be the sequence-to-sequence transformation:</p><formula xml:id="formula_72">Seq(V ) ? Seq(T(V )), (x 1 , . . . , x L ) ? (?(x 1 ), . . . , ?(x, . . . , x L ))<label>(62)</label></formula><p>and let ?(x 0 , . . . ,</p><formula xml:id="formula_73">x L ) = (x 1 ? x 0 , . . . , x L ? x L?1 ). Then e 1 ? (3) e 2 , ?(??(x)) = 1 ? 2 , ?(x)<label>(63)</label></formula><p>Proof. We use the notation ?(x) k = ?(x 1 , . . . , x k ). By induction:</p><formula xml:id="formula_74">e 1 ? (3) e 2?(2)ei , ?(??(x)) (64) = 1?k1&lt;k2?L 1 , ?(x) k1 ? 1 , ?(x) k1?1 2 ? (2) e i , ?(x) k2 ? 2 ? (2) e i , ?(x) k2?1 (65) = L?1 k=1 1 , ?(x) k 2 ? (2) e i , ?(x) k+1 ? 2 ? (2) e i , ?(x) k (66) = L?1 k=1 1 , ?(x) k 1?l?k 2 , ?(x) l x i l+1 ? 2 , ?(x) l?1 x i l (67) = L?1 k=1 1 , ?(x) k 2 , ?(x) k x i k+1 = L?1 k=1 1 2 , ?(x) k x i k+1 (68) = ( 1 2 ) ? (2) e i , ?(x) L .<label>(69)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILS ON COMPUTATIONS</head><p>Here we give further information on the implementation of LS2T layers detailed in the main text. For simplicity, we fix the state-space of sequences to be V = R d from here onwards. We also remark that although some of the following considerations and techniques might look unusual for the standard ML audience, they are well-known in the signatures community <ref type="bibr" target="#b21">(Morrill et al., 2020)</ref> D.1 VARIATIONS Truncation degree. To reiterate from Section 2, for a given static feature map ? :</p><formula xml:id="formula_75">R d ? V the Seq2Tens feature map ? : Seq(R d ) ? T(V ) represents a sequence x = (x 1 , . . . , x L ) ? Seq(R d ) as a tensor in T (V ), ?(x) = (? m (x)) m?0 , ? m (x) = 1?i1&lt;???&lt;im?L ?(x i1 ) ? ? ? ? ? ?(x im ),<label>(70)</label></formula><p>where ? m : Seq(R d ) ? V ?m is given by a summation over all noncontiguous length-m subsequences of x with non-repeating indices. Therefore, for a sequence of length L ? N, ? m can have potentially non-zero terms for m ? L. An empirical observation is that for most datasets computing everything up to the Lth level is redundant in the sense that usually the first M ? N levels already contain most of the information a discriminative or a generative model picks up on where M L. It is thus better treated as a hyperparameter, which we call "order" in the main text.</p><p>Below we take for brevity ?(x i ) = x i ? V = R d since with other maps ? simply amounts to replacing</p><formula xml:id="formula_76">x i ? R d by ?(x i ) ? R e .</formula><p>Distinguishing functionals across levels. Let us consider the LS2T map??, each output coordinate of which is given by a linear functional of ?, i.e.??(x) = 1 , ?(x) , . . . , n , ?(x), for a sequence x ? Seq(R d ) and a collection of rank-1 elements ? = ( k ) n k=1 ? T (R d ). Then, a single output coordinate of? ? may be written for 1 ? k ? n as</p><formula xml:id="formula_77">k , ?(x) = m=0 k m , ? m (x) ,<label>(71)</label></formula><p>for k = ( k m ) m?0 , i.e. we take inner products of tensors that are of the same degree, and then sum these up. We found that rather than taking the summation across tensor levels, it is beneficial to treat the linear functional on each level of the free algebra as an independent output to hav?</p><formula xml:id="formula_78">? m,? (x) = ( 1 m , ? m (x), . . . , n m , ? m (x) ) and??(x) = (? m,? (x)) m?0 ,<label>(72)</label></formula><p>where now?? has output dimensionality (M ? n) with M ? N the truncation degree of ? as detailed in the previous paragraph. Hence this modification scales the output dimension by M , but it will be important for the next step we discuss. It is for this modification that in <ref type="figure" target="#fig_7">Figure 2</ref>, each output of a LS2T layer has dimensionality n ? m for a width-n and order-m LS2T map, while in <ref type="figure" target="#fig_10">Figure 5</ref> the B-LS2T layer has output dimensionality h ? 4, since we set n = h and m = 4.</p><p>The need for normalization. Here we motivate the need to follow each LS2T layer by some form of normalization. Let x = (x 1 , . . . , x L ) ? Seq(R d ) be a sequence. Let ? ? R be a scalar and define y = ?x ? Seq(R d ) a scaled version of x. Let us investigate how the features change:</p><formula xml:id="formula_79">? m (y) = 1?i1&lt;???&lt;i M ?L y i1 ? ? ? ? ? y im = 1?i1&lt;???&lt;i M ?L (?x i1 ) ? ? ? ? ? (?x im ) (73) = ? m 1?i1&lt;???&lt;i M ?L x i1 ? ? ? ? ? x im ,<label>(74)</label></formula><p>and therefore we have ?(y) = (? m (y)) m?0 = (? m ? m (x)) m?0 , which analogously translates into the low-rank Seq2Tens map sinc?</p><formula xml:id="formula_80">? m,? (y) = ( 1 m , ? m (y) , . . . , n m , ? m (y) ) = ( 1 m , ? m ? m (x) , . . . , n m , ? m ? m (x) ) (75) = ? m ( 1 m , ? m (x) , . . . , n m , ? m (x) ).<label>(76)</label></formula><p>From this point alone, it is easy to see that ? m , and thus? m,? will move across wildly different scales for different values of m, which is inconvenient for the training of neural networks. To counterbalance this, we used a batch normalization layer after each LS2T layer in Sections 5.1, 5.2 that computes mean and variance statistics across time and the batch itself, while for the GP-VAE in Section 5.3 we used a layer normalization that computes the statistics only across time.</p><p>Sequence differencing. In both Section 5.1 and Section 5.3, we precede each LS2T layer by a differencing layer and a time-embedding layer.</p><p>Let ? : Seq(R d ) ? Seq(R d ) be the discrete difference operator defined for a sequence x = (x 1 , . . . , x L ) ? Seq(R d ) as</p><formula xml:id="formula_81">?x := (x 1 , x 2 ? x 1 , . . . , x L ? x L?1 ) ? Seq(R d ),<label>(77)</label></formula><p>where we made the simple identification that x 0 ? 0, i.e. for all sequences we first concatenate a 0 observation along the time axis, in the signature learning community this is called basepoint augmentation <ref type="bibr" target="#b21">Morrill et al. (2020)</ref>, which is beneficial for two reasons: (i) now ? preserves the length L of a sequence, (ii) now ? is one-to-one, since otherwise ? would be translation invariant, i.e. it would map all sequences which are translations of each other to the same output.</p><p>To motivate differencing, first let us consider ? m (x) for m = 1, and for brevity denote ?x i = x i ? x i?1 for i = 1, . . . , L and the convention x 0 = 0. Then, we may write</p><formula xml:id="formula_82">? 1 (x) = L i=1 ?x i = x L ,<label>(78)</label></formula><p>which means that now the first level of the Seq2Tens map is simply point-wise evaluation at the last observation time, and when used as a sequence-to-sequence transformation over expanding windows (i.e. equation 15), it is simply the identity map of the sequence.</p><p>Analogously, for the low-rank map we hav?</p><formula xml:id="formula_83">? 1,? (x) = L i=1 ( 1 1 , ?x i , . . . , n 1 , ?x i ) = ( 1 1 , x L , . . . , n 1 , x L ),<label>(79)</label></formula><p>which is simply a linear map applied to x in an observation-wise manner. The higher order terms, ? m (x) and? m,? (x) can generally be written as</p><formula xml:id="formula_84">? m (x) = 1?i1&lt;...im?L ?x i1 ? ? ? ? ? ?x im ,<label>(80)</label></formula><formula xml:id="formula_85">and? m,? = 1?i1&lt;???&lt;im?L ( z 1 m,1 , ?x i1 ? ? ? z 1 m,m , ?x im , . . . , z n m,1 , ?x i1 ? ? ? z n m,m , ?x im )<label>(81)</label></formula><p>for some rank-1 degree-m tensors k m = z k m,1 ? ? ? ? ? z k m,m for k = 1, . . . , n. We observed that this way the higher order terms are relatively stable across time as the length of a sequence increases, while without differencing they can become unstable, exhibit high oscillations, or simply blow-up.</p><p>An additional benefit of taking differences is that the maps ? and?? become warping invariant, that is, invariant to time warpings. It is easy to see this by checking that if x i = x i?1 then ?x i = 0 and all the corresponding terms in the summations equation 80 and equation 81 are zeros. Time-embeddings. By time-embedding, we mean adding as an extra coordinate to an input sequence the observation times (t i , x i ) i=1,...,L ? Seq(R d+1 ). Some datasets already come with a pre-specified observation-grid, in which case we can use that as a time-coordinate at every use of a time-embedding layer. If there is no pre-specified observation grid, we can simply add a normalized and equispaced coordinate, i.e. t i = i/L. Time-embeddings can be beneficial preceding both convolutional layers and LS2T layers. For convolutions, it allows to learn features that are not translation invariant <ref type="bibr">(Liu et al., 2018a)</ref>. For the LS2T layer, the interpretation is slightly different. Note that in both Sections 5.1 and 5.3, we employ the time-embedding before the differencing block. This can be equivalently reformulated as after differencing adding an additional constant coordinate to the sequence, i.e.</p><formula xml:id="formula_86">(t i ?t i?1 , x i ?x i?1 ) i=1,...,L ? Seq(R d+1 ),</formula><p>where t i ? t i?1 = 1/L is simply a constant. This is motivated by Lemma B.8, which states that the map ? : Seq(R d ) ? T (V ) is injective for sequences with a constant coordinate. Thus, the time-embedding before the differencing block is equivalent to adding a constant coordinate after the differencing block, and its purpose is to guarantee injectivity of ?.</p><p>Delay embeddings and convolutions. A useful preprocessing technique for time series are delay embeddings, which simply amount to augmenting the state-space of sequences with a certain number of previous observations, motivated by Takens' theorem <ref type="bibr" target="#b36">(Takens, 1981;</ref><ref type="bibr" target="#b30">Sauer et al., 1991)</ref>, which ensures that a high-dimensional dynamical system can be reconstructed from low-dimensional observations. Theorem 2.1 guarantees that if ? is a universal feature map on the state-space then ? is universal. The most straightforward nonlinearity to take as ? is a multilayer perceptron (MLP), that is, ? = ? D ?? ? ??? 1 with ? j (x) = ?(W j x+b j ). By combining such dense layers with delay embeddings (lags), one recovers a temporal convolution layer, i.e. ? j (x l i ) = ? l k=0 W j,k x i?k + b j , which motivates the use of convolutions in the preprocessing layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 RECURSIVE COMPUTATIONS</head><p>Next, we show how the computation of the maps ? m and ? m,? can be formulated as a joint recursion over the tensor levels and the sequence itself.</p><p>Since ? m is given by a summation over all noncontiguous length-m subsequences with nonrepetitions of a sequence x ? Seq(R d ), simple reasoning shows that ? m obeys the recursion across m and time for 2 ? l ? L and 1 ? m</p><formula xml:id="formula_87">? m (x 1 , . . . x l ) = ? m (x 1 , . . . x l?1 ) + ? m?1 (x 1 , . . . , x l?1 ) ? x l ,<label>(82)</label></formula><p>with the initial conditions ? 0 ? 1, ? 1 (x 1 ) = x 1 and ? m (x 1 ) = 0 for m ? 2.</p><formula xml:id="formula_88">Let = ( m ) m?0 ? T (R d )</formula><p>be a sequence of rank-1 tensors with m = z m,1 ?? ? ??z m,m ? (R d ) ?m a rank-1 tensor of degree-m. Then, m , ? m (x) may be computed analogously to equation 82 using the recursion for 2 <ref type="bibr">87)</ref> and the initial conditions can be rewritten as the identities z 0,0 , ? 0 = 1, z m,1 , ? 1 (x) = z m,1 , x and z m,1 ? ? ? ? ? z m,m , ? m (x 1 ) = 0 for 2 ? m.</p><formula xml:id="formula_89">? l ? L, 1 ? m m , ? m (x 1 , . . . , x l ) = z m,1 ? ? ? ? ? z m,m , ? m (x 1 , . . . , x l ) (83) = z m,1 ? ? ? ? ? z m,m , ? m (x 1 , . . . , x l?1 ) (84) + z m,1 ? ? ? ? ? z m,m?1 , ? m?1 (x 1 , . . . , x l?1 ) z m,m , x l (85) = m , ? m (x 1 , . . . x l?1 ) (86) + z m,1 ? ? ? ? ? z m,m?1 , ? m?1 (x 1 , . . . , x l?1 ) z m,m , x l<label>(</label></formula><p>A slight inefficiency of the previous recursion given in equation 86, equation 87 is that one generally cannot substitute m?1 , ? m (x 1 , . . . , x l?1 ) for the z m,1 ? ? ? ? ? z m,m?1 , ? m (x 1 , . . . , x l?1 ) term in equation 87, since m?1 = z m,1 ? ? ? ? ? z m,m?1 generally. This means that to construct the degree-m linear functional m , ? m (x 1 , . . . , x l ) , one has to start from scratch by first constructing the degree-1 term z m,1 , ? 1 first, then the degree-2 term z m,1 ? z m,2 , ? 2 , and so forth. This further means in terms of complexities that while equation 82 has linear complexity in the largest value of m, henceforth denoted by M ? N, equation 86, equation 87 has a quadratic complexity in M due to the non-recursiveness of the rank-1 tensors ( m ) m = (z m,1 ? ? ? ? ? z m,m ) m .</p><p>The previous observation indicates that an even more memory and time efficient recursion can be devised by parametrizing the rank-1 tensors ( m ) m in a recursive way as follows:</p><formula xml:id="formula_90">let 1 = z 1 ? R d and define m = m?1 ? z m ? (R d ) ?m for 2 ? m, i.e. m = z 1 ? ? ? ? ? z m for {z 1 , . . . z m } ? R d .</formula><p>This parametrization indeed allows to substitute m?1 in equation 87, which now becomes</p><formula xml:id="formula_91">m , ? m (x 1 , . . . , x l ) = m , ? m (x 1 , . . . , x l?1 + m?1 , ? m?1 (x 1 , . . . , x l?1 ) z m , x l ,<label>(88)</label></formula><p>and hence, due to the recursion across m for both m and ? m , it is now linear in the maximal value of m, denoted by M ? N. This results in a less flexible, but more efficient LS2T, due to the additional added recursivity constraint on the rank-1 elements. We refer to this version as the recursive variant, while to the non-recursive construction as the independent variant.</p><p>Next, we show how the previous computations can be rewritten as a simple RNN-like discrete dynamical system. For simplicity, we consider the recursive formulation, but the independent variant can also be formulated as such with a larger latent state size. Let ( j ) j=1,...,n be n ? N different rank-1 recursive elements, i.e. j = ( j m ) m?0 , j m = z j 1 ? . . . z j m for z j m ? R d , m ? 0 and j = 1, . . . , n. Also, denote h j m,i := j m , ? m (x 1 , . . . , x i ) ? R, a scalar corresponding to the output of the jth linear functional on the mth tensor level for the sequence (x 1 , . . . , x i ). We collect all such functionals for given m and i into h m,i := (h 1 m,i , . . . h n m,i ) ? R n , i.e. h m,i =? m,? (x 1 , . . . , x i ).</p><p>Algorithm 1 Computing the LS2T layer with independent tensors across levels 1: Input: Sequences (x j ) j=1,...,nx = (x j 1 , . . . , x j L ) j=1,...,nx ? Seq(R d ), rank-1 tensors ( k ) k=1,...,n = (z k m,1 ? ? ? ? ? z k m,m ) k=1,...,n m=1,..., Save Y m ? ?R[:, :, ] 9: end for 10: Output:</p><formula xml:id="formula_92">M ? T (R d ), LS2T order M ? N 2: Compute A[m, i, j, l, k] ? z j m,k , x i l for m ? {1, . . . , M }, i ? {1, . . . , n x }, j ? {1, . . . , n }, l ? {1,</formula><formula xml:id="formula_93">Sequences (Y 1 , . . . , Y M ) each of shape (n x ? L ? n )</formula><p>Algorithm 2 Computing the LS2T layer with recursive tensors across levels Save Y m ? R[:, :, ] 8: end for 9: Output:</p><formula xml:id="formula_94">1: Input: Sequences (x j ) j=1,...,nx = (x j 1 , . . . , x j L ) j=1,...,nx ? Seq(R d ), rank-1 tensors ( k ) k=1,...,n = (z k 1 ? ? ? ? ? z k m ) k=1,...,n m=1,...,M ? T (R d ), LS2T order M ? N 2: Compute A[m, i, j, l] ? z j m , x i l for m ? {1, . . . , M }, i ? {1, . . . , n x }, j ? {1,</formula><formula xml:id="formula_95">Sequences (Y 1 , . . . , Y M ) each of shape (n x ? L ? n )</formula><p>Additionally, we collect all weight vectors z j m ? R d for a given m ? N into the matrix Z m := (z 1 m , . . . , z n m ) ? R n?d . Then, we may write the following vectorized version of equation 88:</p><formula xml:id="formula_96">h 1,i = h 1,i?1 + Z 1 x i , (89) h m,i = h m,i?1 + h m?1,i?1 Z m x i for m ? 2,<label>(90)</label></formula><p>with the initial conditions h m,0 = 0 ? R n for all m ? 1, and denoting the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ALGORITHMS</head><p>We have shown previously that one may compute ? ? (x 1 , . . . , x i ) = (? m,? (x 1 , . . . , x i )) m?0 recursively in a vectorized way for a given sequence (x 1 , . . . , x i ) ? Seq(R d ). Now, in Algorithms 1 and 2, we additionally show how to further vectorize the previous computations across time and the batch. For this purpose, let (x j ) j=1,...,n X ? Seq(R d ) be n X ? N sequences in R d and ( k ) k=1,...,n ? T (R d ) be n be rank-1 tensors in T (R d ).</p><p>Additionally, we adopted the notation for describing algorithms from Kir?ly &amp; Oberhauser (2019). For arrays, 1-based indexing is used. Let A and B be k-dimensional arrays with size (n 1 ?? ? ??n k ), and let i j ? {1, . . . , n j } for j ? {1, . . . , k}. Then, the following operations are defined:   8.1 ? 10 ?4 1.2 ? 10 ?1 1.7 ? 10 ?3 4.5 ? 10 ?3 9.9 ? 10 ?3 1.7 ? 10 ?3 2.5 ? 10 ?3 3.4 ? 10 ?3 64 8.5 ? 10 ?4 2.3 ? 10 ?1 1.8 ? 10 ?3 4.5 ? 10 ?3 9.9 ? 10 ?3 1.8 ? 10 ?3 2.6 ? 10 ?3 3.4 ? 10 ?3 128 9.7 ? 10 ?4 4.6 ? 10 ?1 2.1 ? 10 ?3 4.9 ? 10 ?3 1.0 ? 10 ?2 2.1 ? 10 ?3 2.9 ? 10 ?3 3.8 ? 10 ?3 256</p><p>1.1 ? 10 ?3 9.3 ? 10 ?1 2.4 ? 10 ?3 5.2 ? 10 ?3 1.1 ? 10 ?2 2.4 ? 10 ?3 3.2 ? 10 ?3 4.0 ? 10 ?3 512 1.3 ? 10 ?3 1.8 ? 10 0 3.2 ? 10 ?3 6.0 ? 10 ?3 1.1 ? 10 ?2 3.0 ? 10 ?3 4.0 ? 10 ?3 4.8 ? 10 ?3 1024 1.9 ? 10 ?3 3.7 ? 10 0 4.4 ? 10 ?3 7.1 ? 10 ?3 1.2 ? 10 ?2 4.4 ? 10 ?3 5.1 ? 10 ?3 6.0 ? 10 ?3 (iii) The shift along axis j by +m for m ? N:</p><formula xml:id="formula_97">A[. . . , :, +m, :, . . . ][. . . , i j?1 , i j , i j+1 , . . . ] := A[. . . , i j?1 , i j ? m, i j+1 , . . . ], if i j &gt; m, 0, if i j ? m.</formula><p>(iv) The Hadamard product of arrays A and B:</p><formula xml:id="formula_98">(A B)[i 1 , . . . , i k ] := A[i 1 , . . . , i k ] ? B[i 1 , . . . , i k ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 COMPLEXITY ANALYSIS</head><p>We give a complexity analysis of Algorithms 1 and 2. Inspection of Algorithm 1 says that it has O(M 2 ? n x ? L ? n ? d) complexity in both time and memory with an additional memory cost of storing the O(M 2 ? n ? d) number of parameters, the rank-1 elements ( k m ) m , which are stored in terms of their components z k m,j ? R d . In contrast, Algorithm 1 has a time and memory cost of O(M ? n x ? L ? n ? d), thus linear in M , and the recursive rank-1 elements are now only an additional O(M ? n ? d) number of parameters.</p><p>Additionally to the big-O bounds on complexities, another important question is how well the computations can be parallelized, which can have a larger impact on computations when e.g. running on GPUs. Observing the algorithms, we can see that they are not completely parallelizable due to the cumsum ( ) operations in Lines 6, 8 (Algorithm 1) and Lines 4, 6 (Algorithm 2). The cumulative sum operates recursively on the whole time axis, therefore it is not parallelizable, but can be computed very efficiently on modern architectures.</p><p>To gain further intuition about what kind of performance one can expect for our LS2T layers, we benchmarked the computation time of a forward pass for varying sequence lengths and varying hyperparameters of the model. For comparison, we ran the same experiment with an LSTM layer and a Conv1D layer with a filter size of 32. The input is a batch of sequences of shape (n X ? L ? d), while the output has shape (n X ? L ? h), where d ? N is the state-space dimension of the input sequences, while h ? N is simply the number of channels or hidden units in the layer. For our layers, we used our own implementation in Tensorflow, while for LSTM and Conv1D, we used the Keras implementation using the Tensorflow backend.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we report the average computation time of a forward pass over 100 trials, for fixed batch size n X = 32, state-space dimension d = 64, output dimension h = 64 and varying sequence lengths L ? {32, 64, 128, 256, 512, 1024}. LS2T and LS2T-R respectively refer to the independent and recursive variants, and M ? N denotes the truncation degree. We can observe that while the LSTM practically scales linearly in L, the scaling of LS2T is sublinear for all practical purposes, exhibiting a growth rate that is more close to that of the Conv1D layer, that is fully parallelizable. Specifically, while the LSTM takes 3.7 seconds to make a forward pass for L = 1024, all variants of the LS2T layer take less time than that by a factor of at least a 100. This suggests that its computations are highly parallelizable across time. Additionally, we observe that LS2T exhibits a more aggressive growth rate with respect to the parameter M due to the quadratic complexity in M (although the numbers show only linear growth), while LS2T-R scales very favourably in M as well due to the linear complexity (the results indicate a sublinear growth rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 INITIALIZATION</head><p>Below we detail the initialization procedure used by our models for the parameters ? of the LS2T layer, where ? = ( k ) n k=1 ? T (R d ). As before, each k = ( k m ) m?0 ? T (R d ) is given as a sequence of rank-1 tensors, such that k m = z k m,1 ? ? ? ? ? z k m,m with z k m,j ? R d for the independent variant, while k m = z k 1 ? ? ? ? ? z k m with z m ? R d for the recursive variant. Hence, by initialization, we mean the initialization of the components z k m,j or z k m . To find a satisfactory initialization scheme, we took as starting point the Glorot <ref type="bibr">(Glorot &amp; Bengio, 2010)</ref> initialization, which specifies that for a layer with input dimension n in and output dimension n out , the weights should be independently drawn from a centered distribution with variance 2/(n in + n out ), where the distribution that is used is usually a uniform or a Gaussian.</p><p>Independent variant. We first consider the independent variant of the algorithm. The weights are given as the rank-1 tensors k m = z k m,1 ? ? ? ? ? z k m,m ? (R d ) ?m for k = 1, . . . , n and m ? 0.</p><p>Denote z k m,j = (z k m,j,1 , . . . , z k m,j,d ) ? R d , and assume that for a given m ? N that each of z k m,j,p are drawn independently from some distribution with E[z k m,j,p ] = 0 and E[z k m,j,p ] 2 = ? 2 m for j = 1, . . . , m and p = 1, . . . , d.</p><p>Then, for a given multi-index i = (i 1 , . . . , i m ) ? {1, . . . , d} m , the ith coordinate of k m is given as</p><formula xml:id="formula_101">k m,i = z k m,1,i1 ? ? ? z k m,m,im<label>(93)</label></formula><p>and has as its first two moments</p><formula xml:id="formula_102">E[ k m,i ] = 0 and E[ k m,i ] 2 = ? 2m m<label>(94)</label></formula><p>due to the independence of the corresponding terms in the product. Therefore, to have E[ k m,i ] 2 = 2/(d m + n ), where we made the substitutions n in = d m and n out = n , we can simply set</p><formula xml:id="formula_103">? 2 m = m 2 d m + n .<label>(95)</label></formula><p>Recursive variant. In the recursive variant, the weights themselves are constructed recursively as k m = z k 1 ? ? ? ? ? z k m for k = 1, . . . , n and m ? 0.</p><p>Thus, for i = (i 1 , . . . , i m ) ? {1, . . . , d} m , the ith component of k m is given as</p><formula xml:id="formula_105">k m,i = z k 1,i1 ? ? ? z k m,im ,<label>(97)</label></formula><p>and if we assume that for a given m ? N, z k m,im is drawn from a centered distribution with variance ? 2 m , then we have E[ k m,i ] = 0 and E[ k m,i ] 2 = ? 2 1 ? ? ? ? 2 m ,</p><p>which means that now our goal is to have ? 2 1 ? ? ? ? 2 m = 2/(d m + n ) for all m ? 1, which is achievable inductively by</p><formula xml:id="formula_107">? 2 1 = 2 d + n and ? 2 m+1 = d m + n d m+1 + n for m ? 1.<label>(99)</label></formula><p>For both variants of the algorithms, we used the above described initialization schemes, where the tensor component z's were drawn from a centered uniform or a Gaussian distribution with the specified variances. Although the resulting weight tensors k m were of neither distribution, they had the pre-specified first two moments, that seemed sufficient to successfully train models with LS2T layers when combined with succeeding normalization layers as described in Appendix D.1.</p><p>However, we remark that when Glorot &amp; Bengio (2010) derived their weight initialization scheme, they considered a fully linear regime across layers, while for x = (x 1 , . . . , x L ) ? Seq(R d ), the features ? m (x 1 , . . . x L ) on which the weight tensors k m act will be highly nonlinear for any given m ? 2 with increasing levels of nonlinearity for larger values of m. Therefore, it is highly likely that better initialization schemes can be derived by studying the distribution of ? m (x), that might also make it possible to abandon the normalization layers succeeding the LS2T layers and still retain the layers' ability to learn relevant features of the data. Alternatively, data dependent initializations could also prove useful here, such as the LSUV initialization <ref type="bibr" target="#b20">(Mishkin &amp; Matas, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILS ON EXPERIMENTS</head><p>In the following, we give details on the time series classification (Appendix E.1), mortality prediction (Appendix E.2) and sequential data imputation (Appendix E.3) experiments. For running all experiments, we used GPU-based computations on a set of computing nodes, that were equipped with 11 NVIDIA GPUs in total: 4 Tesla K40Ms, 5 Geforce 2080 TIs and 2 Quadro GP100 GPUs. The benchmarks and code used to run the experiments using Tensorflow as backend are available at https://github.com/tgcsaba/seq2tens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 TIME SERIES CLASSIFICATION</head><p>Problem formulation. Classification is a traditional task in discriminative supervised machine learning: let X be the data space and Y = {1, . . . , c} the discrete output space that consists of only categorical values with c ? N the total number of classes. The problem is then to predict the corresponding labels of a set of unlabelled examples X = (</p><formula xml:id="formula_108">x i ) n X i=1 given a set of labelled examples (X, y) = (x i , y i ) n X i=1 ? X ? Y.</formula><p>In the context of time series classification (TSC), the data space X = Seq(R d ) is the space of multivariate sequences, i.e. x i = (x i,j ) lx i j=1 where l xi ? N is the length of the sequence x i that can change from instance from instance. Datasets. <ref type="table" target="#tab_6">Table 5</ref> details the datasets from <ref type="bibr" target="#b2">Baydogan (2015)</ref> that were used for the TSC experiment. The columns are defined as follows: n c denotes the number of classes, d the dimension of the state space, L x the range of sequence lengths, n X and n X respectively denote the number of examples in the prespecified training and testing sets. As preprocessing, the state space dimensions were normalized to zero mean and unit variance. From the experiment, we excluded the datasets LP1, LP2, . . . LP5, because all of these contain a very low number of training examples (n X &lt; 50 for 4 out of 5), and also a low signal-to-noise ratio (around 60%-80% accuracy achieved by non-DL, classic time series classifiers), which is arguably not the setting when deep learning becomes particularly relevant.</p><p>Baselines. The benefit of the multivariate TSC archive <ref type="bibr" target="#b2">(Baydogan, 2015)</ref> is that there exist several publications which report the test set results of their respective TSC models, which makes it possible to directly compare against them. We included all results among the comparison that we are aware of: DTW i <ref type="bibr" target="#b29">(Sakoe &amp; Chiba, 1978)</ref>, ARKernel <ref type="bibr">(Cuturi &amp; Doucet, 2011)</ref>, SMTS (Baydogan &amp; Runger, 2015a), LPS <ref type="bibr" target="#b4">(Baydogan &amp; Runger, 2015b</ref><ref type="bibr">), gRSF (Karlsson et al., 2016</ref><ref type="bibr">), mvARF (Tuncel &amp; Baydogan, 2018</ref>, MUSE <ref type="bibr" target="#b31">(Sch?fer &amp; Leser, 2017)</ref>, <ref type="bibr">MLSTMFCN (Karim et al., 2019)</ref>.</p><p>Additionally, a recent survey paper <ref type="bibr">(Ismail Fawaz et al., 2019)</ref> benchmarked a range of DL models for TSC, however, they only considered a subset of these multivariate datasets. Therefore, so as to have results across the whole archive, we borrow the two strongest models as baselines, FCN and ResNet, and train them across the whole archive. In fact, we also chose the FCN as the base model to upgrade with LS2T layers, specifically for its strong performance and simplicity, since FCN is a vanilla CNN model consisting of three temporal convolution layers with kernel sizes (8, 5, 3) and filters <ref type="bibr">(128,</ref><ref type="bibr">256,</ref><ref type="bibr">128)</ref>, where each layer is succeeded by batch normalization and relu activation. We denote this as FCN 128 , while FCN h refers to an FCN with filters (h, 2h, h). The ResNet is a more complicated, residual network <ref type="bibr">(He et al., 2016)</ref> consisting of three FCN blocks of widths <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">128)</ref> with skip-connections in-between, where the width of the convolutional layers in each FCN block are now uniform (hence, the middle convolutional layer also has h filters rather than 2h). For more details, refer to Ismail Fawaz et al. <ref type="formula" target="#formula_0">(2019)</ref>; <ref type="bibr" target="#b43">Wang et al. (2017)</ref>. Also note that for MLSTMFCN the same results are reported as the ones in <ref type="bibr" target="#b31">Sch?fer &amp; Leser (2017)</ref>.</p><p>Architecture. The structure of a generic FCN h -LS2T d n model of FCN width-h, LS2T width-n, LS2T-depth d and LS2T-order m is visualized on <ref type="figure" target="#fig_7">Figure 2</ref>, where the FCN block is additionally augmented with a time-embedding preceding each convolution compared to the vanilla FCN, while the deep LS2T block uses both time-embedding and difference layers before each LS2T layer. The usefulness of these is discussed in Appendix D.1. There is a shortcut connection coming from the INPUT layer, that is added to the input of the first TIME + DIFF layer in the LS2T block, allowing the first LS2T layer to access the input data additionally to the FCN output. Also, there is another shortcut connection coming from the output of the FCN block and added to the output of the LS2T block, which allows the FCN to directly affect the classification performance, hence, allowing the LS2T block to focus on learning global temporal interactions with the localized interactions coming from the FCN block. Both skip-connections use a time-distributed linear projection layer to match the dimension of the residual branch, and the shortcut from the FCN output also uses a GAP layer to pool over the time axis before being added to the final output that the classification layer receives. 3.5 ? 10 4 1.5 ? 10 3 FCN64-LS2T 3 64 1.2 ? 10 5 2.4 ? 10 3 FCN128 2.7 ? 10 5 3.5 ? 10 3 FCN128-LS2T 3 64 3.4 ? 10 5 3.7 ? 10 3 RESNET 5.2 ? 10 5 2.4 ? 10 3</p><p>Parameter comparison. <ref type="table" target="#tab_7">Table 6</ref> depicts the median number of trainable parameters for the models considered by us in this experiment and their median absolute deviation. While the smallest model, LS2T 3 64 , has about the third of the parameters as FCN 64 -LS2T 3 64 due to the added FCN 64 block in the latter, FCN 64 -LS2T 3 64 still has about half as many parameters as FCN 128 as it is a much thinner network. On the other hand, FCN 128 -LS2T 3 64 uses an FCN 128 block with a LS2T 3 64 block on top and additional skip-connections, so it is not surprising that its number of parameters are between FCN 128 and ResNet, with ResNet being the largest model due to it being a residual network of three FCN blocks of various sizes. At the same time, parameter counting might not be a good proxy for measuring the size of deep learning models generally <ref type="bibr" target="#b19">(Maddox et al., 2020)</ref>, and even more so when the layer types constituting the different models also vary additionally to the number of parameters.</p><p>Training details. For the training of all models, an ADAM optimizer <ref type="bibr">(Kingma &amp; Ba, 2015)</ref> was used with an initial learning rate of ? = 1?10 ?3 , and we employed a learning rate decay of ? = 1/2 after 100 epochs of no improvement in the training loss, and stopping early after no improvement over 500 epochs in the loss, after which the lowest loss parameter set was used. The maximum number of epochs for all were set to 2000, except for ResNet it was set to 1500, since that is what <ref type="bibr">Ismail Fawaz et al. (2019)</ref> used. The batch size was set to b = max(min(0.1?n X , b max ), b min ), where for the models LS2T 3 64 , FCN 128 , FCN 64 -LS2T 3 64 , FCN 128 -LS2T 3 64 the values were b max = 16 and b min = 4, for ResNet b max = 64 and b min = 4 were used. This is also the same setting as how FCN 128 and ResNet were trained in Ismail <ref type="bibr">Fawaz et al. (2019)</ref> with the exception that they did not 1.000 0.930 1.000 0 .997 1.000 1.000 1.000 1.000 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000) DIGITSHAPES 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000 <ref type="formula" target="#formula_89">(</ref>  <ref type="figure" target="#fig_1">Figure 3</ref>: Box-plot of classification accuracies (left) and critical difference diagram (right). cap the batch size at b min = 4 that for small datasets (n X &lt; 40) made their training unstable, which is why on some of these datasets our baselines, FCN 128 and ResNet are stronger. We also remark that before we introduced the skip-connections in the FCN-LS2T architecture <ref type="figure" target="#fig_7">(Figure 2)</ref>, training was considerably more unstable for this model, and at that point, using the SWATS optimizer <ref type="bibr">(Keskar &amp; Socher, 2017)</ref> in place of ADAM could provide some improvements on the results; while after upgrading the architecture, changing the optimizer did not seem to make a difference. <ref type="table" target="#tab_8">Table 7</ref>, where for the models that we trained ourselves, i.e. FCN 128 , ResNet, LS2T 3 64 , FCN 64 -LS2T 3 64 , FCN 128 -LS2T 3 64 , we report the mean and standard deviation of test set accuracies over 5 model trains. <ref type="figure" target="#fig_1">Figure 3</ref> depicts the box-plot distributions of classification accuracies and the corresponding critical difference (CD) diagram. The CD diagram depicts the mean ranks of each method averaged over datasets with a calculated CD region using the Nemenyi test <ref type="bibr" target="#b24">(Nemenyi, 1963)</ref> for an alpha value of ? = 0.1. For the Bayesian signed-rank test <ref type="bibr" target="#b5">(Benavoli et al., 2014)</ref>, we used the implementation from https://github.com/janezd/baycomp, and the resulting posterior probabilities are compared in <ref type="table" target="#tab_1">Table 1</ref>, Section 5.1. The posterior distributions themselves are visualized on <ref type="figure" target="#fig_8">Figure 4</ref> The region of practical equivalence (rope) was set to rope = 1 ? 10 ?3 , that is, two accuracies were practically equivalent if they are at most the given distance from each other. For the visualizations and computation of probabilities, the posteriors were evaluated using n = 10 5 Monte Carlo samples. Problem formulation. Mortality prediction in healthcare is a form of binary classification using medical datasets. This kind of data is very heterogenous with the input space being a combination of dynamic and static features, i.e. X = Seq(R d ) R e , and the class distributions often also being highly imbalanced. Also among the dynamic features there can often be missing values, in fact they are usually only observed very sparsely. Hence, it is not guaranteed that for a time series x ? Seq(R d ), all coordinates of an observation x i,tj are observed for a given time-point t j . In other words, we are given for every x i ? Seq(R d ) an additional observation mask m i = (m i,tj ) Li j=1 ? Seq <ref type="figure">({0, 1} d )</ref>, that specifies whether a given coordinate of x i was observed at time t j or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The full table of results is in in</head><p>Dataset. We consider the PHYSIONET2012 dataset for this task, that consists of medical time series of 12,000 ICU stays over at least 48 hours. Overall, 6 static features were recorded and potentially up to 37 TS values were measured both irregularly and asynchronously for each stay with certain dimensions completely missing in some cases. The task is to predict the in-hospital mortality of patients during their hospital stay. From an ML point of view, this is a difficult dataset due missing values, low signal-to-noise ratio, and imbalanced class distributions with a prevalence ratio of around 14%. For comparability of results, we use the same train-val-test split as in Horn et al. Preprocessing. For missing TS values, we use a three-step imputation method: (1) compute the mean value of each dynamic feature across the training dataset, (2) if a TS value is missing at time t = 0, impute it with the mean, (3) for missing TS values at time t &gt; 0, use forward imputation on a roll-forward basis. We remark that it would have also been possible to use the GP-VAE imputation from the following experiment, but our aim was to keep the two experiments separate, in particular, to allow fair comparability with the results in Horn et al. <ref type="bibr">(2020)</ref>. Furthermore, we make the information about missing values available to the model using the augmentation defined in <ref type="bibr">Che et al. (2018, eq. 9)</ref>, which consists of adding as extra coordinates the observation mask and the number of time steps elapsed since an observation was made, both for each dynamic feature. The static features are handled by tiling along the time axis and adding them as extra coordintaes. Finally, all static and dynamic features are normalized to zero mean and unit variance using the training set statistics.</p><p>Baselines. As baselines, we use the experiments conducted in <ref type="bibr">Horn et al. (2020)</ref>, that includes SOTA architectures for irregularly sampled data such as their SEFT-ATTN, GRU-D <ref type="bibr" target="#b14">(Che et al., 2018)</ref>, IP-NETS (Shukla &amp; Marlin, 2019), PHASED-LSTM <ref type="bibr" target="#b23">(Neil et al., 2016)</ref>, TRANSFORMER <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> and LATENT-ODE <ref type="bibr" target="#b27">(Rubanova et al., 2019)</ref>. Together these methods make up a very strong baseline to compare against. However, the main question for us still is whether we can improve on the vanilla FCN model with the FCN-LS2T architecture <ref type="figure" target="#fig_7">(Figure 2)</ref>, since our aim is simply to demonstrate that LS2T layers can serve as useful building blocks in deep learning models via their ability to capture non-local interactions in heterogeneous time series and sequences.</p><p>Hyperparameter selection. We train two models on this task, FCN and FCN-LS2T. To keep the experiment fair, we align with the experimental setting in Horn et al. (2020) and follow their hyperparameter selection procedure using randomized search. For both models, we uniformly sample 20 hyperparameter settings from the hyperparameter grid specified as follows: (1) for FCN-LS2T, the FCN width from h ? {64, 128, 256}. the LS2T width from n ? {64, 128, 256}, the LS2T order from m ? {2, 3, 4}, LS2T depth from d ? {1, 2, 3} and whether to use the recursive or independent LS2T formulation (see Appendix D.2); (2) for FCN, the width from h ? {64, 128, 256}; (3) for both models, we sample the dropout preceding the classification layer from r 1 ? {0.0, 0.1, 0.2, 0.3, 0.4}, the spatial dropout that follows all convolutional and LS2T layers from r 2 ? {0.0, 0.1, 0.2, 0.3, 0.4}. For training, we also sample for both models the batch size used from b ? {4, 8, 16, 32} and the initial learning rate from ? ? {1 ? 10 ?3 , 5 ? 10 ?4 , 2.5 ? 10 ?4 , 1 ? 10 ?4 }. For both architectures, we train a model for each of the 20 hyperparameter settings, and then select the setting which provides the best performance on the validation set. The best model is selected by computing a composite z-score on the validation set, which consists of computing a z-score across the realizations for each metric, that is, ACCURACY, AUPRC, AUROC, and then taking a sum of these z-scores.</p><p>Training details. Last but not least, we specify the training methodology. Similarly to <ref type="bibr">Horn et al. (2020)</ref>, rather than utilizing class weights during training to deal with unbalanced class distributions, we use a generator which feeds balanced batches to the model during each training iteration. This approach is more beneficial for small batch training on such heavily unbalanced datasets, such as the current one. Then, we define an epoch as the number of training iterations required to see all examples from the class with the lowest prevalence ratio. The maximum number of epochs is set to 200, and we stop early after 50 epochs of no improvement over the validation AUPRC, after which the model is restored to the best parameter set according to this metric. We also employ a learning rate decay of ? = 1/2 after 10 epochs of no improvement and only until the learning rate reaches ? min = 1 ? 10 ?4 . Clearly, using the same validation set for early stopping and selecting the hyperparameters introduces a bias in the model selection, however, this is partially remedied by using for hyperparameter selection a composite of three metrics, rather than just the AUPRC.</p><p>Evaluation. After selecting the best hyperparameters, we independently train and evaluate on the test set each model 5 times. The means and standard deviations of the resulting performance metrics over these 5 model trains are what displayed in <ref type="table" target="#tab_2">Table 2</ref>. The best found hyperparameter settings are the following: (1) for FCN-LS2T, FCN width h = 64, LS2T width n = 256, LS2T order m = 3, LS2T depth d = 3, recursive formulation, dropout r 1 = 0.3, spatial dropout r 2 = 0.4, batch size b = 32, inital learning rate ? = 1 ? 10 ?4 ; (2) for FCN, FCN width h = 256, dropout r 1 = 0.4, spatial dropout r 2 = 0.3, batch size b = 4, initial learning rate ? = 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 GENERATIVE SEQUENTIAL DATA IMPUTATION</head><p>Problem formulation. Imputation of sequential data can be formulated as a problem of generative unsupervised learning. The input space is given as X = Seq(R d ) and we are given a number of ex-</p><formula xml:id="formula_109">amples X = (x i ) n X i=1 ? Seq(R d ) with x i = (x i,tj ) Li j=1 .</formula><p>Similarly to before, there are missing values in the input sequences, that is, we are given for every <ref type="figure">1} d )</ref>, that specifies whether a given coordinate of x i was observed at time t j or not. The task in this case is specifically to model the distribution of the unobserved coordinates given the observed coordinates potentially at different time-points.</p><formula xml:id="formula_110">x i ? Seq(R d ) an additional observation mask m i = (m i,tj ) Li j=1 ? Seq({0,</formula><p>Model details. We expand on the GP-VAE (Fortuin et al., 2020) model in details. Let x = (x i ) i=1,...,L ? Seq(R d ) be a sequence of length L ? N. The model assumes that x is noisily generated time-point-wise conditioned on discrete-time realizations of a latent process denoted by</p><formula xml:id="formula_111">z = (z i ) i=1,...,L ? Seq(R d ), p ? (x i |z i ) = N (x i | g ? (z i ), ? 2 I d ),<label>(100)</label></formula><p>where g ? : R d ? R d is the time-point-wise decoder network parametrized by ?, while ? 2 ? R is the observation noise variance. The temporal interdependencies are modelled in the latent space by assigning independent Gaussian process (GP) priors <ref type="bibr" target="#b44">(Williams &amp; Rasmussen, 2006)</ref> to the coordinate processes of z, i.e. denoting z i = (z j i ) j=1,...,d ? R d , it is assumed that z j ? GP(m(?), k(?, ?)), where m : R ? R and k : R ? R ? R are the mean and covariance functions. The authors propose to use the Cauchy kernel as covariance function, defined as</p><formula xml:id="formula_112">k(?, ? ) =? 2 1 + (? ? ? ) 2 l 2 ?1 ,<label>(101)</label></formula><p>which can be seen as an infinite mixture of RBF kernels, allowing one to model temporal dynamics on multiple length scales. For the variational approximation <ref type="bibr" target="#b9">(Blei et al., 2017;</ref><ref type="bibr" target="#b46">Zhang et al., 2018)</ref>, an amortized Gaussian (Gershman &amp; Goodman, 2014) is used that factorizes across the latent space dimensions, but not across the observation times:</p><p>q ? (z 1 , . . . , z L | x 1 , . . . x L ) = q ? (z 1 1 , . . . z 1 L | x 1 , . . . , x L ) ? ? ? , q ? (z d 1 , . . . z d L | x 1 , . . . x L ) (102) = N (z 1 1 , . . . , z 1 L | m 1 , A 1 ) ? ? ? N (z d 1 , . . . , z d L | m d , A d ),</p><p>where m j ? R L are the posterior means and A j ? R L?L are the posterior covariance matrices for j = 1, . . . , d . In general, estimating the full covariance matrices A j ? R L?L from a single data example x = (x 1 , . . . , x L ) ? Seq(R d ) is an ill-posed problem. To circumvent the curse of dimensionality in the matrix estimation while allowing for long-range correlations in time, a structured precision matrix representation is used, such that A ?1 j = B j B j with B j ? R L?L a lower bidiagonal matrix, such as in <ref type="bibr">Dorta et al. (2018)</ref>; <ref type="bibr" target="#b8">Blei &amp; Lafferty (2006)</ref>; <ref type="bibr" target="#b1">Bamler &amp; Mandt (2017)</ref>, which results in a tridiagonal precision matrix and a potentially dense covariance matrix.</p><p>Training across the whole dataset X = (x i ) n X i=1 ? Seq(R d ) is coupled through the decoder and encoder parameters ? and ?, and the ELBO is computed as</p><formula xml:id="formula_114">1 n X n X i=1 log p(x i ) ? 1 n X n X i=1 Li j=1 E q ? (zi,j | xi) [log p ? (x i,j | z i,j )] (104) ? ?D KL q ? (z i | x i )p(z i )) ,<label>(105)</label></formula><p>where the log-likelihood term is only computed across observed features as was done in <ref type="bibr" target="#b22">Nazabal et al. (2018)</ref>. Similarly to ?-VAEs <ref type="bibr">(Higgins et al., 2017)</ref>, ? is used to rebalance the KL term, now to account for the missingness rate.</p><p>Baselines. Additionally to the baseline GP-VAE, the reported baseline results are the same ones as in Fortuin et al. <ref type="formula" target="#formula_0">(2020)</ref>, which are mean imputation, forward imputation, VAE <ref type="bibr">(Kingma &amp; Welling, 2013)</ref>, HI-VAE <ref type="bibr" target="#b22">(Nazabal et al., 2018)</ref> and BRITS <ref type="bibr" target="#b12">(Cao et al., 2018)</ref>. Among these, the VAE based models are Bayesian and provide a probability measure on possible imputations, while the mean/forward imputation methods and the RNN based BRITS only provide a single imputation. Datasets. <ref type="table" target="#tab_10">Table 8</ref> details the datasets used, which are the same ones as considered in <ref type="bibr">Fortuin et al. (2020)</ref>. The columns are defined as: n c ? N denotes the number of classes if the dataset is labelled, m ? (0, 1) denotes ratio of missing data, d ? N denotes the state space dimension of sequences, L x ? N denotes the sequence length, n X , n X ? N denote the number of examples in the respective training and testing sets. For Sprites no labels are available, while for Physionet all examples are in the training set and no ground truth values are available. For HMNIST, the MNAR version was used, the most difficult missingness mechanism (Fortuin et al., 2020). Experiment details. As depicted in <ref type="figure" target="#fig_10">Figure 5</ref>, the difference between the original GP-VAE model and ours is that is that we additionally employ a single bidirectional Seq2Tens block (B-LS2T) in the encoder network following the convolutional layer, but preceding the time-distributed dense layers. The motivation for this is that the original encoder only takes local sequential structure into account using the convolutional layer. Hence, it does not exploit global sequential information, which might limit the expressiveness of the encoder network. This limitation can lead to suboptimal inference, due to the fact that the encoder is not able to represent a rich enough subset of the variational family of distributions. This is called the amortization gap in the literature <ref type="bibr">(Cremer et al., 2018)</ref>.</p><p>We have thus hypothesized that by incorporating a bidirectional LS2T layer into the model that takes sequential structure into account not only locally, but globally, we can improve the expressiveness of the encoder network, that can in turn improve on the variational approximation. However, it should be noted that according to the findings of Cremer et al. (2018), a larger encoder network can potentially result in the variational parameters being overfitted to the training data, and can degrade the generalization on unseen data examples. Therefore, the main question is whether increasing the expressiveness of the encoder will improve the quality of the variational approximation on both seen and unseen examples, or will it lead to overfitting to the seen examples?</p><p>Another interesting question that we have not considered experimentally, but could lead to improvements is the following. The time-point-wise decoder function assumes that d ? N is large enough, so that z ? Seq(R d ) is able to fully represent x ? Seq(R d ) in a time-point-wise manner including its dynamics. Although in theory the GP prior should be able to learn the temporal dynamics in the latent space, this might again only be possible for a large enough latent state size d . In practice, it could turn out to be more efficient to use some of the contextual information in the decoder network as well, either locally, using e.g. a CNN, or globally, using e.g. LS2T layers or RNNs/LSTMs. Implementation. For the implementation of the GP-VAE, we used the same one as in Fortuin et al. <ref type="formula" target="#formula_0">(2020)</ref>, which implements it using Keras and Tensorflow. The bidirectional LS2T layer used our own implementation based on the same frameworks. The hyperparameters of the models, which are depicted in Appendix A in Fortuin et al. <ref type="formula" target="#formula_0">(2020)</ref>, were left unchanged. The only change we concocted is the B-LS2T layer in the encoder network as depicted in <ref type="figure" target="#fig_10">Figure 5</ref>. The width of the B-LS2T layer was set to be the same as the convolutional layer, and M = 4 tensor levels were used. The parametrization of the low-rank LS2T layer used the independent formulation as detailed in Appendix D.2.</p><p>We also made a simple change to how the data is fed into the encoder. In the original model, the missing values were imputed with 0, while we instead used the forward imputed values. This was necessary due to the difference operation preceding the B-LS2T layer in <ref type="figure" target="#fig_10">Figure 5</ref>. With the zero imputation, the coordinates with missing values exhibited higher oscillations after differencing, while with forward imputation the missing values were more well-behaved. A simple way to see this is that, if there were no preprocessing and convolutional layers in <ref type="figure" target="#fig_10">Figure 5</ref> preceding the difference block, then this step would be equivalent to imputing the missing values with zero after differencing.</p><p>Result details. <ref type="table" target="#tab_3">Table 3</ref> shows the achieved performance on the datasets with our upgraded model, GP-VAE (B-LS2T), compared against the original GP-VAE (Fortuin et al., 2020) and the baselines. The reported results are negative log-likelihood (NLL), mean squared error (MSE) and AUROC on HMNIST, while on Sprites the MSE is reported and on Physionet the AUROC score. As Sprites is unlabeled, downstream classification performance (AUROC) is undefined on this dataset, while Physionet does not have ground truth values for the missing entries, and reconstruction error (MSE,NLL) is not defined. The only missing entry is Sprites NLL, which was omitted to preserve space. We observe that increasing the expressiveness of the encoder did manage to improve the results on HM-NIST and Physionet. The only case where no improvement was observable is Sprites, where the GP-VAE already achieved a very low MSE score of M SE = 2 ? 10 ?3 .</p><p>To gain some intuition whether the lack of improvement on Sprites was due to the GP-VAE's performance already being maxed out, or there was some other pathology in the model, we further investigated the Sprites dataset and found a bottleneck in both the original and enhanced GP-VAE models. Due to the high dimensionality of the state space of input sequences, d = 12288, the width of the first convolutional layer in the encoder network was set to h = 32 in order to keep the number of parameters in the layer manageable and be able to train the model with a batch size of n = 64, while all subsequent layers had a width of h = 256. Thus, to see if this was indeed an information bottleneck, we increased the convolution width to h = 256 and decreased the batch size to n = 16, with all other hyperparameters unchanged. Then, we trained using this modification both the baseline GP-VAE and our GP-VAE (B-LS2T) five times on the Sprites datasets. The achieved MSE scores were (i) GP-VAE (base): M SE = 1.4 ? 10 ?3 ? 4.1 ? 10 ?5 , (ii) GP-VAE (B-LS2T): M SE = 1.3?10 ?3 ?4.9?10 ?5 . Therefore, the smaller convolutional layer was indeed causing an information bottleneck, and by increasing its width to be on par with the other layers in the encoder, we managed to improve the performance of both models. The improvement on the GP-VAE (B-LS2T) was larger, which can be explained by the observation that lifting the information bottleneck additionally allowed the benefits of the B-LS2T layer to kick in, as detailed previously.</p><p>To sum up, we have empirically validated the hypothesis that capturing global information in the encoder was indeed beneficial, and managed to improve on the results even on unseen examples in all cases. The experiment as a whole supports that our introduced LS2T layers can serve as useful building blocks in a wide range of models, not only discriminative, but also generative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing</head><p>Reconstructed Original <ref type="figure">Figure 6</ref>: Reconstructions from the Sprites dataset with the images with missingness (top), reconstructed (middle) and original (bottom).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>iii) By taking ? : R d ? T(V ), ? 1 (x) the polynomial map and ? m (x) = 0 for m ? 2 one recovers the iterated sums of Diehl et al. (2019) and Kir?ly &amp; Oberhauser (2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>B. 3</head><label>3</label><figDesc>SEQ2TENS MAKES SEQUENCES OF DIFFERENT LENGTH COMPARABLE The simplest kind of a sequence is a string, that is a sequence of letters. Strings are determined by (i) what letters appear in them, (ii) in what order the letters appear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>T ) of length m and thus comparing ? m (x) and ? m (y) is meaningful even when x and y are of different length. It is instructive to spell out in detail how m-mers are a special case of Seq2Tens, Example B.9, and how it generalizes, Example B.11. Example B.9. Let X = {a, b, c} and ? : X ? V = R 3 defined by mapping a, b, c ? X to the unit vectors e 1 , e 2 , e 3 ? V , so that ?(a) = (1, e 1 , 0, 0, ...) ? T(V ), ?(b) = (1, e 2 , 0, . . .) ? T(V ), and ?(c) = (1, e 3 , 0, . . .) ? T(V ). For the sequence x = (a, a, b, c) ? Seq(X ) we get ?(x) =?(a)?(a)?(b)?(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. . . , L} and k ? {1, . . . , m} 3: for m = 1 to M do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. . . , n } and l ? {1, . . . , L} 3: Assign R ? A[1, :, :, :] 4: Save Y 1 ? R[:, :, ] 5: for m = 2 to M do 6: Update R ? A[m, :, :, :] R[:, :, + 1] 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(i) The cumulative sum along axis j: A[. . . , :, , :, . . . ][. . . , i j?1 , i j , , i j+1 . . . ] := ij ?=1 A[. . . , i j?1 , ?, i j+1 , . . . ]. (ii) The slice-wise sum along axis j: A[. . . , :, ?, :, . . . ][. . . , i j?1 , i j+1 , . . . ] := nj ?=1 A[. . . , i j?1 , ?, i j+1 , . . . ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Depiction of the models used for time series classification. LS2T 3 only consists of a deep LS2T block (yellow), while FCN-LS2T 3 also precedes it with an FCN block (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Posterior distribution plots of pairwise Bayesian signed-rank test comparisons E.2 MORTALITY PREDICTION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(2020) available at https://github.com/ExpectationMax/medical_ts_datasets, where the 12,000 examples were split in a ratio of 64-16-20. Additionally, examples not containing any TS information were excluded from the dataset, for a list of these see Horn et al. (2020, App. A.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Encoder in GP-VAE (B-LS2T).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>T(V ) consisting of M tensors m ? V ?m , yields a linear functional on T(V ); e.g. if V = R d and we identify m in coordinates as m = ( i1,...,im m ) i1,...,im?{1,...,d} then</figDesc><table><row><cell>M</cell><cell>M</cell><cell></cell><cell></cell></row><row><cell>, t :=</cell><cell>m , t m =</cell><cell>i1,...,im m</cell><cell>t i1,...,im</cell></row><row><cell>m=0</cell><cell>m=0 i1,...,im?{1,...,d}</cell><cell></cell><cell></cell></row></table><note>m .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Posterior probabilities given by a Bayesian signed-rank test comparison of the proposed methods against the baselines. {&gt;}, {&lt;}, {=} refer to the respective events that the row method is better, the column method is better, or that they are equivalent.</figDesc><table><row><cell>MODEL</cell><cell></cell><cell>LS2T 3 64</cell><cell></cell><cell cols="3">FCN64-LS2T 3 64</cell><cell cols="3">FCN128-LS2T 3 64</cell></row><row><cell></cell><cell>p(&gt;)</cell><cell>p(=)</cell><cell>p(&lt;)</cell><cell>p(&gt;)</cell><cell>p(=)</cell><cell>p(&lt;)</cell><cell>p(&gt;)</cell><cell>p(=)</cell><cell>p(&lt;)</cell></row><row><cell>SMTS (BAYDOGAN &amp; RUNGER, 2015A)</cell><cell cols="9">0.180 0.000 0.820 0.010 0.000 0.990 0.008 0.000 0.992</cell></row><row><cell>LPS (BAYDOGAN &amp; RUNGER, 2015B)</cell><cell cols="9">0.191 0.002 0.807 0.012 0.001 0.987 0.006 0.001 0.993</cell></row><row><cell>MVARF (TUNCEL &amp; BAYDOGAN, 2018)</cell><cell cols="9">0.011 0.140 0.849 0.000 0.126 0.874 0.000 0.088 0.912</cell></row><row><cell>DTW (SAKOE &amp; CHIBA, 1978)</cell><cell cols="9">0.033 0.000 0.967 0.001 0.000 0.999 0.000 0.000 1.000</cell></row><row><cell>ARKERNEL (CUTURI &amp; DOUCET, 2011)</cell><cell cols="9">0.100 0.097 0.803 0.000 0.021 0.979 0.000 0.015 0.985</cell></row><row><cell>GRSF (KARLSSON ET AL., 2016)</cell><cell cols="9">0.481 0.011 0.508 0.028 0.013 0.960 0.022 0.013 0.965</cell></row><row><cell>MUSE (SCH?FER &amp; LESER, 2017)</cell><cell cols="9">0.405 0.128 0.467 0.001 0.074 0.925 0.001 0.077 0.922</cell></row><row><cell>MLSTMFCN (KARIM ET AL., 2019)</cell><cell cols="9">0.916 0.043 0.041 0.123 0.071 0.807 0.055 0.110 0.835</cell></row><row><cell>FCN128 (WANG ET AL., 2017)</cell><cell cols="9">0.998 0.002 0.000 0.363 0.186 0.451 0.169 0.011 0.820</cell></row><row><cell>RESNET (WANG ET AL., 2017)</cell><cell cols="9">0.998 0.002 0.001 0.056 0.240 0.704 0.016 0.048 0.935</cell></row><row><cell>LS2T 3 64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">0.000 0.001 0.999 0.000 0.001 0.999</cell></row><row><cell>FCN64-LS2T 3 64</cell><cell cols="3">0.999 0.001 0.000</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.020 0.387 0.593</cell></row><row><cell cols="10">archive, which makes it possible to compare against several well-performing competitor methods</cell></row><row><cell cols="10">from the TSC community. These baselines are detailed in Appendix E.1. This archive was also</cell></row><row><cell cols="10">considered in a recent popular survey paper on DL for TSC (Ismail Fawaz et al., 2019), from where</cell></row><row><cell cols="10">we borrow the two best performing models as DL baselines: FCN and ResNet. The FCN is a</cell></row><row><cell cols="10">fully convolutional network which stacks 3 convolutional layers of kernel sizes (8, 5, 3) and filters</cell></row><row><cell cols="10">(128, 256, 128) followed by a global average pooling (GAP) layer, hence employing global param-</cell></row><row><cell cols="10">eter sharing. We refer to this model as FCN 128 . The ResNet is a residual network stacking 3 FCN</cell></row><row><cell cols="10">blocks of various widths with skip-connections in between (He et al., 2016) and a final GAP layer.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FCN-LS2T and FCN on PHY-SIONET2012 with the results from Horn et al. (2020).</figDesc><table><row><cell>MODEL</cell><cell>ACCURACY</cell><cell>AUPRC</cell><cell>AUROC</cell></row><row><cell>FCN-LS2T</cell><cell>84.1 ? 1.6</cell><cell>53.9 ? 0.5</cell><cell>85.6 ? 0.5</cell></row><row><cell>FCN</cell><cell>80.7 ? 1.7</cell><cell>52.8 ? 1.3</cell><cell>85.6 ? 0.2</cell></row><row><cell>GRU-D</cell><cell>80.0 ? 2.9</cell><cell>53 .7 ? 0 .9</cell><cell>86.3 ? 0.3</cell></row><row><cell>GRU-SIMPLE</cell><cell>82.2 ? 0.2</cell><cell>42.2 ? 0.6</cell><cell>80.8 ? 1.1</cell></row><row><cell>IP-NETS</cell><cell>79.4 ? 0.3</cell><cell>51.0 ? 0.6</cell><cell>86 .0 ? 0 .2</cell></row><row><cell>PHASED-LSTM</cell><cell>76.8 ? 5.2</cell><cell>38.7 ? 1.5</cell><cell>79.0 ? 1.0</cell></row><row><cell>TRANSFORMER</cell><cell>83 .7 ? 3 .5</cell><cell>52.8 ? 2.2</cell><cell>86.3 ? 0.8</cell></row><row><cell>LATENT-ODE</cell><cell>76.0 ? 0.1</cell><cell>50.7 ? 1.7</cell><cell>85.7 ? 0.6</cell></row><row><cell>SEFT-ATTN.</cell><cell>75.3 ? 3.5</cell><cell>52.4 ? 1.1</cell><cell>85.1 ? 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of GP-VAE (B-LS2T) with the baseline methods</figDesc><table><row><cell>METHOD</cell><cell></cell><cell>HMNIST</cell><cell></cell><cell>SPRITES</cell><cell>PHYSIONET</cell></row><row><cell></cell><cell>NLL</cell><cell>MSE</cell><cell>AUROC</cell><cell>MSE</cell><cell>AUROC</cell></row><row><cell>MEAN IMPUTATION</cell><cell>-</cell><cell>0.168 ? 0.000</cell><cell>0.938 ? 0.000</cell><cell>0.013 ? 0.000</cell><cell>0.703 ? 0.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning. arXiv preprint arXiv:1603.03788, 2016.Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, and Danilo P Mandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. Foundations and Trends? in MachineLearning,5):249-429, 2016. Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Conference on learning theory, pp. 698-728, 2016. Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In Proceedings of the 35th International Conference on Machine Learning, pp. 1078-1086, 2018. PhysioBank, PhysioToolkit, and Physionet, 2000. Benjamin Graham. Sparse arrays of signatures for online character recognition. arXiv preprint arXiv:1308.0371, 2013. Alex Graves and J?rgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602 -610, 2005. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645-6649. IEEE, 2013b. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016. I. Higgins, Lo?c Matthey, A. Pal, C. Burgess, Xavier Glorot, M. Botvinick, S. Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017. Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt. Set functions for time series. In ICML, 2020. Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-fcns for time series classification. Neural Networks, 116:237 -245, 2019. ISSN 0893-6080. Isak Karlsson, Panagiotis Papapetrou, and Henrik Bostr?m. Generalized random shapelet forests. Data Min. Knowl. Discov., 30(5):1053-1085, September 2016. ISSN 1384-5810. Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to SGD. arXiv preprint arXiv:1712.07628, 2017. Valentin Khrulkov, Oleksii Hrinchuk, and Ivan Oseledets. Generalized tensor models for recurrent neural networks. arXiv preprint arXiv:1901.10801, 2019. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Franz J Kir?ly and Harald Oberhauser. Kernels for sequentially ordered data. Journal of Machine Learning Research, 2019. Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Tensor regression networks. arXiv preprint arXiv:1707.08308, 2017. Serge Lang. Algebra. Springer-Verlag New York, 2002. C Leslie and R Kuang. Fast string kernels using inexact matching for protein sequences. Journal of Machine Learning Research, 2004. Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder, 2018. Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-Philippe Morency. Learning representations from imperfect time series data via tensor rank regularization. arXiv preprint arXiv:1907.01011, 2019. Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An intriguing failing of convolutional neural networks and the coordconv solution. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 9628-9639, Red Hook, NY, USA, 2018a. Curran Associates Inc.</figDesc><table><row><cell>Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4</cell></row><row><cell>(2):251-257, 1991.</cell></row><row><cell>Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, and Qibin Zhao. Deep multimodal multi-</cell></row><row><cell>linear fusion with high-order polynomial pooling. In Advances in Neural Information Processing</cell></row><row><cell>Systems, pp. 12136-12145, 2019.</cell></row><row><cell>Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain</cell></row><row><cell>Muller. Deep learning for time series classification: a review. Data Mining and Knowledge</cell></row><row><cell>Discovery, 33(4):917-963, Jul 2019. ISSN 1573-756X.</cell></row><row><cell>Fazle Karim,</cell></row></table><note>N Cristianini and J Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge, 2000. Marco Cuturi and Arnaud Doucet. Autoregressive Kernels For Time Series. arXiv e-prints, art.arXiv:1101.0673, Jan 2011. Janez Dem?ar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine learning research, 7(Jan):1-30, 2006.J Diehl, K Ebrahimi-Fard, and N Tapia. Time-warping invariants of multidimensional time series. arXiv preprint arXiv:1906.05823, 2019. Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell, and Ivor Simpson. Structured uncertainty prediction networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5477-5485, 2018.K. Ebrahimi-Fard and F. Patras. Cumulants, free cumulants and half-shuffles. Proceedings of the Royal Society, 2015. Vincent Fortuin, Dmitry Baranchuk, Gunnar R?tsch, and Stephan Mandt. GP-VAE: Deep probabilis- tic time series imputation. In International Conference on Artificial Intelligence and Statistics, pp. 1651-1661. PMLR, 2020. Samuel J. Gershman and Noah D. Goodman. Amortized inference in probabilistic reasoning. Cog- nitive Science, 36, 2014.R Giles. A generalization of the strict topology. Transactions of the American Mathematical Society, 1971. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256, 2010.AL Goldberger, LAN Amaral, L Glass, JM Hausdorff, P Ch Ivanov, RG Mark, JE Mietus, GB Moody, CK Peng, and HE Stanley. Components of a new research resource for complex physiologic signals.Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with deep bidirectional lstm. In 2013 IEEE workshop on automatic speech recognition and understanding, pp. 273-278. IEEE, 2013a. Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur- rent neural networks.Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. arXiv preprint arXiv:1806.00064, 2018b.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Forward pass computation time in seconds on a Gefore 2080Ti GPU for varying sequence length L, fixed batch size N = 32, state-space dimension d = 64 and output dimension h = 64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Specification of datasets used for benchmarking</figDesc><table><row><cell>DATASET</cell><cell>nc</cell><cell>d</cell><cell>Lx</cell><cell>n X</cell><cell>n X</cell></row><row><cell>ARABIC DIGITS</cell><cell>10</cell><cell>13</cell><cell cols="2">4-93 6600</cell><cell>2200</cell></row><row><cell>AUSLAN</cell><cell>95</cell><cell>22</cell><cell cols="2">45-136 1140</cell><cell>1425</cell></row><row><cell>CHAR. TRAJ.</cell><cell>20</cell><cell>3</cell><cell>109-205</cell><cell>300</cell><cell>2558</cell></row><row><cell>CMUSUBJECT16</cell><cell>2</cell><cell>62</cell><cell>127-580</cell><cell>29</cell><cell>29</cell></row><row><cell>DIGITSHAPES</cell><cell>4</cell><cell>2</cell><cell>30-98</cell><cell>24</cell><cell>16</cell></row><row><cell>ECG</cell><cell>2</cell><cell>2</cell><cell>39-152</cell><cell>100</cell><cell>100</cell></row><row><cell>JAP. VOWELS</cell><cell>9</cell><cell>12</cell><cell>7-29</cell><cell>270</cell><cell>370</cell></row><row><cell>KICK VS PUNCH</cell><cell>2</cell><cell>62</cell><cell>274-841</cell><cell>16</cell><cell>10</cell></row><row><cell>LIBRAS</cell><cell>15</cell><cell>2</cell><cell>45</cell><cell>180</cell><cell>180</cell></row><row><cell>NETFLOW</cell><cell>2</cell><cell>4</cell><cell>50-997</cell><cell>803</cell><cell>534</cell></row><row><cell>PEMS</cell><cell cols="2">7 963</cell><cell>144</cell><cell>267</cell><cell>173</cell></row><row><cell>PENDIGITS</cell><cell>10</cell><cell>2</cell><cell>8</cell><cell cols="2">300 10692</cell></row><row><cell>SHAPES</cell><cell>3</cell><cell>2</cell><cell>52-98</cell><cell>18</cell><cell>12</cell></row><row><cell>UWAVE</cell><cell>8</cell><cell>3</cell><cell>315</cell><cell>896</cell><cell>3582</cell></row><row><cell>WAFER</cell><cell>2</cell><cell>6</cell><cell>104-198</cell><cell>298</cell><cell>896</cell></row><row><cell>WALK VS RUN</cell><cell>2</cell><cell cols="2">62 128-1918</cell><cell>28</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Number of trainable parameters</figDesc><table><row><cell>MODEL</cell><cell cols="2">TRAINABLE PARAMETERS</cell></row><row><cell></cell><cell>MEDIAN</cell><cell>MED. ABS. DEV.</cell></row><row><cell>LS2T 3 64</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Classifier accuracies on the multivariate TSC datasets with the best and second best highlighted for each row in bold and italic, respectively.</figDesc><table><row><cell>DATASET</cell><cell cols="2">ARKERNEL DTW</cell><cell>LPS</cell><cell cols="5">SMTS GRSF MVARF MUSE MLSTMFCN</cell><cell>FCN128</cell><cell>RESNET</cell><cell>LS2T 3 64</cell><cell>FCN64-LS2T 3 64</cell><cell>FCN128-LS2T 3 64</cell></row><row><cell>ARABICDIGITS</cell><cell>0.988</cell><cell cols="2">0.908 0.971</cell><cell>0.964</cell><cell>0.975</cell><cell>0.952</cell><cell>0.992</cell><cell>0.990</cell><cell>0.995(0.001)</cell><cell>0.995(0.002)</cell><cell>0.979(0.002)</cell><cell>0 .996 (0.001)</cell><cell>0.997(0.001)</cell></row><row><cell>AUSLAN</cell><cell>0.918</cell><cell cols="2">0.727 0.754</cell><cell>0.947</cell><cell>0.955</cell><cell>0.934</cell><cell>0.970</cell><cell>0.950</cell><cell>0.979(0.003)</cell><cell>0.971(0.003)</cell><cell>0.987(0.002)</cell><cell>0.996(0.001)</cell><cell>0 .995 (0.001)</cell></row><row><cell>CHAR. TRAJ.</cell><cell>0.900</cell><cell cols="2">0.948 0.965</cell><cell cols="2">0.992 0 .994</cell><cell>0.928</cell><cell>0.937</cell><cell>0.990</cell><cell>0.992(0.001)</cell><cell>0.985(0.002)</cell><cell>0.980(0.003)</cell><cell>0.993(0.001)</cell><cell>0.995(0.000)</cell></row><row><cell>CMUSUBJECT16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Specification of datasets used for imputation</figDesc><table><row><cell>DATASET</cell><cell>nc</cell><cell>m</cell><cell>d</cell><cell>Lx</cell><cell>n X</cell><cell>n X</cell></row><row><cell>HMNIST</cell><cell cols="2">10 0.45</cell><cell>28 ? 28</cell><cell cols="3">10 60000 10000</cell></row><row><cell>SPRITES</cell><cell>-</cell><cell>0.6</cell><cell>64 ? 64 ? 3</cell><cell>8</cell><cell>9000</cell><cell>2664</cell></row><row><cell>PHYSIONET</cell><cell>2</cell><cell>0.82</cell><cell>35</cell><cell>48</cell><cell>3997</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is necessary to counteract the fact the magnitude of ? grows with the sum of the elements in the sequence</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>CT is supported by the "Mathematical Institute Award" from the Mathematical Institute at the University of Oxford. PB is supported by the Engineering and Physical Sciences Research Council [EP/R513295/1]. HO is supported by the EPSRC grant "Datasig" [EP/S026347/1], the Alan Turing Institute, and the Oxford-Man Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting the past and the future in protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Soda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Pollastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="380" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multivariate time series classification datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Baydogan</surname></persName>
		</author>
		<ptr target="http://mustafabaydogan.com" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a symbolic representation for multivariate time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mustafa Gokce Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Time series representation and similarity based on local autopatterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">C</forename><surname>Mustafa Gokce Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bayesian wilcoxon signed-rank test based on the dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Mangili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Zaffalon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Ruggeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Should we really use post-hoc tests based on mean-ranks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Mangili</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Time for a change: a tutorial for comparing multiple classifiers through bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Dem?ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2653" to="2688" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oberhauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08897</idno>
		<title level="m">Adapted topologies and higher rank signatures</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Perez Arribas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<title level="m">Deep signature transforms. 33rd Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6775" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via an n-way generalization of &quot;Eckart-young&quot; decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jih-Jie</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterated integrals and exponential homomorphisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. London Math. Soc</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="502" to="512" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integration of paths, geometric invariants and a generalized Baker-Hausdorff formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. of Math</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integration of paths -a faithful representation of paths by non-commutative formal power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4537</idno>
		<title level="m">Rough paths, signatures and the modelling of functions on streams</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02139</idno>
		<title level="m">Rethinking parameter counting in deep models: Effective dimensionality revisited</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<title level="m">All you need is a good init</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A generalised signature method for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeline</forename><surname>Fermanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00873</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Handling incomplete heterogeneous data using vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Nazabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03653</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distribution-free Multiple Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nemenyi</surname></persName>
		</author>
		<ptr target="https://books.google.nl/books?id=nhDMtgAACAAJ" />
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tensor-train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2295" to="2317" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reutenauer</surname></persName>
		</author>
		<title level="m">Free Lie Algebras. Clarendon press -Oxford</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<title level="m">Principles of Mathematical Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Yorke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casdagli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Embedology</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical Physics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="579" to="616" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multivariate time series classification with weasel+muse. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07782</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Floris</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical systems and turbulence</title>
		<meeting><address><addrLine>Warwick</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">General tensor discriminant analysis and gabor features for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1700" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bayesian learning from sequential data using gaussian processes with signature covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Autoregressive forests for multivariate time series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa Gokce</forename><surname>Kerem Sinan Tuncel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why are big data matrices approximately low rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Advances in variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>B?tepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2008" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning efficient tensor representations with ring-structured networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8608" to="8612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
