<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP FEATURE SELECTION-AND-FUSION FOR RGB-D SEMANTIC SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejiao</forename><surname>Su</surname></persName>
							<email>yuejiao@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Jiang</surname></persName>
							<email>jiangzhiyu@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP FEATURE SELECTION-AND-FUSION FOR RGB-D SEMANTIC SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGB-D Semantic Segmentation</term>
					<term>Multi- modality</term>
					<term>Skip-connection</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene depth information can help visual information for more accurate semantic segmentation. However, how to effectively integrate multi-modality information into representative features is still an open problem. Most of the existing work uses DCNNs to implicitly fuse multi-modality information. But as the network deepens, some critical distinguishing features may be lost, which reduces the segmentation performance. This work proposes a unified and efficient feature selectionand-fusion network (FSFNet), which contains a symmetric cross-modality residual fusion module used for explicit fusion of multi-modality information. Besides, the network includes a detailed feature propagation module, which is used to maintain low-level detailed information during the forward process of the network. Compared with the state-of-the-art methods, experimental evaluations demonstrate that the proposed model achieves competitive performance on two public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Semantic segmentation refers to the pixel-wise classification of images according to semantic information. Besides widely-used visual information, the depth information is regarded as another supplementary information to improve the scenario understanding performance due to the development of depth sensors. Depth modality contains 3D geometric information, which is insensitive to illumination changes and can distinguish objects with similar appearance. Therefore, depth cues can make up for some of the defects of semantic segmentation using only visual cues. RGB-D semantic segmentation is very important for many applications such as autonomous driving <ref type="bibr" target="#b0">[1]</ref>, robot vision and understanding <ref type="bibr" target="#b1">[2]</ref>, and land cover classification <ref type="bibr" target="#b2">[3]</ref>, etc. 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.. *Corresponding author: Zhiyu <ref type="bibr">Jiang (jiangzhiyu@nwpu.edu.cn)</ref> With the development of deep learning, two-stream networks have achieved remarkable performance in RGB-D semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. As we all know, the information of RGB and depth modalities are complementary. However, how to effectively fuse RGB and depth information into a unified and distinguishing representation is still a basic yet difficult issue in RGB-D semantic segmentation. There are many methods to try to solve this problem. FuseNet <ref type="bibr" target="#b6">[7]</ref> and RedNet <ref type="bibr" target="#b7">[8]</ref> integrated different modalities by directly adding the feature maps of depth to RGB. RFBNet <ref type="bibr" target="#b8">[9]</ref> proposed a residual fusion block to achieve bottom-up interaction and fusion between two modalities. ACNet <ref type="bibr" target="#b9">[10]</ref> proposed an attention complementary module to assign different modalityweights for better integration. RDFNet <ref type="bibr" target="#b10">[11]</ref> used singlemodality residual learning to learn residual RGB and depth features and their combinations to exploit the complementary characteristics. Although these methods provide structured models to integrate the two kinds of information, it is still an unresolved problem to ensure that the network makes full use of the information from both modalities for fine semantic segmentation.</p><p>Moreover, the loss of detailed information during downsampling is an inherent property of convolution operation. For the encoding part of the segmentation network, reducing the resolution of the feature map to a very small level through various pooling layers is not conducive to accurate mask generation, which can lead to inaccurate segmentation results. To further make up for the information lost in the encoding stage, U-Net <ref type="bibr" target="#b11">[12]</ref> proposed skip-connection to reuse the feature to assist up-sampling learning and recover the fine segmentation results. Although it can realize the reuse of some lost features, it lacks pertinence and does not explicitly model the recovery of detailed information.</p><p>To solve the above problems, this work proposes a novel feature selection-and-fusion network to explicitly strengthen features in RGB-D semantic segmentation model from two aspects: multi-modality representations and decoder features.</p><p>The key idea of our proposed network is to select discriminative information from one modality to supplement the other modality to obtain well-informed representation. In addition, this work focuses on the lost information in the encoder and finds ways to make it helpful in predicting the final result.</p><p>These two aspects correspond to two modules respectively. For the former, the Symmetric Cross-modality Residual Fusion module (SCRF) is designed to effectively fuse the complementary information of two modalities, while maintaining the specificity of the specific modality during the information interaction process at the encoder stage. For the latter, a Detailed Feature Propagation module (DFP) is designed to encourage the network to spotlight the missing vital details in the encoder and reuse them in the decoder to improve the segmentation performance. Both modules are designed as two steps: feature selection and feature fusion.</p><p>The main contributions of this work are described as follows:</p><p>? In order to solve the multi-modality information fusion in RGB-D semantic segmentation, this work designs the SCRF module in FSFNet. The core of the module is the cross-modality residual connection, which can retain the advantages of the residual connection and can explicitly select and fuse complementary information into distinguishing and effective representations.</p><p>? In response to the loss of some important information during the down-sampling process, this work designs the DFP module in the network. The DFP module firstly selects vital information that may be lost in the encoder stage by attention mechanism. And then the module propagates and fuse the selected features with decoder features for further segmentation.</p><p>? With proposed modules, our FSFNet uses a relatively simple architecture to achieve excellent performance. We verify the effectiveness of FSFNet and its modules through a series of experiments and achieve competitive or superior performance on NYUDv2 and SUN RGB-D datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The main difference between RGB-D semantic segmentation and RGB semantic segmentation is that the former not only use visual information but also leverages scene depth information to achieve better accuracy <ref type="bibr" target="#b12">[13]</ref>. According to whether the deep learning methods are used or not, RGB-D semantic segmentation methods are roughly divided into traditional and deep learning-based methods. In the early stage, researchers preferred to use depth information directly. Silberman et al. <ref type="bibr" target="#b13">[14]</ref> recovered support relationships by parsing indoor scenes into floor, walls, supporting surfaces, and object regions using depth information. Ren et al. <ref type="bibr" target="#b14">[15]</ref> achieved high labeling accuracy by a combination of color and depth features using kernel descriptors, and by combining MRF with segmentation tree. Gupta et al. <ref type="bibr" target="#b15">[16]</ref> proposed algorithms for object boundary detection and hierarchical segmentation that generalize the gPb-ucm <ref type="bibr" target="#b16">[17]</ref> approach by making effective use of depth information.</p><p>Although the traditional methods can mine the internal relationship between RGB and depth information by explicitly utilizing depth information, they need a lot of prior knowledge and specific descriptors. Compared with traditional methods, deep learning-based approaches implicitly utilizes deep information to assist RGB semantic segmentation through various networks. Couprie et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> regarded the depth information as another channel to concatenate with RGB image and then extract features with RGB semantic segmentation network. However, RGB channel and depth channel contain inconsistent features and cannot be processed by shared network feature extractors. Besides, Gupta et al. <ref type="bibr" target="#b4">[5]</ref> encoded depth into HHA (Horizontal disparity, Height above ground, and Angle with gravity). Some studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref> used two-stream networks to process the RGB images and HHA information. These methods prove that depth data can improve the performance of semantic segmentation. Many of the later researchers focus on the effective fusion of multi-modality data. Proposed by Hazirbas et al. <ref type="bibr" target="#b6">[7]</ref>, FuseNet integrated depth characteristics into RGB feature mapping by dense fusion or sparse fusion strategies as the network deepens. Lin et al. <ref type="bibr" target="#b20">[21]</ref> divided the image into several branches with different scene resolutions based on depth information aiming at the multi-scale problem. Compared with single-scale networks, multi-scale networks <ref type="bibr" target="#b21">[22]</ref> have better segmentation performance, but they also require more computation. Different from the previous work, this work extends the idea of residual connection <ref type="bibr" target="#b22">[23]</ref> to multi-modality and designs a SCRF module based on multi-modal residual connection to clearly promote multi-modality feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The RGB-D semantic segmentation model is usually based on the encoder-decoder architecture. However, a superior encoder can unify both the complementary characteristics of two modalities and their specific characteristics into effective representation. To this end, inspired by the residual connection <ref type="bibr" target="#b22">[23]</ref>, we put forward a unified encoder framework that includes the symmetric cross-modality residual fusion module as described in Section 3.2, which aims to encourage explicit fusion of cross-modality information and preserve single-modality specific characteristics as completely as possible. In addition, in the encoder phase, a lot of detailed information is lost due to cascading downsampling, which is fatal to semantic segmentation. Our goal is to automatically reuse the essential details lost in the encoder stage and make them helpful to form the final segmentation mask. For this purpose, the detailed feature propagation module is proposed between encoder and decoder, as described in Section 3.3.</p><p>The overall frame diagram of the proposed method is shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Take the three-channel RGB image I R ? R H?W ?3 and three-channel HHA image I H ? R H?W?3 as input, our network selects and enhances the information representation capability of two modalities through cascaded SCRF modules, and at the same time encourages to save the specific features of the specific modality as much as possible. In addition, partial details of the encoder are selected and transmitted by the DFP module to the corresponding stage of the decoder to make full use of the lost important details. Each component will be described in more details in the remaining parts of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Symmetric Cross-modality Residual Fusion</head><p>To encourage the complementary fusion of information between two modalities, we consider this issue from two aspects. Firstly, we argue that if the complementary part can be modeled more explicitly, cross-modality information fusion can be accomplished better. For this motivation, the SCRF module in every layer is designed into two steps, feature selection and feature fusion, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Unlike other work using the simple single-modality residual con-nection <ref type="bibr" target="#b10">[11]</ref>, we propose a cross-modality residual connection to select and fuse complementary features from another modality to make both steps effectively. Secondly, as we know, shallow features can identify edge information, while deep features can study the global context to locate salient objects <ref type="bibr" target="#b23">[24]</ref>. Therefore, the deep features are very irregular, while the shallow features are very noisy and chaotic, and a lot of previous work lacked cross-level interaction in the fusion branch. So in this paper, each SCRF module of the fusion branch is cascaded from shallow to deep, so that the network can gradually select complementary features across levels for joint decision-making. Assume that the RGB feature map of j-th layer is F j rgb ? R H?W?C , and the HHA feature map of j-th layer is F j hha ? R H?W?C . For simplicity, H, W, and C are the height, width, and channel of the feature map respectively. So the selection process can be denoted as:</p><formula xml:id="formula_0">S j hha = f s1 (F j hha ), S j rgb = f s2 (F j rgb ),<label>(1)</label></formula><p>where S j rgb and S j hha are selected RGB and HHA features of j-th layer. They are automatically selected and they can help improve the distinguishing ability of another modality. Note that f s1 (?) and f s2 (?) are feature selection operations. In practice, we use 1 ? 1 convolution with stride 1.</p><p>And then the selected features are sent to the feature fusion step. Suppose that the fusion feature of the previous layer of j-th layer is F j?1 fuse ? R H?W?C . Then the feature after fusion is:</p><formula xml:id="formula_1">F j fuse = [f down (F j?1 fuse ), f conv (S j hha + F j rgb ), f conv (S j rgb + F j hha )].<label>(2)</label></formula><p>Here F j fuse is the concatenated fused feature. f down (?) and f conv (?) mean the down-sample and convolution operation respectively. Note that when j = 1, only the last two items in equation <ref type="formula" target="#formula_1">(2)</ref> are concatenated.</p><p>Therefore cross-modality residual function we defined includes feature selection, feature fusion, and the final convolution part, that is:</p><formula xml:id="formula_2">f r1 : F j rgb ? f conv (S j hha + F j rgb ), f r2 : F j hha ? f conv (S j rgb + F j hha ),<label>(3)</label></formula><p>where f r1 (?) and f r2 (?) are the cross-modality residual functions we defined. The simple but effective SCRF module not only continues the advantages of residual connection, that is, it can accelerate the convergence of the network, but also it uses adaptive weight learning to intelligently and explicitly select and fuse supplementary features from another modality. And our work enables the network to automatically select deep or shallow features by cascading the module. These all allow us to effectively aggregate multi-modality information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detailed Feature Propagation</head><p>As mentioned above, the semantic information of some small-scale objects and outlines will be lost in the downsampling stage, which is unfavorable for semantic segmentation. Therefore, inspired by the idea of skip-connection <ref type="bibr" target="#b11">[12]</ref>, we design the detailed feature propagation module to reuse the lost but essential information, as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>.</p><p>The DFP module is still designed with the same two steps as the SCRF module, i.e. feature selection and feature fusion. Different from SCRF, DFP automatically selects some detailed features that may be lost in the encoder stage by spatial-wise attention, and then propagates the selected features to the corresponding decoder part for fusion with the features being up-sampled. In this way, the features of some small scale objects and outlines can be well used.</p><p>For feature selection, we make the model select detailed features only from the second and third layers of the encoder. This is because the first layer contains a lot of noise and is very complex, and the features of the fourth layer no longer contain enough detailed features after the cascading of the down-sampling operations. Accordingly, our feature fusion operation is carried out in the corresponding decoder layer, to achieve the enhancement of features in the decoder, which can improve the segmentation ability of some small objects and outlines. Suppose fused feature maps in i-th layer of encoder denoted as F i encoder ? R H?W?C , the feature maps of the corresponding decoder are F m-i decoder ? R H?W?C , then the enhanced decoder features after feature selection-and-fusion can be calculated by:</p><formula xml:id="formula_3">F m-i decoder = f fuse (f select (F i encoder ), F m-i decoder ), i ? {2, 3}, m = 4. (4)</formula><p>Here,F m-i decoder represents the feature maps after fusion, i ? {2, 3} means that the DFP module only works in the second and third layers, while f select (?) and f fuse (?) represents feature selection operation and feature fusion operation, respectively. In practice, we use the spatial-wise attention as the feature selection block to select some useful but may be lost features, and we transfer the processed features to the corresponding decoder layer like skip-connection. Simple concatenation is used as the feature fusion operation.</p><p>Through explicitly selecting and fusing detailed features, the impact of the loss of detailed features in down-sampling on the final segmentation can be reduced. And the semantic information of some small-scale objects and outlines can be preserved to realize the reuse of features, which can play their role in the prediction of the final segmentation mask.</p><p>In addition, we use pyramid supervision <ref type="bibr" target="#b9">[10]</ref> in our work. In other words, the output of the last three layers of the decoder is also used as supervision information to ensure rapid network convergence. The only difference between the supervision information of the intermediate output and the ground truth is the resolution. We obtain the supervision information of the intermediate output through the nearest neighbor interpolation down-sampling. The final loss function is as follows:</p><formula xml:id="formula_4">L final = 3 i=1 ? i l i ,<label>(5)</label></formula><p>where l i and ? i represents the loss function and its weight of the i-th layer, respectively. The weighted cross-entropy loss function is the loss function of each layer. Our hyperparameter settings refer to ACNet <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>NYUDv2 <ref type="bibr" target="#b13">[14]</ref> and SUN RGB-D <ref type="bibr" target="#b24">[25]</ref> datasets are used to evaluate the proposed network.  As with most related work, mean Intersection-over-Union (mIoU) and Pixel Accuracy (Pixel Acc.) are used as our performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>PyTorch framework is used in our work and we use Stochastic Gradient Descent (SGD) optimizer to train our model with a momentum of 0.9 and a weight decay of 0.0005. Initial learning rate in our work is set to 0.02 and decreased at a rate of 0.9. The input RGB-D images are cropped to 480 ? 480, and the batch size is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>To prove the effectiveness of the proposed model, we compare it with several state-of-the-art methods, as shown in <ref type="table" target="#tab_0">Table 1</ref>. The result shows that on NYUDv2 dataset, our FSFNet performs equivalently to the current best model using more simple architecture. And on SUN RGB-D dataset, our FSFNet outperforms other models in mIoU. That's because our model uses cascaded SCRF modules to implicitly integrate multimodality features and uses the DFP module to reuse the selected detailed information from the encoder, which improves the ability to segment small-scale objects and contours.</p><p>In order to verify the effectiveness of the SCRF and DFP modules, we perform ablation studies on the NYUDv2 dataset under the same parameter settings as shown in <ref type="table" target="#tab_0">Table 1</ref>. We use ResNet-101 as our backbone and the first row in <ref type="table">Table 2</ref> is the baseline that fuses RGB and depth feature maps by  <ref type="bibr" target="#b20">[21]</ref> 47.7% -48.1% -ACNet <ref type="bibr" target="#b9">[10]</ref> 48.3% -48.1% -PAP <ref type="bibr" target="#b27">[28]</ref> 50.4% 76.2% 50.5% 83.8% SA-Gate <ref type="bibr" target="#b19">[20]</ref> 52 element-wise summation in each encoder layer. We can observe that SCRF and DFP can improve performance by 2.9% and 1.7% respectively. And when two modules exist at the same time, the performance improvement is more obvious than the baseline. This experiment proves the effectiveness and importance of the two proposed modules.</p><p>In order to display the results of the model more intuitively, we show a part of the visualization results in <ref type="figure" target="#fig_2">Fig. 2</ref>. It can be observed that compared to the baseline, our model has a better segmentation performance on different classes of objects, such as ceiling, table, etc., which can prove that our model combines the characteristics of RGB and depth information well. In addition, our model can distinguish smallscale objects and can perform more accurate contour segmen-tation, which can prove the effectiveness of DFP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this work, we propose a feature selection-and-fusion network to address two main challenges in RGB-D semantic segmentation. Firstly, for the effective fusion of multimodality information, we put forward the cascaded SCRF module to obtain unified multi-modality representations. Secondly, aimed at the loss of detailed information in the downsampling stage, we design the DFP module to make the important details helpful in predicting the results. Experimental results demonstrate our model achieves competitive performance on NYUDv2 and SUN RGB-D datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Overview of the proposed framework. Given RGB and HHA images as input, the upper and lower encoder branches extract the characteristics of specific modalities from input respectively. The middle fusion branch uses cascaded SCRF modules to fuse the two modal features. The fused features of the middle two layers of the fusion branch are selected by the DFP module and propagated to the corresponding decoder layer for joint prediction. (b) Details of SCRF module. It is based on the crossmodality residual connection. The SCRF firstly select features that complement another modality from one modality, and then perform feature fusion between modalities and levels. (c) Details of DFP module. The DFP firstly uses spatial-wise attention to select important but possibly lost detailed information from the fusion features in the middle two layers of the encoder stage, and then merge them with the corresponding features of the decoder stage for the final joint segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>NYUDv2 dataset contains 1449 densely labeled pairs of RGB-D images captured by Microsoft Kinect. There are 795 pairs of images for training and 654 pairs for testing. The SUN RGB-D dataset is the largest RGB-D semantic segmentation dataset currently, with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization results of this work in NYUDv2 (left) and SUN RGB-D (right) test sets. For each dataset, we show (a) RGB image, (b) HHA image, (c) result of baseline, (d) result of ours, and (e) ground truth. The baseline directly adds feature maps in HHA branch into RGB branch. 10,335 densely annotated RGB-D images taken from 20 different scenes. It is captured by four different sensors (Kinect V1, Kinect V2, Xtion, and RealSense). The officially divided training set consists of 5285 pairs of RGB-D images and labels, and the remaining 5050 pairs are used for testing. The number of classes in both datasets is 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparing with state-of-the-art methods on NYUDv2 and SUN RGB-D test sets.</figDesc><table><row><cell>Method</cell><cell cols="2">NYUDv2 mIoU Pixel Acc.</cell><cell cols="2">SUN RGB-D mIoU Pixel Acc.</cell></row><row><cell>3DGNN [26]</cell><cell>43.1%</cell><cell>-</cell><cell>45.9%</cell><cell>-</cell></row><row><cell cols="2">Kong et al. [27] 44.5%</cell><cell>72.1%</cell><cell>45.1%</cell><cell>80.3%</cell></row><row><cell>RedNet [8]</cell><cell>-</cell><cell>-</cell><cell>47.8%</cell><cell>81.3%</cell></row><row><cell>CFN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lightningnet: Fast and accurate semantic segmentation for autonomous driving based on 3D LIDAR point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview deep learning for consistent semantic mapping with RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Intelligent Robots and Systems</title>
		<meeting>IEEE International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic detection of anatomical landmarks on geometric mesh data using deep semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Li</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Hui</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for RGB-D indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1475" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Andr?s</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational context-deformable convnets for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3991" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating depth into semantic segmentation via fusionbased CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">RedNet: Residual encoder-decoder network for indoor RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">RFBNet: Deep multimodal networks with residual fusion blocks for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ACNET: Attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RDFNet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Sang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contour-aware network for semantic segmentation via adaptive depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RGB-(D) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HDPA: Hierarchical deep probability analysis for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
