<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review VISUAL PROMPT TUNING FOR TEST-TIME DOMAIN ADAPTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web Services 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web Services 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web Services 2</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Under review VISUAL PROMPT TUNING FOR TEST-TIME DOMAIN ADAPTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models should have the ability to adapt to unseen data during test-time to avoid performance drop caused by inevitable distribution shifts in real-world deployment scenarios. In this work, we tackle the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data. We propose a simple recipe called data-efficient prompt tuning (DePT) with two key ingredients. First, DePT plugs visual prompts into the vision Transformer and only tunes these source-initialized prompts during adaptation. We find such parameter-efficient finetuning can efficiently adapt the model representation to the target domain without overfitting to the noise in the learning objective. Second, DePT bootstraps the source representation to the target domain by memory bank-based online pseudo labeling. A hierarchical self-supervised regularization specially designed for prompts is jointly optimized to alleviate error accumulation during self-training. With much fewer tunable parameters, DePT demonstrates not only state-of-the-art performance on major adaptation benchmarks, but also superior data efficiency, i.e., adaptation with only 1% or 10% data without much performance degradation compared to 100% data. In addition, DePT is also versatile to be extended to online or multi-source TTA settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks achieve excellent performance when the testing data (target domain) follow the same distribution as the training data (source domain). However, their generalization ability on the testing data is not guaranteed when there is a distribution shift between the source and the target. Even simple domain shift, like common corruptions <ref type="bibr" target="#b11">(Hendrycks &amp; Dietterich, 2019)</ref> or appearance variations <ref type="bibr" target="#b7">(Geirhos et al., 2018)</ref>, can lead to significant performance drop. Solving the problem of domain shift is non-trivial. On one hand, it is impossible to train a single model to cover an infinite number of domains. On the other hand, training individual models for each domain requires lots of annotated samples, which induces significant data collection and labeling costs.</p><p>In this paper, we tackle the practical yet challenging test-time domain adaptation (TTA) problem, where the model learns from labeled source domain data and adapts with the unlabeled target domain data during test-time. Compared with conventional unsupervised domain adaptation (UDA) <ref type="bibr" target="#b23">(Long et al., 2015)</ref>, TTA does not need access to source data during adaptation. This setting avoids the privacy issue of sharing the source data and is desirable for real-world applications.</p><p>The key challenges of TTA lie in two folds. First, how to effectively modulate the source domain initialized model given noisy unsupervised learning objective. Tent  optimizes the parameters of batch normalization layers, which is parameter-efficient but lacks adaptation capacity. On the other hand, SHOT  tunes the feature encoder; AdaContrast ) trains the whole model. Given the current over-parameterized models, these methods are prone to overfitting to the unreliable unsupervised learning objective, especially when the amount of target domain data is limited. Second, given only unlabeled target domain data, use what kind of learning objective for optimization. Existing works propose to use unsupervised objectives, including entropy minimization , self-supervised auxiliary task <ref type="bibr" target="#b32">(Sun et al., 2019b)</ref>, Under review pseudo labeling <ref type="bibr" target="#b18">(Lee et al., 2013)</ref>, or a combination of above objectives <ref type="bibr" target="#b22">Liu et al., 2021b)</ref>. However, these unsupervised objectives are either not well aligned with the main task <ref type="bibr" target="#b22">(Liu et al., 2021b)</ref>, or give noisy supervision . <ref type="figure">Figure 1</ref>: The test-time adaptation performance of different methods with respect to the data ratio on the VisDA dataset. The number in the legend denotes the number of tunable parameters. DePT-G outperforms previous SOTA AdaContrast on 100% target data with only 0.19% tunable parameters. The superiority of DePT is more significant on the low data settings.</p><p>In this work, we propose Data-efficient test-time adaptation via Prompt Tuning (DePT). Our recipe is simple yet effective, where two key ingredients are proposed to address the two challenges mentioned above, see in <ref type="figure">Fig. 2</ref>. First, a collection of learnable visual prompts <ref type="bibr" target="#b14">(Jia et al., 2022)</ref> are trained with labeled source data alongside the vision Transformer <ref type="bibr" target="#b4">(Dosovitskiy et al., 2020)</ref>. In the adaptation phase, we only finetune the prompts and the classification head, while freezing the backbone parameters. Although the knowledge learned in the source domain is retained in the unchanged backbone, tuning solely the prompts can effectively modulate the model for target domain adaptation. Second, for the learning objective given only unlabeled target domain data, DePT bootstraps the source domain initialized model by a memory bank refined pseudo labeling mechanism. To further alleviate the accumulation of errors during self-training, we designed a hierarchical self-supervised regularization term for the prompts to encourage better target domain representation learning. These two objectives are complementary and jointly optimized.</p><p>The two ingredients in DePT offer multiple merits and lead to state-of-the-art performance on domain adaptation benchmarks: VisDA-C <ref type="bibr" target="#b26">(Peng et al., 2017)</ref> and <ref type="bibr">DomainNet-126 (Peng et al., 2019)</ref>. Tuning prompts during test-time is much more parameterefficient than full fine-tuning. As illustrated in <ref type="figure">Fig. 1</ref>, with only 0.19% tunable parameters, DePT achieves 1.2% higher performance than previous state-of-the-art AdaContrast . Moreover, parameter efficiency brings data efficiency to DePT. Compared with AdaContrast, DePT has significant improvement in the low data regime. For example, with only 1% of unlabeled target domain data in VisDA-C, DePT achieves 87.97% average accuracy, which surpasses AdaContrast by 7.1%. DePT also demonstrates superior performance with 85.9% average accuracy in VisDA-C under the more challenging online TTA setting, where test samples come sequentially in a stream. Besides, we also show that DePT is flexible to extend to more DA scenarios like multi-source <ref type="bibr" target="#b27">(Peng et al., 2019)</ref>  Unsupervised domain adaptation (UDA) setting is commonly used, where unlabeled data from the testing distribution (target domain) is available during training along with the labeled data from the training distribution (source domain). Many studies attempt to solve this problem using style transfer <ref type="bibr" target="#b13">(Hoffman et al., 2018;</ref><ref type="bibr" target="#b34">Tang et al., 2021;</ref><ref type="bibr" target="#b33">Taigman et al., 2016)</ref>, feature alignment <ref type="bibr" target="#b23">(Long et al., 2015;</ref><ref type="bibr" target="#b30">Sun et al., 2017;</ref><ref type="bibr" target="#b27">Peng et al., 2019)</ref> or learning domain-invariant feature via adversarial training <ref type="bibr" target="#b6">(Ganin et al., 2016;</ref><ref type="bibr" target="#b36">Tzeng et al., 2017)</ref>. However, the UDA setting has a strong assumption that the source and target domain data are both accessible during training, which is not always true as it is difficult to access source data after the model is deployed. Moreover, these methods usually need to retrain the whole framework if new domains come.</p><p>Test-time adaptation (TTA)/Source-free domain adaptation (SFDA) is a more challenging and practical setting where no source domain data is available during adaptation. Such property makes TTA avoid the privacy issue and transmission burden of source data sharing, and enables the pipeline of training the model once and adapting the model to any unknown test distributions. Some recent works explore the TTA setting with different unsupervised objectives and model modulation methods. Test-time training (TTT) <ref type="bibr" target="#b32">(Sun et al., 2019b)</ref> introduces a self-supervised rotation prediction task as the test-time optimization objective. Tent  modulates the batch normalization parameters by minimizing entropy. SHOT  adapt the feature extractor to the target domain with a combination of entropy minimization and pseudo labeling. AdaContrast  tunes the whole model with memory bank-based pseudo labeling and contrastive loss. DePT has several critical differences from these approaches. Most of these methods are proposed for ConvNet, while we switch to vision transformer. For model modulation, we freeze the backbone parameters and learn the target-specific prompts. For the learning objective, we jointly optimize an online memory bank pseudo labeling loss and a hierarchical self-supervised loss specially designed for the prompts. DePT is also flexible on a variety of DA scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SELF-SUPERVISED LEARNING</head><p>Self-supervised learning methods have made great progress in unsupervised visual representation learning. Early attempts use pretext tasks, e.g. colorization <ref type="bibr" target="#b40">(Zhang et al., 2016)</ref>, rotation prediction <ref type="bibr" target="#b8">(Gidaris et al., 2018)</ref>, jigsaw <ref type="bibr" target="#b25">(Noroozi &amp; Favaro, 2016)</ref>, as the self-supervised objective. Another large body of works focus on discriminative approaches <ref type="bibr" target="#b3">(Chen et al., 2020;</ref><ref type="bibr" target="#b10">He et al., 2020)</ref>, named contrastive learning, which push the representation of positive pairs closer and spread negative pairs further. Recent works show that with a carefully designed structure or learning objective, unsupervised features can be learned without discrimination between instances <ref type="bibr" target="#b9">(Grill et al., 2020;</ref><ref type="bibr" target="#b1">Caron et al., 2021)</ref>. Besides unsupervised visual representation learning, researchers also explore applying self-supervised objectives for model robustness  and domain adaptation <ref type="bibr" target="#b31">(Sun et al., 2019a;</ref><ref type="bibr">b)</ref>. In our method, inspired by BYOL <ref type="bibr" target="#b9">(Grill et al., 2020)</ref> and DINO <ref type="bibr" target="#b1">(Caron et al., 2021)</ref>, we propose a hierarchical self-supervised regularization for the visual prompts that are jointly optimized with pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PROMPT TUNING</head><p>The concept of prompt originates from NLP. Prompts are text instructions that are prepended to the input as a hint to inform the pre-trained language model about the task <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>. Besides text prompts that are mostly constructed via heuristics, soft prompts, i.e., continuous vectors that are optimized via gradient descent, achieve comparable performance with full fine-tuning but have much less tunable parameters <ref type="bibr" target="#b19">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b21">Liu et al., 2021a)</ref>. Prompt tuning is also explored in vision tasks. L2P  is proposed for continual learning. VPT <ref type="bibr" target="#b14">(Jia et al., 2022</ref>) is a highly related work proposed for transfer learning. VPT inserts randomly initialized prompts to the pre-trained model, then optimizes them with the downstream task's label. In contrast, we initialize the prompts with the source domain data and optimize them with unlabeled target domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We address the close-set test-time domain adaptation (TTA) setting where source data is not accessible during adaptation. In source domain training, n s labeled samples {x i s , y i s } ns i=1 from the source domain D s are given, where x i s ? X s and y i s ? Y s are data and the corresponding labels. A model has a general architecture with parameters ? trained to learn the function f s : X s ? Y s . We assume the model is parameterized by a neural network and consists of two modules: the feature encoder g s : X s ? R d and a classifier h s : R d ? R C , where d is the dimension of the feature and C is the number of classes. During target domain adaptation, unlabeled data</p><formula xml:id="formula_0">{x i t } nt i=1 from target domain D t is available, where x i t ? X t .</formula><p>The goal of TTA is to adapt the source domain trained model f s to target domain f t : X t ? Y t . We assume the target domain share the same label space with source, i.e., Y s = Y t = Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VISUAL PROMPT TUNING</head><p>How to modulate the model during TTA is a non-trivial problem. The optimization objective of TTA is unsupervised and usually noisy, making full parameter tuning sensitive, easy to diverge, and prone Figure 2: The overview of DePT. We plug prompts into ViT, as shown in (A). We split ViT into multiple stages and prepend prompts to the input of each stage. The prompts along with the backbone parameters are initialized with labeled source domain data. During adaptation, only prompts and the classification head (in red) are finetuned, while the backbone is frozen (in blue). By varying the number of stages and prompts, we can easily alter the number of tunable parameters for different adaptation capacities. (B) The proposed online memory bank refined pseudo-labeling and hierarchical self-supervised objective for test-time training.</p><p>to overfitting to the noise, especially when the number of target data is limited. On the other hand, tuning little parameters has limited adaptation capacity. See Appendix B.1 for examples. Therefore, we propose to adopt visual prompt tuning to solve this dilemma.</p><p>Visual prompts are a set of learnable continuous parameters that are prepended to the input of Transformer layers, see in <ref type="figure">Fig. 2 (A)</ref>. For a Vision Transformer (ViT), the input image is first divided into patches and then embedded into d-dimensional image tokens E 0 . Similar to <ref type="bibr" target="#b14">Jia et al. (2022)</ref>, we concatenate the prompts with CLS token and image tokens to the input of Transformer layers after the image embedding layer. In order to adjust the adaptation capacity, DePT inserts new prompts at different levels of Transformer layers. To be specific, we split the Transformer layers into M stages, where each stage has m = N/M layers. A collection of p prompts P are inserted into the first layer of each stage by concatenating with the image and CLS tokens. We select M = {1, 4, 12} to form three variants of DePT: DePT-Shallow (S), DePT-Group (G) and DePT-Deep (D). The prompt augmented ViT is formulated as:</p><formula xml:id="formula_1">[c i , Z i , E i ] = L i ([c i?1 , P i?1 , E i?1 ]) i = 1, m + 1, 2m + 1, . . . , N ? m [c i , Z i , E i ] = L i ([c i?1 , Z i?1 , E i?1 ]) i = else y = head(c N )<label>(1)</label></formula><p>where c i and Z i represent the CLS token and the aggregated prompt features computed by the i-th Transformer layer, respectively. By altering the number of stages M and the number of prompts p, we can easily control the number of tunable parameters for different adaptation capacity.</p><p>The prompt augmented ViT model learns the function f s : X s ? Y s by labeled source data with cross-entropy loss, where all parameters, including the prompts and the backbone, are optimized. In the test-time adaptation phase, we initialize the target model with the source model's weights. We only fine-tune the parameters of the prompts and the classification heads, while all other parameters are fixed. The knowledge learned in the source domain is retained in the frozen backbone. The prompts are finetuned to learn the target-specific knowledge to adjust the representations nonlinearly through self-attention. The classification head is trained to learn a new decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING OBJECTIVE</head><p>Our learning objective consists of two parts. First, an online pseudo labeling with memory bank refinement mechanism is used to bootstrap the source domain learned representation to the target domain. Second, we design a self-supervised objective on the CLS token and the aggregated prompt to forma hierarchical regularization to further reduce the noise in the pseudo labels and improve the target representation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">PSEUDO LABELING</head><p>Pseudo labeling has widely been used in semi/self-supervised learning. Inspired by <ref type="bibr" target="#b35">Tarvainen &amp; Valpola (2017)</ref>; <ref type="bibr" target="#b29">Sohn et al. (2020)</ref>; , we use a student-teacher model and an online memory bank refinement to generate pseudo labels. To be specific, see in <ref type="figure">Fig. 2 (B)</ref>, the student f t and the teacher f t share the same architecture, and are both initialized with the source model's weight ? s at the beginning of adaptation. The teacher is not updated by gradients, but by exponential moving average (EMA) from the student's weights. The teacher maintains an memory bank that stores the feature embeddings and predicted probabilities of target samples. The memory bank is online updated as a first-in-first-out queue from training mini-batches. Given an unlabeled target domain data x t , a weak and a strong random image transformation: T w and T s are applied. The embedding of the weakly augmented view is first obtained from the student. The pseudo label is then obtained by a soft voting, i.e. average of the probability, from the top-k nearest neighbors in the memory bank:? t = arg max V(f t (T w (x t ))), where V denotes the soft voting with memory bank. The student is trained to enforcing the prediction for the strong-augmented view to match? t with a cross-entropy loss:</p><formula xml:id="formula_2">L pl = 1 n t nt i=1 H(? t , f t (T s (x t ))),<label>(2)</label></formula><p>where H(a, b) = ?a log b. The details can be found in Appendix A.1. Intuitively, the student is fast updating during training, making the pseudo label produced by the student to be noisy. The EMA updated teacher can be seen as an ensemble of the student along training. Thus the memory bank maintained by the teacher stores more stable and high-quality predictions. The soft voting further leverages nearby data point cluster to produce more accurate pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">HIERARCHICAL SELF-SUPERVISED REGULARIZATION</head><p>Due to the domain shift, the model trained on the source domain cannot directly produce good representations of the target data, i.e., the representation of images from the same class may spread apart. Although using the student-teacher model and the memory bank refinement can significantly improve the quality of pseudo-labels, they still cannot completely avoid the accumulation of errors during self-training. Therefore, we design a hierarchical self-supervised regularization for the proposed prompt augmented ViT to improve the model's representation of target data.</p><p>For the prompt augmented ViT, the CLS token of the output of ViT encoder c N serves as the holistic image representation for classification. The prompts work similarly to the CLS token: prepending to the input of Transformer layers. With self-attention, the prompts can inject domain-specific information into other tokens, and aggregate instance features from other tokens. Each prompt will attend to different attributes and features of the image. The prompts at the output of the Transformer layer, which we call aggregated prompts, serve as a fine-grained representation of the image. Moreover, the aggregated prompts from different stages may attend to distinctive levels of features and thus form a hierarchical representation. Inspired by <ref type="bibr" target="#b1">Caron et al. (2021)</ref> and <ref type="bibr" target="#b9">Grill et al. (2020)</ref>, given different views of the same image, we push both their holistic feature representations (CLS token) and their hierarchical fine-grained representations (aggregated prompts) closer.</p><p>We use the DINO <ref type="bibr" target="#b1">(Caron et al., 2021)</ref> framework here with a few key modifications. The student encoder g t and EMA teacher encoder g t are reused with the ones in the pseudo labeling section. A series of projection heads q t : R d ? R K are introduced to project the CLS token c t or the aggregated prompts Z i of the target sample x t into K dimensions, where each stage of the prompt augmented ViT has its own projection head. Two random strong-augmented views of the target sample is generated. One is passed to the student, and the other one is passed to the teacher. The projected CLS token and aggregated prompts are obtained from the student:</p><formula xml:id="formula_3">c t = q t (c t ), {Z i = q t (Z i )} i?I</formula><p>and similarly for the teacherc t , {Z i } i?I , where I = {m, 2m, . . . , N } is the index of the last Transformer layer in each stage. For the holistic regularization on the CLS token, we apply the DINO loss. The probability distribution of the student output is obtained by applying a softmax function: P t (x t ) = softmax(c t ), and so is the teacher's P t (x t ). The student aims at matching the teacher's predicted distribution by minimizing the cross-entropy loss: L ss cls = H(P t (x t ), P t (x t )).</p><p>(3) The technique of centering and sharpening of DINO are also used to avoid collapse, where the details can be found in Appendix A.2. For the hierarchical fine-grained regularization, we minimize the mean squared error between the aggregated prompts of the student and teacher:</p><formula xml:id="formula_4">L ss prompt = 1 M i?I p j=1 2 ? 2 ? Z j i ,Z j i ||Z j i || 2 ? ||Z j i || 2 .<label>(4)</label></formula><p>In addition, although the regularization is applied on the aggregated prompts one by one, i.e. the i-th aggregated prompt of the student is pushed closer to the i-th one of the teacher, we find the model tend to learn the trivial solution where all aggregated prompt become similar. To avoid this, we add a diversity term to encourage different prompts to attend to different features, thereby enabling them to learn rich target-specific knowledge. Specifically, we maximize the cosine distance among the p aggregated prompts of the student:</p><formula xml:id="formula_5">L ss div = 1 M i?I p j=1 p k=j 1 ? Z j i ,Z k i ||Z j i || 2 ? ||Z k i || 2 ,<label>(5)</label></formula><p>where j and k are the index of the aggregated prompts at the output of the i-th Transformer layer. Therefore, DePT minimizes the following loss function for TTA:</p><formula xml:id="formula_6">L = ?L pl + ? 1 L ss cls + ? 2 L ss prompt ? ?L ss div .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>We evaluate the effectiveness and versatility of DePT in a variety of TTA scenarios, covering two major domain adaptation benchmarks below.</p><p>VisDA-C Peng et al. <ref type="formula" target="#formula_1">(2017)</ref> is a large-scale domain adaptation benchmark that focuses on 12-class synthesis-to-real object recognition task. We use the training set as the source and the validation set as the target. The source domain contains 152k synthetis images generated by rendering 3D models while the target domain has 55k real object images sampled from Microsoft COCO. <ref type="bibr">-126 Peng et al. (2019)</ref> provides images from multiple domains for classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DomainNet</head><p>Since the original DomainNet dataset has noisy labels, we follow the previous work <ref type="bibr" target="#b28">(Saito et al., 2019;</ref> to use a subset of DomainNet, which contains 126 classes from 4 domains (Real, Sketch, Clipart, Painting). Besides one-on-one adaptation, we also explore multi-source adaptation on the DomainNet-126 dataset.</p><p>Models Different from previous works that use ConvNet as the backbone, e.g. ResNet26/50/101, we use ViT-B as a strong backbone network for adaptation. We use the model and ImageNet pretrained weight from the timm library <ref type="bibr" target="#b39">(Wightman, 2019)</ref> in our experiments.</p><p>Baselines We compare our method with both classical UDA methods and test-time adaptation methods. For UDA methods, we compare to DANN <ref type="bibr" target="#b5">(Ganin &amp; Lempitsky, 2015)</ref>, CDAN <ref type="bibr" target="#b24">(Long et al., 2018)</ref>, CAN <ref type="bibr" target="#b16">(Kang et al., 2019)</ref>, SWD <ref type="bibr" target="#b17">(Lee et al., 2019)</ref> and MCC <ref type="bibr" target="#b15">(Jin et al., 2020)</ref>. Note that all UDA methods need access to both source and target domain data, while ours doesn't. For TTA methods, we compare with Tent , SHOT , and AdaContrast .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VISDA-C DATASET RESULTS</head><p>DePT demonstrates state-of-the art performance with much less tunable parameters. Tab. 1 compares DePT with UDA and TTA methods. Without access to source data, DePT with more strong ViT-B backbone achieves better performance than the UDA methods. Among TTA methods, we reimplement the state-of-the-art AdaContrast  with the ViT-B backbone for a fair comparison. Even with much less tunable parameters, DePT-G (0.16M) and DePT-D (0.47M) outperforms the strong baseline AdaContrast (85.8M) by +1.4% and +1.5%. For DePT-S, with only 0.048M tunable parameters, it still has a huge improvement (+16.5%) compared with no adaptation. Although the backbone parameters of ViT are frozen, tuning the prompt at the input can effectively adjust the representation for the target domain.</p><p>DePT gains more advantages under limited data settings. Previous TTA methods usually assume all target domain data (testing data) can be used for adaptation. However, although no label is required, obtaining a large amount of target domain data still requires high costs in some real-world applications. It is highly desirable that adaptation methods can adapt with as little target data as possible. Therefore, we investigate the impact of the target domain data quantity on the performance of the TTA methods by evaluating the TTA performance with 1%, 10%, 50%, and 100% of target data, see in <ref type="figure" target="#fig_0">Fig. 3 (A)</ref>. DePT shows strong data efficiency. DePT-G still has 87.97% average accuracy under 1% data, which is only 2.59% lower than with 100% data. In contrast, AdaContrast's average accuracy is only 80.87% at 1% data, which is 8.48% lower than 100% data. For DePT-D, its adaptation capacity is the highest when target data is abundant, e.g., in 100% data. DePT-D has more decline than DePT-G and DePT-S under 1% data, but is still superior to AdaContrast. For DePT-S, the absolute performance is not high, but it shows the best robustness against data amount changes, which only decreases 1.17% under 1% data compared to 100% data.</p><p>DePT has strong performance on the more challenging online TTA setting. In the online setting, the model adapts to target data that comes in batch sequentially in a stream. Each data batch can only be used for training once. We do not apply learning rate decay and turn off the memory bank and directly use the model prediction as the pseudo label. The bottom of Tab. 1 shows the performance of DePT online. DePT achieves 85.9% average accuracy, surpassing AdaContrast by +4.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DOMAINNET DATASET RESULTS</head><p>DePT consistently improves across seven domain shifts. Tab. 2 shows the comparison between DePT and other TTA methods on the seven domain shifts in the DomainNet-126 dataset. All three variants of DePT show consistent improvement on all seven domain shifts compared with no adaptation. DePT-G performs the best among them and achieves 81.7% average accuracy. Although the proposed DePT-G and DePT-D have much smaller tunable parameters, they outperform the previous SOTA AdaContrast. In the online setting, DePT achieves 3.1% higher performance than AdaCon-  trast. We also find that models with ViT-B backbone demonstrate stronger robustness against domain shifts than the R50 backbone.</p><p>Beyond one-on-one adaptation. We further evaluate DePT to multi-source TTA setting, details of the setting can be found in Appendix A.3. See in Tab. 3, DePT makes the best use of multi-source domain knowledge by utilizing a set of domain-specific prompts and a shared domain-agnostic prompt. In contrast, SHOT  makes one-by-one domain adaptation separately and then makes predictions by an ensemble, which does not jointly optimize for multi-source domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS</head><p>Visual prompt tuning helps to learn better target representation. <ref type="figure" target="#fig_0">Fig. 3 (B)</ref> shows the t-sne visualization of the target feature embedding before and after adaptation. Before the adaptation, the feature embeddings of the source model on the target data are mixed and difficult to distinguish due to domain shift. After adaptation, the embeddings are more scattered, and the number of samples that fall into the wrong cluster is less. This experiment proves that although the backbone is fixed, DePT still learns the knowledge of the target domain by the prompt and improves the target representation through attention layers.  Prompt tuning is highly effective for adaptation. Tab. 4 shows the impact of the number of prompts on VisDA-C dataset. Increasing the number of stages or the number of prompts will introduce more tunable parameters during TTA. As the VisDA-C validation set has abundant unlabeled data, increasing tunable parameters usually results in better adaptation performance, e.g., DePT-G-100 and DePT-D-50 achieve SOTA adaptation performance. Surprisingly, even with only one prompt, DePT still achieves decent adaptation accuracy. Contribution of each loss. Tab. 5 shows the contribution of each loss to the final performance. The pseudo labeling and memory bank refinement can make better pseudo label generation, and achieves the performance of 86.9. By adding the DINO regularization on the CLS token, the model learns better representation. This regularization works jointly with pseudo labeling to the performance of 88.3. We tried to further add hierarchical regularization on the prompts. However, direct adding regularization makes prompts to collapse. We further introduce the diversity loss of prompts and finally boost the performance to 90.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a data-efficient test-time adaptation approach via visual prompt tuning. We pretrain a collection of visual prompts alongside the network with labeled source domain data. In the adaptation phase, we only adapt the prompts to learn target-specific knowledge with the backbone fixed. A flexible scheme is proposed to insert a certain number of prompts to the input of each stage of the Transformer. By doing so, we can easily control the adaptation capacity and tunable parameters by altering the number of stages and prompts. An online pseudo labeling with memory bank refinement is jointly learnt with a hierarchical self-supervised objective for test-time prompt tuning. We extensively evaluate the proposed method with VisDA-C and DomainNet-126 datasets and show that our simple recipe can lead to SOTA performance in multiple settings and superior data efficiency. In addition, we further verify the effectiveness and flexibility of DePT on online and multi-source adaptation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>A.1 PSEUDO LABELING DETAILS During adaptation, we use a student-teacher model and an online memory bank refinement to generate pseudo labels. The student and the teacher share the same prompt-augmented ViT architecture. They are both initialized with the source model's weight ? s at the beginning of adaptation. As the student is fast updating during training, the pseudo label directly produced by the student is usually unstable and noisy. We introduce a slow momentum teacher model whose weights ? t exponential moving average updated by the student's weights ? t :</p><formula xml:id="formula_7">? t = m? t + (1 ? m)? t ,<label>(7)</label></formula><p>where ? t is the student's weights, ? t is the teacher's weights, and m is the momentum for updating. The teacher can be seen as an ensemble of student along training, thus is more stable and has higher pseudo label accuracy.</p><p>The teacher maintains an online memory bank B with size N to refine the pseudo labels from the student via soft voting, where the memory bank stores the feature embeddings and the predicted probabilities {c i , p i } N i=1 of weakly augmented target data from the teacher. The memory bank B is initialized by randomly selected N samples, and then updated with data from training mini-batch as a first-in-first-out queue.</p><p>Given an unlabeled target domain data x t , a weak and a strong random image transformation T w and T s are applied. The student produces the feature embedding c t based on the weakly augmented view T w (x t ). To refine the prediction of the student, we use c t to retrieve top-k nearest neighbors in the memory bank B by measuring the cosine distance between c t and {c i } N i=1 . The refined hard pseudo label is obtained by averaging the probability of the top-k neighbors followed by a arg max operation:?</p><formula xml:id="formula_8">t = arg max 1 k k i=1 p i<label>(8)</label></formula><p>Inspired by FixMatch <ref type="bibr" target="#b29">(Sohn et al., 2020)</ref>, the student is trained to enforce the cross-entropy loss against the model's output for the strongly-augmented view:</p><formula xml:id="formula_9">L P L = 1 n t nt i=1 H(? t , f t (T s (x t )))<label>(9)</label></formula><p>A.2 SELF-SUPERVISED OBJECTIVE DETAILS</p><p>We use the DINO <ref type="bibr" target="#b1">(Caron et al., 2021)</ref> framework for self-supervised learning with a few key modifications. We reuse the student and teacher model in the pseudo labeling with the DINO framework, which are both initialized by the source model. We apply the DINO loss on the CLS token. The sharpening and centering operation is also used to avoid collapse. To be specific, the sharpening operation is applied for both the student and teacher:</p><formula xml:id="formula_10">P t (x t ) (i) = exp(c (i) t /? ) K k=1 exp(c (k) t /? )<label>(10)</label></formula><p>The teacher is further applied a centering operation:</p><formula xml:id="formula_11">P t (x t ) (i) = exp((c (i) t ? c (i) )/? ) K k=1 exp((c (k) t ? c (k) )/? )<label>(11)</label></formula><p>where ? and ? are the temperature for the student and teacher, respectively. c is the center that is exponential moving average updated for more stable estimation. The multi-crop and local-to-global regularization are not applied in DePT. More details can be found the <ref type="bibr" target="#b1">(Caron et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 EXTENDING TO MULTI-SOURCE DOMAIN TTA SETTING</head><p>The multi-source domain test-time adaptation setting aims at training a model from multiple source domains and adapt it to the unseen target domain. Many works aims at learning a domain-invariant feature across source domains. However, such methods are different to learn domain-invariant features if the source domain is diverse. They also can't capture domain-specific knowledge.</p><p>Our method is versatile to extend to multi-source domain test-time adaptation setting. See in <ref type="figure">Fig.  4</ref>, the basic idea is that all domains share a domain-agnostic backbone while each domain has its own prompts to learn the domain-specific knowledge. During source training, each domain prompts along with a shared general prompts are randomly initialized. In each training iteration, we attractively sample images from each domain. The backbone, shared prompts and the corresponding domain prompts are optimized via supervised loss. During the TTA phase, prompts from all domain are averaged and concatenated with the shared prompts as the initialization. Then the optimization is the same as one-on-one TTA adaptation.</p><p>In our experiments, we use three domains in the DomainNet-126 as the source domains, and use the left one as the target. <ref type="figure">Figure 4</ref>: Our method is easy to extend to multi-source domain adaptation setting, where only the corresponding phase needs to be modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 IMPLEMENTATION DETAILS</head><p>We use PyTorch for all implementation. For source training, we initialized the ViT-B-16 with Im-ageNet pretrained weights from timm library <ref type="bibr" target="#b39">(Wightman, 2019)</ref>. We follow  to randomly split the source data with 90% for training and 10% for validation. For source training, we train around 11k iterations. For target training, we train around 6k iterations for all datasets unless noted. For all experiments, we use SGD optimizer with momentum 0.9 and weight decay 1e-4, and cosine annealing on the learning rate. The training batch size is set as 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 MOTIVATION OF USING PROMPT TUNING</head><p>How to modulate the model is vital for the TTA setting. Since there is only unlabeled target data during adaptation, the model can only learn through the unsupervised learning objective. Current unsupervised learning objectives are not always true and usually exist some noises and errors. If there are too many parameters to modulate, the current over-parameterized model is prone to overfitting to the noise of the unsupervised objective, especially when the amount of unlabeled target data is limited. On the other hand, if modulating too few parameters, although overfitting will be avoided, the adaptation capacity of the model will be limited.</p><p>Here we perform an experiment to verify our claim. We use AdaContrast  and Tent  as examples to modulate the model. Among them, AdaContrast finetunes all parameters of the model. Tent only adjusts the transformation parameters of the BatchNormalization layers, which only contain very few parameters. For ViT, we adjust the transformation parameters of the LayerNorm (LN) layer instead. For these two model modulation approaches, we use the learning objective of AdaContrast for training, i.e., MoCo-based self-supervised combined with pseudo labeling. <ref type="figure">Figure 5</ref>: The curve of average accuracy of evaluation v.s. training steps with 100% data and 1% data on VisDA-C dataset. <ref type="figure">Fig. 5</ref> shows the results of training the above two modulation approaches with 100% and 1% VisDA-C unlabeled target domain data. Under 100% data, AdaContrast adapts well to the target domain. This is because the target domain of VisDA-C has 55K sufficient data, making AdaContrast not easy to overfit. For LN, its performance is limited by the adaptation capacity. Under 1% of the data, with the increasing of training steps, AdaContrast over-fits obviously, as the performance of adaptation gradually declines. LN does not overfit but is still limited by the adaptation capacity, and the performance is not very good. In the setting of TTA, no target domain label is available, and we currently do not have a metric to know when to stop adaptation.</p><p>A method that can avoid over-fitting and have enough adaptation capacity is necessary. Therefore, we are motivated to explore using visual prompt tuning to solve this dilemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ABLATION ON THE HYPER-PARAMETER CHOICE</head><p>Tab. 6 shows the impact of the hyper-parameter choice, including learning rate, the teacher temperature in the DINO loss, and the weight to balance training loss.</p><p>For visual prompt tuning, we find that a larger learning rate would benefit the performance. For example, learning rates of 1e-3 and 5e-3 lead to a decent performance. Continuing to amplify the learning rate may lead to instability in training.</p><p>The Teacher temperature is a hyper-parameter in the DINO loss that controls the sharpness of the teacher output distribution. A large temperature makes the teacher tend to output a uniform distribution, which makes the initial optimization difficult. The performance of small temperature is slightly worse. Using a warmup mechanism on the teacher's temperature is a good balance.</p><p>We finally investigate the effect of weights among the proposed losses in Eq. 6 . We set ? to 1, and adjust the value of ? 1 = ? 2 = ?. We set ? = ?/20. We found that a small ? value results in better performance, e.g., below 0.1.</p><p>Despite the relative performance varies by different hyper-parameter choices, DePT still consistently demonstrates state-of-the-art performance compared to other methods, which also shows the robustness of DePT against the choice of hyper-parameters. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(A) Performance comparison on VisDA-C dataset with different data ratio for TTA. DePT shows less performance degradation when the training data reduces. (B) t-SNE visualization of the DePT-G model before and after adaptation on VisDA-C dataset. Different color denotes different classes. By only tuning the prompts, DePT can effectively adapt the model to the target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DA) aims at reducing the performance drop introduced by distribution shift. Some highly-related DA settings are introduced in the following.</figDesc><table><row><cell>domain adaptation.</cell></row><row><cell>2 RELATED WORK</cell></row><row><cell>2.1 DOMAIN ADAPTATION</cell></row><row><cell>Domain adaptation (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on VisDA-C train ? val adaptation. Upper table: UDA methods. Middle table: TTA methods. Bottom table: online setting. All models with ViT-B backbone are reproduced by us, while the results with R101 backbone are copied from the original paper. Bold and underline denote the first and second high results.</figDesc><table><row><cell>Method</cell><cell cols="9">Backbone plane bcycl bus car horse knife mcycl person plant sktbrd train truck Avg.</cell></row><row><cell>DANN</cell><cell>R101</cell><cell cols="3">81.9 77.7 82.8 44.3 81.2 29.5 65.1</cell><cell>28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8 57.4</cell></row><row><cell>CDAN</cell><cell>R101</cell><cell cols="3">85.2 66.9 83.0 50.8 84.2 74.9 88.1</cell><cell>74.5</cell><cell>83.4</cell><cell>76.0</cell><cell cols="2">81.9 38.0 73.9</cell></row><row><cell>CAN</cell><cell>R101</cell><cell cols="3">97.0 87.2 82.5 74.3 97.8 96.2 90.8</cell><cell>80.7</cell><cell>96.6</cell><cell>96.3</cell><cell cols="2">87.5 59.9 87.2</cell></row><row><cell>SWD</cell><cell>R101</cell><cell cols="3">90.8 82.5 81.7 70.5 91.7 69.5 86.3</cell><cell>77.5</cell><cell>87.4</cell><cell>63.6</cell><cell cols="2">85.6 29.2 76.4</cell></row><row><cell>MCC</cell><cell>R101</cell><cell cols="3">88.7 80.3 80.5 71.5 90.1 93.2 85.0</cell><cell>71.6</cell><cell>89.4</cell><cell>73.8</cell><cell cols="2">85.0 36.9 78.8</cell></row><row><cell>Source</cell><cell>R101</cell><cell>57.2 11.1 42.4 66.9 55.0</cell><cell>4.4</cell><cell>81.1</cell><cell>27.3</cell><cell>57.9</cell><cell>29.4</cell><cell>86.7</cell><cell>5.8 43.8</cell></row><row><cell>Tent</cell><cell>R101</cell><cell cols="3">86.5 23.0 79.8 51.9 78.3 17.6 87.6</cell><cell>64.2</cell><cell>79.9</cell><cell>23.6</cell><cell>63.9</cell><cell>1.1 57.3</cell></row><row><cell>SHOT</cell><cell>R101</cell><cell cols="3">95.3 87.5 78.7 55.6 94.1 94.2 81.4</cell><cell>80.0</cell><cell>91.8</cell><cell>90.7</cell><cell cols="2">86.5 59.8 83.0</cell></row><row><cell>AdaCon</cell><cell>R101</cell><cell cols="3">97.0 84.7 84.0 77.3 96.7 93.8 91.9</cell><cell>84.8</cell><cell>94.3</cell><cell>93.1</cell><cell cols="2">94.1 49.7 87.2</cell></row><row><cell>Source</cell><cell>ViT-B</cell><cell cols="3">99.0 69.1 78.2 71.9 87.7 58.4 96.4</cell><cell>35.0</cell><cell>52.7</cell><cell>92.5</cell><cell cols="2">96.0 17.6 71.2</cell></row><row><cell>AdaCon</cell><cell>ViT-B</cell><cell cols="3">99.5 94.2 91.2 83.7 98.9 97.7 96.8</cell><cell>71.5</cell><cell>96.0</cell><cell>98.7</cell><cell cols="2">97.9 45.0 89.2</cell></row><row><cell>DePT-S (Ours)</cell><cell>ViT-B</cell><cell cols="3">98.7 90.1 87.4 70.7 98.9 96.0 96.8</cell><cell>75.2</cell><cell>91.9</cell><cell>97.9</cell><cell cols="2">96.2 52.5 87.7</cell></row><row><cell>DePT-G (Ours)</cell><cell>ViT-B</cell><cell cols="3">99.2 93.0 93.4 85.6 99.4 98.6 96.7</cell><cell>76.2</cell><cell>98.2</cell><cell>97.9</cell><cell cols="2">96.6 52.0 90.6</cell></row><row><cell>DePT-D (Ours)</cell><cell>ViT-B</cell><cell cols="3">99.4 93.8 94.4 87.5 99.4 98.0 96.7</cell><cell>74.3</cell><cell>98.4</cell><cell>98.5</cell><cell cols="2">96.6 51.0 90.7</cell></row><row><cell>Online Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdaCon</cell><cell>ViT-B</cell><cell cols="3">95.4 75.2 83.4 64.2 95.8 93.2 92.4</cell><cell>65.3</cell><cell>92.6</cell><cell>82.9</cell><cell cols="2">95.1 39.7 81.3</cell></row><row><cell>DePT-G (Ours)</cell><cell>ViT-B</cell><cell cols="3">98.3 87.6 88.9 62.1 98.4 95.2 95.9</cell><cell>67.1</cell><cell>95.2</cell><cell>97.3</cell><cell cols="2">94.9 50.1 85.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) of seven domain shifts on DomainNet-126. All models with ViT-B backbone are reproduced by us, while the performance of other baselines with R50 backbone are copied from the original paper. Bold and underline denote the first and second high results.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>R?C</cell><cell>R?P</cell><cell>P?C</cell><cell>C?S</cell><cell>S?P</cell><cell>R?S</cell><cell>P?R</cell><cell>Avg.</cell></row><row><cell>Source</cell><cell>R50</cell><cell>55.5</cell><cell>62.7</cell><cell>53.0</cell><cell>46.9</cell><cell>50.1</cell><cell>46.3</cell><cell>75.0</cell><cell>55.6</cell></row><row><cell>TENT</cell><cell>R50</cell><cell>58.5</cell><cell>65.7</cell><cell>57.9</cell><cell>48.5</cell><cell>52.4</cell><cell>54.0</cell><cell>67.0</cell><cell>57.7</cell></row><row><cell>SHOT</cell><cell>R50</cell><cell>67.7</cell><cell>68.4</cell><cell>66.9</cell><cell>60.1</cell><cell>66.1</cell><cell>59.9</cell><cell>80.8</cell><cell>67.1</cell></row><row><cell>AdaContrast</cell><cell>R50</cell><cell>70.2</cell><cell>69.8</cell><cell>68.6</cell><cell>58.0</cell><cell>65.9</cell><cell>61.5</cell><cell>80.5</cell><cell>67.8</cell></row><row><cell>Source</cell><cell>ViT-B</cell><cell>68.7</cell><cell>75.9</cell><cell>69.8</cell><cell>67.7</cell><cell>74.1</cell><cell>60.4</cell><cell>86.4</cell><cell>71.8</cell></row><row><cell>AdaContrast</cell><cell>ViT-B</cell><cell>81.8</cell><cell>81.8</cell><cell>82.0</cell><cell>75.8</cell><cell>82.2</cell><cell>73.7</cell><cell>90.4</cell><cell>81.1</cell></row><row><cell>DePT-S (Ours)</cell><cell>ViT-B</cell><cell>81.2</cell><cell>81.4</cell><cell>80.9</cell><cell>75.1</cell><cell>81.7</cell><cell>73.1</cell><cell>88.9</cell><cell>80.3</cell></row><row><cell>DePT-G (Ours)</cell><cell>ViT-B</cell><cell>83.3</cell><cell>81.6</cell><cell>82.6</cell><cell>77.8</cell><cell>81.3</cell><cell>75.1</cell><cell>89.9</cell><cell>81.7</cell></row><row><cell>DePT-D (Ours)</cell><cell>ViT-B</cell><cell>82.8</cell><cell>82.5</cell><cell>82.8</cell><cell>76.9</cell><cell>81.2</cell><cell>74.6</cell><cell>89.8</cell><cell>81.5</cell></row><row><cell>Online Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdaContrast</cell><cell>ViT-B</cell><cell>72.1</cell><cell>77.9</cell><cell>71.8</cell><cell>70.1</cell><cell>75.7</cell><cell>65.5</cell><cell>87.9</cell><cell>74.2</cell></row><row><cell>DePT-G (Ours)</cell><cell>ViT-B</cell><cell>74.8</cell><cell>78.3</cell><cell>74.2</cell><cell>72.5</cell><cell>77.2</cell><cell>68.5</cell><cell>87.2</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (%) of multi-source DA setting on DomainNet-126 dataset. R denotes the remaining three domains except the single source.</figDesc><table><row><cell>Multi-source (R ?)</cell><cell>Backbone</cell><cell>R ?R</cell><cell>R ?C</cell><cell>R ?S</cell><cell>R ?P</cell><cell>Avg.</cell></row><row><cell>ERM</cell><cell>ViT-B</cell><cell>88.7</cell><cell>76.7</cell><cell>69.2</cell><cell>78.7</cell><cell>78.3</cell></row><row><cell>SHOT</cell><cell>ViT-B</cell><cell>90.7</cell><cell>83.1</cell><cell>75.9</cell><cell>83.4</cell><cell>83.2</cell></row><row><cell>DePT-G (Ours)</cell><cell>ViT-B</cell><cell>91.0</cell><cell>83.7</cell><cell>78.3</cell><cell>84.0</cell><cell>84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of the effect of number of prompts in VisDA-C dataset. We provide the average TTA accuracy and the number of tunable parameters.</figDesc><table><row><cell>Prompt Num</cell><cell>1</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>50</cell><cell></cell><cell>100</cell></row><row><cell>Model</cell><cell cols="8">Acc Params/M Acc Params/M Acc Params/M Acc Params/M</cell></row><row><cell>DePT-S</cell><cell>83.8</cell><cell>0.009</cell><cell>86.0</cell><cell>0.016</cell><cell>87.7</cell><cell>0.048</cell><cell>87.6</cell><cell>0.086</cell></row><row><cell>DePT-G</cell><cell>85.4</cell><cell>0.010</cell><cell>88.4</cell><cell>0.040</cell><cell>90.6</cell><cell>0.16</cell><cell>90.6</cell><cell>0.32</cell></row><row><cell>DePT-D</cell><cell>86.3</cell><cell>0.018</cell><cell>89.0</cell><cell>0.10</cell><cell>90.7</cell><cell>0.47</cell><cell>90.4</cell><cell>0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation of the contribution of each component to the final performance on VisDA-C.</figDesc><table><row><cell cols="2"># PL MB CLS Reg. Prompt Reg. Prompt Div. DePT-G</cell></row><row><cell>0</cell><cell>73.2</cell></row><row><cell>1</cell><cell>85.6</cell></row><row><cell>2</cell><cell>86.9</cell></row><row><cell>3</cell><cell>89.3</cell></row><row><cell>4</cell><cell>88.2</cell></row><row><cell>5</cell><cell>90.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation of the impact of hyper-parameter choices of DePT-G on VisDA-C.</figDesc><table><row><cell>LR</cell><cell>2e-4</cell><cell>5e-4</cell><cell>1e-3</cell><cell>5e-3</cell></row><row><cell>Avg Acc</cell><cell>89.6</cell><cell>90.0</cell><cell>90.4</cell><cell>90.6</cell></row><row><cell>Teacher Temp</cell><cell>0.04</cell><cell>0.05</cell><cell>0.07</cell><cell>0.04-0.07</cell></row><row><cell>Avg Acc</cell><cell>90.1</cell><cell>90.2</cell><cell>90.6</cell><cell>90.5</cell></row><row><cell>?</cell><cell>0.01</cell><cell>0.05</cell><cell>0.1</cell><cell>0.5</cell></row><row><cell>Avg Acc</cell><cell>90.1</cell><cell>90.2</cell><cell>90.6</cell><cell>89.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive test-time adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12119</idno>
		<title level="m">Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="464" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Haris</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10285" to="10295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6028" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
	</analytic>
	<monogr>
		<title level="m">Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ttt++: When does self-supervised test-time training fail or thrive?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Bastien Van Delft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21808" to="21820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8050" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Correlation alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Test-time training for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crossnorm and selfnorm for generalization under distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tent: Fully test-time adaptation by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10726</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to prompt for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
