<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Representation Learning from Multiple Domains for Few-shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Representation Learning from Multiple Domains for Few-shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we look at the problem of few-shot classification that aims to learn a classifier for previously unseen classes and domains from few labeled samples. Recent methods use adaptation networks for aligning their features to new domains or select the relevant features from multiple domain-specific feature extractors. In this work, we propose to learn a single set of universal deep representations by distilling knowledge of multiple separately trained networks after co-aligning their features with the help of adapters and centered kernel alignment. We show that the universal representations can be further refined for previously unseen domains by an efficient adaptation step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient. Our code will be available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As deep neural networks progress to dramatically improve results in most of standard computer vision tasks, there is a growing community interest for more ambitious goals. One of them is to improve the data efficiency of the standard supervised methods that rely on large amount of expensive and time-consuming hand-labeled data. Just like the human intelligence is capable of learning concepts from few labeled samples, few-shot learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref> aims at adapting a classifier to accommodate new classes not seen in training, given a few labeled samples from these classes.</p><p>Earlier works in few-shot learning focus on evaluating their methods in homogeneous learning tasks, e.g. Ominglot <ref type="bibr" target="#b24">[25]</ref>, miniImageNet <ref type="bibr" target="#b53">[53]</ref>, tieredImageNet <ref type="bibr" target="#b43">[43]</ref>, where both the meta-train and meta-test examples are sampled from a single data distribution (or dataset). Recently, the interest of the community has shifted to a more realistic and challenging experimental setting, where the goal is to learn few-shot models that can generalize not only within  <ref type="figure">Figure 1</ref>. Universal Representation Learning (URL). To learn universal representations from multiple domains that can generalize to previously unseen domains, one strategy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref> is to learn one feature extractor for each domain and learn to retrieve or combine feature extractors for the target task during meta-test stage as in (a). We propose a universal representation network (b) which is learned by distilling knowledge learned from multiple datasets {f ? * ? } K ? to one single feature extractor f ? shared across all domains. In metatest stage, we use a linear transformation A ? that further refines the universal representations for better generalization to unseen domains. Our universal representation network achieves better generalization performance than using multiple domain-specific ones while being more efficient than (a). a single data distribution but also to previously unseen data distributions. To this end, Triantafillou et al. <ref type="bibr" target="#b52">[52]</ref> propose a new heterogeneous benchmark, Meta-Dataset that consists of ten datasets from different domains for meta-training and meta-test. While, initially two domains were kept as unseen domains, later three more unseen domains are included to meta-test the generalization ability of learned models.</p><p>While the few-shot methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b53">53]</ref>, which were proposed before Meta-Dataset was available, can be directly applied to this new benchmark with minor modifi-cations, they fail to cope with domain gap between train and test datasets and thus obtain subpar performance on Meta-Dataset. Recently several few-shot learning methods are proposed to address this challenge, which can be coarsely grouped into two categories, adaptation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">44]</ref> and feature selection based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. CNAPS <ref type="bibr" target="#b44">[44]</ref> consists of an adaptation network that modulates the parameters of both a feature extractor and classifier for new categories by encoding the data distribution of few training samples. Simple CNAPS <ref type="bibr" target="#b1">[2]</ref> extends CNAPS by replacing its parametric classifier with a non-parametric classifier based on Mahalanobis distance and shows that adapting the classifier from few samples is not necessary for good performance. SUR <ref type="bibr" target="#b12">[13]</ref> and URT <ref type="bibr" target="#b28">[29]</ref> further show that adaptation for the feature extractor can also be replaced by a feature selection mechanism. In particular, both methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref> learn a separate deep network for each training dataset in an offline stage, employ them to extract multiple features for each image, and then select the optimal set of features either based on a similarity measure <ref type="bibr" target="#b12">[13]</ref> or on learning an attention mechanism <ref type="bibr" target="#b28">[29]</ref>. However, despite their good performance, SUR and URT are computationally expensive and require multiple forward passes through multiple networks during inference time.</p><p>In this work, we propose an efficient and high performance few-shot method based on multi-domain learning. Like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, our method builds on multi-domain representations that are learned in an offline stage. However, we learn a single set of universal representations (a single deep neural network) over multiple domains which has a fixed computational cost regardless of the number of domains at inference unlike them. Similar to the adaptation based techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">44]</ref>, our method further employs a simple adaptation strategy to learn the domain specific representations from few samples (an illustration in <ref type="figure">Fig. 1</ref>).</p><p>In particular, we propose to distill the knowledge from multiple domains to a single model, which can efficiently leverage useful information from multiple diverse domains. Learning multi-domain representations is a challenging task and requires to leverage commonalities in the domains while minimizing interference (negative transfer) (e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b56">56]</ref>) between them. To mitigate this, we align the intermediate representations of our multi-domain network with the ones of the domain-specific networks after carefully aligning each space by using small task-specific adapters and Centered Kernel Alignment (CKA) <ref type="bibr" target="#b21">[22]</ref>. Finally, inspired from the use of Mahalanobis distance in <ref type="bibr" target="#b1">[2]</ref>, we adapt the learned multi-domain features into the new task by mapping them into a task-specific space. However, unlike <ref type="bibr" target="#b1">[2]</ref>, we learn the parameters of this mapping via adaptation in a discriminative way. We rigorously evaluate our method in Meta-Dataset benchmark and show that our method outperforms the stateof-the-art few-shot methods significantly in both seen and unseen domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-learning based few-shot classification. One approach that directly trains a model to perform few-shot classification end-to-end is meta-learning. Meta-learning approaches for few-shot learning can be broadly divided into two groups, metric-based and optimization-based approaches. The key idea in the former group is to map raw images to vector representations and use nearest neighbor classifiers with different distance functions by learning discriminative feature spaces with Siamese networks <ref type="bibr" target="#b20">[21]</ref>, producing a weighted nearest neighbor classifier <ref type="bibr" target="#b53">[53]</ref>, representing each class with the average of the samples in the support set <ref type="bibr" target="#b48">[48]</ref>. The latter group focuses on learning models that can quickly adapt to new tasks from few samples in support. The successful methods include MAML <ref type="bibr" target="#b13">[14]</ref> that poses learning to learn problem in a bi-level optimization where the weights of the network are modeled as a function of the initial network weights, Reptile <ref type="bibr" target="#b34">[35]</ref> that alleviates the expensive second order derivative computation in MAML by a first order approximation, MAML++ <ref type="bibr" target="#b0">[1]</ref> that introduces multiple speed and stability improvements over MAML.</p><p>Transfer learning based few-shot classification. There are also simple yet effective methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref> that first learn a neural network on all the available training data and transfer it to few-shot tasks in test time. Baseline++ <ref type="bibr" target="#b5">[6]</ref> only updates a parametric classifier with cosine distance, while Meta-Baseline <ref type="bibr" target="#b6">[7]</ref> fine-tunes entire network with a nearestcentroid cosine similarity and a scale parameter. Dhillon et al. <ref type="bibr" target="#b10">[11]</ref> explore fine-tuning in a transductive setting, where the query set is assumed to be available at the same time.</p><p>Cross-domain few-shot classification. Recent few-shot techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">44]</ref> focus on few-shot learning that generalizes to unseen domains at test time in the recently proposed Meta-Dataset <ref type="bibr" target="#b52">[52]</ref>. CNAPS <ref type="bibr" target="#b44">[44]</ref> adapts the parameters of feature encoder and classifier by conditioning them on current input task via FiLM layers <ref type="bibr" target="#b39">[39]</ref> which is further extended in Simple CNAPS <ref type="bibr" target="#b1">[2]</ref> adopts a non-parametric classifier using a simple class-covariance-based distance metric, namely the Mahalanobis distance. In contrast SUR <ref type="bibr" target="#b12">[13]</ref> stores the domain-specific knowledge by learning an independent feature extractor for each domain, and automatically selects the most relevant representations for a new task by linearly combining features from domain-specific features. URT <ref type="bibr" target="#b28">[29]</ref> instead meta-learns the feature selection mechanism for new tasks by using Transformer layers. Like SUR and URT, our method uses multi-domain features but in a more efficient way, by learning a single network over multiple domains. Our method requires significantly less network capacity and compute load than theirs. In addition, similar to Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, we map our features to a task-specific space before applying the nearest neighbor classifier but we learn the parameters of this mapping from each support set.</p><p>Knowledge distillation. Our work is related to knowledge distillation (KD) methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50]</ref> that distills the knowledge of an ensemble of large teacher models to a small student neural network at the classifier <ref type="bibr" target="#b16">[17]</ref> and intermediate layers <ref type="bibr" target="#b45">[45]</ref>. Born-Again Neural Networks <ref type="bibr" target="#b14">[15]</ref> uses KD proposes to consecutively distill knowledge from an identical teacher network to a student network, which is further applied to few-shot learning in <ref type="bibr" target="#b51">[51]</ref> and multi-task learning in <ref type="bibr" target="#b9">[10]</ref>. Most similar to our work, Li and Bilen <ref type="bibr" target="#b26">[27]</ref> apply knowledge distillation to align features of a student multitask network to multiple single-task learning networks by introducing task-specific adapters. While we use task-specific adapters to align the features across multiple networks like <ref type="bibr" target="#b26">[27]</ref>, we apply the alignment to a more challenging setting of multi-domain learning where there are substantial gap between different domains unlike their method that is shown to work in multi-task learning where multiple tasks are sampled from a single data distribution. To this end, we incorporate a more effective feature matching loss inspired from Centered Kernel Alignment (CKA) to align features in presence of large domain gap. Universal representation. A representation that works equally well in multiple domain, termed universal representation, is introduced in <ref type="bibr" target="#b2">[3]</ref>. To learn a universal representation in multiple domains, SUR <ref type="bibr" target="#b12">[13]</ref> and URT <ref type="bibr" target="#b28">[29]</ref> propose to learn an independent model for each domain and learn to retrieve or blend appropriate models for a new task in few-shot classification. Alternatively, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> propose to learn a single network to perform image classification on very different domains by sharing a large majority of parameters across domains and encoding domain-specific information via normalization layers <ref type="bibr" target="#b2">[3]</ref>, light-weight residual adapters <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref>, Feature-wise Linear Modulate (FiLM) <ref type="bibr" target="#b39">[39]</ref>. Our method is inspired from these methods, thus we learn universal representations without any domain-specific weights and use them in few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the problem setting, introduce our method in two parts, multi-domain feature learning and feature adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Few-shot Task Formulation</head><p>Few-shot classification aims at learning to classify samples from a small training set with only few samples for each class. The task contains two sets of images: a support set</p><formula xml:id="formula_0">S = {(x i , y i )} |S| i=1</formula><p>that contains |S| image and label pairs respectively that define the classification task and a query set</p><formula xml:id="formula_1">Q = {(x j )} |Q| i=1</formula><p>that contains |Q| samples to be classified. In words, we would like to learn a classifier on the support set that can accurately predict the labels of the query set.</p><p>As in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, we solve this problem in two steps: i) a meta-training step where a learning algorithm receives a large dataset D b and outputs a general feature extractor f , ii) a meta-test step where the target tasks (S, Q) are sampled from another large dataset D t by taking the subsets of the dataset to build S and Q. Note that D b and D t contain mutually exclusive classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning multiple domain representations</head><p>Our focus is to learn few-shot image classification that generalizes not only within previously seen visual domains but also to unseen ones. As it is challenging to obtain the domain-specific knowledge from only few samples in a previously unseen domain, inspired from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">41]</ref> we hypothesize that using domain-agnostic or universal representations is the key to the success of cross-domain generalization. To this end, we propose learning a multi-domain network that works well for all the domain-specific tasks simultaneously and use this network as a feature extractor for the target tasks.</p><p>Let assume that D b consists of K subdatasets, each sampled from a different domain. One potential solution is train a multi-domain network by jointly optimizing its parameters over the images from all K domains (datasets):</p><formula xml:id="formula_2">min ?,?? K ? =1 1 |D ? | x,y?D? (h ?? ? f ? (x), y),<label>(1)</label></formula><p>where is cross-entropy loss, f is a multi-domain feature extractor that takes an image as input and outputs a d dimensional feature and is parameterized by a single set of parameters ? which is shared across K domains. h is a domain-specific classifier that takes in f ? (x) and outputs a probability vector over the target categories and it is parameterized by ? ? . While minimizing Eq. (1) results in a multi-domain feature extractor f , several previous works report that this optimization is problematic due to the interference between the different tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">56]</ref>, varying dataset sizes and difficulty <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref> and often leads to subpar results compared to individual single-domain networks. Motivated by this challenge, we propose a two stage procedure to learn multi-domain representations, inspired by the previous distillation methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. To this end, we first train domain-specific deep networks where each consists of a specific feature extractor f ? * ? and classifier h ? * ? with parameters ? * ? and ? * ? respectively, similarly to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. However, instead of using K domain-specific feature extractors and select the most relevant feature like them, we propose to learn a single multi-domain network that performs well in K domains by distilling the knowledge of K pretrained feature extractors. This has two key advantages over <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. First using a single feature extractor, which has the same capacity with each domain-specific one, is significantly more efficient in terms of run-time and number  <ref type="figure">Figure 2</ref>. Illustration of our proposed method for multi-domain feature learning. Given training images from K different domains, we first train K domain-specific networks f ? * 1 , . . . , f ? * K and their classifiers h ? * 1 , . . . , h ? * K , freeze their weights and distill their knowledge to our multi-domain network by matching their features and predictions through two loss functions f and p respectively. As matching multiple features is challenging, we co-align all the features by using light-weight adaptors A ? 1 , A ? 2 , . . . , A ? K and centered kernel alignment.</p><p>of parameters in the meta-test stage. Second learning to find the most relevant features for a given support and query set in <ref type="bibr" target="#b28">[29]</ref> is not trivial and may also suffer from overfitting to the small number of datasets in the training set, while the multi-domain representations automatically contain the required information from the relevant domains.</p><p>In the second stage, we freeze the pretrained domainspecific feature extractors f ? * ? and transfer their knowledge into the multi-domain model at train time. Knowledge distillation can be performed at the prediction <ref type="bibr" target="#b16">[17]</ref> and feature level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">45]</ref> by minimizing the distance between (i) the predictions of the multi-domain and corresponding singledomain network, and also between (ii) the multi-domain and single-domain features for given training samples. While Kullback-Leibler (KL) divergence is the standard choice for the predictions in <ref type="bibr" target="#b16">[17]</ref>, matching the multi-domain features to multiple single-domain ones simultaneously is an ill-posed problem, as the domain-specific features for a given image x can vary the multi-domain network significantly across different domains. To this end, as in <ref type="bibr" target="#b26">[27]</ref>, we propose to map each domain specific feature into a common space by using adaptors A ?? ? R d?d with parameters ? ? and jointly train them along with the parameters of the multi-domain network:</p><formula xml:id="formula_3">min ?,?? ,?? K ? =1 1 |D? | x,y?D? (h ?? ? f ? (x), y)+ ? p ? p (h ?? ? f ? (x), h ? * ? ? f ? * ? (x)) + ? f ? f (A ?? ? f ? (x), f ? * ? (x))<label>(2)</label></formula><p>where p is KL divergence on network predictions, f is a distance function in the feature space, ? p ? and ? f ? are their domain-specific weights. We illustrate this key idea in <ref type="figure">Fig. 2</ref>.</p><p>In words, the multi-domain network is optimized to match the domain-specific features up to a transformation (i.e. A ?? ) and predict the ground-truth classes y ? . While Li and Bilen <ref type="bibr" target="#b26">[27]</ref> show that L2 distance is effective to match the features across task-agnostic and task-specific networks, which are trained for different tasks on a single domain, here we argue that learning to match features that are trained on substantially diverse domains require better a more complex distance distance function to model nonlinear correlations between the representations. To this end, inspired from <ref type="bibr" target="#b21">[22]</ref>, we propose to adopt the Centered Kernel Alignment (CKA) <ref type="bibr" target="#b21">[22]</ref> similarity index with the rbf kernel that is shown to be capable of meaningful non-linear similarities between representations of higher dimension than the number of data points.</p><p>Next we briefly describe CKA.</p><formula xml:id="formula_4">Suppose M = [f ? (x 1 ), . . . , f ? (x n )] ? R n?d and Y = [f ? * ? (x 1 ), . . . , f ? * ? (x n )]</formula><p>? R n?d denote the features that are computed by the multi-domain and domain-specific networks respectively for a given set of images {x 1 , . . . , x n }. We first compute the Radial Basis Function kernel matrices P and T of M and Y respectively. Then we use two kernel matrices P and T to measure the dissimilarity of X and Y as following:</p><formula xml:id="formula_5">f (M, Y) = 1 ? tr(PHTH)/ tr(PHPH)tr(THTH),<label>(3)</label></formula><p>where tr and H denote the trace of a matrix and centering matrix H n = I n ? 1 n 11 respectively, the second term is the CKA similarity between the multi-domain and domain-specific features. As the original CKA similarity requires the computation of the kernel matrices over the whole datasets, which is not scalable to large datasets, we follow <ref type="bibr" target="#b33">[34]</ref> and compute them over each minibatch in our training. We refer to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature adaptation in meta-test</head><p>During meta-test, given a support set</p><formula xml:id="formula_6">S = {(x i , y i )} |S| i=1</formula><p>of a new learning task, we use the multi-domain model to</p><formula xml:id="formula_7">extract features {f ? (x i )} |S| i=1</formula><p>and adapt them to the target task. To this end, we apply a linear transformation A ? :</p><formula xml:id="formula_8">R d ? R d with learnable parameters ? to the computed features, i.e. {z i } |S| i=1 = {A ? ? f ? (x i )} |S| i=1 where ? ? R d?d .</formula><p>Then we follow a similar pipeline to the one in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">48]</ref> to build a centroid classifier by averaging the embeddings belonging to this class:</p><formula xml:id="formula_9">c j = 1 |S j | zi?Sj z i , S j = {z k : y k = j}, j = 1, . . . , C (4)</formula><p>where C is the number of classes in the support set. Next we estimate the likelihood of a support sample z by:</p><formula xml:id="formula_10">p(y = l|z) = exp(?d(z, c l )) C j=1 exp(?d(z, c j )) ,<label>(5)</label></formula><p>where d(z, c l ) is the negative cosine similarity. We then optimize ? to minimize the following objective on the support set S:</p><formula xml:id="formula_11">min ? 1 |S| xi,yi?S [log(p(y = y i |x i ))].<label>(6)</label></formula><p>Solving Eq. (6) for ? results in high intra-class and low interclass similarity in the adapted space. We then use ? and Eq. (5) to predict the label of the query sample from Q by picking the closest centroid c j . Our meta-test pipeline is illustrated <ref type="figure">Fig. 3</ref>. Discussion. In <ref type="bibr" target="#b1">[2]</ref>, Simple CNAPS uses the (squared) Mahalanobis distance between the features of class centroid and a query image,</p><formula xml:id="formula_12">d(z, c) = 1 2 (f ? (x) ? c ) Q ?1 (f ? (x) ? c )</formula><p>where Q is a covariance matrix specific to the task and class and c is the class centroid in the feature space (before the adaptation). The authors show that considering the class covariance enables better adaptation of the feature extractor to the target task. Our adaptation strategy can be seen as a generalization of the Mahalanobis distance computation. Alternatively, assuming that Q ?1 can be decomposed into a product of a lower triangular matrix and its conjugate transpose, i.e. Q ?1 = LL , one can first pre-transform the features by multiplication, i.e. z = L f ? (x) and then compute the distance between these features and centroids. Similarly, we apply a linear transformation to the features but unlike <ref type="bibr" target="#b1">[2]</ref>, we learn its parameters ? by optimizing Eq. <ref type="bibr" target="#b5">(6)</ref>.  <ref type="figure">Figure 3</ref>. Illustration of adaptation procedure in meta-test. Given a support set and query image, our method learns to map their features to a task-specific space through a linear transformation A ? and assign the query image to the nearest class center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here we first describe the benchmarks, implementation details and competing methods. Then we rigorously compare our method to the state-of-the-art and also study each proposed component in an ablation. We also analyze our method qualitatively. Finally we evaluate our method in a global retrieval task to further evaluate the learned feature representations in few-shot classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Dataset. Meta-Dataset <ref type="bibr" target="#b52">[52]</ref> is a few-shot classification benchmark that initially consisted of ten datasets: ILSVRC 2012 <ref type="bibr" target="#b46">[46]</ref> (ImageNet), Omniglot <ref type="bibr" target="#b24">[25]</ref>, FGVC-Aircraft <ref type="bibr" target="#b30">[31]</ref> (Aircraft), CUB-200-2011 <ref type="bibr" target="#b54">[54]</ref> (Birds), Describable Textures <ref type="bibr" target="#b8">[9]</ref> (DTD), QuickDraw <ref type="bibr" target="#b18">[19]</ref>, FGVCx Fungi <ref type="bibr" target="#b3">[4]</ref> (Fungi), VGG Flower <ref type="bibr" target="#b35">[36]</ref> (Flower), Traffic Signs <ref type="bibr" target="#b17">[18]</ref> and MSCOCO <ref type="bibr" target="#b27">[28]</ref> then further expanded with MNIST <ref type="bibr" target="#b25">[26]</ref>, CIFAR-10 <ref type="bibr" target="#b22">[23]</ref> and CIFAR-100 <ref type="bibr" target="#b22">[23]</ref>. We follow the standard procedure and use the first eight datasets for meta-training, in which each dataset is further divided into train, validation and test set with disjoint classes. The evaluation within these datasets is used to measure the generalization ability in the seen domains. The rest five datasets are reserved as unseen domain for meta-test for measuring the cross-domain generalization ability. Implementation details. We use PyTorch <ref type="bibr" target="#b38">[38]</ref> library to implement our method. In all experiments we build our method on ResNet-18 <ref type="bibr" target="#b15">[16]</ref> backbone for both single-domain and multi-domain networks. In the multi-domain network, we share all the layers but the last classifier across the domains. For training single-domain models, we strictly follow the training protocol in <ref type="bibr" target="#b12">[13]</ref>, use a SGD optimizer with a momentum and the cosine annealing learning scheduler with the same hyperparameters. For our multi-domain network, we use the same optimizer and scheduler as before, train it for 240,000 iterations. We set ? f and ? p as 4 for Ima- (1) without the proposed distillation method. As additional baseline, we include the best performing method in <ref type="bibr" target="#b52">[52]</ref>, i.e. Proto-MAML <ref type="bibr" target="#b52">[52]</ref>, and as well as the state-of-the-art methods, OHB-E <ref type="bibr" target="#b47">[47]</ref>, CNAPS <ref type="bibr" target="#b44">[44]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and the Simple CNAPS [2] 1 . For evaluation, we follow the standard protocol in <ref type="bibr" target="#b52">[52]</ref>, randomly sample 600 tasks for each dataset, and report average accuracy and 95% confidence score in all experiments. We reproduce results by training and evaluating SUR <ref type="bibr" target="#b12">[13]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and Simple CNAPS <ref type="bibr" target="#b1">[2]</ref> using their code for fair comparison as recommended by Meta-Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>As in Meta-Dataset [52], we sample each task with varying number of ways and shots and report the results in Table 1. Our method outperforms the state-of-the-art methods in seven out of eight seen datasets and four out of five unseen datasets. We also compute average rank as recommended in <ref type="bibr" target="#b52">[52]</ref>, our method ranks 1.3 in average and the state-of-theart methods SUR and URT rank 5.0 and 4.4, respectively. More specifically, we obtain significant better results than the second best approach on Aircraft (+2.8), Birds (+2.1), Texture (+4.2), and VGG Flower (+1.5) for seen domains <ref type="bibr" target="#b0">1</ref> Results of Proto-MAML <ref type="bibr" target="#b52">[52]</ref>, BOHB-E <ref type="bibr" target="#b47">[47]</ref>, and CNAPS <ref type="bibr" target="#b44">[44]</ref> are obtained from Meta-Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying-Way Five-Shot</head><p>Five-Way One-Shot  and Traffic Sign (+6.1) 2 and MSCOCO (+3.8). The results show that jointly learning a single set of representations provides better generalization ability than fusing the ones from multiple single-domain feature extractors as done in SUR and URT. Notably, our method requires less parameters and less computations to run during inference than SUR and URT, as it runs only one universal network to extract features, while both SUR and URT need to pass the query set to multiple single-domain network.</p><p>We also see that our method outperforms two strong baselines, Best SDL and MDL in all datasets except in Quick-Draw. This indicates that i) universal representations are superior to the single-domain ones while generalizing to new tasks in both seen and unseen domains, while requiring significantly less number of parameters (1 vs 8 neural networks), ii) our distillation strategy is essential to obtain good <ref type="bibr" target="#b1">2</ref> The accuracy of all methods on Traffic Sign is different from the one in the original papers as one bug has been fixed in Meta-Dataset repository.</p><p>See https://github.com/google-research/ meta-dataset/issues/54 for more details.   <ref type="table">Table 4</ref>. Comparison of different classifiers that are incorporated to our method during meta-test stage. NCC, MD, LR, SVM denote nearest center classifier, Mahalanobis distance, logistic regression, support vector machines respectively. multi-domain representations. While MDL outperforms the best SDL in certain domains by transferring representations across them, its performance is lower in other domains than SDL, possibly due to negative transfer across the significantly diverse domains. Surprisingly, MDL achieves the third best in average rank, indicating the benefit of multidomain representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further results</head><p>Varying-way five-shot setting. After reporting results in a broad range of varying shots (e.g. up to 100 shots in some extreme cases), we further analyze our method for 5-shot setting with varying number of categories. To this end, we follow the setting in <ref type="bibr" target="#b11">[12]</ref>, compare our method to the best three state-of-the-art methods including Simple CNAPS, SUR and URT. In this setting, we sample a varying number of ways in Meta-Dataset the same as the standard setting but a fixed number of shots to form balanced support and query sets. As shown in <ref type="table" target="#tab_2">Table 2</ref>, overall performance for all methods decreases in most datasets compared to results in <ref type="table" target="#tab_0">Table 1</ref> indicating that this is a more challenging setting. It is due to that five-shot setting samples much less support images than the standard setting. The ranking of different methods change slightly. The top-2 methods remain the same, while both Simple CNAPS and SUR obtain 3.0 average rank. SUR performs the best on MNIST, Simple CNAPS outperforms others on CIFAR-100 and URT is top-1 on Quick Draw. Ours still achieves significant better performance than other methods on the rest ten datasets. Results in five-way one-shot setting. Next we test an extremely challenging five-way one-shot setting on Meta-Dataset. For each task, only one image per class is seen as support set. This setting is often used in evaluating different methods in a single domain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b53">53]</ref>, while we adopt it for multiple domains. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our method achieves consistent gain as observed in previous two settings, which validates the importance of good universal representations in case of limited labeled samples in metatest. Interestingly, Simple CNAPS achieves better rank than SUR in this setting, which is opposite in previous settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analyses</head><p>Here we conduct an ablation study on different components in our framework by varying the loss function for the distillation, classifier type in meta-test. Different distillation loss functions. First we study different distillation loss functions, including L2 loss, cosine distance, KL divergence and CKA for learning the multidomain networks and report their performances in <ref type="table" target="#tab_4">Table 3</ref>. While we apply KL divergence loss to match the logits of single and multi-domain networks as in <ref type="bibr" target="#b16">[17]</ref>, the other loss functions are used to match the internal representations (features that are fed into classifiers) between those models. Among the individual loss functions, the best results are obtained with either our model with CKA or KL divergence loss, while CKA outperforms KL divergence in the most domains. Although the features are first aligned with an adapter, L2 and cosine loss functions are not sufficient to match features from very diverse domains and further aligning features with CKA is crucial. Note that here L2 baselines corresponds to the method of <ref type="bibr" target="#b26">[27]</ref>. Finally, combining CKA with KL divergence gives the best performance over the multi-domain models that are trained with the individual loss functions. Different classifiers in meta-test. Next we evaluate the proposed adaptive mapping strategy with the nearest neighbor classifier (NCC), described in Section 3.3, to different parametric including Support Vector Machines (SVM), Logistic Regression (LR) as in <ref type="bibr" target="#b51">[51]</ref> and non-parametric classifiers including NCC without the adaptive mapping and NCC with Mahalanobis Distance (NCC+MD) in <ref type="bibr" target="#b1">[2]</ref> in <ref type="table">Table 4</ref>. For non-parametric classifiers, NCC performs best in unseen domains when used with Mahalanobis distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head><p>Omniglot <ref type="table" target="#tab_0">Aircraft  Birds  Textures  Quick Draw  Fungi  VGG Flower  Traffic Sign  MSCOCO  MNIST  CIFAR-10  CIFAR-100   Recall@k  1  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2</ref>   <ref type="table">Table 5</ref>. Global retrieval performance on Meta-Dataset. In addition to few-shot learning experiments, we evaluate our method in a non-episodic retrieval task to further compare the generalization ability of our universal representations. The parametric classifiers, SVM and LR that are trained on the limited support set obtain very competitive results and outperform the non-parametric ones in most domains. Our method, which combines the benefit of parametric and nonparametric classifiers, outperforms SVM, LR and NCC+MD in most seen datasets, while achieves worse in some unseen domains like Traffic Sign and MNIST.</p><p>Qualitative results. We qualitatively analyze our method and compare it to URT <ref type="bibr" target="#b28">[29]</ref> in <ref type="figure" target="#fig_3">Fig. 4</ref> by illustrating the nearest neighbors in four different datasets given a query image (see supplementary for more examples). It is clear that our method produces more correct neighbors than URT. URT retrieves images with more similar colors, shapes and backgrounds, while our method is able to retrieve semantically similar images. It again suggests that our method is able to learn more useful and general representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Global retrieval</head><p>Here we go beyond the few-shot classification experiments and evaluate the generalization ability of our representations that are learned in the multi-domain network in a retrieval task, inspired from metric learning literature <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b55">55]</ref>. To this end, for each test image, we find the nearest images in entire test set in the feature space and test whether they correspond to the same category. For evaluation metric, we use Recall@k which considers the predictions with one of the k closest neighbors with the same label as positive. In <ref type="table">Table 5</ref>, we compare our method with Simple CNAPS in Re-call@1 and Recall@2 (see supplementary for more results). URT and SUR require adaption using support set and no such adaptation in retrieval task is possible, we replace them with two baselines that concatenate or sum features from multiple domain-specific networks. Our method achieves the best performance in ten out of thirteen domains with significant gains in Aircraft, Birds, Textures and Fungi. This strongly suggests that our multi-domain representations are the key to the success of our method in the previous few-shot classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we demonstrate that learning a single set of universal representations integrated with a feature refining step achieves state-of-the-art performance in the recent Meta-Dataset benchmark. To this end, we propose to optimize the parameters of a deep neural network simultaneously over multiple domains by aligning its features with multiple single-domain networks through linear adapters and a loss function that is inspired from CKA. We show that the universal features can be further refined from few examples to unseen tasks by learning a transformation in a similar spirit to distance learning. Our method outperforms the state-ofthe-art techniques while using less number of parameters and being more computationally efficient than other multidomain techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>In all experiments we build our method on ResNet-18 <ref type="bibr" target="#b15">[16]</ref> backbone for both single-domain and multi-domain networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training details of single-domain models</head><p>We train one ResNet-18 model for each training dataset. For optimization, we follow the training protocol in <ref type="bibr" target="#b12">[13]</ref>. Specifically, we use SGD optimizer and cosine annealing for all experiments with a momentum of 0.9 and a weight decay of 7 ? 10 ?4 . The learning rate, batch size, annealing frequency, maximum number of iterations are shown in <ref type="table" target="#tab_7">Table 6</ref>. To regularize training, we also use the exact same data augmentations as in <ref type="bibr" target="#b12">[13]</ref>, e.g. random crops and random color augmentations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training details of our method</head><p>In the multi-domain network, we share all the layers but the last classifier across the domains. To train the multidomain network, we use the same optimizer with a weight decay of 7 ? 10 ?4 and a scheduler as single domain learning model for learning 240,000 iterations. The learning rate is 0.03 and the annealing frequency is 48,000. Similar to <ref type="bibr" target="#b52">[52]</ref> that the training episodes have 50% probability coming from the ImageNet data source, each training batch for our multi-domain network consists of 50% data coming from ImageNet. In other words. The batch size for ImageNet is 64 ? 7 and is 64 for the other 7 datasets.</p><p>We set ? f and ? p as 4 for ImageNet and 1 for other datasets, respectively. And we linearly anneal ? by ? ? ? ? (1 ? t T ), where, t is the current iteration and T is the total number of iterations to anneal ? to zero. Here, T = k ? (anneal. f req.), where anneal. f req. is 48, 000 in this work. We search the k = {1, 2, 3, 4, 5} based on cross-validation over the validation sets of 8 training datasets and k is 5 (i.e. T = 240, 000) for ImageNet, is 2 for Omniglot, Quick Draw, Fungi and is 1 for other datasets. For all experiments, early-stopping is performed based on crossvalidation over the validations sets of 8 training datasets.</p><p>For the optimization of feature adaptation during metatest stage, we initialize ? as an indentity matrix, which allows the NCC to use the original features produced by our universal network and optimize the adaptor ? from a good start point. Similar to the optimization in <ref type="bibr" target="#b12">[13]</ref>, we optimize ? for 40 iterations using Adadelta <ref type="bibr" target="#b57">[57]</ref> as optimizer with a learning rate of 0.1 for first eight datasets and 1 for the last five datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More results</head><p>In this section, we first evaluate each single-domain model for few-shot classification on each test dataset. Then we evaluate the effect of the adaptors for aligning features in knowledge distillation. Then we show complete results on varying-way five-shot and five-way one-shot settings. Finally more qualitative results and global retrieval results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Complete results of single domain learning</head><p>To study the universal representation learning from multiple datasets, we train one network on each training dataset and use each single-domain network as the feature extractor and test it for few-shot classification in each dataset. This involves evaluating 8 single-domain networks on 13 datasets using Nearest Centroid Classifier (NCC). <ref type="table" target="#tab_9">Table 7</ref> shows the results of single domain learning models, where each column present the mean accuracy and 95% confidence interval of a single-domain network trained on one dataset (e.g. ImageNet) and evaluated on 13 test datasets. The average accuracy and 95% confidence intervals computed over 600 few-shot tasks. The numbers in bold indicate that a method has the best accuracy per dataset.</p><p>As shown in <ref type="table" target="#tab_9">Table 7</ref>, the feature of the ImageNet model generalizes well and achieves the best results on four out of eight seen datasets, e.g. ImageNet, Birds, Texture, VGG Flower and four out of five previously unseen datasets, e.g. Traffic Sign, MSCOCO, CIFAR-10, CIFAR-100. The models trained on Omniglot, Aircraft, Quick Draw, and Fungi perform the best on the corresponding datasets while the Omniglot model also generalizes well to MNIST which has the similar style images to Omniglot. We then pick the best performing model, forming the best single-domain model (Best SDL) which serves a very competitive baseline for universal representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Effect of adaptors in knowledge distillation</head><p>In this section, we evaluate our method with adaptors or without adaptors for aligning features when we use CKA for knowledge distillation. From   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Complete results of varying-way five-shot and five-way one-shot</head><p>We further analyze our method for 5-shot setting with varying number of categories. To this end, we follow the setting in <ref type="bibr" target="#b11">[12]</ref>, compare our method to the best three stateof-the-art methods including Simple CNAPS, SUR and URT. In this setting, we sample a varying number of ways in Meta-Dataset the same as the standard setting but a fixed number of shots to form balanced support and query sets. The mean accuracy and 95% confidence interval of our method and compared approaches are depicted in <ref type="table" target="#tab_10">Table 8</ref>. As shown in <ref type="table" target="#tab_10">Table 8</ref>, overall performance for all methods decreases in most datasets compared to results in the conventional setting shown in <ref type="table" target="#tab_0">Table 1</ref> in the paper, indicating that this is a more challenging setting. It is due to that five-shot setting samples much less support images than the standard setting. While both Simple CNAPS and SUR obtain 3.0 average rank. SUR performs the best on MNIST, Simple CNAPS outperforms others on CIFAR-100 and URT is top-1 on Quick Draw. Ours still achieves significant better performance than other methods on the rest ten datasets.</p><p>Results in five-way one-shot setting. Next we test an extremely challenging five-way one-shot setting on Meta-Dataset. For each task, only one image per class is seen as support set. This setting is often used in evaluating different methods in a single domain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b53">53]</ref>   <ref type="table" target="#tab_0">Table 11</ref>. Global retrieval performance on Meta-Dataset (unseen datasets). In addition to few-shot learning experiments, we evaluate our method in a non-episodic retrieval task to further compare the generalization ability of our universal representations.</p><p>adopt it for multiple domains. As shown in <ref type="table" target="#tab_10">Table 8</ref>, our method achieves consistent gain as observed in previous two settings, which validates the importance of good universal representations in case of limited labeled samples in metatest. Interestingly, Simple CNAPS achieves better rank than SUR in this setting, which is opposite in previous settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Qualitatively results</head><p>We qualitatively analyze our method and compare it to the vanilla multi-domain leanring (MDL) baseline, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref> and URT <ref type="bibr" target="#b28">[29]</ref> in Figs. 5 to 17 by illustrating the nearest neighbors in all test datasets given a query image. It is clear that our method produces more correct neighbors than other methods. While other methods retrieves images with more similar colors, shapes and backgrounds, e.g. in <ref type="figure" target="#fig_3">Figs. 13 and 14</ref>, our method is able to retrieve semantically similar images. It again suggests that our method is able to learn more useful and general representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Complete global retrieval results</head><p>Here we go beyond the few-shot classification experiments and evaluate the generalization ability of our representations that are learned in the multi-domain network in a retrieval task, inspired from metric learning literature <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b55">55]</ref>. To this end, for each test image, we find the nearest images in entire test set in the feature space and test whether they correspond to the same category. For evaluation metric, we use Recall@k which considers the predictions with one of the k closest neighbors with the same label as positive. In <ref type="table" target="#tab_0">Tables 10 and 11</ref>, we compare our method with Simple CNAPS in Recall@1, Recall@2, Recall@4 and Recall@8. URT and SUR require adaption using support set and no such adaptation in retrieval task is possible, we replace them with two baselines that concatenate or sum features from multiple domain-specific networks. Our method achieves the best performance in ten out of thirteen domains with significant gains in Aircraft, Birds, Textures and Fungi. This strongly suggests that our multi-domain representations are the key to the success of our method in the previous few-shot classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUR Query</head><p>Nearest Neighbors <ref type="figure">Figure 5</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in ImageNet. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 6</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Omniglot. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 7</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Aircraft. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 8</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Birds. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 9</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Textures. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 10</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Quick Draw. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 11</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Fungi. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 12</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in VGG Flower. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 13</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in Traffic Sign. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure" target="#fig_3">Figure 14</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in MSCOCO. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 15</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in MNIST. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 16</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in CIFAR-10. Green and red colors indicate correct and false predictions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Simple CNAPS SUR Query Nearest Neighbors <ref type="figure">Figure 17</ref>. Qualitative comparison to MDL, Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>, SUR <ref type="bibr" target="#b12">[13]</ref>, and URT <ref type="bibr" target="#b28">[29]</ref> in CIFAR-100. Green and red colors indicate correct and false predictions respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison to URT in four datasets. Green and red colors indicate correct and false predictions respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>? 1.1 51.9 ? 1.1 50.8 ? 1.1 55.8 ? 1.0 53.4 ? 1.1 58.4 ? 1.1 56.2 ? 1.00 56.8 ? 1.1 58.8 ? 1.1 Omniglot 82.7 ? 1.0 67.6 ? 1.2 91.7 ? 0.5 93.2 ? 0.5 93.8 ? 0.4 91.6 ? 0.6 94.1 ? 0.42 94.2 ? 0.4 94.5 ? 0.4 Aircraft 75.3 ? 0.8 54.1 ? 0.9 83.7 ? 0.6 85.7 ? 0.5 86.6 ? 0.5 82.0 ? 0.7 85.5 ? 0.54 85.8 ? 0.5 89.4 ? 0.4 Birds 69.9 ? 1.0 70.7 ? 0.9 73.6 ? 0.9 71.2 ? 0.9 78.6 ? 0.8 74.8 ? 0.9 71.0 ? 1.00 76.2 ? 0.8 80.7 ? 0.8 Textures 68.3 ? 0.8 68.3 ? 0.8 59.5 ? 0.7 73.0 ? 0.6 71.4 ? 0.7 68.8 ? 0.9 71.0 ? 0.80 71.6 ? 0.7 77.2 ? 0.7 Comparison to baselines and state-of-the-art methods on Meta-Dataset. Mean accuracy, 95% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. Average rank is computed according to first 10 datasets as some methods do not report results on last three datasets.</figDesc><table><row><cell>Test Dataset</cell><cell cols="3">Proto-MAML [52] BOHB-E [47] CNAPS [44]</cell><cell>Best SDL</cell><cell>MDL</cell><cell>Simple CNAPS [2]</cell><cell>SUR [13]</cell><cell>URT [29]</cell><cell>Ours</cell></row><row><cell cols="2">ImageNet 46.5 Quick Draw 66.8 ? 0.9</cell><cell>50.3 ? 1.0</cell><cell>74.7 ? 0.8</cell><cell cols="2">82.8 ? 0.6 81.5 ? 0.6</cell><cell>76.5 ? 0.8</cell><cell cols="3">81.8 ? 0.57 82.4 ? 0.6 82.5 ? 0.6</cell></row><row><cell>Fungi</cell><cell>42.0 ? 1.2</cell><cell>41.4 ? 1.1</cell><cell>50.2 ? 1.1</cell><cell cols="2">65.8 ? 0.9 61.9 ? 1.0</cell><cell>46.6 ? 1.0</cell><cell cols="3">64.3 ? 0.95 64.0 ? 1.0 68.1 ? 0.9</cell></row><row><cell>VGG Flower</cell><cell>88.7 ? 0.7</cell><cell>87.3 ? 0.6</cell><cell>88.9 ? 0.5</cell><cell cols="2">87.0 ? 0.6 88.7 ? 0.6</cell><cell>90.5 ? 0.5</cell><cell cols="3">82.9 ? 0.78 87.9 ? 0.6 92.0 ? 0.5</cell></row><row><cell>Traffic Sign</cell><cell>52.4 ? 1.1</cell><cell>51.8 ? 1.0</cell><cell>56.5 ? 1.1</cell><cell cols="2">47.4 ? 1.1 51.0 ? 1.0</cell><cell>57.2 ? 1.0</cell><cell cols="3">51.0 ? 1.11 48.3 ? 1.1 63.3 ? 1.2</cell></row><row><cell>MSCOCO</cell><cell>41.7 ? 1.1</cell><cell>48.0 ? 1.0</cell><cell>39.4 ? 1.0</cell><cell cols="2">53.5 ? 1.0 49.7 ? 1.1</cell><cell>48.9 ? 1.1</cell><cell cols="3">52.0 ? 1.09 51.5 ? 1.1 57.3 ? 1.0</cell></row><row><cell>MNIST</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">89.8 ? 0.5 94.4 ? 0.3</cell><cell>94.6 ? 0.4</cell><cell cols="3">94.3 ? 0.41 90.6 ? 0.5 94.7 ? 0.4</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">67.3 ? 0.8 66.7 ? 0.8</cell><cell>74.9 ? 0.7</cell><cell cols="3">66.5 ? 0.89 67.0 ? 0.8 74.2 ? 0.8</cell></row><row><cell>CIFAR-100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">56.6 ? 0.9 53.6 ? 1.0</cell><cell>61.3 ? 1.1</cell><cell cols="3">56.9 ? 1.10 57.3 ? 1.0 63.6 ? 1.0</cell></row><row><cell>Average Rank</cell><cell>7.8</cell><cell>8.1</cell><cell>6.6</cell><cell>4.8</cell><cell>4.6</cell><cell>5.2</cell><cell>5.0</cell><cell>4.4</cell><cell>1.3</cell></row></table><note>geNet and 1 for other datasets and use early-stopping based on cross-validation over the validations sets of 8 training datasets. We refer to supplementary for more details. Baselines and compared methods. First we compare our method to our own baselines, i) the best single-domain model (Best SDL) where we use each single-domain network as the feature extractor and test it for few-shot classification in each dataset and pick the best performing model. (See supplementary for the complete results) This involves evalu- ating 8 single-domain networks on 13 datasets, serves a very competitive baseline, ii) the vanilla multi-domain learning baseline (MDL) that is learning by optimizing Eq.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Results of Varying-Way Five-Shot and Five-Way One- Shot settings. Mean accuracies are reported and the results with confidence interval are shown in the supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 1.1 57.0 ? 1.1 59.0 ? 1.0 57.0 ? 1.1 58.8 ? 1.1 Omniglot 94.0 ? 0.4 94.1 ? 0.4 94.7 ? 0.4 94.5 ? 0.4 94.5 ? 0.4 Aircraft 87.4 ? 0.5 88.3 ? 0.5 88.9 ? 0.5 89.3 ? 0.4 89.4 ? 0.4 Birds 78.5 ? 0.7 77.5 ? 0.8 80.4 ? 0.7 78.6 ? 0.8 80.7 ? 0.8 Textures 72.8 ? 0.6 73.2 ? 0.7 74.5 ? 0.7 73.3 ? 0.7 77.2 ? 0.7 Quick Draw 81.2 ? 0.6 80.8 ? 0.6 81.9 ? 0.6 81.6 ? 0.6 82.5 ? 0.6</figDesc><table><row><cell>Test Dataset</cell><cell>L2</cell><cell>COSINE</cell><cell>CKA</cell><cell>KL</cell><cell>CKA + KL</cell></row><row><cell cols="4">ImageNet 55.7 Fungi 65.7 ? 0.9 65.9 ? 0.9 66.4 ? 0.9</cell><cell cols="2">67.6 ? 0.9 68.1 ? 0.9</cell></row><row><cell cols="4">VGG Flower 87.5 ? 0.6 85.0 ? 0.6 91.3 ? 0.5</cell><cell cols="2">89.6 ? 0.5 92.0 ? 0.5</cell></row><row><cell cols="4">Traffic Sign 61.6 ? 1.1 59.5 ? 1.1 63.2 ? 1.1</cell><cell cols="2">62.5 ? 1.2 63.3 ? 1.2</cell></row><row><cell>MSCOCO</cell><cell cols="3">53.4 ? 1.0 53.8 ? 1.1 56.6 ? 1.0</cell><cell cols="2">55.6 ? 1.1 57.3 ? 1.0</cell></row><row><cell>MNIST</cell><cell cols="5">94.7 ? 0.3 93.2 ? 0.5 94.7 ? 0.4 95.3 ? 0.4 94.7 ? 0.4</cell></row><row><cell>CIFAR-10</cell><cell cols="3">71.1 ? 0.8 68.1 ? 0.8 73.8 ? 0.7</cell><cell cols="2">72.9 ? 0.8 74.2 ? 0.8</cell></row><row><cell cols="4">CIFAR-100 59.1 ? 1.0 58.1 ? 1.0 62.1 ? 1.0</cell><cell cols="2">60.8 ? 1.0 63.6 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of loss functions for knowledge distillation. Quick Draw 81.8 ? 0.6 80.9 ? 0.7 80.0 ? 0.7 80.0 ? 0.6 82.5 ? 0.6 Fungi 66.2 ? 0.9 57.7 ? 0.9 62.1 ? 0.8 58.5 ? 0.9 68.1 ? 0.9 VGG Flower 91.5 ? 0.5 89.7 ? 0.6 91.1 ? 0.5 91.4 ? 0.6 92.0 ? 0.5 Traffic Sign 49.8 ? 1.1 62.2 ? 1.1 59.7 ? 1.1 65.7 ? 1.2 63.3 ? 1.CIFAR-100 59.1 ? 1.0 60.0 ? 0.9 60.1 ? 1.1 60.5 ? 1.1 63.6 ? 1.0</figDesc><table><row><cell cols="6">Mean accuracy, 95% confidence interval are reported. L2 denotes</cell></row><row><cell cols="6">L2 loss between two feature representations. COSINE represents</cell></row><row><cell cols="6">negative cosine similarity function. KL means KL divergence loss</cell></row><row><cell cols="6">function on the network predictions. All results are obtained with</cell></row><row><cell cols="4">feature adaptation during meta-test stage.</cell><cell></cell><cell></cell></row><row><cell>Test Dataset</cell><cell>NCC</cell><cell>NCC+MD</cell><cell>LR</cell><cell>SVM</cell><cell>Ours</cell></row><row><cell>ImageNet</cell><cell>57.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>0 ? 1.1 53.9 ? 1.0 56.0 ? 1.1 54.5 ? 1.1 58.8 ? 1.1 Omniglot 94.4 ? 0.4 93.8 ? 0.5 93.7 ? 0.5 94.3 ? 0.5 94.5 ? 0.4 Aircraft 88.0 ? 0.5 87.6 ? 0.5 88.3 ? 0.6 87.7 ? 0.5 89.4 ? 0.4 Birds 80.3 ? 0.7 78.3 ? 0.7 79.7 ? 0.8 78.1 ? 0.8 80.7 ? 0.8 Textures 74.6 ? 0.7 73.7 ? 0.7 74.7 ? 0.7 73.8 ? 0.8 77.2 ? 0.72 MSCOCO 54.1 ? 1.0 48.5 ? 1.0 51.2 ? 1.1 50.5 ? 1.0 57.3 ? 1.0 MNIST 91.1 ? 0.4 95.1 ? 0.4 93.5 ? 0.5 95.4 ? 0.4 94.7 ? 0.4 CIFAR-10 70.6 ? 0.7 68.9 ? 0.8 73.1 ? 0.8 72.0 ? 0.8 74.2 ? 0.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Sum 22.1 30.3 84.7 91.8 69.7 80.7 45.9 59.7 66.3 78.2 77.4 84.3 31.9 42.9 85.1 92.1 94.6 97.2 62.6 71.2 98.3 99.2 54.0 68.9 27.8 37.4 Concate 20.2 28.0 84.4 91.5 44.3 58.1 35.5 48.8 68.8 78.2 73.0 80.8 30.7 40.4 83.4 91.3 95.1 97.3 60.7 69.8 98.7 99.3 49.7 65.3 25.4 34.6 MDL 29.8 39.6 89.8 94.3 80.3 87.1 63.2 75.9 67.0 77.1 79.5 85.4 40.2 51.7 86.9 93.3 89.5 94.1 63.6 72.6 97.6 98.8 58.9 72.9 31.6 42.0 Simple CNAPS [2] 34.0 43.8 84.9 91.6 70.5 82.5 55.9 70.5 64.8 76.9 75.3 83.0 29.1 39.0 88.1 94.1 79.9 86.9 65.2 73.8 97.5 98.8 66.2 79.3 33.2 44.2 Ours 36.1 46.2 89.7 94.3 83.3 90.4 66.7 78.9 70.2 80.8 79.9 86.5 44.5 56.2 90.0 94.6 87.9 93.0 67.4 76.3 97.0 98.4 62.1 76.5 35.1 46.1</figDesc><table><row><cell>1</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Training hyper-parameters of single domain learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 ,</head><label>9</label><figDesc>We can see that using adaptors can improve the performance, such as Birds (+1.7) and VGG Flower (+3.6), MSCOCO (+1.3). This indicates that the adaptors A ? help align features between multi-domain and single-domain learning networks which are learned from very different domains. ? 1.0 17.1 ? 0.6 21.7 ? 0.7 25.4 ? 0.8 24.2 ? 0.8 24.1 ? 0.8 32.9 ? 0.9 25.0 ? 0.8 Omniglot 67.4 ? 1.2 93.2 ? 0.5 58.2 ? 1.2 58.7 ? 1.4 57.3 ? 1.4 78.4 ? 1.0 57.6 ? 1.3 54.6 ? 1.3 Aircraft 49.5 ? 0.9 16.8 ? 0.5 85.7 ? 0.5 31.4 ? 0.8 26.0 ? 0.7 23.8 ? 0.6 31.0 ? 0.7 24.6 ? 0.6 Birds 71.2 ? 0.9 13.0 ? 0.6 19.9 ? 0.7 65.0 ? 0.9 19.6 ? 0.7 16.7 ? 0.7 42.8 ? 1.0 28.9 ? 0.8 Textures 73.0 ? 0.6 25.0 ? 0.5 38.6 ? 0.7 42.2 ? 0.7 54.9 ? 0.7 38.6 ? 0.6 54.1 ? 0.7 42.3 ? 0.7 Quick Draw 53.9 ? 1.0 51.0 ? 1.0 38.8 ? 1.0 38.2 ? 1.0 36.8 ? 0.9 82.8 ? 0.6</figDesc><table><row><cell>Test Dataset</cell><cell>Train Dataset</cell><cell>ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell>Vgg Flower</cell></row><row><cell cols="2">ImageNet</cell><cell cols="7">55.8 37.7 ? 0.9</cell><cell>39.7 ? 1.0</cell></row><row><cell cols="2">Fungi</cell><cell>41.6 ? 1.0</cell><cell>9.1 ? 0.5</cell><cell cols="3">14.9 ? 0.7 25.5 ? 0.8 15.6 ? 0.7</cell><cell>12.5 ? 0.6</cell><cell cols="2">65.8 ? 0.9 23.3 ? 0.8</cell></row><row><cell cols="2">VGG Flower</cell><cell cols="2">87.0 ? 0.6 23.8 ? 0.6</cell><cell cols="3">45.5 ? 0.8 62.9 ? 0.8 44.4 ? 0.8</cell><cell>33.4 ? 0.7</cell><cell>79.6 ? 0.7</cell><cell>78.3 ? 0.7</cell></row><row><cell cols="2">Traffic Sign</cell><cell cols="2">47.4 ? 1.1 15.1 ? 0.7</cell><cell cols="3">30.8 ? 0.9 31.0 ? 0.9 38.8 ? 1.1</cell><cell>31.1 ? 0.9</cell><cell>28.0 ? 0.9</cell><cell>30.4 ? 0.9</cell></row><row><cell cols="2">MSCOCO</cell><cell cols="2">53.5 ? 1.0 12.9 ? 0.6</cell><cell cols="3">22.5 ? 0.8 25.1 ? 0.9 23.7 ? 0.8</cell><cell>21.3 ? 0.8</cell><cell>32.5 ? 1.0</cell><cell>25.7 ? 0.8</cell></row><row><cell cols="2">MNIST</cell><cell cols="5">78.1 ? 0.7 89.8 ? 0.5 68.0 ? 0.8 73.0 ? 0.7 64.5 ? 0.8</cell><cell>88.2 ? 0.5</cell><cell>62.2 ? 0.8</cell><cell>72.1 ? 0.7</cell></row><row><cell cols="2">CIFAR-10</cell><cell cols="2">67.3 ? 0.8 28.5 ? 0.6</cell><cell cols="3">41.2 ? 0.7 41.8 ? 0.8 36.9 ? 0.7</cell><cell>40.0 ? 0.7</cell><cell>38.8 ? 0.7</cell><cell>41.3 ? 0.8</cell></row><row><cell cols="2">CIFAR-100</cell><cell cols="2">56.6 ? 0.9 12.3 ? 0.6</cell><cell cols="3">24.3 ? 0.9 28.8 ? 0.9 24.2 ? 0.9</cell><cell>23.4 ? 0.8</cell><cell>25.2 ? 0.9</cell><cell>29.1 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Results of all single domain learning models. Mean accuracy and 95% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen for test only.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Five-Shot</cell><cell></cell><cell></cell><cell cols="2">Five-Way One-Shot</cell><cell></cell></row><row><cell>Test Dataset</cell><cell>Simple CNAPS [2]</cell><cell>SUR [13]</cell><cell>URT [29]</cell><cell>Ours</cell><cell>Simple CNAPS [2]</cell><cell>SUR [13]</cell><cell>URT [29]</cell><cell>Ours</cell></row><row><cell>ImageNet</cell><cell>47.2 ? 1.0</cell><cell>46.7 ? 1.0</cell><cell cols="3">48.6 ? 1.0 49.5 ? 1.0 42.6 ? 0.9</cell><cell cols="3">40.7 ? 1.0 47.4 ? 1.0 48.1 ? 1.0</cell></row><row><cell>Omniglot</cell><cell>95.1 ? 0.3</cell><cell>95.8 ? 0.3</cell><cell cols="3">96.0 ? 0.3 96.4 ? 0.3 93.1 ? 0.5</cell><cell cols="3">93.0 ? 0.7 95.6 ? 0.5 96.1 ? 0.5</cell></row><row><cell>Aircraft</cell><cell>74.6 ? 0.6</cell><cell>82.1 ? 0.6</cell><cell cols="3">81.2 ? 0.6 84.4 ? 0.5 65.8 ? 0.9</cell><cell cols="3">67.1 ? 1.4 77.9 ? 0.9 81.6 ? 0.9</cell></row><row><cell>Birds</cell><cell>69.6 ? 0.7</cell><cell>62.8 ? 0.9</cell><cell cols="3">71.2 ? 0.7 75.6 ? 0.6 67.9 ? 0.9</cell><cell cols="3">59.2 ? 1.0 70.9 ? 0.9 75.7 ? 0.9</cell></row><row><cell>Textures</cell><cell>57.5 ? 0.7</cell><cell>60.2 ? 0.7</cell><cell cols="3">65.2 ? 0.7 65.7 ? 0.7 42.2 ? 0.8</cell><cell cols="3">42.5 ? 0.8 49.4 ? 0.9 52.4 ? 0.9</cell></row><row><cell>Quick Draw</cell><cell>70.9 ? 0.6</cell><cell cols="3">79.0 ? 0.5 79.2 ? 0.5 78.3 ? 0.5</cell><cell cols="4">70.5 ? 0.9 79.8 ? 0.9 79.6 ? 0.9 79.4 ? 0.9</cell></row><row><cell>Fungi</cell><cell>50.3 ? 1.0</cell><cell>66.5 ? 0.8</cell><cell cols="3">66.9 ? 0.9 68.1 ? 0.8 58.3 ? 1.1</cell><cell cols="3">64.8 ? 1.1 71.0 ? 1.0 73.7 ? 1.0</cell></row><row><cell>VGG Flower</cell><cell>86.5 ? 0.4</cell><cell>76.9 ? 0.6</cell><cell cols="3">82.4 ? 0.5 86.3 ? 0.5 79.9 ? 0.7</cell><cell cols="3">65.0 ? 1.0 72.7 ? 0.0 80.0 ? 0.8</cell></row><row><cell>Traffic Sign</cell><cell>55.2 ? 0.8</cell><cell>44.9 ? 0.9</cell><cell cols="3">45.1 ? 0.9 57.6 ? 0.8 55.3 ? 0.9</cell><cell cols="3">44.6 ? 0.9 52.7 ? 0.9 56.4 ? 0.9</cell></row><row><cell>MSCOCO</cell><cell>49.2 ? 0.8</cell><cell>48.1 ? 0.9</cell><cell cols="3">52.3 ? 0.9 54.7 ? 0.8 48.8 ? 0.9</cell><cell cols="3">47.8 ? 1.1 56.9 ? 1.1 58.5 ? 1.0</cell></row><row><cell>MNIST</cell><cell cols="3">88.9 ? 0.4 90.1 ? 0.4 86.5 ? 0.5</cell><cell cols="5">89.4 ? 0.4 80.1 ? 0.9 77.1 ? 0.9 75.6 ? 0.9 78.9 ? 0.8</cell></row><row><cell>CIFAR-10</cell><cell cols="2">66.1 ? 0.7 50.3 ? 1.0</cell><cell>61.4 ? 0.7</cell><cell>64.6 ? 0.7</cell><cell>50.3 ? 0.9</cell><cell cols="3">35.8 ? 0.8 47.3 ? 0.9 53.2 ? 0.8</cell></row><row><cell>CIFAR-100</cell><cell>53.8 ? 0.9</cell><cell>46.4 ? 0.9</cell><cell cols="3">52.5 ? 0.9 54.9 ? 0.8 53.8 ? 0.9</cell><cell cols="3">42.9 ? 1.0 54.9 ? 1.1 61.3 ? 0.9</cell></row><row><cell>Average Rank</cell><cell>3.0</cell><cell>3.0</cell><cell>2.5</cell><cell>1.5</cell><cell>2.8</cell><cell>3.5</cell><cell>2.3</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Results of Five-Way One-Shot and Varying-Way Five-Shot settings. Mean accuracies are reported and the results with confidence interval are reported.Table 9. Results of our method using CKA, CKA without adaptors (i.e. A ? ). Mean accuracy and 95% confidence interval are reported. Here, Ours (CKA w/o A ? ) indicates that adaptors are not applied for aligning features. All results are obtained with feature adaptation during meta-test stage.</figDesc><table><row><cell cols="3">Test Dataset Ours (CKA w/o A ? ) Ours (CKA)</cell></row><row><cell>ImageNet</cell><cell>58.3 ? 1.0</cell><cell>59.0 ? 1.0</cell></row><row><cell>Omniglot</cell><cell>94.4 ? 0.4</cell><cell>94.7 ? 0.4</cell></row><row><cell>Aircraft</cell><cell>88.9 ? 0.5</cell><cell>88.9 ? 0.4</cell></row><row><cell>Birds</cell><cell>78.7 ? 0.8</cell><cell>80.4 ? 0.7</cell></row><row><cell>Textures</cell><cell>74.8 ? 0.7</cell><cell>74.5 ? 0.7</cell></row><row><cell>Quick Draw</cell><cell>82.1 ? 0.6</cell><cell>81.9 ? 0.6</cell></row><row><cell>Fungi</cell><cell>65.4 ? 0.9</cell><cell>66.4 ? 0.9</cell></row><row><cell>VGG Flower</cell><cell>87.5 ? 0.6</cell><cell>91.3 ? 0.5</cell></row><row><cell>Traffic Sign</cell><cell>63.3 ? 1.1</cell><cell>63.2 ? 1.1</cell></row><row><cell>MSCOCO</cell><cell>55.3 ? 1.0</cell><cell>56.6 ? 1.0</cell></row><row><cell>MNIST</cell><cell>94.9 ? 0.4</cell><cell>94.7 ? 0.4</cell></row><row><cell>CIFAR-10</cell><cell>73.4 ? 0.7</cell><cell>73.8 ? 0.7</cell></row><row><cell>CIFAR-100</cell><cell>61.8 ? 1.0</cell><cell>62.1 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Sum 22.1 30.3 39.6 50.0 84.7 91.8 95.8 97.8 69.7 80.7 88.6 94.5 45.9 59.7 72.0 84.1 66.3 78.2 87.3 94.0 77.4 84.3 89.1 92.1 31.9 42.9 54.0 65.4 85.1 92.1 96.7 98.6 Concate 20.2 28.0 36.9 47.8 84.4 91.5 95.8 97.8 44.3 58.1 71.1 82.9 35.5 48.8 62.8 76.0 68.8 78.2 87.3 93.9 73.0 80.8 86.2 90.6 30.7 40.4 51.8 63.0 83.4 91.3 95.2 98.2 MDL 29.8 39.6 49.9 60.9 89.8 94.3 96.8 98.2 80.3 87.1 92.5 95.9 63.2 75.9 84.7 91.6 67.0 77.1 85.4 92.9 79.5 85.4 89.7 92.8 40.2 51.7 63.0 72.4 86.9 93.3 96.6 98.4 Simple CNAPS [2] 34.0 43.8 54.4 65.1 84.9 91.6 95.5 97.5 70.5 82.5 91.3 96.1 55.9 70.5 82.0 90.2 64.8 76.9 87.6 94.4 75.3 83.0 88.0 91.7 29.1 39.0 49.6 61.5 88.1 94.1 97.6 99.2 Ours 36.1 46.2 56.3 66.6 89.7 94.3 97.2 98.3 83.3 90.4 93.7 96.3 66.7 78.9 87.9 94.1 70.2 80.8 87.5 93.8 79.9 86.5 90.5 93.2 44.5 56.2 67.3 76.4 90.0 94.6 97.5 98.9</figDesc><table><row><cell>Test Dataset</cell><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell></cell><cell cols="2">Omniglot</cell><cell></cell><cell></cell><cell cols="2">Aircraft</cell><cell></cell><cell></cell><cell>Birds</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Textures</cell><cell></cell><cell></cell><cell cols="2">Quick Draw</cell><cell></cell><cell></cell><cell>Fungi</cell><cell></cell><cell></cell><cell></cell><cell cols="2">VGG Flower</cell><cell></cell></row><row><cell>Recall@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">, while we</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Global retrieval performance on Meta-Dataset (seen datasets). In addition to few-shot learning experiments, we evaluate our method in a non-episodic retrieval task to further compare the generalization ability of our universal representations.99.3 62.6 71.2 78.9 85.0 98.3 99.2 99.6 99.8 54.0 68.9 81.9 90.6 27.8 37.4 48.4 60.4 Concate 95.1 97.3 98.6 99.2 60.7 69.8 77.4 83.6 98.7 99.3 99.6 99.8 49.7 65.3 79.4 88.9 25.4 34.6 45.3 57.2 MDL 89.5 94.1 96.6 98.3 63.6 72.6 79.9 86.0 97.6 98.8 99.2 99.6 58.9 72.9 84.1 92.2 31.6 42.0 53.4 64.8 Simple CNAPS [2] 79.9 86.9 92.6 96.2 65.2 73.8 81.1 86.6 97.5 98.8 99.3 99.7 66.2 79.3 88.5 94.7 33.2 44.2 57.3 68.7</figDesc><table><row><cell>Test Dataset</cell><cell></cell><cell cols="2">Traffic Sign</cell><cell></cell><cell></cell><cell cols="2">MSCOCO</cell><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Recall@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="21">Sum 94.6 97.2 98.5 Ours 87.9 93.0 96.1 98.2 67.4 76.3 83.0 88.5 97.0 98.4 99.1 99.5 62.1 76.5 86.0 93.3 35.1 46.1 57.8 69.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14493" to="14502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge. online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schroeder</forename><surname>Brigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tasknorm: Rethinking batch normalization for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1153" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bam! born-again multi-task networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selecting relevant features from a multi-domain representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The quick, draw! a.i. experiment. online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowley</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawashima</forename><surname>Takashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Jongmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fox-Gieg</forename><surname>Nick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge distillation for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Imbalance Problems in Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="163" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A universal representation transformer layer for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph representation learning via multi-task knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS GRL Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul A</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards understanding knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5142" to="5151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimized generic feature learning for few-shot classification across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning metrics from teachers: Compact networks for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vacit</forename><surname>Oguz Yazici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2907" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
