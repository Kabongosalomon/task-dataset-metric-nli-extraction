<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion-GAN: Training GANs with Diffusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-11">October 11, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
							<email>zhendong.wang@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
							<email>huangjie.zheng@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mingyuan.zhou@mccombs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion-GAN: Training GANs with Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-11">October 11, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator's timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b14">[Goodfellow et al., 2014]</ref> and their variants <ref type="bibr" target="#b5">[Brock et al., 2018</ref><ref type="bibr" target="#b18">, Karras et al., 2019</ref><ref type="bibr" target="#b40">, Zhao et al., 2020</ref> have achieved great success in synthesizing photo-realistic high-resolution images. GANs in practice, however, are known to suffer from a variety of issues ranging from non-convergence and training instability to mode collapse <ref type="bibr">Bottou, 2017, Mescheder et al., 2018]</ref>. As a result, a wide array of analyses and modifications has been proposed for GANs, including improving the network architectures <ref type="bibr" target="#b18">[Karras et al., 2019</ref><ref type="bibr" target="#b23">, Radford et al., 2016</ref><ref type="bibr">, Sauer et al., 2021</ref><ref type="bibr" target="#b37">, Zhang et al., 2019</ref>, gaining theoretical understanding of GAN training <ref type="bibr" target="#b16">, Heusel et al., 2017</ref><ref type="bibr">, Mescheder et al., 2017</ref>, changing the objective functions <ref type="bibr" target="#b2">, Bellemare et al., 2017</ref><ref type="bibr" target="#b9">, Deshpande et al., 2018</ref><ref type="bibr">, Li et al., 2017a</ref><ref type="bibr">, Nowozin et al., 2016</ref><ref type="bibr" target="#b41">, Zheng and Zhou, 2021</ref><ref type="bibr" target="#b34">, Yang et al., 2021</ref>, regularizing the weights and/or gradients <ref type="bibr" target="#b13">, Fedus et al., 2018</ref><ref type="bibr">, Mescheder et al., 2018</ref><ref type="bibr">, Miyato et al., 2018a</ref><ref type="bibr" target="#b22">, Roth et al., 2017</ref><ref type="bibr" target="#b23">, Salimans et al., 2016</ref>, utilizing side information <ref type="bibr" target="#b32">[Wang et al., 2018</ref><ref type="bibr" target="#b36">, Zhang et al., 2017</ref>, adding a <ref type="figure">Figure 1</ref>: Flowchart for Diffusion-GAN. The top-row images represent the forward diffusion process of a real image, while the bottom-row images represent the forward diffusion process of a generated fake image. The discriminator learns to distinguish a diffused real image from a diffused fake image at all diffusion steps. mapping from the data to latent representation <ref type="bibr" target="#b11">[Donahue et al., 2016</ref><ref type="bibr" target="#b12">, Dumoulin et al., 2016</ref><ref type="bibr">, Li et al., 2017b</ref>, and applying differentiable data augmentation <ref type="bibr" target="#b19">[Karras et al., 2020a</ref><ref type="bibr" target="#b38">, Zhang et al., 2020a</ref><ref type="bibr" target="#b40">, Zhao et al., 2020</ref>.</p><p>A simple technique to stabilize GAN training is to inject instance noise, i.e., to add noise to the discriminator input, which can widen the support of both the generator and discriminator distributions and prevent the discriminator from overfitting <ref type="bibr">Bottou, 2017, S?nderby et al., 2017]</ref>. However, this technique is hard to implement in practice, as finding a suitable noise distribution is challenging . <ref type="bibr" target="#b22">Roth et al. [2017]</ref> show that adding instance noise to the high-dimensional discriminator input does not work well, and propose to approximate it by adding a zero-centered gradient penalty on the discriminator. This approach is theoretically and empirically shown to converge in Mescheder et al. <ref type="bibr">[2018]</ref>, who also demonstrate that adding zero-centered gradient penalties to non-saturating GANs can result in stable training and better or comparable generation quality compared to WGAN-GP . However, <ref type="bibr" target="#b5">Brock et al. [2018]</ref> caution that zero-centered gradient penalties and other similar regularization methods may stabilize training at the cost of generation performance. To the best of our knowledge, there has been no existing work that is able to empirically demonstrate the success of using instance noise in GAN training on high-dimensional image data.</p><p>To inject proper instance noise that can facilitate GAN training, we introduce Diffusion-GAN, which uses a diffusion process to generate Gaussian-mixture distributed instance noise. We show a graphical representation of Diffusion-GAN in <ref type="figure">Figure 1</ref>. In Diffusion-GAN, the input to the diffusion process is either a real or a generated image, and the diffusion process consists of a series of steps that gradually add noise to the image. The number of diffusion steps is not fixed, but depends on the data and the generator. We also design the diffusion process to be differentiable, which means that we can compute the derivative of the output with respect to the input. This allows us to propagate the gradient from the discriminator to the generator through the diffusion process, and update the generator accordingly. Unlike vanilla GANs, which compare the real and generated images directly, Diffusion-GAN compares the noisy versions of them, which are obtained by sampling from the Gaussian mixture distribution over the diffusion steps, with the help of our timestep-dependent discriminator. This distribution has the property that its components have different noise-to-data ratios, which means that some components add more noise than others. By sampling from this distribution, we can achieve two benefits: first, we can stabilize the training by easing the problem of vanishing gradient, which occurs when the data and generator distributions are too different; second, we can augment the data by creating different noisy versions of the same image, which can improve the data efficiency and the diversity of the generator. We provide a theoretical analysis to support our method, and show that the min-max objective function of Diffusion-GAN, which measures the difference between the data and generator distributions, is continuous and differentiable everywhere. This means that the generator in theory can always receive a useful gradient from the discriminator, and improve its performance.</p><p>Our main contributions include: 1) We show both theoretically and empirically how the diffusion process can be utilized to provide a model-and domain-agnostic differentiable augmentation, enabling data-efficient and leaking-free stable GAN training. 2) Extensive experiments show that Diffusion-GAN boosts the stability and generation performance of strong baselines, including StyleGAN2 <ref type="bibr" target="#b20">[Karras et al., 2020b]</ref>, Projected GAN <ref type="bibr">[Sauer et al., 2021]</ref>, and InsGen <ref type="bibr" target="#b34">[Yang et al., 2021]</ref>, achieving state-of-the-art results in synthesizing photorealistic images, as measured by both the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b16">[Heusel et al., 2017]</ref> and Recall score <ref type="bibr">[Kynk??nniemi et al., 2019</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries: GANs and diffusion-based generative models</head><p>GANs <ref type="bibr" target="#b14">[Goodfellow et al., 2014]</ref> are a class of generative models that aim to learn the data distribution p(x) of a target dataset by setting up a min-max game between two neural networks: a generator and a discriminator. The generator G takes as input a random noise vector z sampled from a simple prior distribution p(z), such as a standard normal or uniform distribution, and tries to produce realistic-looking samples G(z) that resemble the data. The discriminator D receives either a real data sample x drawn from p(x) or a fake sample G(z) generated by G, and tries to correctly classify them as real or fake. The goal of G is to fool D into making mistakes, while the goal of D is to accurately distinguish G(z) from x. The min-max objective function of GANs is given by</p><formula xml:id="formula_0">min G max D V (G, D) = E x?p(x) [log(D(x))] + E z?p(z) [log(1 ? D(G(z)))].</formula><p>In practice, this vanilla objective function is often modified to improve the stability and performance of GANs <ref type="bibr" target="#b14">[Goodfellow et al., 2014</ref><ref type="bibr">, Miyato et al., 2018a</ref><ref type="bibr" target="#b13">, Fedus et al., 2018</ref>, but the general idea of adversarial learning between G and D remains the same.</p><p>Diffusion-based generative models <ref type="bibr" target="#b17">[Ho et al., 2020</ref><ref type="bibr" target="#b26">, Sohl-Dickstein et al., 2015</ref><ref type="bibr" target="#b29">, Song and Ermon, 2019</ref> assume p ? (x 0 ) := p ? (x 0:T )dx 1:T , where x 1 , . . . , x T are latent variables of the same dimensionality as the data x 0 ? p(x 0 ). There is a forward diffusion chain that gradually adds noise to the data x 0 ? q(x 0 ) in T steps with pre-defined variance schedule ? t and variance ? 2 :</p><formula xml:id="formula_1">q(x 1:T | x 0 ) := T t=1 q(x t | x t?1 ), q(x t | x t?1 ) := N (x t ; ? 1 ? ? t x t?1 , ? t ? 2 I).</formula><p>A notable property is that x t at an arbitrary time-step t can be sampled in closed form as</p><formula xml:id="formula_2">q(x t | x 0 ) = N (x t ; ?? t x 0 , (1 ?? t )? 2 I), where ? t := 1 ? ? t ,? t := t s=1 ? s .<label>(1)</label></formula><p>A variational lower bound <ref type="bibr" target="#b3">[Blei et al., 2017]</ref> is then used to optimize the reverse diffusion chain as</p><formula xml:id="formula_3">p ? (x 0:T ) := N (x T ; 0, ? 2 I) T t=1 p ? (x t?1 | x t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diffusion-GAN: Method and Theoretical Analysis</head><p>To construct Diffusion-GAN, we describe how to inject instance noise via diffusion, how to train the generator by backpropagating through the forward diffusion process, and how to adaptively adjust the diffusion intensity. We further provide theoretical analysis illustrated with a toy example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Instance noise injection via diffusion</head><p>We aim to generate realistic samples x g from a generator network G that maps a latent variable z sampled from a simple prior distribution p(z) to a high-dimensional data space, such as images. The distribution of generator samples</p><formula xml:id="formula_4">x g = G(z), z ? p(z) is denoted by p g (x) = p(x g | z)p(z)dz.</formula><p>To make the generator more robust and diverse, we inject instance noise into the generated samples x g by applying a diffusion process that adds Gaussian noise at each step. The diffusion process can be seen as a Markov chain that starts from the original sample x and gradually erases its information until reaching a noise level ? 2 after T steps.</p><p>We define a mixture distribution q(y | x) that models the noisy samples y obtained at any step of the diffusion process, with a mixture weight ? t for each step t. The mixture components q(y | x, t) are Gaussian distributions with mean proportional to x and variance depending on the noise level at step t. We use the same diffusion process and mixture distribution for both the real samples x ? p(x) and the generated samples x g ? p g (x). More specifically, the diffusion-induced mixture distributions are expressed as</p><formula xml:id="formula_5">x ? p(x), y ? q(y | x), q(y | x) := T t=1 ? t q(y | x, t), x g ? p g (x), y g ? q(y g | x g ), q(y g | x g ) := T t=1 ? t q(y g | x g , t),</formula><p>where q(y | x) is a T -component mixture distribution, the mixture weights ? t are non-negative and sum to one, and the mixture components q(y | x, t) are obtained via diffusion as in Equation <ref type="formula" target="#formula_2">(1)</ref>, expressed as</p><formula xml:id="formula_6">q(y | x, t) = N (y; ?? t x, (1 ?? t )? 2 I).<label>(2)</label></formula><p>Samples from this mixture can be drawn as t ? p ? := Discrete(? 1 , . . . , ? T ), y ? q(y | x, t).</p><p>By sampling y from this mixture distribution, we can obtain noisy versions of both real and generated samples with varying degrees of noise. The more steps we take in the diffusion process, the more noise we add to y and the less information we preserve from x. We can then use this diffusion-induced mixture distribution to train a timestep-dependent discriminator D that distinguishes between real and generated noisy samples, and a generator G that matches the distribution of generated noisy samples to the distribution of real noisy samples. Next we introduce Diffusion-GAN that trains its discriminator and generator with the help of the diffusion-induced mixture distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Training</head><p>The Diffusion-GAN trains its generator and discriminator by solving a min-max game objective as</p><formula xml:id="formula_7">V (G, D) = E x?p(x),t?p?,y?q(y | x,t) [log(D ? (y, t))] + E z?p(z),t?p?,yg?q(y | G?(z),t) [log(1 ? D ? (y g , t))]. (3)</formula><p>Here, p(x) is the true data distribution, p ? is a discrete distribution that assigns different weights ? t to each diffusion step t ? {1, . . . , T }, and q(y | x, t) is the conditional distribution of the perturbed sample y given the original data x and the diffusion step t. By Equation (2), with Gaussian reparameterization, the perturbation function could be written as y = ??</p><formula xml:id="formula_8">t x + ? 1 ?? t ? , where 1 ?? t = 1 ? t s=1</formula><p>? s is the cumulative noise level at step t, ? is a scale factor, and ? N (0, I) is a Gaussian noise.</p><p>The objective function in Equation (3) encourages the discriminator to assign high probabilities to the perturbed real data and low probabilities to the perturbed generated data, for any diffusion step t. The generator, on the other hand, tries to produce samples that can deceive the discriminator at any diffusion step t. Note that the perturbed generated sample y g ? q(y | G ? (z), t) can be rewritten as y g = ?? t G ? (z) + (1 ?? t )? , ? N (0, I). This means that the objective function in Equation <ref type="formula">(3)</ref> is differentiable with respect to the generator parameters, and we can use gradient descent to optimize it with back-propagation.</p><p>The objective function Equation <ref type="formula">(3)</ref> is similar to the one used by the original GAN <ref type="bibr" target="#b14">[Goodfellow et al., 2014]</ref>, except that it involves the diffusion steps and the perturbation functions. We can show that this objective function also minimizes an approximation of the Jensen-Shannon (JS) divergence between the true and the generated distributions, but with respect to the perturbed samples and the diffusion steps, as follows:</p><formula xml:id="formula_9">D JS (p(y, t)||p g (y, t)) = E t?p? [D JS (p(y | t)||p g (y | t))].<label>(4)</label></formula><p>The JS divergence measures the dissimilarity between two probability distributions, and it reaches its minimum value of zero when the two distributions are identical. The proof of the equality in Equation (4) is given in Appendix C. A natural question that arises from this result is whether minimizing the JS divergence between the perturbed distributions implies minimizing the JS divergence between the original distributions, i.e., whether the optimal generator for Equation <ref type="formula">(3)</ref> is also the optimal generator for D JS (p(x)||p g (x)). We will answer this question affirmatively and provide a theoretical justification in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive diffusion</head><p>With the help of the perturbation function and timestep dependency, we have a new strategy to optimize the discriminator. We want the discriminator D to have a challenging task, neither too easy to allow overfitting the data <ref type="bibr" target="#b19">[Karras et al., 2020a</ref><ref type="bibr" target="#b40">, Zhao et al., 2020</ref> nor too hard to impede learning. Therefore, we adjust the intensity of the diffusion process, which adds noise to both y and y g , depending on how much D can distinguish them. When the diffusion step t is larger, the noise-to-data ratios are higher and the task is harder. We use 1 ?? t to measure the intensity of the diffusion, which increases as t grows. To control the diffusion intensity, we adaptively modify the maximum number of steps T .</p><p>Our strategy is to make the discriminator learn from the easiest samples first, which are the original data samples, and then gradually increase the difficulty by feeding it samples from larger t. To do this, we use a self-paced schedule for T , which depends on a metric r d that estimates how much the discriminator overfits to the data:</p><formula xml:id="formula_10">r d = E y,t?p(y,t) [sign(D ? (y, t) ? 0.5)], T = T + sign(r d ? d target ) * C,<label>(5)</label></formula><p>where r d is the same as in <ref type="bibr" target="#b19">Karras et al. [2020a]</ref> and C is a constant. We calculate r d and update T every four minibatches. We have two options for the distribution p ? that we use to sample t for the diffusion process:</p><formula xml:id="formula_11">t ? p ? := uniform: Discrete 1 T , 1 T , . . . , 1 T , priority: Discrete 1 T t=1 t , 2 T t=1 t , . . . , T T t=1 t ,<label>(6)</label></formula><p>The 'priority' option gives more weight to larger t, which means the discriminator will see more new samples from the new steps when T increases. This is because we want the discriminator to focus on the new and harder samples that it has not seen before, as this indicates that it is confident about the easier ones. Note that even with the 'priority' option, the discriminator can still see samples from smaller t, because q(y | x) is a mixture of Gaussians that covers all steps of the diffusion chain.</p><p>To avoid sudden changes in T during training, we use an exploration list t epl that contains t values sampled from p ? . We keep t epl fixed until we update T , and we sample t from t epl to generate noisy samples for the discriminator. This way, the model can explore each t sufficiently before moving to a higher T . We give the details of training Diffusion-GAN in Algorithm 1 in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical analysis with Examples</head><p>To better understand the theoretical properties of our proposed method, we present two theorems that address two important questions about the use of diffusion-based instance noise injection for training GANs. The proofs of these theorems are deferred to Appendix B. The first question, denoted as (a), is whether adding noise to the real and generated samples in a diffusion process can facilitate the learning. The second question, denoted as (b), is whether minimizing the JS divergence between the joint distributions of the noisy samples and the noise levels, p(y, t) and p g (y, t), can lead to the same optimal generator as minimizing the JS divergence between the original distributions of the real and generated samples, p(x) and p g (x).</p><p>To answer (a), we prove that for any choice of noise level t and any choice of convex function f , the f -divergence <ref type="bibr">[Nowozin et al., 2016]</ref> between the marginal distributions of the noisy real and generated samples, q(y | t) and q(y g | t), is a smooth function that can be computed and optimized by the discriminator. This implies that the diffusion-based noise injection does not introduce any singularity or discontinuity in the objective function of the GAN. The JS divergence is a special case</p><formula xml:id="formula_12">of f -divergence, where f (u) = ? log(2u) ? log(2 ? 2u).</formula><p>Theorem 1 (Valid gradients anywhere for GANs training). Let p(x) be a fixed distribution over X and z be a random noise over another space Z. Denote G ? : Z ? X as a function with parameter ? and input z and</p><formula xml:id="formula_13">p g (x) as the distribution of G ? (z). Let q(y | x, t) = N (y; ?? t x, (1 ?? t )? 2 I), where? t ? (0, 1) and ? &gt; 0. Let q(y | t) = p(x)q(y | x, t)dx and q g (y | t) = p g (x)q(y | x, t)dx. Then, ?t, if function G ? is continuous and differentiable, the f-divergence D f (q(y | t)||q g (y | t))</formula><p>is continuous and differentiable with respect to ?.</p><p>Theorem 1 shows that with the help of diffusion noise injection by q(y | x, t), ?t, y and y g are defined on the same support space, the whole X , and D f (q(y | t)||q g (y | t)) is continuous and differentiable everywhere. Then, one natural question is what if D f (q(y | t)||q g (y | t)) keeps a near constant value and hence provides little useful gradient. Hence, we empirically show that by injecting noise through a mixture defined over all steps of the diffusion chain, there is always a good chance that a sufficiently large t is sampled to provide a useful gradient, via the toy example below.</p><p>Toy example. We use the same simple example from  to illustrate our method. Let x = (0, z) be the real data and x g = (?, z) be the data generated by a one-parameter generator, where z is a uniform random variable in [0, 1]. The JS divergence between the real and the generated distributions, D JS (p(x)||p(x g )), is discontinuous: it is log 2 when ? = 0 and zero otherwise, so it does not provide a useful gradient to guide ? towards zero.</p><p>We introduce diffusion-based noise to both the real and the generated data, as shown in the first row of <ref type="figure" target="#fig_0">Figure 2</ref>. The noisy data, y and y g , have supports that cover the whole space R 2 and their densities overlap more or less depending on the diffusion step t. In the second row, left, of <ref type="figure" target="#fig_0">Figure 2</ref>, we plot how the JS divergence between the noisy distributions, D JS (q(y | t)||q g (y | t)), varies with ? for different t values. The black line with t = 0 is the original JS divergence, which has a discontinuity at ? = 0. As t increases, the JS divergence curves become smoother and have non-zero gradients for a larger range of ?. However, some values of t, such as t = 200 in this example, still have flat regions where the JS divergence is nearly constant. To avoid this, we use a mixture of all steps to ensure that there is always a high chance of getting informative gradients.</p><p>For the discriminator optimization, as shown in the second row, right, of <ref type="figure" target="#fig_0">Figure 2</ref>, the optimal discriminator under the original JS divergence is discontinuous and unattainable. With . The first row plots the distributions of data with diffusion noise injected for t. The second row shows the JS divergence and the optimal discriminator value with and without our noise injection.</p><p>diffusion-based noise, the optimal discriminator changes with t: a smaller t makes it more confident and a larger t makes it more cautious. Thus the diffusion acts like a scale to balance the power of the discriminator. This suggests the use of a differentiable forward diffusion chain that can provide various levels of gradient smoothness to help the generator training.</p><formula xml:id="formula_14">Theorem 2 (Non-leaking noise injection). Let x ? p(x), y ? q(y | x) and x g ? p g (x), y g ? q(y g | x g ), where q(y | x) is the transition density. Given certain q(y | x), if y could be reparameterized into y = f (x) + h( ), ? p( ),</formula><p>where p( ) is a known distribution, and both f and h are one-to-one mapping functions, then we could have p</p><formula xml:id="formula_15">(y) = p g (y) ? p(x) = p g (x).</formula><p>To answer question (b), we present Theorem 2, which shows a sufficient condition for the equality of the original and the augmented data distributions. By Theorem 2, the function f maps each x to a unique y, the function h maps each to a unique noise term, and the distribution of is known and independent of x. Under these assumptions, the theorem proves that the distribution of y is the same as the distribution of y g , if and only if the distribution of x is the same as the distribution of x g . If we take y | t as the y introduced in the theorem, then for ?t, Equation <ref type="formula" target="#formula_6">(2)</ref> fits the assumption made. This means that, by minimizing the divergence between q(y | t) and q g (y | t), which is the same as minimizing the divergence between p(x) | t and p g (x) | t, we are also minimizing the divergence between p(x) and p g (x). This implies that the noise injection does not affect the quality of the generated samples, and we can safely use our noise injection to improve the training of the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Related work</head><p>The proposed Diffusion-GAN can be related to previous works on stabilizing the GAN training, building diffusion-based generative models, and constructing differential augmentation for data-efficient GAN training. A detailed discussion on these related works is deferred to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments to answer the following questions: (a) Will Diffusion-GAN outperform state-of-the-art GAN baselines on benchmark datasets? (b) Will the diffusionbased noise injection help the learning of GANs in domain-agnostic tasks? (c) Will our method improve the performance of data-efficient GANs trained with a very limited amount of data?  <ref type="bibr" target="#b35">[Yu et al., 2015]</ref>, LSUN-Church <ref type="bibr" target="#b35">[Yu et al., 2015]</ref>, AFHQ(Cat/Dog/Wild) <ref type="bibr" target="#b6">[Choi et al., 2020]</ref>, and FFHQ <ref type="bibr" target="#b18">[Karras et al., 2019]</ref>. More details on these benchmark datasets are provided in Appendix E.</p><p>Evaluation protocol. We measure image quality using FID <ref type="bibr" target="#b16">[Heusel et al., 2017]</ref>. Following <ref type="bibr" target="#b18">Karras et al. [2019</ref><ref type="bibr" target="#b20">Karras et al. [ , 2020b</ref>, we measure FID using 50k generated samples, with the full training set used as reference. We use the number of real images shown to the discriminator to evaluate convergence <ref type="bibr" target="#b19">[Karras et al., 2020a</ref><ref type="bibr">, Sauer et al., 2021</ref>. Unless specified otherwise, all models are trained with 25 million images to ensure convergence (these trained with more or fewer images are specified in table captions). We further report the improved Recall score introduced by Kynk??nniemi et al.</p><p>[2019] to measure the sample diversity of generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations and resources.</head><p>We build Diffusion-GANs based on the code of StyleGAN2 <ref type="bibr" target="#b20">[Karras et al., 2020b]</ref>, ProjectedGAN <ref type="bibr">[Sauer et al., 2021]</ref>, and InsGen <ref type="bibr" target="#b34">[Yang et al., 2021]</ref> to answer questions (a), (b), and (c), respectively. Diffusion GANs inherit from their corresponding base GANs all their network architectures and training hyperparamters, whose details are provided in Appendix G. Specifically for StyleGAN2 and InsGen, we construct the discriminator as D ? (y, t), where t is injected via their mapping network. For ProjectedGAN, we empirically find t in the discriminator could be ignored to simplify the implementation and minimize the modifications to ProjectedGAN. More implementation details are provided in Appendix H. By applying our diffusion-based noise injection, we denote our models as Diffusion StyleGAN2/ProjectedGAN/InsGen. In the following experiments, we train related models with their official code if the results are unavailable, while others are all reported from references and marked with * . We run all our experiments with either 4 or 8 NVIDIA V100 GPUs depending on the demands of the inherited training configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to state-of-the-art GANs</head><p>We compare Diffusion-GAN with its state-of-the-art GAN backbone, StyleGAN2 <ref type="bibr" target="#b19">[Karras et al., 2020a]</ref>, and to evaluate its effectiveness from the data augmentation perspective, we compare it with both StyleGAN2 + DiffAug <ref type="bibr" target="#b40">[Zhao et al., 2020]</ref> and StyleGAN2 + ADA <ref type="bibr" target="#b19">[Karras et al., 2020a]</ref>, in terms of both sample fidelity (FID) and sample diversity (Recall) over extensive benchmark datasets.</p><p>We present the quantitative and qualitative results in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 3</ref>. Qualitatively, these generated images from Diffusion StyleGAN2 are all photo-realistic and have good diversity, ranging from low-resolution (32 ? 32) to high-resolution (1024 ? 1024). Additional randomly generated images are can be found in Appendix K. Quantitatively, Diffusion StyleGAN2 outperforms all the GAN baselines in generation diversity, as measured by Recall, on all 6 benchmark datasets and outperforms them in FID by a clear margin on 5 out of the 6 benchmark datasets.</p><p>From the data augmentation perspective, we observe that Diffusion StyleGAN2 always clearly outperforms the backbone model StyleGAN2 across various datasets, which empirically validates our Theorem 2. By contrast, both the ADA <ref type="bibr" target="#b20">[Karras et al., 2020b]</ref> and Diffaug <ref type="bibr" target="#b40">[Zhao et al., 2020]</ref> techniques could sometimes impair the generation performance on sufficiently large datasets, e.g., LSUN-Bedroom and LSUN-Church, which is also observed by Yang et al.</p><p>[2021] on FFHQ. This is possibly because their risk of leaking augmentation overshadows the benefits of data augmentation.</p><p>To investigate how the adaptive diffusion process works during training, we illustrate in <ref type="figure">Figure 4</ref> the convergence of the maximum timestep T in our adaptive diffusion and discriminator outputs. We see that T is adaptively adjusted: The T for Diffusion StyleGAN2 increases as the training goes while the T for Diffusion ProjectedGAN first goes up and then goes down. Note that the T is adjusted according to the overfitting status of the discriminator. The second panel shows that trained with the diffusion-based mixture distribution, the discriminator is always well-behaved and provides useful learning signals for the generator, which validates our analysis in Section 3.4 and Theorem 1.</p><p>Memory and time costs. Generally speaking, the memory and time costs of a Diffusion-GAN are comparable to those of the corresponding GAN baseline. More specifically, switching from ADA <ref type="bibr" target="#b19">[Karras et al., 2020a]</ref> to our diffusion-based augmentation, the added memory cost is negative, the added training time cost is negative, and the added inference time cost is zero. For example, for CIFAR-10, with four NVIDIA V100 GPUs, the training time for each 4k images is around 8.0s for StyleGAN2, 9.8s for StyleGAN2-ADA, and 9.5s for Diffusion-StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Diffusion-GAN for domain-agnostic augmentation</head><p>To verify whether our method is domain-agnostic, we apply Diffusion-GAN onto the input feature vectors of GANs. We conduct experiments on both low-dimensional and high-dimensional feature vectors, for which commonly used image augmentation methods are no longer applicable.</p><p>25-Gaussians Example. We conduct experiments on the popular 25-Gaussians generation task. The 25-Gaussians dataset is a 2-D toy data, generated by a mixture of 25 two-dimensional Gaussian distributions. Each data point is a 2-dimensional feature vector. We train a small GAN model, whose generator and discriminator are both parameterized by multilayer perceptrons (MLPs), with two 128-unit hidden layers and LeakyReLu nonlinearities.</p><p>The training results are shown in <ref type="figure">Figure 5</ref>. We observe that the vanilla GAN exhibits severe mode collapsing, capturing only a few modes. Its discriminator outputs of real and fake samples depart from each other very quickly. This implies a strong overfitting of the discriminator happened so that the discriminator stops providing useful learning signals for the generator. However, Diffusion-GAN successfully captures all the 25 Gaussian modes and the discriminator is under control to continuously provide useful learning signals. We interpret the improvement from two perspectives: First, non-leaking augmentation helps provide more information about the data space; Second, the discriminator is well behaved given the adaptively adjusted diffusion-based noise injection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator outputs of DiffusionGAN</head><p>Real samples Fake samples <ref type="figure">Figure 5</ref>: The 25-Gaussians example. We show the true data samples, the generated samples from vanilla GANs, the discriminator outputs of the vanilla GANs, the generated samples from our Diffusion-GAN, and the discriminator outputs of Diffusion-GAN.</p><p>ProjectedGAN. To verify that our adaptive diffusion-based noise injection could benefit the learning of GANs on high-dimensional feature vectors, we directly apply it onto the discriminator feature space of ProjectedGAN <ref type="bibr">[Sauer et al., 2021]</ref>. ProjectedGANs generally leverage pre-trained neural networks to extract meaningful features for the adversarial learning of the discriminator and generator. Following <ref type="bibr">Sauer et al. [2021]</ref>, we adaptively diffuse the feature vectors extracted by EfficientNet-v0 and keep all the other training part unchanged. We report the performance of Diffusion ProjectedGAN on several benchmark datasets in <ref type="table" target="#tab_1">Table 2</ref>, which verifies that our augmentation method is domain-agnostic. Under the Pro-jectedGAN framework, we see that with noise properly injected onto the high-dimensional feature space, Diffusion ProjectedGAN shows clear improvement in terms of both FID and Recall. We reach state-of-the-art FID results with Diffusion ProjectedGAN on STL-10 and LSUN-Bedroom/Church datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Diffusion-GAN for limited data</head><p>We evaluate whether Diffusion-GAN can provide data-efficient GAN training. We first generate five FFHQ (1024?1024) dataset splits, consisting of 200, 500, 1k, 2k, and 5k images respectively, where 200 and 500 images are considered to be extremely limited data cases. We also consider AFHQ-Cat, -Dog, and -Wild (512 ? 512), each with as few as around 5k images. Motivated by the success of InsGen <ref type="bibr" target="#b34">[Yang et al., 2021]</ref> on small datasets, we build our Diffusion-GAN upon it. We note on limited data, InsGen convincingly outperforms both StyleGAN2+ADA and +DiffAug, and currently holds the state-of-the-art performance for data-efficient GAN training. The results in <ref type="table" target="#tab_2">Table 3</ref> show that our Diffusion-GAN method can help further boost the performance of InsGen in limited data settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present Diffusion-GAN, a novel GAN framework that uses a variable-length forward diffusion chain with a Gaussian mixture distribution to generate instance noise for GAN training. This approach enables model-and domain-agnostic differentiable augmentation that leverages the advantages of diffusion without requiring a costly reverse diffusion chain. We prove theoretically and demonstrate empirically that Diffusion-GAN can prevent discriminator overfitting and provide non-leaking augmentation. We also demonstrate that Diffusion-GAN can produce high-resolution photo-realistic images with high fidelity and diversity, outperforming its corresponding state-of-the-art GAN baselines on standard benchmark datasets according to both FID and Recall.</p><p>Stabilizing GAN training. A root cause of training difficulties in GANs is often attributed to the JS divergence that GANs intend to minimize. This is because when the data and generator distributions have non-overlapping supports, which are often the case for highdimensional data supported by low-dimensional manifolds, the gradient of the JS divergence may provide no useful guidance to optimize the generator <ref type="bibr">, Mescheder et al., 2018</ref><ref type="bibr" target="#b22">, Roth et al., 2017</ref>. For this reason,  propose to instead use the Wasserstein-1 distance, which in theory can provide useful gradient for the generator even if the two distributions have disjoint supports. However, Wasserstein GANs often require the use of a critic function under the 1-Lipschitz constraint, which is difficult to satisfy in practice and hence realized with heuristics such as weight clipping , gradient penalty <ref type="bibr" target="#b15">[Gulrajani et al., 2017]</ref>, and spectral normalization <ref type="bibr">[Miyato et al., 2018a]</ref>.</p><p>While the divergence minimization perspective has played an important role in motivating the construction of Wasserstein GANs and gradient penalty-based regularizations, cautions should be made on purely relying on it to understand GAN training, due to not only the discrepancy between the divergence in theory and the actual min-max objective function used in practice, but also the potential confounding between different divergences and different training and regularization strategies <ref type="bibr" target="#b13">[Fedus et al., 2018</ref><ref type="bibr">, Mescheder et al., 2018</ref>. E.g., Mescheder et al.</p><p>[2018] have provided a simple example where in theory the Wasserstein GAN is predicted to succeed while the vanilla GAN is predicted to fail, but in practice the Wasserstein GAN with a finite number of discriminator updates per generator update fails to converge while the vanilla GAN with the non-saturating loss can slowly converge. <ref type="bibr" target="#b13">Fedus et al. [2018]</ref> provide a rich set of empirical evidence to discourage viewing GANs purely from the perspective of minimizing a specific divergence at each training step and emphasize the important role played by gradient penalties on stabilizing GAN training.</p><p>Diffusion models. Due to the use of a forward diffusion chain, the proposed Diffusion-GAN can be related to diffusion-based (or score-based) deep generative models <ref type="bibr" target="#b17">[Ho et al., 2020</ref><ref type="bibr" target="#b26">, Sohl-Dickstein et al., 2015</ref><ref type="bibr" target="#b29">, Song and Ermon, 2019</ref>] that employ both a forward (inference) and a reverse (generative) diffusion chain. These diffusion-based generative models are stable to train and can generate high-fidelity photo-realistic images <ref type="bibr" target="#b10">[Dhariwal and Nichol, 2021</ref><ref type="bibr" target="#b17">, Ho et al., 2020</ref><ref type="bibr" target="#b10">, Nichol et al., 2021</ref><ref type="bibr">, Ramesh et al., 2022</ref><ref type="bibr" target="#b29">, Song and Ermon, 2019</ref><ref type="bibr" target="#b30">, Song et al., 2021b</ref>. However, they are notoriously slow in generation due to the need to traverse the reverse diffusion chain, which involves going through the same U-Net-based generator network hundreds or even thousands of times <ref type="bibr" target="#b28">[Song et al., 2021a]</ref>. For this reason, a variety of methods have been proposed to reduce the generation cost of diffusion-based generative models <ref type="bibr">[Kong and Ping, 2021</ref><ref type="bibr">, Luhman and Luhman, 2021</ref><ref type="bibr">, Pandey et al., 2022</ref><ref type="bibr" target="#b24">, San-Roman et al., 2021</ref><ref type="bibr" target="#b28">, Song et al., 2021a</ref><ref type="bibr" target="#b33">, Xiao et al., 2022</ref><ref type="bibr" target="#b42">, Zheng et al., 2022</ref>.</p><p>A key distinction is that Diffusion-GAN needs a reverse diffusion chain during neither training nor generation. More specifically, its generator maps the noise to a generated sample in a single step. Diffusion-GAN can train and generate as quickly as a vanilla GAN does with the same generator size. For example, it takes around 20 hours to sample 50k images of size 32 ? 32 from a DDPM <ref type="bibr" target="#b17">[Ho et al., 2020]</ref> on an Nvidia 2080 Ti GPU, but would take less than a minute to do so from Diffusion-GAN.</p><p>Differentiable augmentation. As Diffusion-GAN transforms both the data and generated samples before sending them to the discriminator, we can also relate it to differentiable augmentation <ref type="bibr" target="#b19">[Karras et al., 2020a</ref><ref type="bibr" target="#b40">, Zhao et al., 2020</ref> proposed for data-efficient GAN training. <ref type="bibr" target="#b19">Karras et al. [2020a]</ref> introduce a stochastic augmentation pipeline with 18 transformations and develop an adaptive mechanism for controlling the augmentation probability. <ref type="bibr" target="#b40">Zhao et al. [2020]</ref> propose to use Color + Translation + Cutout as differentiable augmentations for both generated and real images.</p><p>While providing good empirical results on some datasets, these augmentation methods are developed with domain-specific knowledge and have the risk of leaking augmentation into generation <ref type="bibr" target="#b19">[Karras et al., 2020a]</ref>. As observed in our experiments, they sometime worsen the results when applied to a new dataset, likely because the risk of augmentation leakage overpowers the benefits of enlarging the training set, which could happen especially if the training set size is already sufficiently large.</p><p>By contrast, Diffusion-GAN uses a differentiable forward diffusion process to stochastically transform the data and can be considered as both a domain-agnostic and a model-agnostic augmentation method. In other words, Diffusion-GAN can be applied to non-image data or even latent features, for which appropriate data augmentation is difficult to be defined, and easily plugged into an existing GAN to improve its generation performance. Moreover, we prove in theory and show in experiments that augmentation leakage is not a concern for Diffusion-GAN. <ref type="bibr" target="#b31">Tran et al. [2021]</ref> provide a theoretical analysis for deterministic non-leaking transformation with differentiable and invertible mapping functions. <ref type="bibr" target="#b4">Bora et al. [2018]</ref> show similar theorems to us for specific stochastic transformations, such as Gaussian Projection, Convolve+Noise, and stochastic Block-Pixels, while our Theorem 2 includes more satisfying possibilities as discussed in Appendix B.</p><p>p r ,t (y) and p g ,t (y) are continuous functions over y.</p><formula xml:id="formula_16">lim ?y?0 p r ,t (y ? ?y) = lim ?y?0 X p r (x)N (y ? ?y; a t x, b t )dx = X p r (x) lim ?y?0 N (y ? ?y; a t x, b t )dx = X p r (x) lim ?y?0 1 C 1 exp ((y ? ?y) ? a t x) 2 C 2 dx = X p r (x)N (y; a t x, b t )dx = p r ,t (y),</formula><p>where C 1 and C 2 are constants. Hence, p r ,t (y) is a continuous function defined on y. The proof of continuity for p g ,t (y) is exactly the same proof. Then, given g ? is also a continuous function, it is clear to see that D f (p r ,t (y)||p g ,t (y)) is a continuous function over ?.</p><p>Next, we show that D f (p r ,t (y)||p g ,t (y)) is differentiable. By the chain rule, showing D f (p r ,t (y)||p g ,t (y)) to be differentiable is equivalent to show p r ,t (y), p r ,t (y) and f are differentiable. Usually, f is defined with differentiability <ref type="bibr">[Nowozin et al., 2016]</ref>.</p><formula xml:id="formula_17">? ? p r ,t (a t g ? (z) + b t ) = ? ? X p r (x)N (a t g ? (z) + b t ; a t x, b t )dx = X p r (x) 1 C 1 ? ? exp ||a t g ? (z) + b t ? a t x|| 2 2 C 2 dx, ? ? p g ,t (a t g ? (z) + b t ) = ? ? X p g (x)N (a t g ? (z) + b t ; a t x, b t )dx = ? ? E z ?p(z ) [N (a t g ? (z) + b t ; a t g ? (z ), b t )] = E z ?p(z ) 1 C 1 ? ? exp ||a t g ? (z) + b t ? a t g ? (z )|| 2 2 C 2 ,</formula><p>where C 1 and C 2 are constants. Hence, p r ,t (y) and p r ,t (y) are differentiable, which concludes the proof.</p><p>Proof of Theorem 2. We have p(y) = p(x)q(y | x)dx and p g (y) = p g (x)q(y | x)dx. ? If p(x) = p g (x), then p(y) = p g (y) ? Let y ? p(y) and y g ? p g (y). Given the assumption on q(y | x), we have</p><formula xml:id="formula_18">y = f (x) + g( ), x ? p(x), ? p( ) y g = f (x g ) + g( g ), x g ? p g (x), g ? p(</formula><p>).</p><p>Since f and g are one-to-one mapping functions, f (x) and g( ) are identifiable, which indicates f (x)</p><formula xml:id="formula_19">D = f (x g ) ? x D = x g .</formula><p>By the property of moment-generating functions (MGF), given f (x) is independent with g( ), we have for ?s</p><formula xml:id="formula_20">M y (s) = M f (x) (s) ? M g( ) (s) M yg (s) = M f (xg) (s) ? M g( g ) (s).</formula><p>where M y (s) = E y?p(y) [e s T y ] denotes the MGF of random variable y and the others follow the same form. By the moment-generating function uniqueness theorem, given y D = y g and g( ) D = g( g ), we have M y (s) = M yg (s) and M g( ) (s) = M g( g ) (s) for ?s. Then, we could</p><formula xml:id="formula_21">obtain M f (x) = M f (xg) for ?s. Thus, M f (x) = M f (xg) ? f (x) D = f (x g ) ? p(x) = p(x g ),</formula><p>which concludes the proof.</p><p>Discussion. Next, we discuss which q(y | x) fits the assumption we made on it. We follow the discussion of reparameterization of distributions as used in <ref type="bibr">Kingma and Welling [2014]</ref>. Three basic approaches are:</p><p>1. Tractable inverse CDF. In this case, let ? U(0, I), and ?( , y, x) be the inverse CDF of q(y | x). From ?( , y, x), if y = f (x) + g( ), for example, y ? Cauchy(x, ?) and y ? Logistic(x, s), then Theorem 2 holds.</p><p>2. Analogous to the Gaussian example, y ? N (x, ? 2 I) ? y = x + ? ? , ? N (0, I). For any "location-scale" family of distributions we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable , and let g(.) = location + scale ? . Examples: Laplace, Elliptical, Student's t, Logistic, Uniform, Triangular, and Gaussian distributions.</p><p>3. Implicit distributions. q(y | x) could be modeled by neural networks, which implies y = f (x) + g( ), ? p( ), where f and g are one-to-one nonlinear transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Derivations</head><p>Derivation of equality in JSD JSD(p(y, t), p g (y, t))</p><formula xml:id="formula_22">= 1 2 D KL p(y, t) p(y, t) + p g (y, t) 2 + 1 2 D KL p g (y, t) p(y, t) + p g (y, t) 2 = 1 2 E y,t?p(y,t) log 2 ? p(y, t) p(y, t) + p g (y, t) + 1 2 E y,t?pg(y,t) log 2 ? p g (y, t) p(y, t) + p g (y, t) = 1 2 E t?p?(t),y?p(y | t) log 2 ? p(y | t)p ? (t) p(y | t)p ? (t) + p g (y | t)p ? (t) + 1 2 E t?p?(t),y?pg(y | t) log 2 ? p g (y | t)p ? (t) p(y | t)p ? (t) + p g (y | t)p ? (t) = E t?p?(t) 1 2 E y?p(y | t) log 2 ? p(y | t) p(y | t) + p g (y | t) + 1 2 E y?pg(y | t) log 2 ? p g (y | t) p(y | t) + p g (y | t) = E t?p?(t) [JSD(p(y | t), p g (y | t))].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of toy example</head><p>Here, we provide the detailed analysis of the JS divergence toy example.</p><p>Notation. Let X be a compact metric set (such as the space of images [0, 1] d ) and Prob(X ) denote the space of probability measures defined on X . Let P r be the target data distribution and P g 1 be the generator distribution. The JSD between the two distributions P r , P g ? Prob(X ) is defined as:</p><formula xml:id="formula_23">D JS (P r ||P g ) = 1 2 D KL (P r ||P m ) + 1 2 D KL (P g ||P m ),<label>(7)</label></formula><p>where P m is the mixture (P r + P g )/2 and D KL denotes the Kullback-Leibler divergence, i.e., D KL (P r ||P g ) = X p r (x) log( pr(x) p ? (x) )dx. More generally, the f -divergence <ref type="bibr">[Nowozin et al., 2016]</ref> between P r and P g is defined as: where the generator function f : R + ? R is a convex and lower-semicontinuous function satisfying f (1) = 0. We refer to <ref type="bibr">Nowozin et al. [2016]</ref> for more details.</p><formula xml:id="formula_24">D f (P r ||P g ) = X p g (x)f p r (x) p g (x) dx,<label>(8)</label></formula><p>We recall the typical example introduced in Arjovsky and Bottou <ref type="bibr">[2017]</ref> and follow the notations.</p><p>Example. Let Z ? U [0, 1] be the uniform distribution on the unit interval. Let X ? P r be the distribution of (0, Z) ? R 2 , which contains a 0 on the x-axis and a random variable Z on the y-axis. Let X g ? P g be the distribution of (?, Z) ? R 2 , where ? is a single real parameter. In this case, the D JS (P r ||P g ) is not continuous,</p><formula xml:id="formula_25">D JS (P r ||P g ) = 0 if ? = 0, log 2 if ? = 0.</formula><p>which can not provide a usable gradient for training. The derivation is as follows:</p><formula xml:id="formula_26">D JS (P r ||P g ) = 1 2 E x?pr(x) log 2 ? p r (x) p r (x) + p g (x) + 1 2 E y?pg(y) log 2 ? p g (y) p r (y) + p g (y) = 1 2 E x1=0,x2?U [0,1] log 2 ? 1[x 1 = 0] ? U (x 2 ) 1[x 1 = 0] ? U (x 2 ) + 1[x 1 = ?] ? U (x 2 ) + 1 2 E y1=?,y2?U [0,1] log 2 ? 1[y 1 = ?] ? U (y 2 ) 1[y 1 = 0] ? U (y 2 ) + 1[y 1 = ?] ? U (y 2 ) = 1 2 log 2 ? 1[x 1 = 0] 1[x 1 = 0] + 1[x 1 = ?] x 1 = 0 + 1 2 log 2 ? 1[y 1 = ?] 1[y 1 = 0] + 1[y 1 = ?] y 1 = ? = 0 if ? = 0, log 2 if ? = 0.</formula><p>Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero . This happens to be the case when two low dimensional manifolds intersect in general position . To avoid the potential issue caused by having non-overlapping distribution supports, a common remedy is to use Wasserstein-1 distance which in theory can still provide usable gradient . In this case, the Wasserstein-1 distance is |?|.</p><p>Diffusion-based noise injection In general, with our diffusion noise injected, we could have,</p><formula xml:id="formula_27">p r ,t = X p r (x)N (y; ?? t x, (1 ?? t )? 2 I)dx p g ,t = X p g (x)N (y; ?? t x, (1 ?? t )? 2 I)dx D JS (p r ,t ||p g ,t ) = 1 2 E p r ,t log 2p r ,t p r ,t + p g ,t + 1 2 E p g ,t log 2p g ,t p r ,t + p g ,t</formula><p>For the previous example, we have Y t and Y g,t such that,</p><formula xml:id="formula_28">Y t = (y 1 , y 2 ) ? p r ,t = N (y 1 | 0, b t )f (y 2 ), Y g,t = (y g,1 , y g,2 ) ? p g ,t = N (y g,1 | a t ?, b t )f (y g,2 ),</formula><p>where f (?) = 1 0 N (? | a t Z, b t )U (Z)dZ, a t and b t are abbreviations for ?? t and (1 ?? t )? 2 . The supports of Y t and Y g,t are both the whole metric space R 2 and they overlap with each other depending on t, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. As t increases, the high density region of Y t and Y g,t get closer since the weight a t is decreasing towards 0. Then, we derive the JS divergence,</p><formula xml:id="formula_29">D JS (p r ,t ||p g ,t ) = 1 2 E y1?N (y1 | 0,bt),y2?f (y2) log 2 ? N (y 1 | 0, b t )f (y 2 ) N (y 1 | 0, b t )f (y 2 ) + N (y 1 | a t ?, b t )f (y 2 ) + 1 2 E yg,1?N (yg,1 | 0,bt),yg,2?f (yg,2) log 2 ? N (y g,1 | a t ?, b t )f (y g,2 ) N (y g,1 | 0, b t )f (y g,2 ) + N (y g,1 | a t ?, b t )f (y g,2 ) = 1 2 E y1?N (0,bt) log 2 ? N (y 1 | 0, b t ) N (y 1 | 0, b t ) + N (y 1 | a t ?, b t ) + 1 2 E yg,1?N (at?,bt) log 2 ? N (y g,1 | a t ?, b t ) N (y g,1 | 0, b t ) + N (y g,1 | a t ?, b t )</formula><p>which is clearly continuous and differentiable.</p><p>We show this D JS (p r ,t ||p g ,t ) with respect to increasing t values and a ? grid in the second row of <ref type="figure" target="#fig_0">Figure 2</ref>. As shown in the left panel, the black line with t = 0 shows the origianl JSD, which is not even continuous, while as the diffusion level t increments, the lines become smoother and flatter. It is clear to see that these smooth curves provide good learning signals for ?. Recall that the Wasserstein-1 distance is |?| in this case. Meanwhile, we could observe with an intense diffusion, e.g., t = 800, the curve becomes flatter, which indicates smaller gradients and a much slower learning process. This motivates us that an adaptive diffusion could provide different level of gradient smoothness and is possibly better for training. The right panel shows the optimal discriminator outputs over the space X . With diffusion, the optimal discriminator is well defined over the space and the gradient is smooth, while without diffusion the optimal discriminator is only valid on two star points. Interestingly, we find that smaller t drives the optimal discriminator to become more assertive while larger t makes discriminator become more neutral. The diffusion here works like a scale to balance the power of the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Dataset descriptions</head><p>The CIFAR-10 dataset consists of 50k 32 ? 32 training images in 10 categories. The STL-10 dataset originated from ImageNet <ref type="bibr" target="#b8">[Deng et al., 2009]</ref> consists of 100k unlabeled images in 10 categories, and we resize them to 64 ? 64 resolution. For LSUN datasets, we sample 200k images from LSUN-Bedroom, use the whole 125k images from LSUN-Church, and resize them to 256 ? 256 resolution for training. The AFHQ datasets includes around 5k 512 ? 512 images per category for dogs, cats, and wild life; we train a separate network for each of them. The FFHQ contains 70k images crawled from Flickr at 1024 ? 1024 resolution and we use all of them for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Diffusion-GAN</head><p>while i ? number of training iterations do</p><p>Step I: Update discriminator ? Sample minibatch of m noise samples {z 1 , z 2 , . . . , z m } ? p z (z).</p><p>? Obtain generated samples {x g,1 , x g,2 , . . . , x g,m } by x g = G(z).</p><p>? Sample minibatch of m data examples {x 1 , x 2 , . . . , x m } ? p(x).</p><p>? Sample {t 1 , t 2 , . . . , t m } from t epl list uniformly with replacement.</p><p>? For j ? {1, 2, . . . , m}, sample y j ? q(y j |x j , t j ) and y g,j ? q(y g,j |x g,j , t j ) ? Update discriminator by maximizing Equation <ref type="formula">(3)</ref>.</p><p>Step II: Update generator ? Sample minibatch of m noise samples {z 1 , z 2 , . . . , z m } ? p z (z) ? Obtain generated samples {x g,1 , x g,2 , . . . , x g,m } by x g = G(z).</p><p>? Sample {t 1 , t 2 , . . . , t m } from t epl list with replacement.</p><p>? For j ? {1, 2, . . . , m}, sample y g,j ? q(y g,j |x g,j , t j ) ? Update generator by minimizing Equation <ref type="formula">(3)</ref>.</p><p>Step III: Update diffusion if i mod 4 == 0 then Update T by Equation <ref type="formula" target="#formula_10">(5)</ref> Sample t epl = [0, . . . , 0, t 1 , . . . , t 32 ], where t k ? p ? for k ? {1, . . . , 32}. p ? is in Equation <ref type="formula" target="#formula_11">(6)</ref>. {t epl has 64 dimensions.} end if end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Algorithm</head><p>We provide the Diffusion-GAN algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hyperparameters</head><p>Comparing to GANs, Diffusion-GAN introduces a new hyperparameter d target , which is a threshold to identify whether the current discriminator is overfitting. We find StyleGAN2-based models are not sensitive to the values of d target , so we set d target = 0.6 for them across all dataset, only except that we set d target = 0.8 for FFHQ (d target = 0.8 for FFHQ is slightly better than 0.6 in FID). We report d target of Diffusion ProjectedGAN for our experiments in <ref type="table" target="#tab_4">Table 4</ref>. We also evaluated two t sampling distribution p ? , ['priority', 'uniform'], defined in Equation <ref type="formula" target="#formula_11">(6)</ref>. In most cases, 'priority' works slightly better, while in some cases, such as FFHQ, 'uniform' is better. Overall, we didn't modify anything in the model architectures and training hyperparameters, such as learning rate and batch size. The forward diffusion configuration and model training configurations are as follows.</p><p>Diffusion config. For our diffusion-based noise injection, we set up a linearly increasing schedule for ? t , where t ? {1, 2, . . . , T }. For pixel level injection in StyleGAN2, we follows <ref type="bibr" target="#b17">[Ho et al., 2020]</ref> and set ? 0 = 0.0001 and ? T = 0.02. We adaptively modify T ranging from T min = 5 to T max = 1000. The image pixels are usually rescaled to [?1, 1] so we set the Guassian noise standard deviation ? = 0.05. For feature level injection in Diffusion ProjectedGAN, we set ? 0 = 0.0001, ? T = 0.01, T min = 5, T max = 500 and ? = 0.5. We list all these values in <ref type="table" target="#tab_5">Table 5</ref> Model config. For StyleGAN2-based models, we borrow the config settings provided by <ref type="bibr" target="#b19">Karras et al. [2020a]</ref>, which include <ref type="bibr">['auto', 'stylegan2', 'cifar', 'paper256', '</ref>  Diffusion config for pixel, priority ? 0 = 0.0001, ? T = 0.02, T min = 5, T max = 1000, ? = 0.05 Diffusion config for pixel, uniform ? 0 = 0.0001, ? T = 0.02, T min = 5, T max = 500, ? = 0.05</p><p>Diffusion config for feature ? 0 = 0.0001, ? T = 0.01, T min = 5, T max = 500, ? = 0.5 'stylegan2']. We create the 'stl' config based on 'cifar' with a small modification that we change the gamma term to be 0.01. For ProjectedGAN models, we use the recommended default config <ref type="bibr">[Sauer et al., 2021]</ref>, which is based on <ref type="bibr">FastGAN [Liu et al., 2020]</ref>. We report the config settings used for our experiments in <ref type="table" target="#tab_7">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Implementation details</head><p>We implement an additional diffusion sampling pipeline, where the diffusion configurations are set in Appendix G. The T in the forward diffusion process is adaptively adjusted and clipped to [T min , T max ]. As illustrated in Algorithm 1, at each update step, we sample t from t epl for each data point x, and then use the analytic Gaussian distribution at diffusion step t to sample y. Next, we use y and t instead of x for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion StyleGAN2.</head><p>We inherit all the network architectures from StyleGAN2 implemented by <ref type="bibr" target="#b19">Karras et al. [2020a]</ref>. We modify the original mapping network, which is there for label conditioning and unused for unconditional image generation tasks, inside the discriminator to inject t. Specifically, we change the original input of mapping network, the class label c, to our discrete value timestep t. Then, we train the generator and discriminator with diffused samples y and t.</p><p>Diffuson ProjectedGAN. To simplify the implementation and minimize the modifications to ProjectedGAN, we construct the discriminator as D ? (y), where t is ignored. Our method is plugged in as a data augmentation method. The only change in the optimization stage is that the discriminator is fed with diffused images y instead of original images x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffuson InsGen.</head><p>To simplify the implementation and minimize the modifications to InsGen, we keep their contrastive learning part untouched. We modify the original discriminator network to inject t similarly to Diffusion StyleGAN2. Then, we train the generator and discriminator with diffused samples y and t.</p><p>I Ablation on the mixing procedure Note the mixing procedure described in Equation <ref type="formula" target="#formula_11">(6)</ref>, referred to as "priority mixing" in what follows, is designed based on our intuition. Here we conduct an ablation study on the mixing procedure by comparing the priority mixing with uniform mixing on three representative datasets. We report in <ref type="table">Table 7</ref> the FID results, which suggest that uniform mixing could work better than priority mixing in some dataset, and hence Diffusion-GAN may be further  StyleGAN2 based models, we borrow the config settings provided by <ref type="bibr" target="#b19">Karras et al. [2020a]</ref>, which includes <ref type="bibr">['auto', 'stylegan2', 'cifar', 'paper256', 'paper512', 'paper1024']</ref>. We create the 'stl' config based on 'cifar' with small modifications that we change the gamma term to be 0.01. For ProjectedGAN models, we use the recommended default config <ref type="bibr">[Sauer et al., 2021]</ref>, which is based on FastGAN.</p><p>improved by optimizing its mixing procedure according to the training data. While optimizing the mixing procedure is beyond the focus of this paper, it is worth further investigation in future studies. <ref type="table">Table 7</ref>: Ablation study on the mixing procedure. "Priority Mixing" refers to the mixing procedure in Equation <ref type="formula" target="#formula_11">(6)</ref> and "Uniform Mixing" refers to sample t uniformly at random. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J More GAN variants</head><p>To further validate our noise injection via diffusion-based mixtures, we add our diffusionbased training into two more representative GAN variants: <ref type="bibr">DCGAN [Radford et al., 2015]</ref> and <ref type="bibr">SNGAN [Miyato et al., 2018b]</ref>, which have quite different GAN architectures compared to StyleGAN2. We provide the FIDs for CIFAR-10 in <ref type="table" target="#tab_9">Table 8</ref>. We observe that both Diffusion-DCGAN and Diffusion-SNGAN clearly outperform their corresponding baseline GANs.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The toy example inherited from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Randomly generated images from Diffusion StyleGAN2 trained on CIFAR-10, CelebA, STL-10, LSUN-Bedroom, LSUN-Church, and FFHQ datasets. Plot of adaptively adjusted maximum diffusion steps T and discriminator outputs of Diffusion-GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>We show the data distribution and DJS(Pr||Pg).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>More generated images for LSUN-Bedroom (FID 1.43, Recall 0.58) and LSUN-Church (FID 1.85, Recall 0.65) from Diffusion ProjectedGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>More generated images for AFHQ-Cat (FID 2.40), AFHQ-Dog (FID 4.83) and AFHQ-Wild (FID 1.51) from Diffusion InsGen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>More generated images for FFHQ from Diffusion StyleGAN2 (FID 3.71, Recall 0.43).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image generation results on benchmark datasets: CIFAR-10, CelebA, STL-10, LSUN-Bedroom, LSUN-Church, and FFHQ. We highlight the best and second best results in each column with bold and underline, respectively. Lower FIDs indicate better fidelity, while higher Recalls indicate better diversity.</figDesc><table><row><cell></cell><cell cols="2">CIFAR-10</cell><cell cols="2">CelebA</cell><cell cols="2">STL-10</cell><cell cols="4">LSUN-Bedroom LSUN-Church</cell><cell cols="2">FFHQ</cell></row><row><cell>Methods</cell><cell cols="2">(32 ? 32)</cell><cell cols="2">(64 ? 64)</cell><cell cols="2">(64 ? 64)</cell><cell cols="2">(256 ? 256)</cell><cell cols="2">(256 ? 256)</cell><cell cols="2">(1024 ? 1024)</cell></row><row><cell></cell><cell>FID</cell><cell cols="3">Recall FID Recall</cell><cell>FID</cell><cell cols="2">Recall FID</cell><cell>Recall</cell><cell>FID</cell><cell>Recall</cell><cell cols="2">FID Recall</cell></row><row><cell>StyleGAN2 [Karras et al., 2020a]</cell><cell>8.32  *</cell><cell>0.41  *</cell><cell>2.32</cell><cell>0.55</cell><cell>11.70</cell><cell>0.44</cell><cell>3.98</cell><cell>0.32</cell><cell>3.93</cell><cell>0.39</cell><cell>4.41</cell><cell>0.42</cell></row><row><cell cols="2">StyleGAN2 + DiffAug [Zhao et al., 2020] 5.79  *</cell><cell>0.42  *</cell><cell>2.75</cell><cell>0.52</cell><cell>12.97</cell><cell>0.39</cell><cell>4.25</cell><cell>0.19</cell><cell>4.66</cell><cell>0.33</cell><cell>4.46</cell><cell>0.41</cell></row><row><cell cols="3">StyleGAN2 + ADA [Karras et al., 2020a] 2.92  *  0.49  *</cell><cell>2.49</cell><cell>0.53</cell><cell>13.72</cell><cell>0.36</cell><cell>7.89</cell><cell>0.05</cell><cell>4.12</cell><cell>0.18</cell><cell>4.47</cell><cell>0.41</cell></row><row><cell>Diffusion StyleGAN2</cell><cell>3.19</cell><cell>0.58</cell><cell>1.69</cell><cell>0.67</cell><cell>11.43</cell><cell>0.45</cell><cell>3.65</cell><cell>0.32</cell><cell>3.17</cell><cell>0.42</cell><cell>2.83</cell><cell>0.49</cell></row><row><cell cols="13">Datasets. We conduct experiments on image datasets ranging from low-resolution (e.g.,</cell></row><row><cell cols="13">32 ? 32) to high-resolution (e.g., 1024 ? 1024) and from low-diversity to high-diversity:</cell></row><row><cell cols="10">CIFAR-10 [Krizhevsky, 2009], STL-10 [Coates et al., 2011], LSUN-Bedroom</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Domain-agnostic experiments on ProjectedGAN.</figDesc><table><row><cell>Domain-agnostic Tasks</cell><cell cols="8">CIFAR-10 (32 ? 32) STL-10 (64 ? 64) LSUN-Bedroom (256 ? 256) LSUN-Church (256 ? 256) FID Recall FID Recall FID Recall FID Recall</cell></row><row><cell cols="2">ProjectedGAN [Sauer et al., 2021] 3.10</cell><cell>0.45</cell><cell>7.76</cell><cell>0.35</cell><cell>2.25</cell><cell>0.55</cell><cell>3.42</cell><cell>0.56</cell></row><row><cell>Diffusion ProjectedGAN</cell><cell>2.54</cell><cell>0.45</cell><cell>6.91</cell><cell>0.35</cell><cell>1.43</cell><cell>0.58</cell><cell>1.85</cell><cell>0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FFHQ (1024 ? 1024) FID results with 200, 500, 1k, 2k, and 5k training samples; AFHQ (512 ? 512) FID results. To ensure convergence, all models are trained across 10M images for FFHQ and 25M images for AFHQ. We bold the best number in each column.</figDesc><table><row><cell>Models</cell><cell>FFHQ (200) FFHQ (500) FFHQ (1k) FFHQ (2k) FFHQ (5k) Cat</cell><cell cols="2">Dog Wild</cell></row><row><cell cols="3">InsGen [Yang et al., 2021] 2.60  Diffusion InsGen 102.58 54.762 34.90 18.21 9.89 63.34 50.39 30.91 16.43 8.48 2.40 4.83</cell><cell>1.51</cell></row></table><note>* 5.44* 1.77*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>dtarget for Diffusion ProjectedGAN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Diffusion config.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The config setting of StyleGAN2 based models and ProjectedGAN based models. For</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>FIDs on CIFAR-10 for DCGAN, Diffusion-DCGAN, SNGAN, and Diffusion-SNGAN.</figDesc><table><row><cell>, 2018b] Diffusion-SNGAN</cell></row></table><note>DCGAN [Radford et al., 2015] Diffusion-DCGAN SNGAN [Miyato et al.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For notation simplicity, g and G both denote the generator network in GANs in this paper.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Related work B Proof</head><p>Proof of Theorem 1. For simplicity, let x ? P r ,</p><p>Since N (y; a t x, b t I) is assumed to be an isotropic Gaussian distribution, for simplicity, in what follows we show the proof in uni-variate Gaussian, which could be easily extended to multi-variate Gaussian by the production rule. We first show that under mild conditions, the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk4_qw5xe" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Cramer distance as a solution to biased Wasserstein gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10743</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AmbientGAN: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy7fDog0b" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<title level="m">StarGAN v2: Diverse image synthesis for multiple domains. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8185" to="8194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative modeling using the sliced Wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=AAWuCvzaVt" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Martin Arjovsky, and Aaron Courville. Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Many paths to equilibrium: GANs do not need to decrease a divergence at every step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes. CoRR, abs/1312.6114, Aditya Ramesh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
	</analytic>
	<monogr>
		<title level="m">Hierarchical text-conditional image generation with CLIP latents</title>
		<meeting><address><addrLine>Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Projected GANs converge faster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>abs/1503.03585</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Amortised MAP inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Casper Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Husz?r</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1RP6GLle" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=St1giarCHLP" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PxTIG12RRHS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On data augmentation for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1882" to="1897" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tackling the generative learning trilemma with denoising diffusion GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data-efficient instance generation from instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9378" to="9390" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1lxKlSKPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational heteroencoder randomized GANs for joint image-text modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1x5wRVtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7559" to="7570" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploiting chain rule and Bayes&apos; theorem to compare probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=f-ggKIDTu5D" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Truncated diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09671</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">K More generated images We provide more randomly generated images for LSUN-Bedroom, LSUN-Church, AFHQ, and FFHQ datasets in Figure 7, Figure 8, and Figure 9</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
