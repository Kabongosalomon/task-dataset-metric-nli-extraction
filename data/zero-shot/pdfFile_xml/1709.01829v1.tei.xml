<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Proposal Networks for Weakly Supervised Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
							<email>zhouyanzhao215@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
							<email>qiang.qiu@duke.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Soft Proposal Networks for Weakly Supervised Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object localization remains challenging, where only image labels instead of bounding boxes are available during training. Object proposal is an effective component in localization, but often computationally expensive and incapable of joint optimization with some of the remaining modules. In this paper, to the best of our knowledge, we for the first time integrate weakly supervised object proposal into convolutional neural networks (CNNs) in an end-to-end learning manner. We design a network component, Soft Proposal (SP), to be plugged into any standard convolutional architecture to introduce the nearly cost-free object proposal, orders of magnitude faster than state-of-the-art methods. In the SP-augmented CNNs, referred to as Soft Proposal Networks (SPNs), iteratively evolved object proposals are generated based on the deep feature maps then projected back, and further jointly optimized with network parameters, with image-level supervision only. Through the unified learning process, SPNs learn better object-centric filters, discover more discriminative visual evidence, and suppress background interference, significantly boosting both weakly supervised object localization and classification performance. We report the best results on popular benchmarks, including PASCAL VOC, MS COCO, and ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of object proposal methods greatly drives the progress of the object localization. With the popularity of deep learning, object detection is evolving from pipelined frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to unified frameworks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, thanks to the unprecedentedly learning capability of convolutional neural networks (CNNs) and abundant object bounding box annotations.  Despite the unified frameworks achieve remarkable performance in supervised object detection, they can not be directly applied to weakly supervised object localization where only image-level labels, i.e., the presence or absence of object categories, are available during training.</p><p>To tackle the problem of weakly supervised object localization, many of the conventional methods follow a multiinstance learning (MIL) framework by using object proposal methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. The learning objective is designed to choose an instance (a proposal) from each bag (an image with multiple proposals) to minimize the image classification error; however, the pipelined proposaland-classification method is sub-optimal as the two steps can not be jointly optimized. Recent research <ref type="bibr" target="#b5">[6]</ref> demonstrates that the convolutional filters in CNN can be seen as object detectors and their feature maps can be aggregated to produce Class Activation Map (CAM) <ref type="bibr" target="#b35">[36]</ref>, which specifies the spatial distribution of discriminative patterns for different image classes. This end-to-end network demonstrates a surprising capability to localize objects under weak supervision. However, without the prior knowledge of informative object regions during training, conventional CNNs can be misled by co-occurrence patterns and noisy backgrounds, <ref type="figure">Fig. 2</ref>. The weakly supervised setting increases the importance of high-quality object proposals, but the problem to integrate the proposal functionality into a unified framework for weakly supervised object localization remains open.</p><p>In this paper, we design a network component, Soft Proposal (SP), to be plugged into standard convolutional architectures for nearly cost-free object proposal (?0.9ms per image, 10?faster than RPN <ref type="bibr" target="#b21">[22]</ref>, 200?faster than Edge-Boxes <ref type="bibr" target="#b36">[37]</ref>), <ref type="figure" target="#fig_1">Fig. 1</ref>. CNNs using SP module are referred to as Soft Proposal Networks (SPNs). In SPNs, iteratively evolved object proposals are projected back on the deep feature maps, and further jointly optimized with network parameters, using image-level labels only. We further apply the SP module to successful CNNs including CNN-S, VGG, and GoogLeNet, and upgrade them to Soft Proposal Networks (SPNs), which can learn better object-centric filters and discover more discriminative visual evidence for weakly supervised localization tasks.</p><p>The meaning of the word "soft" is threefold. First of all, instead of extracting multiple materialized proposal boxes, we predict objectness score for each receptive field, based on the deep feature maps. Next, the proposal couples with deep activation in a probabilistic manner, which not only avoids threshold tuning but also aggregates all information to improve performance. Last but not least, the proposal iteratively evolves along with CNN filters updating.</p><p>To summarize, the main contributions of this paper are:</p><p>? We design a network component, Soft Proposal (SP), to upgrade conventional CNNs to Soft Proposal Networks (SPNs), in which the network parameters can be jointly optimized with the nearly cost-free object proposal.</p><p>? We upgrade successful CNNs to SPNs, including CNN-S, VGG16, and GoogLeNet, and improve the state-of-the-art of weakly supervised object localization by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly supervised object localization problems are often solved with a pipelined approach, i.e., an object proposal method <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref> is first applied to decompose images into object proposals, with which a latent variable learning method, e.g., multi-instance learning (MIL), is used to iteratively perform proposal selection and classifier estimation  <ref type="figure">Figure 2</ref>. Visualization of Class Activation Maps (CAM) <ref type="bibr" target="#b35">[36]</ref> for generic CNN and the proposed SPN. CNNs can be misled by noisy backgrounds, e.g., grass for "cow", and co-occurrence patterns, e.g., rail for "train", and thus miss informative object evidence. In contrast, SPNs focus on informative object regions during training to discover more fine-detailed evidence, e.g., hands for "person", while suppressing background interference. Best viewed in color. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. With the popularity of deep learning, the pipelined approaches have been evolving to end-toend MIL networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref> by learning convolutional filters as detectors and using response maps to localize objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Proposal</head><p>Conventional object proposal methods, e.g., Selective Search (SS) <ref type="bibr" target="#b29">[30]</ref> and EdgeBoxes (EB) <ref type="bibr" target="#b36">[37]</ref>, use redundant proposals generated with hand-craft features to hypothesize objects locations. Region Proposal Network (RPN) regresses object locations using deep convolutional features <ref type="bibr" target="#b21">[22]</ref>, reports the state-of-the-art proposal performance. The success of RPN roots in the localization capability of deep convolutional features; however, such capability is not available until the network is well trained with precise annotations about object locations, i.e., bounding boxes, which limits its applicability to weakly supervised methods.</p><p>Our SPN is specified for weakly supervised object localization task with only image-level annotations, i.e., presence or absence of object categories. The key difference between our method to existing ones is that the "soft" proposal is an objectness confidence map instead of materialized boxes. Such a proposal couples with convolutional activation and evolves with the deep feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weakly Supervised Localization</head><p>Pipelined methods. Weakly supervised localization methods often use a stepwise strategy, i.e., first extracting candidate proposals and then learning classification model together with selecting proposals to localize objects. Many approaches have been explored to prevent the learning procedure from getting stuck to a local minimum, e.g., prior regularization <ref type="bibr" target="#b2">[3]</ref>, multi-fold learning <ref type="bibr" target="#b7">[8]</ref>, and smooth optimization methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3]</ref>. One representative method is WSDDN <ref type="bibr" target="#b4">[5]</ref>, which significantly improves the object detection performance by performing proposal selection together with classifier learning. ContextLoc <ref type="bibr" target="#b13">[14]</ref> updates WSDDN by introducing two context-aware modules which try to expand or contract the fixed proposals in learning procedure to leverage the surrounding context to improve localization. Attention net <ref type="bibr" target="#b28">[29]</ref> computes an attention score for each precomputed object proposals. ProNet <ref type="bibr" target="#b26">[27]</ref> uses parallel CNN streams for multiple scales to propose possible object regions and then classify these regions via cascaded CNNs.</p><p>To the best of our knowledge, we are the first to integrate proposal step into CNNs and achieve jointly updating among proposal generation, object region selection, and object detector estimation under weak supervision.</p><p>Unified frameworks. Another line of research shows up in weakly supervised localization uses unified network frameworks to perform both localization and classification. The essence of the method Oquab et al. <ref type="bibr" target="#b19">[20]</ref> is that the deep feature maps are interpreted as a "bag" of instances, where only the highest responses of feature maps contribute to image label prediction in an MIL-like learning procedure. Zhou et al. <ref type="bibr" target="#b35">[36]</ref> achieve remarkable localization performance by leveraging a global average pooling layer behind the top convolutional layer to aggregate class-specific activation. In the following works, Zhang et al. <ref type="bibr" target="#b34">[35]</ref> formulate such a class activation procedure as conditional probability backward propagation along convolutional layers to localize discriminative patterns in generic CNNs. Bency et al. <ref type="bibr" target="#b1">[2]</ref> propose a heuristic search strategy to hypothesize locations of feature maps in a multi-scale manner and grade the corresponding receptive fields by the classification layer.</p><p>The main idea of these methods is that the convolutional filters can behave as detectors to activate locations on the deep feature maps, which provide informative evidence for image classification. Despite the simplicity and efficiency of these networks, they are observed missing useful object evidence, as well as being misled by complex backgrounds. The reason behind this phenomenon can be that the filters learned for common object classes are challenged with object appearance variations and background complexity. Our proposed SPN targets at solving such problems by utilizing image-specific objectness prior and coupling it with the network learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Soft Proposal Network</head><p>In this section, we present a network component, Soft Proposal (SP), to be plugged into standard convolutional architectures for nearly cost-free object proposal. CNNs using SP module are referred to as Soft Proposal Networks (SPNs), <ref type="figure" target="#fig_3">Fig. 3</ref>. Despite the SP module can be inserted after any CNN layer, we apply it after the last convolutional layer where the deep features are most informative. For weakly supervised object localization, SPN has an spatial pooling layer with the output features connected to image labels, as illustrated later.</p><p>In the learning procedure of SPN, the Soft Proposal Generation step spotlights potential object locations via performing graph propagation over the receptive fields of deep responses, and the Soft Proposal Coupling step aggregates feature maps with the generated proposal map. With iterative proposal generation, coupling, and activation, SPN performs weakly supervised learning in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Soft Proposal Generation</head><p>The proposal map, M ? R N ?N , is an objectness map generated by SP module based on the deep feature maps,   </p><formula xml:id="formula_0">(i, j) on U l has a deep feature vector u l ij = U l ?,i,j ? R K from all K channels of U l . To generate M , a fully connected directed graph G is first constructed by connecting every location on U l , with the weight matrix D ? R N 2 ?N 2</formula><p>where D iN +j,pN +q indicating the weight of edge from node (i, j) to node (p, q).</p><p>To calculate the weight matrix D, two kinds of objectness measures are utilized: 1). Image regions from the same object category share similar deep features. 2). Neighboring regions exhibit semantic relevance. The objectness confidence are reflected with a dissimilarity measure that combines feature difference and spatial distance, as D iN +j,pN +q u l ij ?u l pq ?L(i?p, j?q), and L(a, b) exp(? a 2 +b 2 2 2 ), where is empirically set as 0.15N in all experiments. And then the weights of the outbound edges of each node are normalized to 1, i.e.,</p><formula xml:id="formula_1">D a,b = D a,b N a=1 D a,b .</formula><p>With the weight matrix D defining the edge weight between nodes, a graph propagation algorithm, i.e., random walk <ref type="bibr" target="#b17">[18]</ref>, is utilized to generate the proposal map M . The random walk algorithm iteratively accumulates objectness confidence at the nodes that have high dissimilarity with their surroundings. A node receives confidence from inbound directed edges, and then the confidence among the nodes can be diffused along the outbound directed edges which are connected to all other nodes, <ref type="figure" target="#fig_5">Fig. 4</ref>. In this procedure, a location transfer confidence to others via globally objectness flow, which not only collects local object evidence but also depresses noise regions. For the convenience of random walk operation, we first reshape the 2D proposal map M to a vector with N 2 element, initialized with the value 1 N 2 . M is updated with iteratively multiplying with the weight matrix D, as</p><formula xml:id="formula_2">M ? D ? M.</formula><p>(1)</p><p>The above procedure is a variant of the eigenvector centrality measure <ref type="bibr" target="#b18">[19]</ref>, which outputs a proposal map to indicate the objectness confidence of each location on the deep feature maps. Note that the weight matrix D is conditional on the deep feature maps U l , and U l is conditional on the convolutional filters of the l-th layer, W l , in the learning procedure. To show such dependency, Eq. 1 is updated as</p><formula xml:id="formula_3">M ? D U l (W l ) ? M.<label>(2)</label></formula><p>The random walk procedure can be seen as a Markov chain that can reach unique stable state because the chain is ergodic, a property which emerges from the fact that the graph G is by construction strongly connected <ref type="bibr" target="#b12">[13]</ref>. Given deep feature maps U , Eq. 2 usually reaches its stable state in about ten iterations, and the output M is reshaped from a vector to a 2D proposal map M ? R N ?N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Soft Proposal Coupling</head><p>The proposal map generated with the deep feature maps in a weakly supervised manner can be regarded as a kind of objectness map, which indicates possible object regions. From the perspective of image representation, the proposal map spotlights "regions of interest" that are informative to image classification. M can be integrated into the end-toend learning via SP module, <ref type="figure" target="#fig_1">Fig. 1</ref>, to aggregate the imagespecific discriminative patterns from deep responses.</p><p>In the forward propagation of a SP-augmented CNN, i.e., SPN, each feature map of the coupled V ? R N ?N is the Hadamard product of the corresponding feature map of U and M ,</p><formula xml:id="formula_4">V k = U l k (W l ) ? M, k=1,2,...,K,<label>(3)</label></formula><p>where the subscript k denotes the channel index and "?" denotes element-wise multiplication. The coupled feature maps V pass forward to predict scores y ? R C of C classes, and then the prediction error E = (y, t) of each sample comes out according to the image labels t. (?) is the loss function. In the back-propagation procedure of SPN, the gradient is apportioned by M , as</p><formula xml:id="formula_5">W l = W l + ?W (M ) ?W (M ) = ?? ?E ?W l (M )<label>(4)</label></formula><p>where ? is the network learning rate. ?W (M ) means that W l is conditional on M , as the gradients of filters ?E ?W l are conditional on M , Eq. 7. Since W l is conditional on M , the SPN learns more informative image regions in each image and depresses noisy backgrounds. </p><formula xml:id="formula_6">M ? D U l (W l ) ? M 5:</formula><p>until stable state reached <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_7">V = U l (W l ) ? M , feed forward. 7: W l = W l + ?W (M ), backward. 8:</formula><p>for all the convolutional layers l do 9:</p><p>U l = W l * U l?1 10: end for <ref type="bibr">11:</ref> until Learning converges Given the Soft Proposal Generation defined by Eq. 2, the Soft Proposal Coupling defined by Eq. 3, and the back propagation procedure defined by Eq. 4, it is clear that U l , W l , and M are conditional on each other. During training, once the convolutional filters W l changed by Eq. 4, U l will also change. Once U l is updated, a random walk procedure, described in Sec. 3.1, is utilized to update the proposal map M . The proposal map M helps SPNs to progressively spotlight feature maps U l and learn discriminative filters W l , thus the proposals and filters are jointly optimized in SPNs, <ref type="figure" target="#fig_3">Fig. 3</ref>. The procedure is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weakly Supervised Activation</head><p>The weakly supervised learning task is performed by firstly using an spatial pooling layer to aggregate deep feature maps to a feature vector, and connecting such a feature vector to image categories with a fully connect layer, <ref type="figure" target="#fig_3">Fig. 3</ref>. Such an architecture uses weak supervision posed from the end of the network, i.e., the image category annotations, to activate potential object regions.</p><p>In the forward propagation of SPN, proposal map M is generated by the SP module inserted behind the l-th convolutional layer. The feature maps U l is computed as</p><formula xml:id="formula_8">U l j = ( i?Sj U l?1 i * W l ij + b l j ) ? M,<label>(5)</label></formula><p>where S j is a selection of input maps, b l j is the additive bias, and W l ij is the convolutional filters between the i-th input map in U l?1 and the j-th output map in U l .</p><p>In the backward propagation of SPN, the error propagates from layer l + 1 to layer l via the ?, as</p><formula xml:id="formula_9">? l = ?E ?U l = ?E ?U l+1 ?U l+1 ?U l = ? l+1 ?[(U l * W l+1 + b l ) ? M ] ?U l = ? l+1 * W l+1 ? M,<label>(6)</label></formula><p>which indicates that the proposal map M spotlights not only informative regions on feature maps but also worth-learning locations. Since the M flows along with gradients ?, inserting one SP module after the top convolutional layer can effect all CNN filters.</p><p>Once ? l is calculated, we can immediately compute the gradients for filters as</p><formula xml:id="formula_10">?E ?W l ij = p,q (? l j ) pq (x l?1 i ) pq = p,q (? l+1 j * W l+1 j? ) pq M pq (x l?1 i ) pq ,<label>(7)</label></formula><p>and compute the gradients for bias as</p><formula xml:id="formula_11">?E ?b l ij = p,q (? l j ) pq = p,q (? l+1 j * W l+1 j? ) pq M pq ,<label>(8)</label></formula><p>where W l+1 j? denotes the filters of layer l +1 that are used to calculate U l j , and (x l?1 i ) pq denotes the patch centered (p, q) on U l?1 i . With Eq. 7 and Eq. 8, the proposal map M which indicates the objectness confidence of an image combines with the gradient maps in the weakly supervised activation procedure, driving SPN to learn more useful patterns.</p><p>For weakly supervised object localization, we calculate the response map R c for the c-th class, similar to <ref type="bibr" target="#b35">[36]</ref>, R c = k w k,c ?? k ? M where? k is the k-th feature map of the last convolutional layer, w k,c is the weight value of the fully connected layer which connects the c-th output node and the k-th feature vector, <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We upgrade state-of-the-art CNN architectures, e.g., VGG16 and GoogLeNet, to SPNs, and evaluate them on popular benchmarks. In Sec. 4.1, we compare SPN with conventional object proposal methods, showing that it can generate high-quality proposals with negligible computational overhead. In Sec. 4.2, on a weakly supervised pointbased object localization task, we demonstrate SPNs can learn better object-centric filters, which produce precise responses on class-specific objects. In Sec. 4.3, SPNs are further tested on a weakly supervised object bounding box localization task, validating its capability of discovering more fine-detailed visual evidence in complex cluttered scenes. In Sec. 4.4, the significant improvement of classification performance on PASCAL VOC <ref type="bibr" target="#b9">[10]</ref> (20-classes, ?10k images), MS COCO <ref type="bibr" target="#b15">[16]</ref> (80-classes, ?160k images), and ImageNet <ref type="bibr" target="#b22">[23]</ref> (1000-classes, ?1300k images), shows the superiority of SPNs beyond weakly supervised object localization tasks 2 . We train SPNs using SGD with cross- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ObjectEnergy(%) Time(ms) Selective Search <ref type="bibr" target="#b29">[30]</ref> 53.7 2000 EdgeBoxes <ref type="bibr" target="#b36">[37]</ref> 58.8 200 RPN (supervised) <ref type="bibr" target="#b21">[22]</ref> 63.3 10.5 SPN (weakly supervised) 62.2 0.9 entropy loss. We use a weight decay of 0.0005 with a momentum of 0.9 and set the initial learning rate to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proposal Quality</head><p>On the VOC2007 dataset, we assess the quality of proposals by an Object Energy metric defined below. For the compared Selective Search <ref type="bibr" target="#b29">[30]</ref>, EdgeBoxes <ref type="bibr" target="#b36">[37]</ref> and RPN <ref type="bibr" target="#b21">[22]</ref> methods, the energy value of a pixel is the sum of scores of the proposal boxes that cover the pixel. Therefore, all objectness values in an image constitute an energy map that indicates the informative object regions predicted by the method. For the SPN, we produce Object Energy maps by rescaling proposal maps to the image size, <ref type="figure" target="#fig_7">Fig. 5</ref>. We further normalize each energy map and compute the sum of Object Energy of pixels those fall into ground-truth bounding boxes as the Object Energy.</p><p>It can be seen from the definition that the Object En-  ergy values range in [0.0, 1.0], which indicates how many informative object areas in the image are spotlighted by the method. The second column in Tab. 1 demonstrates that the proposals generated by SPN are of high-quality. The Object Energy of SPN proposals is significantly larger than those of Selective Search and EdgeBoxes, which usually produce redundant proposals and cover many background regions. Surprisingly, The Object Energy of SPN proposals obtained by weakly supervised learning is comparable to that of supervised RPN method (62.2% vs. 63.2%). It can be seen in <ref type="figure" target="#fig_9">Fig. 6(a)</ref> that the proposed SPN can spotlight small objects significantly better than the Selective Search and Edge-Boxes methods, despite that the proposal maps are based on low-resolution deep feature maps. <ref type="figure" target="#fig_9">Fig. 6(b)</ref> demonstrates that the SPN proposals can iteratively evolve and jointly optimize with network filters during the end-to-end training. Moreover, the implementation of SPN is simple and naturally compatible with GPU parallelization. It can be seen from the third column of Tab. 1 that the proposed SP module can introduce weakly supervised object proposal to CNNs in a nearly cost-free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pointing Localization</head><p>Pointing without prediction. To evaluate whether the proposed SPN can learn more discriminative filters which are effective to produce accurate response maps, we test it on the weakly supervised pointing task. We select three successful CNNs, including CNN-S <ref type="bibr" target="#b6">[7]</ref>, VGG16 <ref type="bibr" target="#b24">[25]</ref>, and GoogLeNet <ref type="bibr" target="#b27">[28]</ref> and upgrade them to SPNs by inserting the SP module after their last convolution layers, <ref type="figure" target="#fig_3">Fig. 3</ref>. All SPNs are fine-tuned on the VOC2007 training set with same hyper-parameters, and we calculate the response maps as described in Sec. 3.3 with ground-truth labels for pointing localization. Following the setting of c-MWP <ref type="bibr" target="#b34">[35]</ref>, a stateof-the-art method, we calculate the accuracy of pointing lo-  <ref type="figure">Figure 7</ref>. Examples of pointing localization, which shows that SPN is effective in complex scenes: a) Noisy co-occurrence patterns, e.g., leaves for "potted plant". b) Small objects, e.g., "apple" in hand. c) Cluttered backgrounds, e.g., "car" on the street. d) Infrequent form, e.g., closed "umbrella". Best viewed in color.</p><p>calization as below: a hit is counted if the pixel of maximum response falls in one of the ground truth bounding boxes of the cued object category within 15 pixels tolerance. Otherwise, a miss is counted. We measure the per-class localization accuracy by Acc = Hits Hits+M isses . The overall results are the mean value of per-class point localization accuracy.</p><p>For the VOC2007 dataset, we use two test sets, i.e., All and Difficult (Diff.) <ref type="bibr" target="#b34">[35]</ref>. All means the overall test set and Diff. means a difficult subset which has mixed categories and contains small objects. As shown in Tab. 2, upgrading conventional CNNs to SPNs brings significant performance improvement. Specifically, the SP-VGGNet outperforms c-MWP by 7.5% (87.5 % vs 80.0 %) for All and 11.3% (78.1% vs 66.8%) for Diff.. The SP-GoogLeNet outperforms c-MWP by 3.1% and 6.8% for All and Diff., respectively. The significant improvement of pointing localization performance validates the effectiveness of the SP module for guiding SPNs to learn better object-centric filters, which can pick up accurate object responses.</p><p>We made multiple observations in Tab. 2. 1). SP-VGGNet has better performance than SP-GoogLeNet on pointing localization. The reason can be that the receptive fields of SP-VGGNet are smaller than that of SP-GoogLeNet. Without much overlap between receptive fields, the objectness propagation in SP module can be more effective.  than that on All, which shows that the proposal functionality of SPNs is particularly effective in cluttered scenes.</p><p>Pointing with prediction. We further test SPN on a more challenging pointing-with-prediction task. The task requires the network output not only the correct prediction of the presence/absence of the object categories in test images, but also the correct pointing localization of objects, i.e., the point of maximum response falls in one of the ground truth bounding boxes within 18 pixels tolerance <ref type="bibr" target="#b19">[20]</ref>.</p><p>We upgrade a pre-trained VGG16 model to SPN and respectively fine-tune it on VOC2012 and COCO2014 dataset for 20 epochs. Results are reported in Tab. 3. Without multiscale setting, SPN outperforms the state-of-the-art method <ref type="bibr" target="#b1">[2]</ref> by a significant margin (5.8% mAP for VOC2012, 6% mAP for COCO2014). This evaluation demonstrates that the Soft Proposal module endows CNNs accurate localization capability while keeping its classification ability. In Sec. 4.4, we will show that upgrading CNNs to SPNs can even improve the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Bounding Box Localization</head><p>Although without object-level annotations involved in the learning phase, our method can also be used to estimate object bounding boxes with the help of response maps. We calculate each response map with ground truth labels and convert them to binary maps with the mean value as thresholds. We then rescale them to the original image size and extract the tightest box covering the foreground pixels as the predicted object bounding box.</p><p>The Correct Localization (CorLoc) metric <ref type="bibr" target="#b8">[9]</ref> is used   <ref type="figure">Figure 8</ref>. Bounding box localization results on the VOC2007 test set. By activating fine-detailed evidence like arm or leg for "person", paw for "cat", and texture fragments for "sofa", the estimated bounding boxes are more precise than those by WSDDN.</p><p>to evaluate the bounding box localization performance. It can be seen in Tab. 4 that the mean CorLoc of our method outperforms the state-of-the-art ContextLoc method <ref type="bibr" target="#b13">[14]</ref> by about 5%. Surprisingly, on the "dog", "cat", "horse", and "person" classes, SPN outperforms the compared method up to 20-30%. It can be seen from <ref type="figure">Fig. 8</ref> that the conventional method tends to use the most discriminative part for each category, e.g., faces, while SPN can discover more fine-detailed object evidence, e.g., hands and legs, thanks to the objectness prior introduced by the SP module. On the "sofa" and "table" classes, our method outperforms other methods by 10%, demonstrating the capability of SPN to correctly localize the occluded objects, <ref type="figure">Fig. 8</ref>, which shows that the graph propagation in the Soft Proposal Generation step helps to find object fragments of similar appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Classification</head><p>Although to predict the presence/absence of object categories in an image does not require accurate located and comprehensive visual cues, the proposal functionality of SPNs which highlights informative regions while suppressing disturbing backgrounds during training should also benefit the classification performance.</p><p>We use GoogLeNetGAP <ref type="bibr" target="#b35">[36]</ref>, a simplified version of GoogLeNet, as the baseline. By inserting SP module after the last convolution layer, the GoogLeNetGAP is upgraded to a SPN. The SPN is trained on the ILSVRC2014 dataset, i.e., ImageNet, for 90 epochs with the SGD method. It can be seen in the second column of Tab. 6 that the SPN significantly outperforms the baseline GoogLeNetGAP by 1.5%, which shows that the SPNs can learn more informative feature representation. We then fine-tune each trained model on COCO2014 and VOC2007 by 50 and 20 epochs to assess the generalization capability of SPN. As shown in the third column of Tab. 6. SP-GoogLeNetGAP surpasses the baseline by a large margin, e.g., 4.5% on VOC2007. This further demonstrates that the weakly supervised object proposal is effective for both localization and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a simple yet effective technique, Soft Proposal (SP), to integrate nearly cost-free object proposal into CNNs for weakly supervised object localization. We designed the SP module to upgrade conventional CNNs, e.g., VGG and GoogLeNet, to Soft Proposal Networks (SPNs). In SPNs, iteratively evolved object proposals are generated based on the deep feature maps then projected back, leading filters to discover more fine-detailed evidence through the unified learning procedure. SPNs significantly outperforms state-of-the-art methods on weakly supervised localization and classification tasks, demonstrating the effectiveness of coupling object proposal with network learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t P r o p o s a l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Soft Proposal (SP) module can be inserted after any CNN layer. A proposal map M is generated based on deep feature maps U and then projected back, which results in feature maps V . During the end-to-end learning procedure, M iteratively evolves and jointly optimizes with the feature maps to spotlight informative object regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The first row shows the Soft Proposal Network architecture. The second row illustrates the evolution of the proposal map during training epochs (corresponding to the outer loop of Algorithm 1). The third row presents the evolution of the response map for "cow". The proposal map produced by SP module iteratively evolves and jointly optimizes with convolutional filters during the learning phase, leading SPN to discover fine-detailed visual evidence for localization. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Soft Proposal Generation in a single SPN feedforward pass (corresponding to the inner loop of Algorithm 1). Experimentally, the generation reaches stable in about ten iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Consider a SP module is inserted after the l-th convolutional layer, let U l ? R K?N ?N denote the deep feature maps of the l-th convolutional layer, where K is the number of feature maps (channels), N ? N denotes the spatial size of a feature map. Each location</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Proposal examples. The first row presents input images. The second row presents proposal coupled images, by composing the proposal map with the original images. The third row shows top-100 scored receptive fields according to the proposal map. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>(a) Object Energy curves. The x-coordinate is the ratio between the object area to the image size, and y-coordinate is the Object Energy. The curves are produced by using a 3-polynomial regression on the dots, each of which denotes an image. (b) Evolution of Object Energy during the learning procedure. Best viewed zooming on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Learning SPN with Soft Proposal CouplingInput: Training images with category labels Output: Network parameters, proposal map for each image.</figDesc><table><row><cell cols="2">1: repeat</cell></row><row><cell>2:</cell><cell>initial each element in M with 1 N 2</cell></row><row><cell>3:</cell><cell>repeat</cell></row><row><cell>4:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Proposal quality evaluation on VOC2007 test set. The Object Energy in the second column indicates the percentage of spotlighted object areas. Note that RPN is learned with object bounding box annotations (supervised) while SPN is learned with image label annotations (weakly supervised). The third column describes the average time cost per image. RPN and SPN are tested with a NVIDIA Tesla K80 GPU while Selective Search and Edge-Boxes are tested on CPU due to algorithm complexity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2). The accuracy improvement on Diff. is larger</figDesc><table><row><cell>Method</cell><cell>CNN-S</cell><cell>VGG16</cell><cell>GoogLeNet</cell></row><row><cell>Center</cell><cell cols="2">69.5/42.6 69.5/42.6</cell><cell>69.5/42.6</cell></row><row><cell>Grad [24]</cell><cell cols="2">78.6/59.8 76.0/56.8</cell><cell>79.3/61.4</cell></row><row><cell cols="3">Deconv [33] 73.1/45.9 75.5/52.8</cell><cell>74.3/49.4</cell></row><row><cell>LRP [1]</cell><cell>68.1/41.3</cell><cell>-</cell><cell>72.8/50.2</cell></row><row><cell>CAM [36]</cell><cell>-</cell><cell>-</cell><cell>80.8/61.9</cell></row><row><cell>MWP [35]</cell><cell cols="2">73.7/52.9 76.9/55.1</cell><cell>79.3/60.4</cell></row><row><cell cols="3">c-MWP [35] 78.7/61.7 80.0/66.8</cell><cell>85.1/72.3</cell></row><row><cell>SPN</cell><cell cols="2">81.8/66.7 87.5/78.1</cell><cell>88.2/79.1</cell></row><row><cell cols="4">Table 2. Pointing localization accuracy (%) on VOC2007 test set</cell></row><row><cell cols="4">(All/Diff.). Center is a baseline method which uses the image</cell></row><row><cell cols="3">centers as estimation of object centers.</cell><cell></cell></row><row><cell cols="2">Method</cell><cell cols="2">mAP (%)</cell></row><row><cell cols="2">Dataset</cell><cell cols="2">VOC COCO</cell></row><row><cell cols="3">Oquab et al. [20] 74.5</cell><cell>41.2</cell></row><row><cell cols="2">Sun et al. [27]</cell><cell>74.8</cell><cell>43.5</cell></row><row><cell cols="2">Bency [2]</cell><cell>77.1</cell><cell>49.2</cell></row><row><cell>SPN</cell><cell></cell><cell>82.9</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mean Average Precision (mAP) of location prediction on VOC2012 val. set and COCO2014 val. set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean Bilen et al. [4] 66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5 60.0 19.8 39.2 41.7 30.1 50.2 44.1 43.7 Wang et al. [31] 80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5 Cinbis et al. [8] 65.3 55.0 52.4 48.3 18.2 66.4 77.8 35.6 26.5 67.0 46.9 48.4 70.5 69.1 35.2 35.2 69.6 43.4 64.6 43.7 52.0 WSDDN [5] 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5 ContextLoc [14] 83.3 68.6 54.7 23.4 18.3 73.6 74.1 54.1 8.6 65.1 47.1 59.5 67.0 83.5 35.3 39.9 67.0 49.7 63.5 65.2 55.1 SP-VGGNet 85.3 64.2 67.0 42.0 16.4 71.0 64.7 88.7 20.7 63.8 58.0 84.1 84.7 80.0 60.0 29.4 56.3 68.1 77.4 30.5 60.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Correct Localization rate (CorLoc [9]) on the positive trainval images of the VOC2007 dataset (%).</figDesc><table><row><cell>Our response map</cell><cell>SPN</cell><cell>WSDDN</cell></row><row><cell>sofa cat person</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Table 5. Bounding box localization errors on ILSVRC2014 val. set. Classification results. The second column is the top-1/top-5 error rate (%) on ILSVRC2014 val. set. The third and fourth column are mAP (%) on VOC2007 test set and COCO val. set.</figDesc><table><row><cell>Method</cell><cell cols="5">CAM c-MWP MWP Fb[35] SPN</cell></row><row><cell>Error (%)</cell><cell>48.1</cell><cell>57.0</cell><cell>38.7</cell><cell>38.8</cell><cell>36.3</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">ImageNet COCO VOC</cell></row><row><cell cols="4">GoogLeNetGAP[36] 35.0/13.2</cell><cell>54.4</cell><cell>83.4</cell></row><row><cell cols="2">SP-GoogLeNetGAP</cell><cell cols="2">33.5/12.7</cell><cell>56.0</cell><cell>84.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please refer to supplementary materials for more results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are very grateful for support by NSFC grant 61671427, BMSTC grant Z161100001616005.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random walks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lov?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics, Paul erdos is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The mathematics of networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The new palgrave encyclopedia of economics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps. International Conference on Learning Representations (ICLR Workshop)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pronet: Learning to propose object-specific boxes for cascaded neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-learning scene-specific pedestrian detectors using a progressive latent model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<idno>abs/1611.07544</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A self-paced multiple-instance learning framework for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Topdown neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
