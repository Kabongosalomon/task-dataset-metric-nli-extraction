<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FaceDancer: Pose-and Occlusion-Aware High Fidelity Face Swapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rosberg</surname></persName>
							<email>felix.rosberg@berge.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Berge Consulting</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<settlement>Halmstad</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<settlement>Halmstad</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Alonso-Fernandez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<settlement>Halmstad</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristofer</forename><surname>Englund</surname></persName>
							<email>cristofer.englund@hh.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<settlement>Halmstad</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FaceDancer: Pose-and Occlusion-Aware High Fidelity Face Swapping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1: Face swapping results generated by FaceDancer.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a new single-stage method for subject agnostic face swapping and identity transfer, named FaceDancer. We have two major contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature Similarity Regularization (IFSR). The AFFA module is embedded in the decoder and adaptively learns to fuse attribute features and features conditioned on identity information without requiring any additional facial segmentation process. In IFSR, we leverage the intermediate features in an identity encoder to preserve important attributes such as head pose, facial expression, lighting, and occlusion in the target face, while still transferring the identity of the source face with high fidelity. We conduct extensive quantitative and qualitative experiments on various datasets and show that the proposed FaceDancer outperforms other state-of-the-art networks in terms of identity transfer, while having significantly better pose preservation than most of the previous methods. Code available at https:// github.com/felixrosberg/FaceDancer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face swapping is a challenging task aiming at shifting the identity of a source face into a target face, while preserv-ing the descriptive face attributes such as facial expression, head pose, and lighting of the target face. The idea of generating such non-existent face pairs has a vast range of applications in the film, game, and entertainment industry <ref type="bibr" target="#b1">[2]</ref>. Therefore, face swapping has rapidly attracted increased research interest in computer vision and graphics. The challenge in swapping faces remains in achieving a high fidelity identity transfer from the source face with a set of attributes which need to be consistent with those in the target face.</p><p>There exist two mainstream approaches for face synthesis: source-oriented and target-oriented methods. The former approaches initially synthesize a source face with the attributes captured in the target face, which is then followed by blending the source face into the target counterpart <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. These techniques still have difficulties in handling lighting, occlusion, and complexity. The latter approach directly convert the identity of the target face into one in the source face <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b2">[3]</ref>. These methods particularly rely on Generative Adversarial Networks (GAN) using a one-stage optimization setting. This helps preserve the target image attributes, such as pose and lighting, without requiring any additional processing step, e.g. by learning perceptual and deep features already in the training stage <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>In this work 1 , we introduce a novel, target-oriented, and single-stage method, named FaceDancer, to deal with challenges, e.g., lighting, occlusion, pose, and semantic structure (See <ref type="figure">Fig. 1</ref>). FaceDancer is simple, fast, and accurate. Our core contribution is twofold: First, we introduce an Adaptive Feature Fusion Attention (AFFA) module, which adaptively learns during training to produce attention masks that can gate features. Inspired by the recent methods <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b38">[39]</ref>, the AFFA module is embedded in the decoder and learns attribute features without requiring any additional facial segmentation process. The incoming feature maps in AFFA are features that have been conditioned on the source identity information, but also the skip connection of the unconditioned target information in the encoder (See <ref type="figure" target="#fig_0">Fig. 2)</ref>. The AFFA module, in a nutshell, allows FaceDancer to learn which conditioned features (e.g. identity information) to discard and which unconditioned features (e.g. background information) to keep in the target face. Our experiments show that gating from the AFFA module considerably improves the identity transfer.</p><p>Second, we present an Interpreted Feature Similarity Regularization (IFSR) method for boosting the attribute preservation. IFSR regularizes FaceDancer to enhance the preservation of facial expression, head pose, and lighting while still transferring the identity with high fidelity. More specifically, IFSR explores the similarity between intermediate features in the identity encoder by comparing the cosine distance distributions of these features in the target, source, and generated face triplets learned from a pretrained state-of-the-art identity encoder, ArcFace <ref type="bibr" target="#b11">[12]</ref> (See <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>We conduct extensive quantitative and qualitative experiments on the FaceForensic++ <ref type="bibr" target="#b33">[34]</ref> and AFLW2000-3D <ref type="bibr" target="#b43">[44]</ref> datasets and show that the proposed FaceDancer significantly outperforms other state-of-the-art networks in terms of identity transfer, while maintaining significantly better pose preservation than most of the previous methods. To address the scalability of our network, we further apply FaceDancer to low resolution images with harsh distortions and qualitatively show that FaceDancer can still improve the pose preservation in contrast to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are two leading approaches for face swapping: source-and target-oriented methods. Although our proposed method falls into the later category, we here provide a brief review of the literature related to both approaches.</p><p>Source-oriented approaches first transform the source face to match the expression and posture of the target face, and then blend with the target frame. One of the earliest approaches is The Digital Emily project <ref type="bibr" target="#b1">[2]</ref> which performs face swapping through expensive and time-consuming 3D scanning of a single actor. Getting one face ready with this method to insert in a scene can, however, take months. Banz et al. <ref type="bibr" target="#b3">[4]</ref> presents an early approach for utilizing 3D Morphable Models (3DMM) <ref type="bibr" target="#b12">[13]</ref> to generate source faces with matching target attributes. This approach, however, comes with the cost that for each image the subject hair must be carefully marked out. Nirkin et al. <ref type="bibr" target="#b30">[31]</ref> also utilizes 3DMM to extract pose and expression coefficients from the target face. These coefficients are then employed for reconstructing the source face. The reconstructed image is finally combined with the output from a facial segmentation network in order to automate the entire face swapping process. This method, however, struggles with textures and lighting conditions. FSGAN <ref type="bibr" target="#b29">[30]</ref> introduces a reenactment network particularly designed to reenact the source face based on the target landmarks. In this work, the blending process is performed in an additional step which combines outputs from a segmentation network together with outputs from an inpainting network. This method also struggles with lighting conditions. More importantly, due to relying on the target landmarks for reenactment, the reenacted source falls short in having an effective identity transfer.</p><p>Target-oriented approaches mostly rely on generative models to manipulate features of an encoded target face, together with a semi-supervised loss function or a regularization method to preserve attributes. Almost all of these methods, including ours, utilize facial recognition models in order to extract identity information to be later used for conditioning of the target features. FaceShifter <ref type="bibr" target="#b26">[27]</ref> robustly transfers the identity while maintaining attributes by having an attribute encoder-decoder model trained in a semi-supervised fashion. This model is coupled with a generator that injects the source identity information and adaptively learns to gate features between the generator and the attribute model. FaceShifter also has a secondary stage to improve the occlusion awareness. This approach succeeds well with identity and occlusion, but struggles with hard poses, which is solved by our new IFSR loss function. SimSwap <ref type="bibr" target="#b7">[8]</ref> has an encoder-decoder model that utilizes the identity information to manipulate the bottleneck features. To preserve attributes, SimSwap uses a modified version of the feature matching loss from pix2pixHD <ref type="bibr" target="#b37">[38]</ref>. This approach achieves state-of-the-art performance for preserving the pose at an arguably large trade-off for the identity transferability. HifiFace <ref type="bibr" target="#b38">[39]</ref> uses a combination of GANs and 3DMMs to achieve state-of-the-art identity performance. Although HifiFace produces high resolution photo-realistic face swaps, it, however, seems not to improve the pose considerably and performs worse than SimSwap. In addition, HifiFace relies on a 3DMM model, which particularly works well with high resolution images only <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our approach differs from these methods in that ours rely on the identity encoder for simplicity and can handle harsh  <ref type="bibr" target="#b3">4)</ref>. Note that, the ArcFace model has two instances just to avoid having otherwise multiple intersecting arrows in the figure. Right: RB stands for ResBlock. X s is the source face, X t is the target face, X c is the changed face, z id is the identity vector extracted from ArcFace, w id is the mapped identity vector, h is an incoming feature map, and z a is a skip connection feature map. The layer Resample represents either an average pooling operation abbreviated as 'down' or a bilinear upsampling indicated as 'up' or an identity function shown as 'none'. The layer Concat RB concatenates h and z a without the AFFA module.</p><p>image distortions such as artifacts that emerge in low resolution images. Our method also reaches state-of-the-art identity performance and improves the pose preservation in contrast to HifiFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section describes the FaceDancer network architecture shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, together with the AFFA module, the IFSR method, and loss functions. Throughout this paper, we use the following notations: X t refers to the target face which is the face image to be manipulated, X s defines the source face which is the image of the face whose identity is transferred, and X c is the changed face representing the manipulated target face with the identity of the source face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>FaceDancer involves a generator and a discriminator forming a conditional GAN model coupled with a mapping network and ArcFace <ref type="bibr" target="#b11">[12]</ref> as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Generator: The generator G relies on an U-Net like encoder-decoder architecture combined with a mapping network M (See <ref type="figure" target="#fig_0">Fig. 2</ref>). The encoder consists of a set of residual blocks with gradually increasing number of filters. The decoder also involves a set of residual blocks, each of which employs either Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b7">[8]</ref> or an AFFA module or a concatenation layer for exploiting encoded skip connections. The main aim of G is to generate X c from the encoded image X t while conditioning the feature maps on the mapped identity vector w id extracted from X s as shown on the left in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Discriminator: The discriminator D used for the adversarial loss is the same as the one in StarGan-v2 <ref type="bibr" target="#b8">[9]</ref> and Hi-fiFace <ref type="bibr" target="#b38">[39]</ref>, with the exception that we omit the multi-task discrimination, since we use the hinge loss.</p><p>Mapping network: FaceDancer has a mapping network M to boost the performance of G as already shown in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The mapping network learns to transform the initial identity distribution to a new distribution in order to particularly inject the identity information. The M network consists of four fully-connected layers (FC) combined with leaky ReLU as non-linearity in all layers except the last <ref type="figure" target="#fig_0">(Fig. 2)</ref>.</p><p>ArcFace: To extract and inject identity information from the source image X s , FaceDancer employs a pretrained state-of-the-art identity encoder, ArcFace <ref type="bibr" target="#b11">[12]</ref>, coming with a ResNet50 backbone <ref type="bibr" target="#b15">[16]</ref>. The resulting ArcFace output is an identity vector with the size of 512 that serves as an input to FaceDancer. The ArcFace model is also used for the computation of IFSR (Section 3.3) and the identity loss (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Adaptive Feature Fusion Attention (AFFA) Module</head><p>The AFFA module is inspired by previous works such as the Adaptive Attentional Denormalization layer in FaceShifters <ref type="bibr" target="#b26">[27]</ref> and the Semantic Facial Fusion module in HifiFaces <ref type="bibr" target="#b38">[39]</ref>. Unlike the former method where a separate attribute encoder-decoder model exists, we here keep everything condensed within the generator. In contrast to the latter method, which utilizes segmentation masks for supervision, we here avoid introducing any additional need to compute such segmentation masks for each training sample by letting AFFA adaptively learn attention masks. In this regard, AFFA employs the information from skip connections in the generator encoder and forces the generator to learn whether it should rely on features (z a ) from skip connections or features (h) from the decoder conditioned on the source identity <ref type="figure" target="#fig_0">(Fig. 2</ref>). This way, AFFA can implicitly learn to extract relevant descriptive face features. Instead of naively concatenating or adding the two feature maps (h and z a ), AFFA first concatenates the feature maps and then passes them through a few learnable layers <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Finally, AFFA produces an attention mask m with the same filter number as in h and z a . The following equation is used to gate and fuse h and z a :</p><formula xml:id="formula_0">h = h ? m + (1 ? m) ? z a ,<label>(1)</label></formula><p>where h denotes the final fused feature map between h and z a . We experimentally demonstrate the impact of the AFFA module by comparing with cases where either concatenation or addition is individually used to incorporate the information from skip connections in the generator encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Interpreted Feature Similarity Regularization (IFSR)</head><p>Target-oriented face swapping methods rely particularly on semi-supervised or unsupervised techniques to make sure that the output image maintains the target attributes. To favor the preservation of attributes, we regularize the FaceDancer training by employing intermediate features captured by the ArcFace <ref type="bibr" target="#b11">[12]</ref> identity encoder described in section 3.1. This idea of using pretrained identity encoders for exploring facial expressions is also supported by the recent work in <ref type="bibr" target="#b35">[36]</ref>.</p><p>To investigate which layers of ArcFace are responsible for facial expressions and, thus, contribute more to the attribute preservation, we perform a pre-study on a state-ofthe-art face swapping model FaceShifter <ref type="bibr" target="#b26">[27]</ref>. Note that, since the source code of FaceShifter <ref type="bibr" target="#b26">[27]</ref>, to the best of our knowledge, is not public, we here use our implementation of FaceShifter with minor modifications. For instance, in our implementation, the generator down samples to a resolution of 8 ? 8, instead of 2 ? 2. We also incorporate the weak feature matching loss from <ref type="bibr" target="#b7">[8]</ref> together with the L1 reconstruction loss, instead of L2. Next, we use our baseline implementation of FaceShifter to perform random face swaps between identities in the VGGFace2 data set <ref type="bibr" target="#b6">[7]</ref>. We then compare the cosine distances not only between the target and the generated face swaps, but also between the source and generated face pairs for exploring the intermediate features in each block in the ArcFace backbone. In addition, we compute the distance for intermediate features between negative pairs (imposters) of identities as qualitative reference. All these measured distributions of distances help us determine which layers, i.e., intermediate feature maps, are useful for preserving attribute information. For example, if there is a small distance between the target face and the generated face swap, it indicates that the intermediate features from that layer contain more attribute information. For this purpose, we also define a margin m i for each ith layer based on the computed mean distances. The motivation for the margin is to regularize the generator to match the mean of the distribution, instead of completely minimizing the distance. The final regularization equation is as follows:</p><formula xml:id="formula_1">L if sr = n i=k min(1 ? cos(I (i) (X t ), I (i) (X c )) ? m i ? s, 0),<label>(2)</label></formula><p>where I (i) denotes the ith intermediate feature map in the identity encoder ArcFace, m i represents the aforementioned margin for the ith layer, s is a hyperparameter that scales the margin, cos(.) represents the cosine similarity between two feature maps, k and n respectively denote the index of the first and final blocks, from which intermediate feature maps are extracted. Note that the feature maps are initially reshaped to a vector to have the appropriate dimensionality for the cosine similarity operation. In our experiments, k and n are set to 2 and 13, respectively. The main role of the margin scale s is to control the amount of feature similarity that can deviate from the margin. The lower the value of s, the stricter is the similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>During training, FaceDancer employs various loss functions: Identity loss, reconstruction loss, perceptual loss, adversarial loss regularized with our IFSR method, and gradient penalty for the discriminator. See <ref type="figure" target="#fig_0">Fig. 2</ref> for an overview of how these loss functions interact with inputs and outputs.</p><p>The identity loss is used to transfer the source identity as follows:</p><formula xml:id="formula_2">L i = 1 ? cos(I(X s ), I(X c )) ,<label>(3)</label></formula><p>where I is the identity encoder ArcFace and cos(.) denotes the cosine similarity. The output of I is the identity embedding vector z id (See <ref type="figure" target="#fig_0">Fig. 2)</ref>.</p><p>The reconstruction loss is used to make sure that when the target X t and source X s are the same images, the final result X c should be equal to the target image on a pixel-wise level. This reconstruction loss is defined as follows:</p><formula xml:id="formula_3">L r = ||X t ? X c ||, ifX t = X s 0, otherwise. ,<label>(4)</label></formula><p>To further strengthen the above behavior and improve the semantic understanding of the image, a perceptual loss is deployed. The motivation is that deep features as a perceptual loss have shown to be robust in many reconstruction tasks <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The perceptual loss is defined as:</p><formula xml:id="formula_4">L p = n i=0 ||P (i) (X t ) ? P (i) (X c )||, if X t = X s 0, otherwise. ,<label>(5)</label></formula><p>where P (i) denotes the ith feature map output of the VGG16 model <ref type="bibr" target="#b19">[20]</ref> pretrained on Imagenet <ref type="bibr" target="#b9">[10]</ref> and n is the final index of outputs before the down sample step within the VGG16 model. In our experiments, n is 4.</p><p>Furthermore, we utilize the cycle consistence loss to motivate the model to keep important attributes and structures within the target image <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The cycle consistence loss is formulated as follows:</p><formula xml:id="formula_5">L c = ||X t ? G(X c , I(X t ))|| ,<label>(6)</label></formula><p>where I denotes the identity encoder ArcFace and G is the generator.</p><p>For adversarial loss L adv we use the hinge loss <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The discriminator is regularized with a gradient penalty term L gp <ref type="bibr" target="#b13">[14]</ref>. The total loss function for the generator G is a weighted sum of above losses, formulated as:</p><formula xml:id="formula_6">L G = L adv + ? i L i + ? r L r + ? p L p + ? c L c + ? if sr L if sr ,<label>(7)</label></formula><p>where ? i = 10, ? r = 5, ? p = 0.2, ? c = 1 and ? if sr = 1. The weighting for L gp (? gp ) is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Implementation Details: FaceDancer is trained on the datasets VGGFace2 <ref type="bibr" target="#b6">[7]</ref> and LS3D-W <ref type="bibr" target="#b5">[6]</ref>. All faces are aligned with five point landmarks extracted with Reti-naFace <ref type="bibr" target="#b10">[11]</ref>. The alignment is performed to match the input into ArcFace <ref type="bibr" target="#b11">[12]</ref>. We keep all images in the data sets. Arc-Face is pretrained on MS1M <ref type="bibr" target="#b14">[15]</ref> with a ResNet50 backbone. We used the Adam <ref type="bibr" target="#b25">[26]</ref> optimizer with ? 1 = 0, ? 2 = 0.99, a learning rate of 0.0001, and exponential learning rate decay of 0.97 every 100K steps. The target (X t ) and source (X s ) images are randomly augmented with brightness, contrast, and saturation. Each configuration is trained for 300K steps for the ablation study ( <ref type="table" target="#tab_1">Table 2 and Table 3</ref>). We further train all the best performing configurations in the ablation studies (B, C, D) up to 500K steps to compare with the recent works using a batch size of 10. Image resolution for all of our models are 256 ? 256. There is a 20% chance that an image pair is the same, with at least one pair in the batch being the same. Margin scale s in Eq. 2 is set to 1.2. <ref type="table">Table 1</ref>:</p><p>Quantitative experiments on FaceForen-sics++ <ref type="bibr" target="#b33">[34]</ref>. See <ref type="table" target="#tab_1">Table 2</ref> for the definition of each FaceDancer configuration (Config B to D). These models has been trained for 500k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ID? Pose? Exp? FID? FaceSwap <ref type="bibr" target="#b0">[1]</ref> 54. <ref type="bibr" target="#b18">19</ref> 2.51 N/A N/A FaceShifter <ref type="bibr" target="#b26">[27]</ref> 97.38 2.96 N/A N/A MegaFS <ref type="bibr" target="#b44">[45]</ref> 90.83 2.64 N/A N/A FaceController <ref type="bibr" target="#b39">[40]</ref> 98.27 2.65 N/A N/A HifiFace <ref type="bibr" target="#b38">[39]</ref> 98.48 2.63 N/A N/A SimSwap <ref type="bibr" target="#b7">[8]</ref> 92 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Results</head><p>We perform quantitative evaluation of FaceDancer using the FaceForensics++ <ref type="bibr" target="#b33">[34]</ref> dataset and compare it to the other state-of-the-art face swapping networks, such as Sim-Swap <ref type="bibr" target="#b7">[8]</ref>, FaceShifter <ref type="bibr" target="#b26">[27]</ref>, HifiFace <ref type="bibr" target="#b38">[39]</ref>, and FaceController <ref type="bibr" target="#b39">[40]</ref>. The metrics evaluated are identity retrieval (ID), pose error, expression error, and Frech?t Inception Distance (FID) <ref type="bibr" target="#b16">[17]</ref>. For the identity retrieval, we initially perform random swaps for each image in the test set and then retrieve the correct identity with a secondary identity encoder, CosFace <ref type="bibr" target="#b36">[37]</ref>. To compare pose, we use the pose estimator in <ref type="bibr" target="#b34">[35]</ref> and report the average L2 error. The expression metric is often omitted for comparison due to poor accessibility of models. However, we here use an implementation of an expression embedder <ref type="bibr" target="#b32">[33]</ref> and report the average L2 error. FID is calculated between the swapped version of the test set and the unaltered test set and helps demonstrate when a model has problems with lighting, occlusion, visual quality, and posture.</p><p>Similar to the previous works <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we sample 10 frames from each video in FaceForensic++ which yields a test data set of 10K. As shown in <ref type="table">Table 1</ref> our method FaceDancer outperforms all the previous works by leading to the highest identity retrieval performance. Regarding the pose metric, we have comparable results, i.e., FaceDancer achieves the second-lowest pose error (2.04) after SimSwap <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>For the qualitative evaluation, we compare the performance of our model FaceDancer with the recent state-ofthe-art works SimSwap <ref type="bibr" target="#b7">[8]</ref>, FaceShifter <ref type="bibr" target="#b26">[27]</ref>, HifiFace <ref type="bibr" target="#b38">[39]</ref>, and FaceController <ref type="bibr" target="#b39">[40]</ref> as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We here note that SimSwap <ref type="bibr" target="#b7">[8]</ref> is the only work coming with a public and easy to access model. Due to this fact, we have more  <ref type="bibr" target="#b7">[8]</ref>, FaceShifter <ref type="bibr" target="#b26">[27]</ref>, HifiFace <ref type="bibr" target="#b38">[39]</ref>, and FaceController <ref type="bibr" target="#b39">[40]</ref>. in depth comparison with SimSwap, whereas for the other baseline models we show qualitative results for sample images only reported in these works. <ref type="figure" target="#fig_1">Fig. 3</ref> shows that our model FaceDancer behaves similar to SimSwap, but one can easily notice the substantially improved identity transfer in our results. FaceShifter performs good identity transfer and is able to transfer relevant attributes such as facial hair while preserving occlusion and the identity face shape. FaceShifter, however, struggles with lighting and gaze direction as it heavily relies on the second stage model. FaceController exhibits good identity transferability and decent pose error, however, still fails noticeably often with the gaze direction. Our approach FaceDancer deals with all these problems better. Finally, HifiFace demonstrates promising results regarding all these metrics, particularly when it comes to the facial shape. For instance, HifiFace exhibits better face shape preservation of the identity than our model. Otherwise, it is not feasible to compare qualitatively with HifiFace since our model FaceDancer quantitatively performs better (See <ref type="table">Table 1</ref>).</p><p>Furthermore, to address the scalability of our model, we qualitatively analyze the performance of FaceDancer compared to SimSwap on low resolution face images. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that FaceDancer has enough capacity to capture the semantic structure of the face images even under low resolution cases. FaceDancer is able to maintain the pixelation artifacts, while SimSwap either produces a smooth face or completely fails, as depicted in the first row of <ref type="figure" target="#fig_2">Fig. 4</ref>. FaceDancer also works well on videos without any temporal information. We refer to supplementary materials for video results. In supplementary materials we also include further results of images in higher resolution, further comparisons, occlusion, difficult poses, extreme cases and finally failure cases. Failure usually occurs when the face poses away from the camera or the face pose is an uncommon angle represented in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We here ablate different FaceDancer components (such as the AFFA module and the IFSR method) and compare  <ref type="table" target="#tab_1">Table 2</ref> for the definition of each FaceDancer configuration (Config B to D). <ref type="figure">Figure 5</ref>: Illustration of the impact of IFSR. Config A given in the 3rd column here shows results once IFSR is omitted during training as described in <ref type="table" target="#tab_1">Table 2.</ref> to two baselines as shown in <ref type="table" target="#tab_1">Table 2</ref>. The ablations shown in <ref type="table" target="#tab_1">Table 2</ref> are evaluated on FaceForensic++ <ref type="bibr" target="#b33">[34]</ref> and the ablations shown in <ref type="table" target="#tab_2">Table 3</ref> are evaluated on AFLW2000-3D <ref type="bibr" target="#b43">[44]</ref>. Baseline 1 and 2 respectively employ concatenation and addition in order to fuse feature maps from the decoder and skip connection. For baseline 1, baseline 2, configuration A and configuration B, the feature fusion is performed at top three resolutions (256, 128, 64). For configuration C, we use concatenation at resolution 256 and AFFA at resolution 128, 64, and 32 are processed. Configuration D is the same as C, but with two additional AFFA modules and skips at resolution 16 and 8 <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Configuration E is the same as D with the only difference that the mapping network (M ) in the generator is omitted <ref type="figure" target="#fig_0">(Fig.  2)</ref>. We refer to the supplementary materials for detailed figures of the baselines and configurations. All ablation configurations are trained for 300K steps. As reported in <ref type="table" target="#tab_1">Table 2</ref>, baselines 1 and 2 achieve the lowest pose errors, however, with the cost of having either high FID score or poor identity performance. Configuration A improves identity performance but does not use IFSR which leads to poor pose error, expression error, and FID. Since Configuration B employs IFSR, it improves the expression and pose problem, however, still struggles with the FID. Configuration C overcomes these problems and achieves state-of-the-art performance on identity. Adding two more AFFA modules in lower resolutions in the decoder slightly disrupts the identity performance, but improves the other metrics further. This is mainly because Configuration D fuses more features from the target face. The last row in this table shows that the mapping information employed by the FaceDancer generator improves identity transfer and FID with expression error as trade off.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we also provide the total runtime performance for each FaceDancer configuration. Inference and memory consumption profiling are done on a single Nvidia RTX 3090 with a batch size of 32. Profiling includes inference for ArcFace.</p><p>The contribution of IFSR and AFFA becomes clearer when we ablate on the pose challenging dataset AFLW2000-3D <ref type="bibr" target="#b43">[44]</ref>  <ref type="table" target="#tab_2">(Table 3</ref>). We here use AFLW2000-3D as the target data set and FaceForsenic++ as the source data set. In this case, after randomly swapping all faces in AFLW2000-3D with faces from FaceForensic++, we try to retrieve the original identity in FaceForensic++. <ref type="table" target="#tab_2">Table 3</ref> depict that baseline 1 still performs the best for pose, but falls short on the other metrics. Configurations A through D perform significantly better for ID, however, they have comparable pose error and similar or better expression error. Configuration E demonstrates the impact of not having the mapping network M used in FaceDancer. Configuration E falls short for identity perfor- mance and pose error ( <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our findings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of the AFFA Module</head><p>In this section, we provide a comprehensive study showing the role of the Adaptive Feature Fusion Attention (AFFA) module. For this purpose, we first trained FaceDancer using AFFA in the three upper resolutions of the decoder (256, 128, 64). However, this leads to noticeable color defects in the swapped face images <ref type="figure">(Fig. 5</ref>). Experimental findings reported in <ref type="figure" target="#fig_4">Fig. 7</ref> show that this is due to the usage of AFFA in the end of FaceDancer generator. The attention maps generated at the resolution of 256 are mostly gray, as depicted in <ref type="figure" target="#fig_4">Fig. 7</ref>. We hypothesize that in the highest layer of the generator, the attention maps are far away from fusing the feature maps as expected. As shown in <ref type="table" target="#tab_1">Table 2</ref>, baselines 1 and 2 do not have any problems with the color defect as demonstrated by the substantially lower FID scores compared to configurations A and B which rely on AFFA at resolution 256. To remedy this problem, we replace the final AFFA module with a simple concatenation operation while adding an AFFA module to resolution 32. In <ref type="figure" target="#fig_4">Fig. 7</ref>, we show examples of attentions maps for each configuration in <ref type="table" target="#tab_1">Table 2</ref> at each resolution of the decoder where the FaceDancer generator uses an AFFA module.   <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis of IFSR</head><p>We now provide a comprehensive experimental evaluation on the role of the Interpreted Feature Similarity Regularization (IFSR) method.</p><p>We start investigating intermediate features within the ArcFace ResNet50 backbone by comparing the cosine distance between feature maps computed for the target face, the source face, the changed face, and negative pairs using the VGGFace2 <ref type="bibr" target="#b6">[7]</ref> dataset. This process is repeated for each residual block output in ArcFace. The changed face is obtained by deploying a pretrained implementation of FaceShifter <ref type="bibr" target="#b26">[27]</ref>, briefly detailed in Section 3.3. As shown in <ref type="figure" target="#fig_3">Fig. 6(a)</ref>, the changed and target faces (c2t) share significantly more similar features than those observed between the changed and source faces (c2s) in the early ArcFace layers. This behavior is not observed in the final residual block, as depicted in <ref type="figure" target="#fig_3">Fig. 6(b)</ref>. This strongly suggests that the identity encoder contains important information such as pose, expression and occlusions in the earlier layers while the final blocks store identity information. To demonstrate the separability of the c2t and c2s distributions in <ref type="figure" target="#fig_3">Fig. 6</ref>, we calculate the equal error rate (EER) between these distributions. As shown by the EER plots in <ref type="figure" target="#fig_3">Fig. 6(c)</ref>, the c2t and c2s distributions are completely separable until block 14. Afterwards, the EER jumps to more than 50%, which means that the c2t distribution moves to the right of c2s, i.e., X c shares more identity attributes with X s in contrast to X t in the same layer. This confirms that X c successfully captures the identity of X s . The qualitative impact of our proposed IFSR method is shown in <ref type="figure">Fig. 5</ref>. Without IFSR, the pasted face effect and lack of expression preservation become more apparent. Note that the layers and information from IFSR come from a frozen identity encoder. Therefore, any pretrained face swap framework could here be employed to calculate the IFSR margins. IFSR itself does not contain any learnable parameter. The process is just needed to gain an interpretable insight of what kind of information the layers contain (expression, pose, color, lightning, identity, etc.), and how to define the margins for IFSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce FaceDancer as a new singlestage face swapping model that quantitatively reaches stateof-the-art. FaceDancer has a novel regularization component IFSR which utilizes intermediate features to preserve attributes such as pose, facial expression, and occlusion. Furthermore, the AFFA module in FaceDancer drastically improves identity transfer without a significant trade off for visual quality and attribute preservation when coupled with IFSR. FaceDancer is limited in two main aspects, transferring face shape and the need of calculating IFSR margins from a pretrained face swap model. Future directions for the latter is to figure out how to calculate the margins adaptively online. IFSR can potentially be used to compress complex face swap (or even image translation) models. Trying to combine IFSR with 3DMM to gain strong pose, occlusion and face shape preservation would be interesting future work.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our proposed single-stage face swapping network FaceDancer. Left: The information flow in FaceDancer during training. Black lines indicate standard information flow, while red lines are the cycle consistency loss information flow and dashed lines represent inputs for losses (Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparing our model FaceDancer with SimSwap</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison on low resolution images. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Cosine similarity between intermediate features between changed and target faces (c2t), changed and source faces (c2s), and different identities (Negative Samples). (a) Distances between features from first block of ArcFace. (b) Distances between features from final block of ArcFace. (c) Equal error rates (EER) between the distance distributions for intermediate features in every block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between example attention maps at different resolutions for different configurations in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Further results from FaceDancer and comparison with SimSwap. The left column display images with challenging occlusion or lightning. The right column display images with challenging facial poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Further results in extreme and difficult cases and comparisons with SimSwap. The left column display face swaps with challenging images, such as extreme occlusions and expressions. The right column display failure cases. Most failure cases occur when the face is posing away from the camera, but can also occur in rare lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Overview of baseline 1 and baseline 2 of the FaceDancer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Overview of configuration A, B (the same structure) and C of the FaceDancer. Note that the difference between the Configs A and B is the IFSR module, which is not included in the Config A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Overview of configuration E of the FaceDancer and block details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablative analysis together with the runtime performance. Inference time is given in millisecond and memory usage in GB. All models in this table were trained for 300k iterations. Concatenation instead of AFFA at resolution 256 + one extra AFFA modules at resolution 32. See supplementary materials for detailed figures for each configuration.</figDesc><table><row><cell cols="4">Config IFSR AFFA Concat final skip* 6 skips Mapping</cell><cell>ID?</cell><cell cols="4">Pose? Exp? FID? Inference Memory</cell></row><row><cell>Baseline 1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.66</cell><cell>1.97</cell><cell>8.20 16.72</cell><cell>74.9</cell><cell>1.25</cell></row><row><cell>Baseline 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.61</cell><cell>1.87</cell><cell>7.97 13.51</cell><cell>70.2</cell><cell>1.25</cell></row><row><cell>A</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.14</cell><cell>3.61</cell><cell>9.82 31.63</cell><cell>75.8</cell><cell>1.18</cell></row><row><cell>B</cell><cell></cell><cell>-</cell><cell>-</cell><cell>96.96</cell><cell>2.48</cell><cell>8.25 23.11</cell><cell>75.8</cell><cell>1.18</cell></row><row><cell>C</cell><cell></cell><cell></cell><cell>-</cell><cell>98.57</cell><cell>2.27</cell><cell>7.98 14.59</cell><cell>78.3</cell><cell>1.26</cell></row><row><cell>D</cell><cell></cell><cell></cell><cell></cell><cell>97.53</cell><cell>2.04</cell><cell>7.76 13.50</cell><cell>78.2</cell><cell>1.27</cell></row><row><cell>E</cell><cell></cell><cell></cell><cell>-</cell><cell>97.38</cell><cell>2.07</cell><cell>5.73 14.68</cell><cell>64.6</cell><cell>1.21</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Ablative analysis using AFLW2000-3D [44] as tar-</cell></row><row><cell cols="4">get and FaceForensics++ [34] as source. See Table 2 for</cell></row><row><cell cols="2">configuration details.</cell><cell></cell><cell></cell></row><row><cell>Config</cell><cell>ID?</cell><cell cols="2">Pose? Exp? FID?</cell></row><row><cell>Baseline 1</cell><cell>89.10</cell><cell>5.63</cell><cell>5.34 19.26</cell></row><row><cell>Baseline 2</cell><cell>94.95</cell><cell>6.23</cell><cell>5.60 21.30</cell></row><row><cell>A</cell><cell cols="3">98.50 14.97 7.07 40.34</cell></row><row><cell>B</cell><cell>97.95</cell><cell>5.86</cell><cell>5.74 21.50</cell></row><row><cell>C</cell><cell>97.65</cell><cell>5.82</cell><cell>4.13 18.50</cell></row><row><cell>D</cell><cell>97.10</cell><cell>5.75</cell><cell>4.15 20.41</cell></row><row><cell>E</cell><cell>95.45</cell><cell>6.16</cell><cell>4.19 18.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://drive.google.com/drive/folders/1hHjK0W-Oo1HD6OZb97IdSifPs4_c6NNo?usp=sharing</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this section we show further results, including edge cases and failure cases. You can also find video results here 2 or at the github page. You will also find network structures of the remaining configurations that was tested in the ablations study below. <ref type="figure">Figure 8</ref>: Face swap matrix results from FaceDancer. When source and target is the same, the result visually appears to be perfect reconstruction.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faceswap</surname></persName>
		</author>
		<idno>2022-02-18</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Creating a photoreal digital actor: The digital emily project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lambeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 Conference for Visual Media Production</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards open-set identity preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6713" to="6722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exchanging faces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="669" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE international conference on automatic face &amp; gesture recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SimSwap: An Efficient Framework For High Fidelity Face Swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renwang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2003" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multilevel face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4685" to="4694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d morphable face models-past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romdhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">and Bill Christmas. Resolution-aware 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Ho</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
		<respStmt>
			<orgName>University of Surrey</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advancing high fidelity identity swapping for forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5074" to="5083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tran</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparing facial expressions for face swapping evaluation with supervised contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rosberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristofer</forename><surname>Englund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faceforen-sics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A compact embedding for facial expression similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5683" to="5692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hififace: 3d shape and semantic prior guided high fidelity face swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<editor>Zhi-Hua Zhou</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main Track</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facecontroller: Controllable attribute editing for face in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3083" to="3091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One shot face swapping on megapixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="4834" to="4844" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
