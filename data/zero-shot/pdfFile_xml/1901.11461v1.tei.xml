<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
						</author>
						<title level="a" type="main">GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mesh models are a promising approach for encoding the structure of 3D objects. Current mesh reconstruction systems predict uniformly distributed vertex locations of a predetermined graph through a series of graph convolutions, leading to compromises with respect to performance or resolution. In this paper, we argue that the graph representation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we propose a system which properly benefits from the advantages of the geometric structure of graphencoded objects by introducing (1) a graph convolutional update preserving vertex information;</p><p>(2) an adaptive splitting heuristic allowing detail to emerge; and (3) a training objective operating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our proposed method is evaluated on the task of 3D object reconstruction from images with the ShapeNet dataset, where we demonstrate state of the art performance, both visually and numerically, while having far smaller space requirements by generating adaptive meshes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Surfaces in our physical world exhibit highly non-uniform curvature; compare a plane's wing to a microscopic screw that fastens its engine. Traditionally, deep 3D understanding systems have relied upon representations such as voxels and point clouds, which capture structure with either uniform volumetric or surface detail <ref type="bibr" target="#b5">(Choy et al., 2016;</ref><ref type="bibr" target="#b55">Wu et al., 2016;</ref><ref type="bibr" target="#b12">Fan et al., 2017)</ref>. By representing unimportant, or uninteresting, regions with high detail, these systems scale poorly to higher resolutions and object complexity (see <ref type="bibr">Figure 1a and 1b</ref>  issue through intermediary representations <ref type="bibr" target="#b51">(Tatarchenko et al., 2017;</ref><ref type="bibr" target="#b19">H?ne et al., 2017;</ref><ref type="bibr" target="#b48">Smith et al., 2018;</ref><ref type="bibr" target="#b58">Zhang et al., 2018)</ref>, these either continue to rely on sparse volumetric units, or maintain uniform detail through alternative representations.</p><p>A triangle mesh is a graph-based shape representation that encodes 3D structure through a set of vertices and corresponding planar faces. Recently, advances in deep learning on graphs have enabled mesh-based 3D shape methods <ref type="bibr" target="#b32">(Kato et al., 2017;</ref><ref type="bibr" target="#b54">Wang et al., 2018;</ref><ref type="bibr" target="#b30">Kanazawa et al., 2018;</ref><ref type="bibr" target="#b27">Jack et al., 2018;</ref><ref type="bibr" target="#b23">Henderson &amp; Ferrari, 2018;</ref><ref type="bibr" target="#b17">Groueix et al., 2018b)</ref>. However, these approaches produce mesh predictions which uniformly space vertices and faces over their surface, providing no significant improvement over the previously highlighted representations (see <ref type="figure" target="#fig_0">Figure 1c</ref>) and hence, not exploiting the mesh representation advantages. In particular, by placing many vertices in regions of fine detail while using large triangles to summarize nearly planar regions, one could define adaptive meshes (see <ref type="figure" target="#fig_0">Figure  1d</ref>), enabling flexible scaling by effectively localizing complexity in object surfaces, and allowing for the 3D structure of complicated shapes to be encoded with smaller space requirements and higher precision.</p><p>arXiv:1901.11461v1 [cs.CV] 31 Jan 2019</p><p>Moreover, these deep learning mesh-based approaches rely on Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b35">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b9">Defferrard et al., 2016;</ref><ref type="bibr" target="#b8">Cucurull et al., 2018;</ref><ref type="bibr" target="#b54">Wang et al., 2018)</ref>. Although effective in many node/graph classification and regression tasks, we argue that GCNs may be inadequate for understanding, reconstructing or generating 3D structure as they may induce over-smoothing while aggregating neighboring information at vertex level . This aggregation bias could in turn lead to a harder learning problem when vital information held at each vertex cannot be derived from its neighbors, and as a direct consequence must not be lost.</p><p>Last, an important question when reconstructing 3D objects is how to define a loss between a prediction and its target. A common approach is to employ the Chamfer Distance over some parametrization of the two surfaces <ref type="bibr" target="#b0">(Barrow et al., 1977;</ref><ref type="bibr" target="#b25">Insafutdinov &amp; Dosovitskiy, 2018;</ref><ref type="bibr" target="#b54">Wang et al., 2018;</ref><ref type="bibr" target="#b16">Groueix et al., 2018a;</ref><ref type="bibr" target="#b50">Sun et al., 2018;</ref><ref type="bibr" target="#b12">Fan et al., 2017)</ref>. However, this loss penalizes the point positions exclusively, and thus, its direct application to mesh vertex positions leads poor accuracy, as no information of the faces they define is provided and the placement of vertices over a surface is, to a large degree, arbitrary. In addition, this local loss function takes no consideration of the global structure of the predicted object, preventing class-specific attributes to emerge and creating global inconsistencies.</p><p>Therefore in this paper, we aim to address the abovementioned limitations by introducing an adaptive mesh reconstruction system, called Geometrically Exploited Object Metrics (GEOMetrics), which properly capitalizes on the advantages and geometric structure of graph-encoded objects. GEOMetrics reformulates graph convolutional layers to prevent vertex smoothing. Moreover, it incorporates an adaptive face splitting heuristic allowing non-uniform detail to emerge. Finally, it introduces a training objective operating both on the local surfaces defined by vertices, via a differentiable sampling procedure, as well as the global structure defined by the graph, through a perceptual loss reminiscent of that of style transfer applications <ref type="bibr" target="#b14">(Gatys et al., 2016;</ref><ref type="bibr" target="#b29">Johnson et al., 2016)</ref>. To the best of our knowledge, our system is the first deep approach to describing shape as an adaptive mesh, through advances in geometrically-aware graph operations. We extensively evaluate our system on the task of 3D object reconstruction from single RGB images and show that the interplay of our introduced components encourages mesh reconstructions, which properly localize detail, while maintaining structural consistency. As a result, we are able to obtain mesh predictions which outperform previous methods and have far smaller space requirements.</p><p>The contributions of this paper can be summarized as:</p><p>? We introduce the Zero-Neighbor GCN (0N-GCN), an extension of <ref type="bibr" target="#b35">Kipf &amp; Welling (2016)</ref>, which allows the information at each vertex to be maintained, and as a result better suits the understanding and reconstruction of 3D meshes. ? We present an adaptive face splitting procedure to encourage local complexity to emerge when reconstructing meshes, taking advantage of the mesh flexible scaling (see <ref type="figure" target="#fig_0">Figure 1d</ref>). ? We propose a training objective, which operates locally and globally over the surface to produce mesh reconstructions, which are highly accurate and benefit from the graceful scaling of mesh representations. ? We highlight through extensive evaluation the substantial benefits provided by the previous contributions and show, on the task of 3D object reconstruction from single RGB images, that by properly exploiting the meshs' properties and geometry, our GEOMetrics system is able to notably outperform prior methods visually and quantitatively, while requiring far less vertices/faces.</p><p>Note that the above-mentioned contributions are not specific to the reconstruction system nor the chosen task and thus, can be easily adapted to arbitrary mesh problems. Code for our system is publicly available on a GitHub repository, to ensure reproducible experimental comparison. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Mesh Reconstruction. Mesh models have only recently been used in generation and reconstruction tasks due to the challenging nature of their complex definition <ref type="bibr" target="#b54">(Wang et al., 2018)</ref>. Recent mesh approaches rely on graph representations of meshes, and use GCNs <ref type="bibr" target="#b35">(Kipf &amp; Welling, 2016)</ref> to effectively process them. Our work most closely relates to Neural 3D Mesh Renderer <ref type="bibr" target="#b32">(Kato et al., 2017)</ref> and Pixel2Mesh <ref type="bibr" target="#b54">(Wang et al., 2018)</ref>, which use deformations of a generic pre-defined input mesh, generally a sphere, to form 3D structures. Similarly, Atlas-Net <ref type="bibr" target="#b16">(Groueix et al., 2018a)</ref> uses deformations over a set of primitive square faces to form 3D shapes. Conceptually similar, there exists numerous papers using class-specific input meshes which are deformed with respect to the given input image <ref type="bibr" target="#b42">(Pontes et al., 2017;</ref><ref type="bibr" target="#b30">Kanazawa et al., 2018;</ref><ref type="bibr" target="#b27">Jack et al., 2018;</ref><ref type="bibr" target="#b23">Henderson &amp; Ferrari, 2018;</ref><ref type="bibr" target="#b17">Groueix et al., 2018b;</ref><ref type="bibr" target="#b31">Kar et al., 2015)</ref>. While effective, these approaches require prior knowledge on the target class or access to a model repository.</p><p>Graph Convolutional Networks. The great success of convolutional neural networks in numerous image-based tasks <ref type="bibr" target="#b20">(He et al., 2016;</ref><ref type="bibr" target="#b24">Huang et al., 2017;</ref><ref type="bibr" target="#b28">J?gou et al., 2017;</ref><ref type="bibr" target="#b3">Casanova et al., 2018)</ref> has led to increasing efforts to extend deep networks to domains where graph-structured data is ubiquitous.</p><p>Early attempts to extend neural networks to deal with arbi-1 https://github.com/EdwardSmith1884/GEOMetrics trarily structured graphs relied on recursive neural networks <ref type="bibr" target="#b13">(Frasconi et al., 1998;</ref><ref type="bibr" target="#b15">Gori et al., 2005;</ref><ref type="bibr" target="#b46">Scarselli et al., 2009)</ref>. Recently, spectral approaches have emerged as an effective alternative which formulates the convolution as an operation on the spectrum of the graph <ref type="bibr" target="#b22">(Henaff et al., 2015;</ref><ref type="bibr" target="#b2">Bruna et al., 2014;</ref><ref type="bibr" target="#b1">Bronstein et al., 2017;</ref><ref type="bibr" target="#b36">Levie et al., 2017)</ref>. Methods operating directly on the graph domain have also been presented. <ref type="bibr" target="#b9">Defferrard et al. (2016)</ref> proposed to approximate the filters using the Chebyshev polynomials applied on the Laplacian operator. This approximation was further simplified by <ref type="bibr" target="#b35">Kipf &amp; Welling (2016)</ref>. Finally, several works have been introduced exploring well-established deep learning ideas and improving previously reported results <ref type="bibr" target="#b10">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b18">Hamilton et al., 2017;</ref><ref type="bibr" target="#b38">Monti et al., 2017;</ref><ref type="bibr" target="#b53">Veli?kovi? et al., 2018)</ref>.</p><p>3D Object Representation. Deep learning approaches for understanding 3D shapes have, for a long time, employed voxels as a default 3D object representation <ref type="bibr" target="#b5">(Choy et al., 2016;</ref><ref type="bibr" target="#b55">Wu et al., 2016;</ref><ref type="bibr" target="#b49">Smith &amp; Meger, 2017;</ref><ref type="bibr" target="#b52">Tulsiani et al., 2017;</ref><ref type="bibr" target="#b56">Wu et al., 2017;</ref>. While straightforward to use, voxels induce a cubic computational cost, scaling poorly to higher resolutions and complex objects. Numerous computationally efficient approaches have arisen, such as octree methods <ref type="bibr" target="#b45">(Riegler et al., 2017;</ref><ref type="bibr" target="#b51">Tatarchenko et al., 2017;</ref><ref type="bibr" target="#b19">H?ne et al., 2017)</ref>, which represent voxel objects with adaptive degrees of detail. Most similar to mesh models are point clouds methods, which represent 3D objects through a set of points in 3D space <ref type="bibr" target="#b12">(Fan et al., 2017;</ref><ref type="bibr" target="#b43">Qi et al., 2017;</ref><ref type="bibr" target="#b25">Insafutdinov &amp; Dosovitskiy, 2018;</ref><ref type="bibr" target="#b40">Novotny et al., 2017)</ref>. Point clouds represent only the surface information of 3D objects, making them more efficient and scalable. However, as they do not define surface information beyond each point's local neighborhood they must be uniformly sampled over a surface, and so to encode high levels of detail, the sampling density over the entire surface must increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In this section, we review GCNs <ref type="bibr" target="#b35">(Kipf &amp; Welling, 2016)</ref>, a key component for mesh generation and reconstruction systems, and outline how the Chamfer Distance has been previously employed as a loss for mesh reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Convolutional Networks</head><p>Let G be a graph with N vertices defined by an adjacency matrix A ? R N ?N and a vertex feature matrix H ? R N ?F , where F denotes the number of features. A GCN layer takes as input both A and H, and produces a new vertex feature matrix H ? R N ?F with F features as follows:</p><formula xml:id="formula_0">H = ?(AHW + b),<label>(1)</label></formula><p>where ? is an arbitrary activation function, and W ? R F ?F and b ? R F are the learnable weight matrix and bias vector, respectively. By stacking multiple such layers, information is exchanged throughout the graph such that, after k layers, the information at a given vertex will, for the first time, reach it's k th depth neighbor. Alternatively, k th depth information can be immediately reached using the k th power of adjacency matrix in Eq. 1 <ref type="bibr" target="#b9">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b36">Levie et al., 2017;</ref><ref type="bibr" target="#b8">Cucurull et al., 2018)</ref>. Note that, as discussed in Section 1, GCN layers equate any given vertex to a summary of its neighbourhood, and their repeated application may over-smooth important local information .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Chamfer Loss: Vertex-To-Point Loss</head><p>The Chamfer Distance between predicted and ground truth objects has become a standard metric for 3D reconstruction <ref type="bibr" target="#b54">(Wang et al., 2018;</ref><ref type="bibr" target="#b25">Insafutdinov &amp; Dosovitskiy, 2018;</ref><ref type="bibr" target="#b16">Groueix et al., 2018a;</ref><ref type="bibr" target="#b50">Sun et al., 2018;</ref><ref type="bibr" target="#b12">Fan et al., 2017)</ref>. This loss is defined as:</p><formula xml:id="formula_1">L Chamfer = p?S min q?? p ? q 2 2 + q?? min p?S p ? q 2 2<label>(2)</label></formula><p>and is computed between two sets of points,? and S, sampled from the predicted surface and the ground truth surface. As discussed in Section 1, this metric performs poorly when directly applied to two sets of mesh vertices as it does not take into account the faces which they define, and because of the difficult learning problem associated with matching highly arbitrary vertex placement on a surface. To avoid these issues, <ref type="bibr" target="#b54">Wang et al. (2018)</ref> define? as a large set of predicted vertex positions, and S as a large, pre-computed set of points sampled from the ground truth surface (see <ref type="figure" target="#fig_2">Figure 3a</ref>). Defining the ground truth set as a dense uniform sampling over the target surface avoids issues with inconsistent vertex positions across similar objects. However, this leads to predictions with a high number of vertices packed tightly over the full surface. In this way, the mesh predictions resemble a point cloud, and thus fail to take advantage of the graceful scaling properties of their representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GEOMetrics Mesh Reconstruction</head><p>In this section, we describe our pipeline for reconstructing adaptive meshes from single images and outline our proposed 0N-GCN as well as our suggested adaptive face splitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Mesh Reconstruction from Images Pipeline</head><p>Figure 2 depicts our mesh reconstruction module, which takes as input a mesh model, defined by a set of vertex positions and an adjacency matrix, together with an RGB image depicting an object view and outputs a new mesh prediction. The module is composed of three distinct phases: feature extraction, mesh deformation and face splitting, which are cascaded m = 3 times to obtain incrementally refined mesh predictions. Note that the initial module takes as input a predefined mesh model (e.g. a sphere), whereas each subsequent module is fed the preceding module's prediction.</p><p>In this manner, the initial mesh is iteratively deformed and updated to match the input image.</p><p>Our feature extraction is based on the method proposed by <ref type="bibr" target="#b54">Wang et al. (2018)</ref>, where the input image is passed through a deep CNN and the features from 4 intermediary layers are outputted. The feature vector for each vertex of the mesh is then defined by projecting the vertices of the input mesh onto the CNN outputs and extracting their corresponding features. In addition, each vertex feature vector is provided its 3D coordinates x, y, z and also, if available, the final feature vector it possessed in the preceding reconstruction module. Our mesh deformation consists of a graph convolutional model, which takes as input a mesh and deforms it by making a residual prediction for the position of each vertex. The residual prediction is then added to the original position to complete the deformation. The graph convolutional model of this mesh deformation phase is made up of a series of the proposed 0N-GCN layers (see Subsection 4.2 for details). Finally, our face splitting phase, described in Subsection 4.3, encourages local complexity to emerge in regions that require additional detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero-Neighbor Graph Convolutional Networks</head><p>As described in Subsection 3.1, a potential shortcoming of the standard GCN formulation is that a vertex has no capacity to maintain and directly draw conclusions upon its own information, as this information is smoothed with outside influence at each layer. This outside influence, while useful in global graph understanding contexts, may be detrimental when vital information held at each vertex cannot be derived from its neighbors. This situation is exemplified by meshes, where, if optimally defined, every vertex defines some new surface structure (see <ref type="figure" target="#fig_0">Figure 1d</ref>). To rectify this problem, we define a Zero-Neighbor update, in which a fraction of a vertex's feature vector are not updated with the neighbors' information. This is accomplished by, instead of applying higher powers of the adjacency matrix to reach further depths, taking the adjacency matrix to the power 0 (equivalent to the identity matrix) to exchange with no further depths:</p><formula xml:id="formula_2">H = HW, H = ?([AH 0:i A 0 H i: ] + b),<label>(3)</label></formula><p>where [?||?] denotes concatenation between vectors and i is a feature index. This 0N-GCN provides a soft middle ground between full exchange of information and no vertex communication, where the network can choose how heavily a portion of the features of a given vertex will be influenced by the rest of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive Face Splitting</head><p>In the final step of each reconstruction module, the mesh's set of vertices is redefined over its surface, by adding vertices in regions of high detail. To do so, we introduce a face splitting method, which adaptively increases the set of vertices and the connections between them by analyzing the local curvature of the surface at each mesh face. The curvature at each face is computed by taking the average of the angle between a face's normal and its neighboring faces' normals. For a given face f , made up of vertices v 1 , v 2 , v 3 , its face normal N f is calculated as:</p><formula xml:id="formula_3">N f = e 1 ? e 2 e 1 ? e 2 ,<label>(4)</label></formula><p>where e 1 = v 1 ? v 2 and e 2 = v 3 ? v 2 . The curvature C f at face f is then computed as:</p><formula xml:id="formula_4">C f = 180 |H f |? i?H f arccos (N f ? N i ),<label>(5)</label></formula><p>where H f is the set of neighboring faces of f . All faces with curvature over a given threshold, ?, are then selected to be updated. A selected face is updated by adding a new vertex to its center and connecting it to its 3 original vertices, creating 3 new faces. As the new vertex positions are defined by the positions of already existing vertices, the gradients from each vertex are easily defined to flow back through all previous modules. In this way, each reconstruction module is able to identify areas of the current mesh which require increased detail and prescribe them a higher vertex density. This allows the mesh to fully take advantage of the scaling properties of its representation, by concentrating the generation process in areas of high detail.</p><formula xml:id="formula_5">(a) Vertex-to-point (b) Point-to-point (c) Point-to-surface</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GEOMetrics Losses</head><p>In this section, we describe the key contributions made to the mesh prediction task when considering 3D geometry. In particular, we introduce a training objective, considering the local topology and the global structure to produce mesh predictions that properly benefit from the graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Differentiable Surface Sampling Losses</head><p>We introduce a differentiable sampling procedure which enables us to penalize vertices by the surface they implicitly define, rather then their explicit position. This approach allows predicted meshes to match the target surface without emulating the target vertex positions, which are entirely arbitrary when randomly sampled from the ground truth, while also optimally positioning their vertices and faces.</p><p>To do so, we define a discrete probability distribution based on the relative area of each face and sample n times from this distribution to determine the number of points to sample per face. Then, we sample the previously chosen number of points uniformly over each corresponding surface.More precisely, given a triangular face defined by vertices v 1 , v 2 , v 3 , following <ref type="bibr" target="#b41">Osada et al. (2002)</ref>, a point r can be sampled uniformly from the surface of the triangle as:</p><formula xml:id="formula_6">r = (1 ? ? u)v 1 + ? u(1 ? w)v 2 + ? uwv 3 ,<label>(6)</label></formula><p>where u, w ? U (0, 1). This formulation allows us to differentiate through the random selection via the reparametriza- tion trick <ref type="bibr" target="#b34">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b44">Rezende et al., 2014)</ref>, as the sampling procedure is defined by a deterministic transformation on the vertex coordinates and the independent stochastic terms.</p><p>We apply this sampling procedure to both the predicted mesh and the ground truth and define an alternative Chamfer loss, which operates over the previously sampled points, rather than the predicted vertices (see <ref type="figure" target="#fig_2">Figure 3b</ref>):</p><formula xml:id="formula_7">L PtP = p?S min q?? p ? q 2 2 + q?? min p?S p ? q 2 2 ,<label>(7)</label></formula><p>where? and S are the sampled points of the predicted mesh and the ground truth, respectively. An algorithmic description of the entire process is provided in the supplementary material. Note this loss differs from the vertex-to-point loss in that it properly penalizes the surface of the predicted mesh instead of the predicted vertex's positions.</p><p>Building on these ideas, we define an improved loss term to more accurately compare the surfaces of two meshes:</p><formula xml:id="formula_8">L PtS = p?S min f ?M dist(p,f ) + q?? min f ?M dist(q, f ),<label>(8)</label></formula><p>whereM and M are the predicted and ground truth meshes, f and f the faces,? and S the set of points sampled from the surfaces ofM and M , and dist is a function computing the distance between a point and a triangular face 2 . This loss is shown in <ref type="figure" target="#fig_2">Figure 3c</ref>, and an algorithmic description can be found in the supplementary material. Note that this loss provides an exact measure of the distance between a point and a mesh surface. This is in contrast to the Chamfer loss in Eq. 2 and the point-to-point loss in Eq. 8, which are faster to compute, but can drastically lose accuracy if too few points are sampled on either surface. A quantitative analysis of the improvement from these losses is provided in the supplementary material through a toy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Global Encoding of Graphs</head><p>In order to consider the global structure of an object during the reconstruction process, we introduce a global mesh loss. This loss relies on features extracted from a pre-trained mesh-to-voxel model, which is designed as an encoderdecoder network. The mesh-to-voxel encoder takes as input a mesh graph and produces a latent embedding, from which the 3D object is reconstructed, through the decoder, in a voxelized format. In this manner, objects with structural similarity in voxel space, will have similar latent representations, without requiring similar placement of vertices. The proposed global mesh loss is then defined as</p><formula xml:id="formula_9">L Latent = ||E(M ) ? E(M )|| 2 2<label>(9)</label></formula><p>where E corresponds to the encoder function of the meshto-voxel network. This process is depicted in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The encoder network of the mesh-to-voxel model is built by stacking 0N-GCN layers, followed by a max pooling operation applied to the set of vertices as in <ref type="bibr" target="#b6">Ciregan et al. (2012)</ref> to produce a single fixed length latent representation. The decoder is a 3D deconvolutional network <ref type="bibr" target="#b5">(Choy et al., 2016)</ref>, in following with the network defined in <ref type="bibr" target="#b49">Smith &amp; Meger (2017)</ref>, to perform image to voxel mappings. The complete mesh-to-voxel network is pre-trained by minimizing the mean squared error on the voxelized representations prior to being used in the GEOMetrics system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Optimization Details</head><p>Finally, we present the complete training objective for our mesh reconstruction system. This function combines our differential surface sampling losses, our global structure loss, along with two regularization techniques defined in <ref type="bibr" target="#b54">Wang et al. (2018)</ref>: an edge length minimizing regularizer L Edge and a Laplacian-maintaining regularizer L ?? , pushing the predicted mesh to be smooth and visually appealing. The final loss function of our system is defined as:</p><formula xml:id="formula_10">L = ? 1 L Latent + ? 2 L PtS + ? 3 L Edge + ? 4 L ?? ,<label>(10)</label></formula><p>where ? i are hyper-parameters weighting the importance of each term. During the initial stages of training we approximate the L PtS term using the defined L PtP loss function for faster computation. Note that the loss L is applied to the output of each mesh reconstruction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we demonstrate our algorithm's ability to reconstruct the surface information of 3D objects from single RGB images by taking advantage of the benefits of the mesh representation. We evaluate on this task across 13 classes of the ShapeNet <ref type="bibr" target="#b4">(Chang et al., 2015)</ref> dataset. In addition, we present an ablation study to demonstrate how our algorithm's individual components contribute to its overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Dataset</head><p>The dataset consists of mesh models, voxel models, and RGB images computed from 13 large classes of CAD models found in the ShapeNet dataset <ref type="bibr" target="#b4">(Chang et al., 2015)</ref>. Mesh models were computed from the CADs by removing all texture information and downscaling their size so that each model possesses less then 2000 vertices, where possible. From these mesh models, voxelized counter parts were produced at 32 3 resolution. From each CAD model, 24 RGB images were produced, from random viewpoints, with the camera projection matrix recorded for use in the feature selection method. The data in each class was then split into a training, validation and test set with a ratio of 70:10:20, respectively. This matches the dataset used for empirical evaluation by <ref type="bibr" target="#b54">Wang et al. (2018)</ref> and <ref type="bibr" target="#b48">Smith et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Implementation Details</head><p>Mesh-to-Voxel Mapping For each class, we train a meshto-voxel mapping from the mesh and voxel ground truths, for use in our latent loss. These mappings are trained with Adam optimizer <ref type="bibr" target="#b33">(Kingma &amp; Ba, 2014)</ref> (? 1 = 0.9, ? 2 = 0.999), a learning rate of 10 ?4 , and a mini-batch size of 16. We train for 10 5 iterations and practice early stopping, with the best model selected from evaluating on the validation set every 100 iterations.</p><p>GEOMetrics We train the full system on each class in our dataset with Adam optimizer <ref type="bibr" target="#b33">(Kingma &amp; Ba, 2014)</ref>, at learning rate of 10 ?4 for 300k iterations, and then again for 150k iterations at a learning rate of 10 ?5 , with minibatch size of 5. We practice early stopping by evaluating on the validation set every 50 iterations. The hyper-parameter settings used, as described in Eq. (10), are ? 1 = .001, ? 2 = 1, ? 3 = 0.3, and ? 4 = 1. As mentioned above, L P tP is employed as a faster approximation to the L P tS loss, specifically for the first 300k iteration. This is because L P tS is slow to compute and we found it sufficient to only use it to finetune pre-trained models. All hyper-parameters were initially tuned on the validation set of the chair class. The generic pre-defined mesh fed to the first reconstruction module is an ellipsoid. A face is split at the end of each module only if the curvature at that face is greater than 70?. Architecture details for all networks is provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Single Image Reconstruction</head><p>We evaluate our method's performance quantitatively by comparing its ability to reconstruct mesh surfaces from sin-gle RGB image to an array of high performing 3D object reconstruction algorithms. To do this, we sample points from both the surface of the predicted object and the groundtruth object and compute the F1 score. In following with <ref type="bibr" target="#b54">(Wang et al., 2018)</ref>, precision and recall are calculated using the percentage of sampled points which exists within a threshold of 0.0001 any sampled point in the compared surface. State of the art results of mesh approaches, N3MR <ref type="bibr" target="#b32">(Kato et al., 2017)</ref> and Pixel2Mesh <ref type="bibr" target="#b54">(Wang et al., 2018)</ref>, a point cloud method, PSG <ref type="bibr" target="#b12">(Fan et al., 2017)</ref> and a voxel baseline, 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref>, are reported from <ref type="bibr" target="#b54">Wang et al. (2018)</ref>. We also compare mesh-based approaches in terms of space requirements (number of vertices). The results of this comparison are summarized in <ref type="table" target="#tab_1">Table 1</ref>. As shown, our GEOMetrics system boasts far higher performance than previous approaches, with an average increase in F1 score of 7.65 points across all classes, and improved score in all classes but one, where we experience a negligible drop of 0.87 points. In addition, in all cases, our system requires notably less vertices than the previous mesh-based state of the art Pixel2Mesh, e.g. cellphone objects require 6? less vertices, whereas lamp objects require 3? less vertices. With an average of 574.06 vertices used across all classes, the vertex requirements drop as much as 4.3? on average, highlighting the potential of the adaptive face splitting. Moreover, when compared to point cloud and voxel baselines, we also exhibits state of the art results.</p><p>Qualitative reconstruction results for each of the 13 classes are displayed in <ref type="figure" target="#fig_6">Figure 7</ref> 3 . We boast highly accurate reconstructions of the input object, effectively capturing both global structure and local detail. In addition, we render an un-smoothed chair reconstruction in <ref type="figure" target="#fig_4">Figure 5</ref> with its edges heavily outlined, demonstrating the obtained diverse vertex density across a single object and highlighting the way our system represents simple surfaces with a small number of faces, and shifts to higher density where required. Lastly, <ref type="figure" target="#fig_5">Figure 6</ref> depicts a visual comparison between GEOMetrics and Pixel2Mesh reconstructions, where we can observe how GEOMetrics is able to provide reconstructions with higher detail (e.g. the sharpness of the chair legs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Single Image Reconstruction Ablation Study</head><p>In this subsection, we study the influence of our system's components and demonstrate their individual importance by comparing our full method's results on the chair class to ablated versions of the system. We assess the impact of our 0N-GCN layers by replacing them with standard GCNs <ref type="bibr" target="#b35">(Kipf &amp; Welling, 2016)</ref> in both the mesh reconstruction as well as the mesh-to-voxel models. We validate the effectiveness of the proposed adaptive face splitting by substituting it by a procedure in which all faces are split at the end of each reconstruction module, keeping uniform detail and maintaining approximately the same the number of vertices as the full approach. We then check the importance of each one of the newly introduced losses by removing them at training time. Note that, when the face sampling losses L PtP and L PtS are removed, we replace them by the vertex-to-point loss (L VtP ) proposed by <ref type="bibr" target="#b54">Wang et al. (2018)</ref>. Finally, we compare our method to the Pixel2Mesh, when roughly equivalent in terms of number of vertices.</p><p>The results of this ablation study are reported in <ref type="table" target="#tab_2">Table 2</ref>. As shown in the table, the biggest effect comes from the introduction of the adaptive face splitting, with a drop of 6.23 points when replacing it with a uniform splitting heuristic. Moreover, assisting the model to give more importance to vertex features through the 0N-GCN also appears to be relevant. The losses proposed to train the whole system also play an important role, as ignoring them leads to a decrease in performance of 3.69 points and 1.02 points, respectively. Moreover, training Pixel2Mesh baseline to use as few vertices as GEOMetrics leads to notably worse performance. These results empirically justify the contributions of our GEOMetrics system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented GEOMetrics, a novel approach for adaptive mesh reconstruction, which focuses on exploiting the geometry of the mesh representation. The GEOMetrics system reformulates GCNs to explicitly preserve local vertex information and incorporates an adaptive face splitting procedure to enhance local complexity when necessary. Furthermore, the system is trained by introducing a training objective which operates both locally and globally at mesh level, and capitalizes on the geometric structure of graph-encoded objects. We demonstrated the potential of the approach through extensive evaluation on the challenging task of 3D object reconstruction from single images of the ShapeNet dataset. Finally, we reported visually appealing state of the art results, outperforming existing mesh-based methods by a large margin, while requiring (on average) as many as 4.3? less vertices. Future research directions include addressing the restrictive constant topology prescribed by the initial mesh object through reconstruction and generation methods, which adapt the topology to match the target mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEOMetrics: Supplemental Material</head><p>A. Point to Surface Loss</p><p>In this section, we describe the the Distance Between Point and Triangle in 3D algorithm <ref type="bibr" target="#b11">(Eberly, 1999)</ref>. For a given point P and triangle T , the algorithm computes the minimum distance between the point and any point contained within the triangle. Assuming the triangle is defined by corner point B and directions E 0 and E 1 , then any point T (s, t) contained in the triangle can be defined by a pair of scalars (s, t) such that T (s, t)</p><formula xml:id="formula_11">= B + sE 0 + tE 1 , where (s, t) ? D = {(s, t) : s ? 0, t ? 0, s + t ? 1}.</formula><p>We can now define the squared distance Q between the point P and any point in the triangle T (s, t) by the following quadratic function:</p><p>Q(s, t) = as 2 + 2bst + ct 2 + 2ds + 2et + f,</p><p>where for clarity we denote a</p><formula xml:id="formula_13">= E 0 ? E 0 , b = E 0 ? E 1 , c = E 1 ? E 1 , d = E 0 ? (B ? P ), e = E 1 ? (B ? P ), and f = (B ? P ) ? (B ? P )</formula><p>. Selecting (s, t) which minimizes Q(s, t) provides the minimum distance between the point P and triangle T . As Q is continuously differentiable, (s, t) can be found at an interior point where ?Q = 0 or at the boundary of the set D.</p><p>In the first case, note ?Q(s , t ) = 0 if and only if s and t satisfy the following:</p><formula xml:id="formula_14">s = be ? cd ac ? b 2 , t = bd ? ae ac ? b 2<label>(12)</label></formula><p>Then if (s , t ) ? D, we have the minimum distance.Otherwise, the distance minimizing (s, t) must lie on the boundary of D, where either {s = 0, t ? [0, 1]}, {s ? [0, 1], t = 0}, or {s ? [0, 1], t = 1 ? s}. In each case Q(s, t), can be reduced to quadratic of one unknown variable, which can be minimized by setting the gradient to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mesh-to-Voxel Mapping Ablation</head><p>In this section, we perform an ablation study over the use of 0N-GCN as building block for our Mesh-to-Voxel Mapping network to highlight its impact with respect to the standard GCN layers. To that end, we compare our model on 3 different object classes to an analogous network composed of standard GCN layers with the same number of parameters. Additionally, we assess the influence of pooling across a set of vertices by comparing it to other forms of aggregation such as the one introduced by the Neural Graph Fingerprint (NGF) model <ref type="bibr" target="#b10">(Duvenaud et al., 2015)</ref>. The results of this ablation study can be found in <ref type="table" target="#tab_3">Table 3</ref> in terms of mean squared error (MSE). As shown in the table, results demonstrate the benefits of the 0N-GCN layers, as well as the max-pooling vertex set aggregation, for this mesh understanding task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Differentiable Surface Loss Algorithms</head><p>This section provides the algorithmic details of both the point-to-point loss (Algorithm 1) as well as the point-tosurface loss (Algorithm 2). Algorithm 1 Point-to-Point Loss 1: Input: Two mesh surfaces M andM , and number of points n 2: for face f in mesh M do 3:</p><formula xml:id="formula_15">A f = Area(f )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>A T += Area(f ) 5: end for 6: Define F s.t. P (F = f ) = A f * 100 A T 7: Define U = U nif orm(0, 1) 8: Define S = [] 9: for i = 0 to n do 10:</p><formula xml:id="formula_16">f ? F 11: v 1 , v 2 , v 3 = vertices(f ) 12: u ? U, w ? U 13: r = (1 ? ? u)v 1 + ? u(1 ? w)v 2 + ? uwv 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>S.append(r) 15: end for 16: Apply lines 1 to 15 to meshM to produce? 17:</p><formula xml:id="formula_17">L PtP = p?S min q?? p ? q 2 2 + q?? min p?S p ? q 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Analysis</head><p>In this section, we present further analysis of GEOMetrics losses to emphasize the benefits of the introduced point-topoint and surface-to-point losses over the vertex-to-point loss. To that end, we design a toy problem, which consists in optimizing the placement of the vertices of an initial square surface to match the surface area of a target triangle in 2D.</p><p>Algorithm 2 Point-to-Surface Loss 1: Input: Two mesh surfaces M andM , and number of points n 2: for face f in mesh M do 3:</p><formula xml:id="formula_18">A f = Area(f ) 4: A T += Area(f ) 5: end for 6: Define F s.t. P (F = f ) = A f * 100</formula><p>A T 7: Define U = U nif orm(0, 1) 8: Define S = [] 9: for i = 0 to n do 10:</p><formula xml:id="formula_19">f ? F 11: v 1 , v 2 , v 3 = vertices(f ) 12: u ? U, w ? U 13: r = (1 ? ? u)v 1 + ? u(1 ? w)v 2 + ? uwv 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>S.append(r) 15: end for 16: Apply lines 1 to 15 to meshM to produce? 17: <ref type="figure">Figure 9</ref> (top) depicts the above-mentioned initial and target surfaces. We optimize the placement of the vertices of the initial square by performing gradient descent on each of the losses independently, and calculate the intersection over union (IoU) of the predicted object and the target triangle. Moreover, in order to assess the impact of the number of points sampled, we repeat this experiment 100 times, increasing the number of sampled points from 1 to 100. <ref type="figure">Figure  8</ref> shows the results of this experiment. Firstly, we observe that the vertex-to-point loss fails to match the target surface entirely, no matter the number of sampled points. Secondly, we observe that the point-to-point loss performance is notably affected by the number of sampled points. While it exhibits poor performance for lower number of sampled points (e.g. below 20), it rapidly improves as the number of sampled points increases, and ultimately, converges to an average performance, which is only slightly lower than that of the point-to-surface loss. Finally, the point-to-surface loss begins with a far higher IoU and remains the stronger option for nearly all numbers of sampled points. <ref type="figure">Figure 9</ref> illustrates qualitative results for the three compared losses when optimizing with 50 points sampled. As can be seen, the point-to-surface deformation of the square better matches the target triangle shape, followed by point-to-point, which somewhat emulates the triangle, and vertex-to-point, which exhibits the poorest results. <ref type="figure">Figure 8</ref>. Comparison of vertex-to-point, point-to-point and surface-to-point losses, in terms of IoU, on a toy problem: optimizing the placement of vertices of a square to match that of a target triangle. Results are compared by increasing the number of sampled points on the surfaces they optimize. Vertex-to-point is the loss employed by <ref type="bibr" target="#b54">Wang et al. (2018)</ref>. Point-to-point and point-to-surface are the losses introduced in our paper. <ref type="figure">Figure 9</ref>. Qualitative comparison of vertex-to-point <ref type="bibr" target="#b54">(Wang et al., 2018)</ref>, point-to-point and surface-to-point losses on the square-totriangle problem when using 50 sampled points. We highlight the correspondence between the points in the initial surface and the target surface, which are chosen to be compared, and show the result of optimizing the placement of the vertices when using each of the losses.</p><formula xml:id="formula_20">L PtS = p?S min f ?M dist(p,f ) + q?? min f ?M dist(q, f )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Architectures</head><p>In this section, we provide details on the architectures of the networks used in the paper. <ref type="table">Table 4</ref> describes the feature extractor network of the mesh reconstruction module. Similarly, <ref type="table">Table 5</ref> specifies the mesh deformation network of the reconstruction module. Finally, <ref type="table">Table 6</ref> and 7 detail the mesh-to-voxel encoder and decoder architectures, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Single Image Reconstruction Visualizations</head><p>Figures 10 and 11 depict additional reconstruction results from each ShapeNet object class, with three objects shown per class. <ref type="table">Table 4</ref>. Feature extraction network: Details of the convolutional neural network architecture used to extract image features. Each layer performs a 2D convolutional, followed by batch normalization <ref type="bibr" target="#b26">(Ioffe &amp; Szegedy, 2015)</ref> and a ReLU activation function <ref type="bibr" target="#b39">(Nair &amp; Hinton, 2010)</ref>. The last row indicates which layer's features are extracted for use in the mesh reconstruction module.  <ref type="table">Table 5</ref>. Mesh deformation network: Details of the graph convolutional network architecture used to compute the mesh deformation in each reconstruction module. Each layer is composed of a 0N-GCN, followed by an ELU activation function <ref type="bibr" target="#b7">(Clevert et al., 2015)</ref>.</p><p>Layers 1 2-4 5 6-7 8 9 10 11 12 13-16 17 Input Feature Dimension 3 60 60 120 120 150 200 210 250 300 300 Output Feature Dimension 60 60 120 120 150 200 210 250 300 300 50 <ref type="table">Table 6</ref>. Mesh-to-voxel encoder: Details of the graph convolutional network architecture used to encode mesh graphs into latent vectors. Each layer is composed of a 0N-GCN followed by an ELU activation function <ref type="bibr" target="#b7">(Clevert et al., 2015)</ref>, except for the final layer which is a max pooling aggregation over the set of vertices.  <ref type="table">Table 7</ref>. Mesh-to-voxel decoder: Details of the 3D convolutional neural network archictecture used to decode latent vectors into voxel grids. Each layer performs a 3D deconvolution <ref type="bibr" target="#b47">(Shelhamer et al., 2017)</ref> with batch normalization <ref type="bibr" target="#b26">(Ioffe &amp; Szegedy, 2015)</ref> and an ELU activation function <ref type="bibr" target="#b7">(Clevert et al., 2015)</ref>, except for the final layer which is a standard 3D convolutional layer. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of 3D shape encoding techniques, including their respective encoding sizes for the level of quality viewed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Mesh reconstruction module, with its three main components highlighted. Feature Extraction describes the process through which image features are extracted for each vertex. Mesh Deformation outlines the deformation of the inputted mesh through 0N-GCN layers. Adaptive Face Splitting illustrates how high curvature faces are split to increase local complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A comparison of different surface losses. Vertex-to-point is the technique used by<ref type="bibr" target="#b54">Wang et al. (2018)</ref>. Point-to-point sampling and point-to-surface sampling are the sampling procedures introduced in our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Mesh-to-voxel Mapping. A encoder-decoder architecture is trained to map ground truth meshes to their corresponding voxelizations. The latent representations produced by the encoder from predicted and ground truth meshes are then compared in our global reconstruction loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Renderings from a single reconstructed chair, demonstrating the variety of different local vertex densities which are produced by our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visual comparison between GEOMetrics and Pixel2Mesh (Wang et al., 2018) chair reconstructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results: renderings of meshes reconstructed from each ShapeNet object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Single image reconstruction results on bench, cabinet, car, cellphone, chair, lamp, monitor, plane and rifle classes.GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects Single image reconstruction results on sofa, speaker, table and watercraft classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). While efforts have been made to rectify this 1 Department of Computer Science, McGill University, Montreal, Canada 2 Mila Qu?bec AI Institute 3 Facebook AI Research. Correspondence to: Edward Smith &lt;edward.smith@mail.mcgill.ca&gt;.</figDesc><table><row><cell>(a) Voxels</cell><cell>(b) Point cloud</cell></row><row><cell>(262, 144 units)</cell><cell>(30, 000 points)</cell></row><row><cell>(c) Uniform mesh</cell><cell>(d) Adaptive mesh</cell></row><row><cell>(2416 vertices)</cell><cell>(120 vertices)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results on ShapeNet 3D object reconstruction reported as per class surface sampling F1 scores and mean F1 score.</figDesc><table><row><cell>Category</cell><cell>3D-R2N2</cell><cell>PSG</cell><cell>N3MR</cell><cell>Vertices</cell><cell>Pixel2Mesh</cell><cell cols="2">Vertices Ours Vertices</cell></row><row><cell></cell><cell cols="3">(Choy et al., 2016) (Fan et al., 2017) (Kato et al., 2017)</cell><cell></cell><cell>(Wang et al., 2018)</cell><cell></cell><cell></cell></row><row><cell>Plane</cell><cell>41.46</cell><cell>68.20</cell><cell>62.10</cell><cell>642</cell><cell>71.12</cell><cell>2466</cell><cell>89.00 645.03</cell></row><row><cell>Bench</cell><cell>34.09</cell><cell>49.29</cell><cell>35.84</cell><cell>642</cell><cell>57.57</cell><cell>2466</cell><cell>72.11 514.54</cell></row><row><cell>Cabinet</cell><cell>49.88</cell><cell>39.93</cell><cell>21.04</cell><cell>642</cell><cell>60.39</cell><cell>2466</cell><cell>59.52 556.68</cell></row><row><cell>Car</cell><cell>37.80</cell><cell>50.70</cell><cell>36.66</cell><cell>642</cell><cell>67.86</cell><cell>2466</cell><cell>74.64 509.33</cell></row><row><cell>Chair</cell><cell>40.22</cell><cell>41.60</cell><cell>30.25</cell><cell>642</cell><cell>54.38</cell><cell>2466</cell><cell>56.61 619.13</cell></row><row><cell>Monitor</cell><cell>34.38</cell><cell>40.53</cell><cell>28.77</cell><cell>642</cell><cell>51.39</cell><cell>2466</cell><cell>59.50 449.65</cell></row><row><cell>Lamp</cell><cell>32.35</cell><cell>41.40</cell><cell>27.97</cell><cell>642</cell><cell>48.15</cell><cell>2466</cell><cell>58.65 743.28</cell></row><row><cell>Speaker</cell><cell>45.30</cell><cell>32.61</cell><cell>19.46</cell><cell>642</cell><cell>48.84</cell><cell>2466</cell><cell>49.53 550.06</cell></row><row><cell>Firearm</cell><cell>28.34</cell><cell>69.96</cell><cell>52.22</cell><cell>642</cell><cell>73.20</cell><cell>2466</cell><cell>88.36 638.35</cell></row><row><cell>Couch</cell><cell>40.01</cell><cell>36.59</cell><cell>25.04</cell><cell>642</cell><cell>51.90</cell><cell>2466</cell><cell>59.54 561.79</cell></row><row><cell>Table</cell><cell>43.79</cell><cell>53.44</cell><cell>28.40</cell><cell>642</cell><cell>66.30</cell><cell>2466</cell><cell>66.33 732.82</cell></row><row><cell>Cellphone</cell><cell>42.31</cell><cell>55.95</cell><cell>27.96</cell><cell>642</cell><cell>70.24</cell><cell>2466</cell><cell>73.65 416.05</cell></row><row><cell>Watercraft</cell><cell>37.10</cell><cell>51.28</cell><cell>43.71</cell><cell>642</cell><cell>55.12</cell><cell>2466</cell><cell>68.32 526.04</cell></row><row><cell>Mean</cell><cell>39.01</cell><cell>48.58</cell><cell>33.80</cell><cell>642</cell><cell>59.72</cell><cell>2466</cell><cell>67.37 574.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>GEOMetrics ablation compared to full method (ours) and Pixel2Mesh. Results reported as mean F1 score on the chair class.</figDesc><table><row><cell cols="5">Ours GCN Unif. Split. No L latent L VtP Pixel2Mesh</cell></row><row><cell>56.61 54.57</cell><cell>50.33</cell><cell>55.59</cell><cell>52.92</cell><cell>38.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mesh-to-Voxel Mapping Reconstruction MSE scores.</figDesc><table><row><cell>Category</cell><cell>Ours</cell><cell>GCN</cell><cell>NGF</cell></row><row><cell>Plane</cell><cell cols="3">0.0089 0.0104 0.0108</cell></row><row><cell>table</cell><cell cols="3">0.0310 0.0393 0.0360</cell></row><row><cell>Chair</cell><cell cols="3">0.0412 0.0526 0.0486</cell></row><row><cell>Mean</cell><cell cols="3">0.0270 0.0341 0.0318</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? 4 ? 4 8 ? 8 ? 8 16 ? 16 ? 16 32 ? 32 ? 32 32 ? 32 ? 32</figDesc><table><row><cell>Layers</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">Output Resolution 4 # Channels</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>8</cell><cell>1</cell></row><row><cell>Stride</cell><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>Type</cell><cell cols="2">DeConv</cell><cell>DeConv</cell><cell>DeConv</cell><cell>DeConv</cell><cell>Conv</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Calculated using an optimized adaptation of the Distance Between Point and Triangle in 3D algorithm<ref type="bibr" target="#b11">(Eberly, 1999)</ref>, details provided in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See supplementary material for additional visualizations.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parametric correspondence and chamfer matching: two new techniques for image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international joint conference on Artificial intelligence</title>
		<meeting>the 5th international joint conference on Artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1977" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the iterative refinement of densely connected representation levels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Autonomous Driving, CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for mesh-based parcellation of the cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wagstyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jakobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Imaging with Deep Learning</title>
		<meeting>the International Conference on Medical Imaging with Deep Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>978-1-5108-3881-9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distance between point and triangle in 3d. Geometric Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eberly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.712151</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2005.1555942</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Atlasnet: A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d-coded: 3d correspondences by deep deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="230" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hierarchical surface prediction for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00710</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to generate and reconstruct 3d meshes with only 2d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09259</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning free-form deformations for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10932</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07549</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1966" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07566</idno>
		<title level="m">Neural 3d mesh renderer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Graph convolutional neural networks with complex rational spectral filters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning 3d object categories by looking around them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="807" to="832" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Image2mesh: A learning framework for single image 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10669</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6620" to="6629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-view silhouette and depth decomposition for high resolution 3d object representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved adversarial systems for 3d object generation and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixel2mesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01654</idno>
		<title level="m">Generating 3d mesh models from single rgb images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning shape priors for singleview 3d completion and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="673" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to reconstruct shapes from unseen classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2263" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
