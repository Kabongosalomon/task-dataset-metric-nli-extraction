<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingcan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
							<email>jiali@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent salient object detection (SOD) methods based on deep neural network have achieved remarkable performance. However, most of existing SOD models designed for low-resolution input perform poorly on high-resolution images due to the contradiction between the sampling depth and the receptive field size. Aiming at resolving this contradiction, we propose a novel one-stage framework called Pyramid Grafting Network (PGNet), using transformer and CNN backbone to extract features from different resolution images independently and then graft the features from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine broken detailed information more holistically, guided by different source feature during decoding process. Moreover, we design an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM to help the network better interact with the attention from different models. We contribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD, containing 5,920 images at 4K-8K resolutions. To our knowledge, it is the largest dataset in both quantity and resolution for high-resolution SOD task, which can be used for training and testing in future research. Sufficient experiments on UHRSD and widely-used SOD datasets demonstrate that our method achieves superior performance compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> aims at identifying and segmenting the most attractive objects in a certain scene. As a pre-processing step, it is widely applied in var-* Correspondence should be addressed to Changqun Xia (Email: xi-achq@pcl.ac.cn ). The code and dataset are available at https:// github.com/iCVTEAM/PGNet. ious computer vision tasks, such as light field segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>, instance segmentation <ref type="bibr" target="#b46">[47]</ref> and video object segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Recently, deep neural networks based salient object detection methods have made remarkable achievements <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. However, most of existing SOD methods perform well within a specific input low-resolution range (e.g., 224 ? 224, 384 ? 384 ). With the rapid development of image capture devices (e.g., smartphone), the resolution (e.g., 1080p, 2K and 4K) of the images accessible to people has far exceeded the range to which existing saliency detection method can be adapted directly. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (c), we fed the high-resolution image directly into the commonly used network with Resnet-18 as the backbone, and comparing ground truth <ref type="figure" target="#fig_0">Fig. 1 (b)</ref> shows that the segmentation result is incomplete and many detail regions are lost. In order to reduce computational consumption and memory usage, existing methods often downsample the input images and then upsample the output results to recover original resolution, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>. This challenge is due to the fact that most low-resolution SOD networks are designed in an Encoder-Decoder style, and as the input resolution increases dramatically, the size of features extracted increases, but the receptive field determined by the network is fixed, making the relative receptive field small, ultimately resulting in the inability to capture global semantics that are vital to SOD task. Since direct processing cannot handle the challenges posed by high resolution, a number of methods have emerged in recent years specifically designed for high-resolution input. There are two representative highresolution SOD methods (HRSOD <ref type="bibr" target="#b39">[40]</ref>, DHQSOD <ref type="bibr" target="#b29">[30]</ref>). HRSOD divides the whole process into global stage, local stage and reorganization stage, where the global stage provides guidance on both the local stage and the crop process. And DHQSOD disentangle the SOD task into classification task and regression task, where the two task is connected by their proposed trimap and uncertainty loss. They generate relatively good saliency maps with sharp boundaries. However, both of the above methods use a multi-stage architecture, dividing SOD into semantic(in low resolution) and detailed (in high resolution) phases. This has led to two new problems: (1) Inconsistent contextual semantic transfer between stages. The intermediate maps obtained in the previous stages are input into the last stage, while the errors are also passed on. Further more, the refinement in the last stage will likely inherit or even amplify the previous errors as there is not enough semantic support, which implies that the final saliency maps are heavily dependent on the performance of the low-resolution network. (2) Time consuming. Compared to the one-stage method, the multi-stage method are not only difficult to parallel but also have the potential problem of increasing number of parameters, which makes it slow.</p><p>Based on the above defects of existing high-resolution methods , we propose a new perspective that since the specific features in a single network cannot settle the paradox of receptive field and detail retention simultaneously, instead we can separately extract two sets of features of different spatial sizes and then graft the information from one branch to the other. In this paper, we rethink the dual-branch architecture and design a novel one-stage deep neural network for high-resolution saliency detection named Pyramid Grafting Network (PGNet). As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> (e), we use both Resnet and Transformer as our Encoders, extracting features with dual spatial sizes in parallel. The transformer branch first decode the features in the FPN style, then pass the global semantic information to the Resnet branch in the stage where the feature maps of two branches have similar spatial sizes. We call this process feature grafting. Eventually, the Resnet branch completes the decoding process with the grafted features. Compared to classic FPNs, we have constructed a higher feature pyramid at a lower cost. To better graft features cross two different types of models, we design the Cross-Model Grafting Module (CMGM) based on the attention mechanism and propose the Attention Guided Loss to further guide the grafting. Considering that supervised deep learning method requires a large amount of high quality data, we have provided a 4K resolution SOD dataset (UHRSD) with the largest number to date in an effort to promote future high-resolution salient object detection research.</p><p>Our major contributions can be summarized as follows:</p><p>? We propose the first one-stage framework named PGNet for high-resolution salient object detection, which uses staggered connection to capture both continuous semantics and rich details.</p><p>? We introduce the Cross-Model Grafting Module to transfer the information from transformer branch to CNN branch, which allows CNN to not only inherit global information but also remedy the defects common to both. Moreover, we design the Attention Guided Loss to further promote the feature grafting.</p><p>? We contribute a new challenging Ultra High-Resolution Saliency Detection dataset (UHRSD) containing 5,920 images of various scenes at over 4K resolution and corresponding pixel-wise salient annotations, which is the largest high-resolution saliency dataset available.</p><p>? Experimental results on both existing datasets and ours demonstrate our method outperforms state-of-the-art methods in accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>During the past decades, a large amount traditional methods have been proposed to solve saliency detection problem <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. However, these methods only focus on the low-level feature and ignore the rich semantic information resulting in unstable performance in complex scenarios.More details can be found in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning-Based Saliency Detection</head><p>Recently, remarkable progress has been made in saliency detection due to the application of deep neural network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. Hou et al. <ref type="bibr" target="#b10">[11]</ref> and Chen et al. <ref type="bibr" target="#b3">[4]</ref> use deep convolutional networks as Encoder to extract multilevel features and design various modules to fuse them in an FPN style. Ma et al. <ref type="bibr" target="#b22">[23]</ref> and Xu et al. <ref type="bibr" target="#b36">[37]</ref> avoid semantic dilution while suppressing loss of detail by experimenting with various feature connection paths. In addition, Wei et al. <ref type="bibr" target="#b32">[33]</ref> generate saliency maps with sharp boundary by explicitly supervising edge pixels. The extensive use of transfomer in vision has also led to new advances in saliency detection. Liu et al. <ref type="bibr" target="#b19">[20]</ref> take use of T2Tvit as backbone and design a multi-tasking decoder with a pure transformer architecture to perform RGB and RGB-D saliency detection. However, these methods are designed for low-resolution scenes and cannot be directly applied to high-resolution scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">High-Resolution SOD</head><p>Nowadays, focusing on high-resolution SOD methods is already trending. Zeng et al. <ref type="bibr" target="#b39">[40]</ref> propose a paradigm for high-resolution salient object detection using GSN for extracting semantic information, and APS guided LRN for optimizing local details and finally GLFN for prediction fusion. Also they contributed the first high-resolution salient object detection dataset (HRSOD). Tang et al. <ref type="bibr" target="#b29">[30]</ref> propose that salient object detection should be disentangled into two tasks. They first design LRSCN to capture sufficient semantics at low resolution and generate the trimap. By introducing the uncertainty loss, the designed HRRN can refine the trimap generated in first stage using low-resolution dataset. However, both of them use multi-stage architecture, which has led to slow inference, making it difficult to meet some real-world application scenarios. And a more serious problem is the semantic incoherence between networks. Thus we aim to design a one-stage deep network to get rid of the above defects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UHR Saliency Detection Dataset</head><p>Available SOD datasets. The existing common SOD datasets usually are in low-resolution (below 500 ? 500 ). What's more, they have the following drawbacks for training high-resolution networks and evaluating high-quality segmentation results. Firstly the low resolution of the images results in insufficient detail information. Secondly, the quality of the edges of annotations is poor <ref type="bibr" target="#b39">[40]</ref> .Lastly, the finer level of annotations is dissatisfied, especially for hardcase annotations which are handled perfunctorily as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (f). The only available high-resolution dataset known is HRSOD <ref type="bibr" target="#b39">[40]</ref>. However, the number of highresolution images in HRSOD is limited.</p><p>UHRSD dataset. For supervised learning, training data is obviously important. Before this, the only available highresolution training set was only 1,610 images, and we experimentally discovered that training only on it was easy to overfit its data distribution, which significantly impacted the model's generalization ability. If the low-resolution datasets are mixed together for training, a lot of noise will be introduced to affect the performance of the high-resolution model. To relief the lack of high-resolution datasets for SOD, we contribute the Ultra High-Resolution for Saliency Detection (UHRSD) dataset with a total of 5,920 images in 4K(3840 ? 2160) or higher resolution, including 4,932 images for training and 988 images for testing. A total of 5,920 images were manually selected from websites (e.g. Flickr Pixabay) with free copyright. Our dataset is diverse in terms of image scenes, with a balance of complex and simple salient objects of various size. Multiple participants in the constructing process to ensure accuracy of salient annotations. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the superiority of our UHRSD. As shown in histogram <ref type="figure" target="#fig_1">Fig. 2</ref> (a) (b), UHRSD datset is much larger than HRSOD datset and to the best of our knowledge is the largest dataset available. The large scale considerably alleviates the issues mentioned above when training high-resolution deep neural networks. In addition, the histogram <ref type="figure" target="#fig_1">Fig. 2(b)</ref> shows that the size of images in UHRSD far exceeds that of the existing high-resolution dataset. Not only that, <ref type="figure" target="#fig_1">Fig. 2(a)</ref> shows the number of pixels at the edges of our images also far surpasses the existing high-resolution dataset by a large margin, which means that UHRSD has richer and more challenging edge details. Lastly, through the comparison among <ref type="figure" target="#fig_1">Fig. 2</ref>(c)-(f), it's evident that UHRSD also has a finer level of annotation for the hard cases than both existing high-resolution dataset and low-resolution dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Staggered Grafting Framework</head><p>The architecture of proposed network is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, the network consists of two encoders and a decoder. To better perform the respective tasks of the two encoders, Swin transformer and Resnet-18 are chosen as encoders. This choice of combination was made for the consideration of balancing efficiency and effectiveness. On the one hand, the transformer encoder is able to get accurate global semantic information in the low-resolution case, and the convolutional encoder can get rich detail with the highresolution input. On the other hand, variability in the features extracted by different models may be complementary to identify saliency more accurately.</p><p>During the encoding process, two encoders are fed with images of different resolutions in order to capture global semantic information and detailed information respectively in parallel. The decoding phase can be divided into three substages, first Swin decoding, followed by grafted feature decoding and finally Resnet decoding in a staggered structure. The feature decoded in the second sub-stage is produced from Cross-Model Grafting Module (CMGM), where the global semantic information is grafted from Swin branch to Resnet branch. Also the CMGM process a matrix named CAM to be supervised. Reviewing the whole process, we construct a higher feature pyramid through two lower pyramid using staggered connection structure as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In other word, the network achieves deeper sampling depth at low computational cost to adapt to the challenge caused by high-resolution input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Extractors</head><p>Countering the massive computational consumption and memory usage generated by high-resolution input, we choose Resnet-18 <ref type="bibr" target="#b9">[10]</ref> and Swin-B <ref type="bibr" target="#b21">[22]</ref> as our backbones to balance performance and efficiency. For Resnet-18 encoder, five feature maps will be generated, which we denote the set as R. The feature map extracted by top 7 ? 7 layer offers limited performance gains but consume huge computational effort, especially for high-resolution input. Thus the utilized features in R can be denoted as {R i |i = 2, 3, 4, 5}. Due to the down-sampling in every stage, for input size</p><formula xml:id="formula_0">H ? W , the size of feature R i is H 2 i ? W 2 i ? (C ? 2 i?1 )</formula><p>, where (C ? 2 i ) is the channel of features. We remove the last stage while adopt the patch embedding feature of Swin transformer, which generates 4 features denoted as {S i |i = 1, 2, 3, 4}. Due to the nature that the embedding dim is fixed in transformer, the input size is 224 ? 224 and the feature size in S is 56 2 i?1 ? 56 2 i?1 ? (64 ? 2 i ) for i = 1, 2, 3 and 14 ? 14 ? 512 for S 4 . The spatial size of R 5 is close to S 2 , hence we chose to graft the features here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cross-Model Grafting Module</head><p>We propose Cross-Model Grafting Module(CMGM) to graft the feature f R5 and f S2 extracted by two different encoders. For feature f S2 , due to the transformer's ability to capture information over long distances, it has global semantic information that is important for saliency detection. In contrast, CNNs perform well at extracting local information thus f R5 have relatively rich details. However, due to the contradiction between feature size and receptive field, in f R5 there will be many noises in the background. For a salient prediction of a certain region, the predictions generated from different features can be roughly summarized as three cases: (a) Both right, (b) Some of them right and (c) Both wrong. Existing fusion method using element-wise operation such as addition and multiplication may work for the first two cases. However, the element-wise operation and the convolutional operation focus on only limited local information, resulting fusion methods hardly remedy for common errors. Compared with the feature fusion, CMGM re-calculates the point-wise relationship between Resnet feature and Transformer feature, transferring the global semantic information from Transformer branch to Resnet branch so as to remedy the common errors. We calculate the error map by E = |G ? P | ? [0, 1],where G is the ground truth and P is the salient prediction map generated by different branchs or CMGM. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the CMGM remedy the common error as expected.</p><p>Specifically, in CMGM it first flatten the f R5 ? ? H?W ?C to f R ? ? 1?C?HW , and do the same to f S2 to get f S . Inspired by the multi-head self-attention mechanism, we apply layer normalization and linear projection on them respectively to get f q R , f v R and f k S . We obtain Z by matrix multiplication, the process can be expressed as follows:</p><formula xml:id="formula_1">Y = softmax(f q R ? f k S T ),<label>(1)</label></formula><formula xml:id="formula_2">Z = Y ? f v R ,<label>(2)</label></formula><p>then we input Z to the linear projection layer and reshape it back to size of ? H?W ?C before feeding into convolutional layer. Two shortcut connections were performed in the process as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. In addition, during the cross attention process, we generate Cross Attention Matrix based on Y , which can be expressed as :</p><formula xml:id="formula_3">CAM = ReLU(BN(Conv(Y + Y T ))),<label>(3)</label></formula><p>The detailed usage of CAM can be found in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attention Guided Loss</head><p>In order for CMGM to better serve the purpose of transferring information from the Transformer branch to the Renset branch, we design the Attention Guided Loss to supervise the Cross Attention Matrix explicitly. We argue that the Cross Attention Matrix should be similar to the attention matrix generated from ground truth, because the salient features should have a higher similarity, in other words the dot product should has a larger activation value. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref> given a salient map M with size H ? W , we first flatten it to M with size 1 ? HW . Then we apply matrix multiplication on M to obtain corresponding attention matrix M a . The process can be denoted as M a = F (M ) and the value of M a xy can be expressed as</p><formula xml:id="formula_4">M a xy = M T x ? M y ,<label>(4)</label></formula><p>Then we use the transformation F(?) to construct G a , RP a , SP a , where G is the ground truth map, RP and SP are salient prediction map generated from feature R 5 and S 2 respectively. We propose the Attention Guided Loss based on weighted binary cross entropy (wBCE) to supervise the Cross Attention Matrix CAM generated from CMGM shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The BCE <ref type="bibr" target="#b5">[6]</ref> can be written as: </p><formula xml:id="formula_5">G xy = 1 log(1 ? P xy ) G xy = 0 ,<label>(5)</label></formula><p>where G xy is the ground truth label of the pixel (x, y), and P xy is the predicted probability in predicted map and both of them are in range[0, 1]. Then our L AG can be expressed as:</p><formula xml:id="formula_6">L AG = ? H i=1 W j=1 (1 + ?? ij ) ? bce (G a ij , CAM ij ) H i=1 W j=1 (1 + ?? ij ) ,<label>(6)</label></formula><p>where ? is a hyperparameter to adjust impact of the weight ? Eq. <ref type="bibr" target="#b6">(7)</ref>. In the Eq. (6), the bce on each pixel is assigned with a weight ?? ij . The use of weight ? serves two purposes: (1) The degree of positive and negative sample imbalance is squared due to the matrix multiplication.(2) As described in Sec. 4.3, we want to remedy the errors common to both of branches. When ?? equals 0, the Eq. (6) becomes usual binary cross entropy loss L bce . The weight ? can be calculated by:</p><formula xml:id="formula_7">?ij = 1 2 (| (G a ij ? RP a ij ) | + | (G a ij ? SP a ij ) |) + 1,<label>(7)</label></formula><p>where RP a and SP a are the attention matrix of RP and SP defined above.</p><p>What's more, we also apply the widely-used IoU loss <ref type="bibr" target="#b23">[24]</ref> to pay more attention to the global structure of the image as suggested by <ref type="bibr" target="#b26">[27]</ref>. The IoU loss L iou can be referred to supplementary materials. In the end, our total loss can be expressed as follow:</p><formula xml:id="formula_8">L total = L P b+i + L AG + 1 8 (L auxiliary b+i ),<label>(8)</label></formula><p>where L b+i = L bce + L iou , and L auxiliary b+i is L b+i applied on the RP and SP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>High-Resolution Datasets.</p><p>The high-resolution datasets available are UHRSD (4,932 images for training and 988 for testing) , HRSOD <ref type="bibr" target="#b39">[40]</ref> (1,610 images for training and 400 for testing). Followed by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>, we also use the DAVIS-S for evaluation.</p><p>Low-Resolution Datasets. DUTS-TR <ref type="bibr" target="#b30">[31]</ref> is used to train the model. In addition, we also evaluate our method on widely-used benchmark datasets: ECSSD <ref type="bibr" target="#b37">[38]</ref> with 1,000 images, DUT-OMRON <ref type="bibr" target="#b38">[39]</ref> with 5,168 images, PASCAL-S <ref type="bibr" target="#b16">[17]</ref> with 850 images, DUTS-TE <ref type="bibr" target="#b30">[31]</ref> with 5,019 images and HKU-IS <ref type="bibr" target="#b15">[16]</ref> with 4,447 images.</p><p>Evaluation Metrics. We use following metrics to evaluate the performance of all methods. Firstly, Mean Absolute Error (MAE), defined as Eq. <ref type="formula" target="#formula_9">(9)</ref> where P is the prediction map and G is the ground truth. The second is Max F -measure (F M ax ? ), which can be calculated by F ? = (1+? 2 )?precision?recall ? 2 ?precision?recall , where ? 2 is set to 0.3 as suggested in <ref type="bibr" target="#b1">[2]</ref>. Then we adopt Structural similarity Measure (S m ) <ref type="bibr" target="#b6">[7]</ref> and E-measure(E ? ) <ref type="bibr" target="#b7">[8]</ref> as many other methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. At last, to better evaluate the boundary quality which is important in High-resolution Saliency Detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>, we adopt the Boundary Displacement Error (BDE) to evaluate the result of high-resolution datasets, where lower values means better boundary quality.</p><formula xml:id="formula_9">MAE = 1 H ? W H i=1 W j=1 |Pij ? Gij|.<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use Pytorch <ref type="bibr" target="#b24">[25]</ref> to implement our model and two RTX 2080Ti GPUs are used for accelerating training. We choose Resnet-18 <ref type="bibr" target="#b9">[10]</ref> and Swin-B 224 <ref type="bibr" target="#b21">[22]</ref> as the backbone for convolutional branch and transformer branch respectively. The whole network is trained end-to-end by using stochastic gradient descent (SGD). We set the maximum learning rate to 0.003 for Swin backbone and 0.03 for others. The learning rate first increases then decays during the training process, what's more Momentum and weight decay are set to 0.9 and 0.0005 respectively. Batchsize is set to 16 and maximum epoch is set to 32. For data augmentation, we use random flip, crop and multi-scale input images <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>. In order to make fair comparisons and fully demonstrate the attributes of our UHRSD, we take three combinations of available datasets to train our model: (1) DUTS-TR (2) DUTS-TR+HRSOD-TR (3) UHRSD-TR+HRSOD-TR. During testing, each image is resized to 1024 ? 1024 and then fed into the network without any post-processing(e.g. CRF <ref type="bibr" target="#b14">[15]</ref>). <ref type="table">Table 1</ref>. Quantitative comparisons with state-of-the-art SOD models on five benchmark datasets in terms of max F-measure, MAE , Emeasure, S-measure and BDE. The best two results are shown in red and green, respectively. D: trained on DUTS-TR, HD: trained on DUTS-TR and HRSOD-TR, UH: trained on UHRSD-TR and HRSOD-TR . The best two results are in red and green fonts.  <ref type="figure">Figure 7</ref>. Visual comparison between our method and SOTA methods. The first four lines are from our UHRSD-TE and the next two lines are from HRSOD-TE. Best viewed by zooming in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the State-of-the-arts</head><p>We compare our proposed PGNet with 11 SOTA methods, including CPD <ref type="bibr" target="#b33">[34]</ref>, SCRN <ref type="bibr" target="#b34">[35]</ref>, DASNet <ref type="bibr" target="#b42">[43]</ref>, F3Net <ref type="bibr" target="#b31">[32]</ref>, GCPA <ref type="bibr" target="#b3">[4]</ref>, ITSD <ref type="bibr" target="#b45">[46]</ref>, LDF <ref type="bibr" target="#b32">[33]</ref>, CTD <ref type="bibr" target="#b44">[45]</ref>, PFS <ref type="bibr" target="#b22">[23]</ref>, HRSOD <ref type="bibr" target="#b39">[40]</ref>, DHQSOD <ref type="bibr" target="#b29">[30]</ref>, where HRSOD and DHQ-SOD are designed for high-resolution salient object detection. All of the above methods use Resnet-50 <ref type="bibr" target="#b9">[10]</ref> as the backbone except for HRSOD which uses VGG16 <ref type="bibr" target="#b27">[28]</ref>. And all of them are trained on DUTS-TR <ref type="bibr" target="#b30">[31]</ref> dataset, except for the marked ones like HRSOD-DH and DHQSOD-DH, which are trained on the mixed dataset (HRSOD <ref type="bibr" target="#b39">[40]</ref> and DUTS-TR). For a fair comparison, we use either the available implementations or the saliency maps provided by the authors. It's worth noting that the vacant lines in Tab. 1 are caused by the fact that one of them is not available so far and the other not being consistent with our test environment. Quantitative Comparison. As mentioned above, for fair comparison we use three settings of train set. As can be seen in Tab. 1, the results of training on either only DUTS-TR or mix of DUTS-TR and HRSOD-TR exceed the SOTA by a large margin on both high-resolution and lowresolution test sets. When using the mixed dataset DUTS-HRSOD, our method has significantly improved on highresolution datasets. There may be discrepancy between the distribution of high-resolution and low-resolution data. This is further supported by the results of training on the UHRSD-HRSOD mixed dataset, where the performance of the high-resolution dataset is significantly improved, especially for UHRSD-TE. This demonstrates that the annotation bias of high-resolution datasets differing from lowresolution datasets has a promotional effect on supervised high-resolution saliency detection method, which is the reason why high-resolution training data with high-quality annotation is in great demand.</p><p>Visual Comparison. To exhibit the characteristics of high-resolution dataset and the superiority of our method on it, <ref type="figure">Fig. 7</ref> shows representative examples of visual comparison of ours with respect to others. As can be seen, our method can capture details well and produce clear boundary (row 1 and 2). More than the high quality boundary, another significant aspect of high-resolution SOD is the ability to segment objects with small sturctures that are easily to overlook in low-resolution cases (row 3, 5 and 6). This also demonstrates the superiority that our method makes the process one-stage. What's more, our method works well even in some extremly complex scenarios such as row 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>To better illustrate the nature of proposed method for high-resolution images, the ablation studies are based on the settings of Ours-UH, which is trained on the mixed dataset UHRSD-TR and HRSOD-TR.</p><p>Ablation Study for Compositions. To prove the effectiveness of proposed feature grafting method including the CMGM and AGL, we report the quantitative performance in Tab. 2. The baseline Resnet-18 and baseline Swin represent the widely-used U-shape network with Resnet-18 backbone and Swin backbone respectively. As can be seen in row 3, our proposed staggered architecture and Cross- Model Grafting Module inherits the strengths of both models. What's more, under the guiding role of AGL, performance has been further improved.</p><p>Ablation Study for Grafting Position. To investigate the impact of grafting position on the network performance, we conduct a series of experiments with different grafting feature pairs. As shown in Tab. 3, starting with the alignment of the last stage of two encoders, the performance gradually improves as the number of staggered layers increase until reaching the best at the pair R 5 ? S 2 . This may be due to the spatial size of feature maps. When the sizes are close, the spatial information in the features extracted from two models corresponds to each other, which in turn promotes the feature grafting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitation</head><p>Our method is simple and fast for one-stage highresolution saliency detection, but the training process is still quite demanding on GPU memory usage, resulting in a high cost of training. What's more, though our method has already a superior input resolution compared to previous SOD methods, the input resolution is not unlimited. For excessive resolution such as 4K, images need to be downsampled first before input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose the Pyramid Grafting Network for one-stage high-resolution salient object detection. The proposed staggered grafting patterns effectively exploit the advantages of each of the two existing different encoder. In addtion the proposed Cross-Model Grafting Modules and Attention Guided Loss cooperate with each other to inherit the advantages and remedy the common defects of the CNN and transformer. It is worth noting that we contribute the first 4K resolution SOD dataset for advancing future studies in high-resolution SOD. Extensive experiments demonstrates that our method not only outperforms the state-ofthe-art methods but also is able to produce high-resolution saliency predictions fast and accurately.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of the results of the different methods. (a) Input image. (b) Ground truth mask. (c) Directly input to Resnet-18 based FPN. (d) Downsample then input to Swin transformer based FPN. (e) Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of the results of the different methods. (a) Comparison of the logarithm of edge pixel amount between our UHRSD and HRSOD. (b) Comparison of the diagonal length between our UHRSD and HRSOD [40] (c) Sample from our UHRSD. (d) Sample from HRSOD. (e) Sample from our UHRSD. (f) Sample from DUTS-TE. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An overview of proposed Pyramid Grafting Network. Dual branches use Resnet and Swin transformer as encoder respectively. The DBn is the Decoder block with n input features, and the specific structure are shown on the right side. The two auxiliary supervisions are used to supervise the RP and SP mentioned in Sec. 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of error elimination through CMGM. (a)(b) are the images and ground truth masks. (c) and (d) are thee salient error map generated from Transformer branch and Resnet branch respectively. (e) shows the error map generated from CMGM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of Cross-Model Grafting Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The construction of attention matrix. The operation is used to create the target and weights for proposed AGL. bce (G xy , P xy ) = log(P xy )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>CPD19.867 .041 .891 .881 62.066 .871 .029 .921 .893 33.971 .894 .055 .884 .878 32.587 .797 .056 .866 .825 .865 .043 .887 .869 SCRN19 .880 .042 .887 .888 75.696 .893 .027 .911 .902 46.592 .904 .051 .880 .887 40.176 .811 .056 .863 .837 .888 .040 .888 .885 DASNet20 .893 .032 .925 .897 69.310 .902 .020 .949 .911 26.761 .914 .045 .892 .889 35.044 .827 .050 .877 .845 .895 .034 .908 .894 F3Net20 .900 .035 .913 .897 65.757 .915 .020 .940 .914 44.760 .909 .046 .887 .890 39.612 .813 .053 .871 .838 .891 .035 .902 .888 GCPA20 .889 .036 .898 .898 74.900 .922 .020 .934 .929 39.160 .912 .047 .886 .896 35.947 .812 .056 .860 .839 .888 .038 .891 .891 ITSD20 .896 .036 .912 .898 87.946 .899 .022 .922 .909 68.256 .911 .045 .895 .897 41.174 .821 .061 .863 .840 .883 .041 .895 .885 LDF20 .904 .032 .919 .904 58.714 .911 .019 .947 .922 35.447 .913 .047 .891 .888 33.775 .820 .051 .873 .838 .898 .034 .910 .892 CTD21 .905 .032 .921 .905 63.907 .904 .019 .938 .911 42.832 .917 .043 .898 .897 33.835 .826 .052 .875 .844 .897 .034 .909 .893 PFS21 .911 .033 .922 .906 63.537 .916 .019 .946 .923 30.612 .918 .043 .896 .897 37.387 .823 .055 .875 .842 .896 .036 .902 .892 .931 .021 .944 .930 46.923 .936 .015 .947 .935 34.957 .931 .037 .904 .912 32.300 .835 .045 .887 .855 .917 .027 .922 .911 Ours-DH .937 .020 .946 .935 45.292 .950 .012 .975 .948 14.463 .935 .036 .905 .912 32.008 .835 .046 .887 .858 .919 .028 .925 .912 Ours-UH .945 .020 .946 .938 57.147 .957 .010 .979 .954 12.725 .949 .026 .916 .935 30.019 .772 .058 .884 .786 .871 .038 .897 .859</figDesc><table><row><cell>Method</cell><cell>F M ax ?</cell><cell cols="2">HRSOD-TE MAE E? Sm BDE F M ax ?</cell><cell cols="3">DAVIS-S MAE E? Sm BDE F M ax ?</cell><cell cols="6">UHRSD-TE MAE E? Sm BDE F M ax DUT-OMRON ? MAE E? Sm F M ax ?</cell><cell>DUTS-TE MAE E? Sm</cell></row><row><cell>HRSOD-DH19</cell><cell cols="5">.905 .030 .934 .896 88.017 .899 .026 .955 .876 44.359</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">.743 .065 .831 .762 .835 .050 .885 .824</cell></row><row><cell cols="6">DHQSOD-DH21 .922 .022 .947 .920 46.495 .938 .012 .947 .920 14.266</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">.820 .045 .873 .836 .900 .031 .919 .894</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Our PGNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours-D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image</cell><cell>GT</cell><cell>Ours-UH</cell><cell>CPD</cell><cell>CTD</cell><cell>DASNet</cell><cell cols="2">F3Net</cell><cell></cell><cell>GCPA</cell><cell></cell><cell>ITSD</cell><cell>LDF</cell><cell>PFS</cell><cell>SCRN</cell></row><row><cell>Image</cell><cell>GT</cell><cell>Ours-UH</cell><cell>DHQSOD</cell><cell>HRSOD</cell><cell>CPD</cell><cell cols="2">CTD</cell><cell></cell><cell>DASNet</cell><cell></cell><cell>F3Net</cell><cell>GCPA</cell><cell>ITSD</cell><cell>LDF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different architectures and compositions.</figDesc><table><row><cell>Composition</cell><cell>F M ax ?</cell><cell>HRSOD-TE MAE E ?</cell><cell>S m</cell></row><row><cell>baseline Resnet-18</cell><cell>.878</cell><cell cols="2">.051 .875 .871</cell></row><row><cell>baseline Swin</cell><cell>.915</cell><cell cols="2">.027 .937 .921</cell></row><row><cell>baseline R+S+CMGM</cell><cell>.940</cell><cell cols="2">.023 .944 .936</cell></row><row><cell>baseline R+S+CMGM+AGL</cell><cell>.945</cell><cell cols="2">.020 .946 .938</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance with the different grafted features. Ri denotes the ith feature of R defined in Sec. 4.2, and Si is similar.</figDesc><table><row><cell>Feature Pair</cell><cell cols="2">HRSOD-TE F M ax ? MAE E ?</cell><cell cols="2">UHRSD-TE F M ax ? MAE E ?</cell></row><row><cell>R 5 ? S 4</cell><cell>.913</cell><cell>.029 .922</cell><cell>.935</cell><cell>.031 .907</cell></row><row><cell>R 5 ? S 3</cell><cell>.939</cell><cell>.022 .937</cell><cell>.947</cell><cell>.026 .912</cell></row><row><cell>R 5 ? S 2</cell><cell>.945</cell><cell>.020 .946</cell><cell>.949</cell><cell>.026 .916</cell></row><row><cell>R 5 ? S 1</cell><cell>.937</cell><cell>.022 .935</cell><cell>.947</cell><cell>.026 .910</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Salient object detection: A survey. Computational visual media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Runmin Cong, and Qingming Huang. Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00651</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10421</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1109.CVPR" />
	</analytic>
	<monogr>
		<title level="m">ieee conf comput vispattern recognit</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Full-duplex strategy for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4922" to="4933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calibrated rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9471" to="9481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic feature integration for simultaneous detection of salient object, edge and skeleton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08595</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual saliency transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyuan</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="4722" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Light field saliency detection with dual local graph learning and reciprocative guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4712" to="4721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramidal feature shrinking for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingcan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeproadmapper: Extracting road topology from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gell?rt</forename><surname>M?ttyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U2-net: Going deeper with nested u-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Osmar R Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangled high quality salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3580" to="3590" />
		</imprint>
	</monogr>
	<note>Shouhong Ding, and Mofei Song</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">F 3 net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4142" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghua</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3004" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7234" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory-oriented decoder for light field salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic context-sensitive filtering network for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1553" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is depth really necessary for salient object detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1745" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Complementary trilateral decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactive two-stream decoder for accurate and fast saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9141" to="9150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-type self-attention guided degraded saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meijun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13082" to="13089" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
