<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paint Transformer: Feed Forward Neural Painting with Stroke Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhua</forename><surname>Liu</surname></persName>
							<email>2songhua.liu@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Paint Transformer: Feed Forward Neural Painting with Stroke Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Equal contribution. ? This work was done when Songhua Liu was an intern at VIS, Baidu. able for training the Paint Transformer, we devise a selftraining pipeline such that it can be trained without any offthe-shelf dataset while still achieving excellent generalization capability. Experiments demonstrate that our method achieves better painting performance than previous ones with cheaper training and inference costs. Codes and models are available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Illustration of our results. The second row demonstrates the progressive painting process. Zoom-in for better view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Neural painting refers to the procedure of producing a series of strokes for a given image and non-photorealistically recreating it using neural networks. While reinforcement learning (RL) based agents can generate a stroke sequence step by step for this task, it is not easy to train a stable RL agent. On the other hand, stroke optimization methods search for a set of stroke parameters iteratively in a large search space; such low efficiency significantly limits their prevalence and practicality. Different from previous methods, in this paper, we formulate the task as a set prediction problem and propose a novel Transformerbased framework, dubbed Paint Transformer, to predict the parameters of a stroke set with a feed forward network. This way, our model can generate a set of strokes in parallel and obtain the final painting of size 512 ? 512 in near real time. More importantly, since there is no dataset avail-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since ancient times, painting has been a fantastic way for human beings to record what they perceive or even how they imagine about the world. Painting has long been known to require professional knowledge/skills and is not easy for ordinary people. Computer-aided art creation largely fills this gap and enables many of us to create our own artistic compositions. Especially with the coming of AI era, natural images can be transformed to be artistic via image style transfer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref> or image-to-image translation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. These previous methods typically formulate image creation as an optimization process in the pixel space <ref type="bibr" target="#b4">[5]</ref> or a feed-forward pixel-wise image mapping with neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>. Nevertheless, different from pixel-wise operations of neural networks, humans create paintings through a stroke-by-stroke procedure, using brushes from coarse to fine. It is of great potential to make machines imitate such a stroke-by-stroke process to generate more authentic and human-creation-like paintings. Besides, it also has the additional benefit of interpreting how a painting can be created step by step, which might be valuable as a teaching tool. Thus, as an emerging research topic, stroke based neural painting is explored to generate a series of strokes for imitating the way that artistic works are created by human painters. Hopefully, with such techniques, the generated paintings can look more like real human created paintings such as oil paint or watercolor.</p><p>Generating stroke sequences for painting process is a challenging task even for skilled human painters, especially when the targets have complex compositions and rich textures. To achieve this goal, some previous works tackle this problem by a sequential process of generating strokes one by one, such as recurrent neural networks (RNN) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6]</ref>, step-wise greedy search <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>, and reinforcement learning (RL) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. There are also methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17]</ref> tackling this problem via stroke parameter searching using an iterative optimization process. Although attractive painting results are generated by these methods, there still exists large room for improvement on both efficiency and effectiveness. Sequence-based methods such as RL are relatively fast in inference but suffer from long training time as well as unstable agents. Meanwhile, optimization-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17]</ref> do not need training, but its optimization process is extremely time consuming. These inconveniences motivate us to explore more efficient and elegant solutions for stroke-based painting generation. Instead of stoke sequence generation, we re-formulate the neural painting task as a feed-forward stroke set prediction problem. Given an initial canvas and a target natural image, our model predicts a set of strokes and then renders them on the initial canvas to minimize the difference between the rendered image and the target one. This procedure is repeated at K coarse-tofine scales. At each scale, its initial canvas is the output of the previous scale. As shown in <ref type="figure">Fig. 1</ref>, high-quality final paintings can be generated. Therefore, the core problem of our method is to train a robust stroke set predictor. Interestingly, object detection is also a typical set prediction problem. We are therefore inspired by recent object detector DETR <ref type="bibr" target="#b1">[2]</ref> and propose our novel Paint Transformer to generate painting via predicting parameters of multiple strokes with a feed forward Transformer. However, different from object detection, no annotated data is available for training a stroke predictor. To overcome such difficulty, we propose a novel self-training pipeline which utilizes synthesized stroke images. Specifically, we first synthesize a background canvas image with some randomly sampled strokes; then, we randomly sample a foreground stroke set, and render them on canvas image to derive a target image. Thus, the training objective of the stroke predictor is to predict the foreground stroke set and minimize the differences between the synthesized canvas image and the target image, where the optimization is conducted on both stroke level and pixel level. Impressively, our self-trained Paint Transformer shows great generalization capability and can work for arbitrary natural images once trained. Extensive experiments demonstrate that our feed-forward method can generate paintings with better quality at lower cost compared to existing methods. Our contributions can be summarized as:</p><p>? We view stroke-based neural painting problem from an innovative perspective of feed-forward stroke set prediction, instead of stroke sequence generation or optimization-based stroke search.</p><p>? A novel Paint Transformer tailored for this task is proposed with a creative self-training strategy to make it well trained without any off-the-shelf dataset.</p><p>? Extensive experiments are conducted to validate our approach and demonstrate that state-of-the-art visual quality is achieved, while maintaining high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stroke Based Painting</head><p>It is not a totally new research topic to teach machines how to paint. Traditional methods usually devise heuristic painting strategies <ref type="bibr" target="#b7">[8]</ref> or greedily select a stroke that minimizes difference from the target image step by step <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. In recent years, RNN and RL are largely applied to generate strokes in a sequential manner. Ha et al. <ref type="bibr" target="#b5">[6]</ref> proposed an RNN-based solution to generate strokes for sketches. Ganin et al. <ref type="bibr" target="#b3">[4]</ref> and Zhou et al. <ref type="bibr" target="#b36">[37]</ref> introduced RL for the sketch synthesis task. These works focus on sketches only, while in <ref type="bibr" target="#b31">[32]</ref> RL-based strokes generation is explored for ink painting. By leveraging strengths of CNN, RNN, GAN, and RL, <ref type="bibr" target="#b9">[10]</ref> provided solutions to generate more photo-realistic paintings. Nevertheless, training a stable RL agent is difficult due to the alternate and adversarial updates of actors, critics, and discriminators. Recently, Zou et al. <ref type="bibr" target="#b38">[39]</ref> proposed a stroke optimization strategy that iteratively searches optimal parameters for each stroke and is possible to be optimized jointly with neural style transfer. Similar idea is also adopted in Kotovenko et al. <ref type="bibr" target="#b16">[17]</ref>.  Although its artistic painting effect is satisfactory, its computational cost largely limits its applicability. Differently, we formulate neural painting as a problem of feed-forward stroke set prediction, in order to seek better trade-off between performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection</head><p>Our Paint Transformer is essentially a set prediction model and is largely inspired by object detection. Pioneering deep object detection models use an inconvenient twostage pipeline <ref type="bibr" target="#b27">[28]</ref>. There are also one-stage object detectors proposed, such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. However, its heavy dependence on post-processing steps such as non-max suppression can still bring much inconvenience. Recently, DETR <ref type="bibr" target="#b1">[2]</ref> employs Transformer <ref type="bibr" target="#b29">[30]</ref> to produce detection results end-to-end and we find DETR quite suitable for our stroke prediction task, since it can perform set prediction without any tricky post-processing. Nevertheless, instead of directly adopting DETR, we add binary neurons to predict a stroke should be kept or not. Besides, our model takes two images (current canvas and target images) as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Framework</head><p>We formulate the neural painting as a progressive stroke prediction process. At each step, we predict multiple strokes in parallel to minimize the difference between current canvas and our target image in a feed-forward fashion. Our Paint Transformer consists of two modules: Stroke Predictor and Stroke Renderer. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, given a target image I t and an intermediate canvas image I c , Stroke Predictor generates a set of parameters to determine current stroke set S r . Then, Stroke Renderer generates the stroke image for each stroke in S r and plots them onto the canvas I c , producing the resulting image I r . We can formulate this process as:</p><formula xml:id="formula_0">I r = P aintT ransf ormer(I c , I t )<label>(1)</label></formula><p>In Paint Transformer, only Stroke Predictor contains trainable parameters, while Stroke Renderer is a parameterfree and differentiable module. To train a Stroke Predictor, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we propose a novel self-training pipeline which utilizes randomly synthesized strokes. In each iteration during training, we first randomly sample a foreground stroke set S f and a background stroke set S b . We then generate a canvas image I c using Stroke Renderer taking as input S b and produce a target image I t by rendering S f onto I c . Lastly, taking I c and I t as input, Stroke Predictor can predict a stroke set S r , after which Stroke Renderer can generate a predicted image I r taking S r and I c as input. In other words, Stroke Predictor is trained under a stroke-image-stroke-image pipeline, where the optimization is conducted on both stroke level and pixel level. Therefore, the training objective for the Stroke Predictor is:</p><formula xml:id="formula_1">L = L stroke (S r , S f ) + L pixel (I r , I t ),<label>(2)</label></formula><p>where L stroke and L pixel are stroke loss and pixel loss separately. Note that strokes used for supervision are randomly synthesized so that we can generate infinite data for training and do not rely on any off-the-shelf dataset. Appealing results can be produced by our self-trained Paint Transformer. We will provide detailed description for each part of our method in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stroke Definition and Renderer</head><p>In this work, we mainly consider straight line stroke, which can be represented by shape parameters and color parameters. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>  RGB values denoted as r, g and b. Thus, a stroke s can be denoted as {x, y, h, w, ?, r, g, b}.</p><p>In the task of neural painting, differentiable rendering is one important problem to synthesize stroke images based on stroke parameters and thereby enable end-to-end training of Stroke Predictor. Recently, deep neural networks have been widely utilized as differentiable renderers as discussed in <ref type="bibr" target="#b13">[14]</ref>. Nevertheless, for the specific stroke definition in this paper, instead of adopting neural networks, we consider a geometric transformation based Stroke Renderer, which does not need training and is differentiable as expected. We denote this Stroke Renderer as:</p><formula xml:id="formula_2">I out = StrokeRenderer(I in , S),<label>(3)</label></formula><p>where I in and I out are input and output canvas separately and S = {s i } n i=1 is a set of n strokes. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, given a primitive brush I b and a stroke s i , we can modify its color and transfer its shape and location in canvas coordinate system, obtaining its rendered stroke imag? I i b . Meanwhile, we generate a single-channel alpha map ? i with the same shape of? i b as a binary mask of s i . Denoting I 0 mid = I in , we can formulate stroke rendering process as:</p><formula xml:id="formula_3">I i mid = ? i ?? i b + (1 ? ? i ) ? I i?1 mid ,<label>(4)</label></formula><p>where the output of the Stroke Renderer is I out = I n mid . Since the whole process can be achieved by linear transformation, Stroke Renderer becomes differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stroke Predictor</head><p>The goal of our Stroke Predictor is to predict a set of strokes which can cover the differences between a intermediate canvas image and a target image. Meanwhile, to achieve a certain degree of abstraction to simulate real painting process, we hope the Stroke Predictor can predict as few strokes, while still covering most areas of differences. To achieve this, inspired by DETR [2], we propose a Transformer-based predictor which takes in I c and I t and generates a stroke set, i.e., S r = StrokeP redictor(I c , I t ).  As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, taking I c , I t ? R 3?P ?P as input, Stroke Predictor first adopts two independent convolution neural networks to extract their feature maps as F c , F t ? R C?P/4?P/4 . Here, P is the pre-defined size of stroke image. Then, F c , F t and a learnable positional encoding are concatenated and flattened as the input of Transformer encoder. In decoder part, following DETR, we use N learnable stroke query vectors as input. Finally, there are two branches of fully-connected layers to predict initial stroke parametersS r = {s i } N i=1 and stroke confidence</p><formula xml:id="formula_5">C r = {c i } N i=1</formula><p>respectively. Here, we add binary neurons for stroke confidence: in forward phase, confidence score c i can be converted to a decision d i = Sign(c i ), where Sign is a binary function, whose value is 1 if c i ? 0 and is 0 otherwise. The decision d i is used to determine whether a predicted stroke should be plotted in canvas. Note that Sign function has zero gradient almost everywhere. In order to enable back propagation, in backward phase, we alternatively utilize Sigmoid function ?(x) to compute gradient as:</p><formula xml:id="formula_6">?d i ?c i = ??(c i ) ?c i = exp(?c i ) (1 + exp(?c i )) 2 .<label>(6)</label></formula><p>Gathering all predicted strokes with positive decisions, we can get the final S r = {s i } n i=1 with n strokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>The major advantage of our proposed self-training pipeline is that we can simultaneously minimize differences between ground truth and prediction on both image level and stroke level. In this section, we will introduce our pixel loss, measurement of differences between strokes, and stroke loss. Pixel Loss. One intuitive goal for neural painting is to recreate a target image. Therefore, pixel-wise loss L pixel between I r and I t is penalized on the image level:</p><formula xml:id="formula_7">L pixel = ||I r ? I t || 1 .<label>(7)</label></formula><p>Stroke Distance. On the stroke level, it is important to define appropriate metrics for measuring the difference be-tween two strokes. First, similar to the object detection task, we define parameter-wise L 1 distance as:</p><formula xml:id="formula_8">D u,v L1 = ||s u ? s v || 1 ,<label>(8)</label></formula><p>where s u and s v denote parameters of strokes u and v respectively. As shown in many object detection works, merely employing the L 1 metric dismisses different scales for big and small strokes. Thus, we further add the Wasserstein distance between two strokes following the idea in rotational object detection <ref type="bibr" target="#b32">[33]</ref>. To be specific, a rotational rectangular stroke with parameters [x, y, w, h, ?] (excluding color parameters) can be viewed as a 2-D Gaussian distribution N (?, ?) by the following equations: ? = (x, y),</p><formula xml:id="formula_9">? 1 2 = cos ? ? sin ? sin ? cos ? w 2 0 0 h 2 cos ? sin ? ? sin ? cos ? = w 2 cos 2 ? + h 2 sin 2 ? w?h 2 cos ? sin ? w?h 2 cos ? sin ? w 2 sin 2 ? + h 2 cos 2 ? .<label>(9)</label></formula><p>Therefore, the Wasserstein distance between two Gaus-</p><formula xml:id="formula_10">sian distributions N (? u , ? u ) and N (? v , ? v ) is: D u,v W = ||? u ? ? v || 2 2 + T r(? u + ? v ? 2(? 1 2 u ? v ? 1 2 u ) 1 2 ),<label>(10)</label></formula><p>where T r(?) denotes the trace of a matrix. Moreover, it is desired that the predicted confidence for a stroke with positive (negative) ground-truth decision should be as high (low) as possible. Let's consider s u as a predicted stroke with confidence c u and s v as a target stroke with groundtruth label g v , where g v = 1 if s v is a valid stroke and g v = 0 if s v is an empty stroke. Therefore, we can utilize binary cross entropy to match the confidence similarity:</p><formula xml:id="formula_11">D u,v bce = ?? r ?g v ?log ?(c u )?(1?g v )?log(1??(c u )),<label>(11)</label></formula><p>where ? r is a weight term controlling recall. Stroke Loss. During training, the number of valid groundtruth strokes is varied. Thus, following DETR <ref type="bibr" target="#b1">[2]</ref>, with the predefined maximum stroke number N , we need to first generate a matching mechanism between the prediction set S r of N strokes and the ground-truth set S g of N strokes (they can be both valid and empty strokes in S g ) to calculate the loss. Following DETR <ref type="bibr" target="#b1">[2]</ref>, we adopt the permutation of strokes that produces the minimal stroke level matching cost to calculate final loss. The optimal bipartite matching is firstly computed leveraging the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>. For a stroke s u in the prediction setS r and a stroke s v in the target set S g , their cost value is: I k t = resize(I t , (P ? 2 k , P ? 2 k ));</p><formula xml:id="formula_12">M u,v = g v (D u,v L1 + D u,v W + D u,v bce ),<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>I k c = resize(I c , (P ? 2 k , P ? 2 k )); I c = patches to image(I k r ); 12: end for 13: I r = crop(I c , size = (H, W )); <ref type="bibr">14:</ref> return I r .</p><p>which means the matching cost for empty target strokes is always 0. Therefore, denoting as X and Y the optimal permutations for predicted strokes and target strokes given by the Hungarian algorithm, respectively, the stroke loss function can be written as:</p><formula xml:id="formula_13">L stroke = 1 n n i=1 (g Yi (? L1 D XiYi L1 + ? W D XiYi W ) + ? bce D XiYi bce ),<label>(13)</label></formula><p>where ? L1 , ? W , and ? bce are weight terms. Moreover, although in the neural painting task, stroke order is of great importance, we ignore the stroke order in the stroke level loss and set the task of regulating stroke order to the image level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>To imitate a human painter, we devise a coarse-to-fine algorithm to generate painting results during inference, as shown in Algorithm 1. Given a real-world image of size H ?W , our Paint Transformer runs on K scales from coarse to fine in order. Painting on each scale is dependent on result of the previous scale. Target image and current canvas would be cut into several non-overlapping P ? P patches before being sent to Stroke Predictor. We set K as follow: <ref type="bibr" target="#b13">(14)</ref> where in the k-th (0 ? k ? K) scale, there are 2 k ? 2 k patches. Each patch would go through Stroke Predictor and then Stroke Renderer module in parallel independently. The painting result on each scale is derived by combining patches of canvas together. <ref type="figure">Figure 5</ref>. Comparison with the state-of-the-art methods: an optimization-based method (Optim) <ref type="bibr" target="#b38">[39]</ref> and an RL-based method <ref type="bibr" target="#b9">[10]</ref>. We also demonstrate our results with different number of rendering scales, where K = 4 is the default setting. MS here denotes using more strokes for Optim, with same number as Ours (K = 4) .  <ref type="table">Table 1</ref>. Quantitative results under different metrics for different methods or settings. Smaller values mean closer to original inputs.</p><formula xml:id="formula_14">K = max(argmin K {P ? 2 K ? max(H, W )}, 0),</formula><p>Optim is applied with the same number of strokes with Ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementing Details</head><p>To train our Paint Transformer, in practice, we set the size of input images P as 32, and the number of strokes N in one patch as 8. The CNNs for image feature extraction consists of three Conv-BatchNorm-ReLU blocks with two 1/2-scale down-sampling operations. For the Transformer, the feature dimension is 256 and both encoder and decoder have 3 layers. During training, we randomly generate parameters of N target strokes from a uniform distribution. To prevent too much stroke-wise overlap and ensure that the number of valid target strokes is varied, we generate strokes for I t in sequence and set the label of a stroke to 0 if it covers more than 60% area of one previous stroke. Hyper-parameters ? r , ? L1 , ? w and ? bce are set to 8, 1, 10, and 1 respectively. We use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a learning rate of 0.0001. We train the model for 30, 000 iterations with a batch size of 128 on a single Nvidia RTX 2080 Ti GPU. The total training time is fewer than 4 hours. For inference, painting results in this paper are all under 512 ? 512 resolution with K = 4 if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ours RL <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art Methods</head><p>Qualitative Comparison. As shown in <ref type="figure">Fig. 5</ref>, we compare our method with two state-of-the-art stroke-based painting generation methods. Comparing with the optimizationbased method (Optim) <ref type="bibr" target="#b38">[39]</ref>, our method can generate more appealing and refreshing results. To be specific, in large texture-less image areas, our method can generate humanlike painting with relative fewer and bigger strokes (row 3, 5 and 7). In small texture-rich image areas, our method can generate painting with clearer texture to preserve content structure. We further implement Optim with more strokes (column 5), however, the aforementioned problem still exists. Compared with the RL-based method <ref type="bibr" target="#b9">[10]</ref>, we can generate more vivid results with clear brushes. Meanwhile, the results of <ref type="bibr" target="#b9">[10]</ref> are somehow blurred and lack of artistic abstraction, it is also too similar to the original images. Quantitative Comparison. We also conduct quantitative comparison for reference. Since one objective of neural painting is to recreate original images, we directly use the pixel loss L pixel and the perceptual loss L pcpt <ref type="bibr" target="#b11">[12]</ref> as evaluation metrics. For real images, we randomly select 100 landscapes from <ref type="bibr" target="#b0">[1]</ref>, 100 artworks from WikiArt <ref type="bibr" target="#b24">[25]</ref>, and 100 portraits from FFHQ <ref type="bibr" target="#b12">[13]</ref> for evaluation. Results shown in <ref type="table">Table 1</ref> are consistent with the previous qualitative analysis: (1) with vivid brush textures, our method can present the original content better than Optim <ref type="bibr" target="#b38">[39]</ref>; <ref type="bibr" target="#b1">(2)</ref> [10] achieves the best content fidelity, however it is weak in abstraction. Then, to compare stroke prediction performance, we send synthesized stroke images to both Paint Transformer and Optim and evaluate their generated strokes with the same metrics as Sec. 3.4. Numeric results show that our method can predict strokes successfully and outperforms other methods. Here, measurements are missing for <ref type="bibr" target="#b9">[10]</ref>, since it has differently parameterized strokes. Efficiency Analysis. We demonstrate efficiency comparison in <ref type="table" target="#tab_4">Table 2</ref>. Training or inference time is measured using a single Nvidia 2080Ti GPU. During inference, since Paint Transformer produces a set of strokes in parallel in a feed-forward manner, it runs significantly faster than optimization baseline <ref type="bibr" target="#b38">[39]</ref> and slightly faster than the RL-based baseline <ref type="bibr" target="#b9">[10]</ref>. As for training, we only need a few hours to train a Stroke Predictor, which is more convenient than both <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b38">[39]</ref> from the perspective of total training time. Besides, our model-free Stroke Renderer and data-free Stroke Predictor are efficient and convenient to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we present ablation study results to verify the effectiveness of each optimization term used for training Paint Transformer. (1) Without the pixel loss, the model fails to learn proper locations to put strokes with proper colors, resulting in wrong colors and dirty textures;</p><p>(2) Without the parameter L 1 loss, the model fails to learn the shapes of strokes and present repeated stroke patterns;</p><p>(3) Without the Wasserstein loss, it seems that the ability of handling strokes with different scales is weakened, with large and vivid strokes vanished; (4) Without the confidence loss, the model cannot decide whether to plot a stroke or not, resulting in too many small strokes totally covering the whole image and previous strokes. We also present quantitative ablation results in <ref type="table">Table 1</ref>, which demonstrates that missing each of proposed metric leads to performance drop. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Extension of Paint Transformer</head><p>Painting with Different Strokes. Notably, once trained with one kind of primitive brush, our Stroke Predictor can be easily transferred to another kind via replacing the primitive brush used in Stroke Renderer. As demonstrated in <ref type="figure" target="#fig_5">Fig. 7</ref>, with a Stroke Predictor trained with oil-painting brush, we can still generate appealing and vivid painting results with rectangle and circle brushes. Stylized Painting. It is also flexible for our method to be integrated with artistic style transfer to generate attractive and stylized paintings. We utilize existing style transfer methods such as LapStyle <ref type="bibr" target="#b19">[20]</ref> and AdaAttN <ref type="bibr" target="#b21">[22]</ref> to generate neural paintings on stylized content images. As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, with this imaginative manner, we can generate stylized paintings with diverse colors and textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>In this paper, we re-formulate the neural painting problem from a stroke set prediction perspective. Leveraging insights from Transformer-based object detection, we propose a novel framework, dubbed Paint Transformer, which can generate paintings from natural images via predicting parameters of multiple strokes with a feed-forward Transformer. Moreover, we propose a novel self-training pipeline that makes it possible to train our Paint Transformer without any manually collected dataset. Experiments demonstrate that our model can generate paintings with better trade-off between artistic abstraction and realism, compared to stateof-the-art methods, while maintaining high efficiency.</p><p>As for our future work, it is a valuable topic to explore more complex strokes with various shapes or color patterns besides straight-line strokes with uniform colors. More advanced stroke rendering systems are required for these stroke settings. It may further improve the painting quality of long but narrow areas if cross-patch context is exploited. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Training Details</head><p>In our training, firstly, we sample 8 strokes on a canvas with 64 ? 64 resolution as background. Then, the background canvas is divided into 4 blocks with 32 ? 32 size. For each block, we further sample another 8 strokes as foreground ones based on the background. The stroke predictor learns to predict these extra strokes. Such operations encourage the stroke predictor to paint from coarse to fine. Thus, it always tends to generate refined strokes to minimize the differences between current canvas and target. During inference, the coarse-to-fine inference process can gradually fill in the canvas and reduce the differences between canvas and the real image. Therefore, our stroke predictor can be generalized from randomly-synthesized dataset to real-world images successfully. More canvastarget-predict pairs (denoted as S b , S f , and S r respectively) during training period are shown in <ref type="figure" target="#fig_7">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Inference Results</head><p>We provide more results including high-resolution (1024 ? 1024) results of our algorithm in <ref type="figure">Fig. 10</ref>. The animated painting process can be found in the attachment or our code page. <ref type="figure">Figure 10</ref>. More inference result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Demonstration of our proposed self-training pipeline for Painter Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of Stroke Renderer and parameter definition of a stroke.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of Stroke Predictor, which contains two convolution networks for feature embedding and a Transformer network for stroke parameter prediction. ? stands for concatenate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>tc</head><label></label><figDesc>= image to patches(I k t , size = (P, P )); = image to patches(I k c , size = (P, P ));</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Ablation study on proposed different loss terms. To illustrate the differences clearly, in each image, an area is enlarged.MethodsReal Images Random StrokesL pixel L pcpt L pixel D L1 D W RL [10] 0.040 0.737 0.058 --Optim [39] 0.059 0.856 0.073 0.137 0.057 Ours 0.056 0.807 0.042 0.083 0.018 w/o L pixel 0.081 1.012 0.068 0.241 0.024 w/o D L1 0.074 0.941 0.077 0.267 0.019 w/o D W 0.069 0.947 0.046 0.113 0.034 w/o D bce 0.071 0.928 0.052 0.093 0.021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results with different brushes, K is set to 3 here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Stylized paintings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Canvas-target-predict pairs in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Stroke Render Stroke Predictor Stroke Render</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>stroke loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Foreground Background</cell><cell>Strokes Strokes</cell><cell>? ?</cell><cell>Blank Canvas Stroke Renderer Stroke Renderer</cell><cell>Canvas Image Target Image</cell><cell>stroke queries</cell><cell>Prediction</cell><cell>Strokes</cell><cell>?</cell><cell>pixel loss Stroke Renderer</cell><cell>Predict Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, shape parameters of a stroke include: center point coordinate x and y, height h, width w and rotation angle ?. Color parameters of a stroke include ?</figDesc><table><row><cell>Shape</cell><cell>Color</cell></row><row><cell>Params</cell><cell>Params</cell></row><row><cell></cell><cell>?</cell></row><row><cell cols="2">Stroke Stroke</cell></row><row><cell cols="2">Render Renderer</cell></row><row><cell>primitive brush</cell><cell>rendered stroke</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 Inference Algorithm of Paint TransformerRequired: A target image I t with shape H ? W ; Stroke Predictor SP ; Stroke Renderer SR. : K = max(argmin K {P ? 2 K ? max(H, W )}, 0); 2: I t = pad(I t , size = (P ? 2 K , P ? 2 K )); 3: I c = blank canvas;4:  for 0 ? k ? K do</figDesc><table><row><cell>5:</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Efficiency of inference and training for different methods.</figDesc><table><row><cell>10] Optim [39]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PaddlePaddle Implementation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Datasets of pictures of natural landscapes. Website</title>
		<ptr target="https://www.kaggle.com/arnaud58/landscape-pictures.7" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cartoongan: Generative adversarial networks for photo cartoonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9465" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesizing programs for images using reinforced adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1666" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A neural representation of sketch drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03477</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Paint by numbers: Abstract image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Haeberli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 17th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Painterly rendering with curved brush strokes of multiple sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 25th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to paint with model-based deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8709" to="8718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Ando</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12057,2020.4</idno>
		<title level="m">Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. Differentiable rendering: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Style transfer by relaxed optimal transport and self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10051" to="10060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking style transfer: From pixels to parameterized brushstrokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Kotovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Heimbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning linear transformations for fast arbitrary style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04537</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Drafting and revision: Laplacian pyramid network for fast high-quality artistic style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5141" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Processing images and video for an impressionist effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Litwinowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 24th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaattn: Revisit attention mechanism in arbitrary neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08410</idno>
		<title level="m">Neural painters: A learned differentiable constraint for generating brushstroke paintings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer with style-attentional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Dae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Hee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5880" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wiki art gallery, inc.: A case for critical thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandy</forename><surname>Mackintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Issues in Accounting Education</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="608" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to cartoonize using white-box cartoon representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8090" to="8099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Artist agent: A reinforcement learning approach to automatic stroke generation in oriental ink painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSAC-TIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1134" to="1144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Apdrawinggan: Generating artistic portrait drawings from face photos with hierarchical gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10743" to="10752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired portrait drawing generation via asymmetric cycle mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8217" to="8225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Strokenet: A neural painting environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingjiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungmoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05977</idno>
		<title level="m">Learning to sketch with deep q networks and demonstrated strokes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Stylized neural painting, 2020. 2, 6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
