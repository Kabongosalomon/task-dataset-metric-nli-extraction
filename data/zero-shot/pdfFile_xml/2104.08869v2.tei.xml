<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ranking Structured Objects with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Damke</surname></persName>
							<email>cdamke@mail.upb.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinz Nixdorf Institute Paderborn University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>H?llermeier</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ranking Structured Objects with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Published at Discovery Science 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph-structured data ? Graph neural networks ? Preference learning ? Learning to rank</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been successfully applied in many structured data domains, with applications ranging from molecular property prediction to the analysis of social networks. Motivated by the broad applicability of GNNs, we propose the family of so-called RankGNNs, a combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are trained with a set of pair-wise preferences between graphs, suggesting that one of them is preferred over the other. One practical application of this problem is drug screening, where an expert wants to find the most promising molecules in a large collection of drug candidates. We empirically demonstrate that our proposed pair-wise RankGNN approach either significantly outperforms or at least matches the ranking performance of the na?ve point-wise baseline approach, in which the LtR problem is solved via GNN-based graph regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bringing a set of objects o 1 , . . . , o N into a particular order is an important problem with many applications, ranging from task planning to recommender systems. In such domains, the criterion defining the underlying order relation typically depends on properties (features) of the objects (for example the price and quality of a product). If the sorting criterion (and hence the relation ) is not explicitly given, one may think of inferring it from exemplary data, often provided in the form of a set of pair-wise orderings o i o j (e.g., representing that the user prefers product o i over product o j ). This gives rise to a machine learning task often referred to as LtR. Thus, the goal is to learn a general ordering strategy (preference model) from sample data of the above kind, which can then be used to sort any new (previously unseen) set of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2104.08869v2 [cs.LG] 11 Oct 2021</head><p>Ranking Structured Objects with Graph Neural Networks -DS <ref type="bibr">'21</ref> While existing state-of-the-art LtR approaches assume that objects o i are represented by feature vectors x i ? R n , in this paper, we will consider the LtR problem for another quite natural and practically important representation, namely the domain of finite graphs. Methods for learning to rank objects represented in the form of graphs can, for example, be used in applications such as drug screening, where the ranked objects are the molecular structures of drug candidates.</p><p>To support the ranking of structured objects such as graphs, existing LtR methods need to be adapted. Previously, Agarwal <ref type="bibr" target="#b0">[1]</ref> has considered the problem of ranking the vertices within a given graph. However, to the best of our knowledge, the graph-wise LtR problem has so far only been described in the context of specific domains, such as drug discovery, where manually chosen graph feature representations were used <ref type="bibr" target="#b1">[2]</ref>. Motivated by the success of GNNs in graph representation learning, we propose a simple architecture that combines GNNs with neural LtR approaches. The proposed approach allows for training ranking functions in an end-to-end fashion and can be applied to arbitrary graphs without the need to manually choose a domainspecific graph feature representation.</p><p>Our neural graph ranking architecture will be introduced in Section 4. Before, the LtR and GNN models that are used in this architecture are described in Section 2 and Section 3, respectively. In Section 5, we evaluate our approach on a selection of graph benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Object Ranking</head><p>LtR approaches are often categorized as point-wise, pair-wise, and list-wise methods. We begin with a short overview of these families. Afterwards, a more in-depth introduction is given to a selection of neural pair-wise approaches that we shall built upon in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of LtR Approaches</head><p>Point-wise methods assume the existence of a (latent) utility function representing the sought preference relation , i.e., that an ordinal or numeric utility score u i ? R can be assigned to</p><formula xml:id="formula_0">each object o i ? O such that ? o i , o j ? O : u i ? u j ? o i o j .</formula><p>Based on training data in the form of exemplary (and possibly noisy) ratings, i.e., object/utility</p><formula xml:id="formula_1">pairs {(x i , u i )} N i=1 ? X ? R, where x i ? X is the feature representation of o i ,</formula><p>the LtR problem can be solved by fitting a model f u : X ? R using standard ordinal or numeric regression methods. Given a new set of objects {o j } M j=1 to be ranked, these objects are then sorted in decreasing order of their estimated utilities f u (o j ). Note that point-wise methods are restricted to linear orders but cannot represent more general relations, such as partial orders.</p><p>Pair-wise methods proceed from training data in the form of a set of ordered object pairs S = {o ai o bi } N i=1 , i.e., relative training information in the form of pair-wise comparisons rather than absolute assessments. Based on such training samples S, the goal is to learn the underlying preference relation . The resulting model f :</p><formula xml:id="formula_2">O ? O ? {0, 1} is a binary classifier, which is supposed to return f (o i , o j ) = 1 iff o i o j .</formula><p>One of the first pair-wise preference methods was the Ranking SVM <ref type="bibr" target="#b2">[3]</ref> -essentially a standard support vector machine (SVM) trained on the differences between vector representations of object preference pairs. Later, Burges et al. <ref type="bibr" target="#b3">[4]</ref> proposed the RankNet architecture, which is also trained using feature vector differences but uses a multilayer perceptron (MLP) instead of an SVM. Since then, multiple extensions of those approaches have been developed <ref type="bibr" target="#b4">[5]</ref>. One commonality between all of them is their training optimization target, namely to minimize the number of predicted inversions, i.e., the number of pairs o i o j with f (o i , o j ) = 0. An important difference between existing pair-wise approaches concerns the properties they guarantee for the learned preference relation; three properties commonly considered are ? reflexivity (?x : x x),</p><p>? antisymmetry (?x, y : x y ? y x), and ? transitivity (?x, y, z : (x y ? y z) ? x z).</p><p>The set of desirable properties depends on the domain. While some approaches guarantee that the learned relation fulfills all three properties <ref type="bibr" target="#b5">[6]</ref>, others, for example, explicitly allow for non-transitivity <ref type="bibr" target="#b6">[7]</ref>.</p><p>Assuming a suitable pair-wise ranking model f was selected and trained, one then typically wants to produce a ranking for some set of objects {o i } M i=1 . To this end, a ranking (rank aggregation) procedure is applied to the preferences predicted for all pairs (o i , o j ). A simple example of such a procedure is to sort objects o i by their Borda count c i = j =i f (o i , o j ), i.e., by counting how often each object o i is preferred over another object. Alternatively, the classifier f can also be used directly as the comparator function in a sorting algorithm; this reduces the number of comparisons from O(M 2 ) to O(M log M ). While the latter approach is much more efficient, it implicitly assumes that f is transitive. The rankings produced by an intransitive sorting comparator are generally unstable, because they depend on the order in which the sorting algorithm compares the objects <ref type="bibr" target="#b7">[8]</ref>. This might not be desirable in some domains.</p><p>List-wise methods generalize the pair-wise setting. Instead of determining the ordering of object pairs, they directly operate on complete rankings (lists) of objects, training a model based on a list-wise ranking loss function. One of the first list-wise losses was proposed by Cao et al. <ref type="bibr" target="#b8">[9]</ref>. Given a set S of objects, their ListNet approach uses a probability distribution over all possible rankings of S and is trained by minimizing the cross-entropy between the model's current ranking distribution and some target distribution. Compared to pair-wise approaches, list-wise methods exhibit a higher expressivity, which can be useful to capture effects such as context-dependence of preferences <ref type="bibr" target="#b9">[10]</ref>. In general, however, if this level of expressiveness is not required, recent results by K?ppel et al. <ref type="bibr" target="#b5">[6]</ref> suggest that the list-wise approaches have no general advantage over the (typically simpler) pair-wise methods. To tackle the graph LtR problem in Section 4, we will therefore focus on the pair-wise approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Pair-wise Ranking Models</head><p>As already stated, we propose a combination of existing LtR methods and GNNs to solve graph ranking problems. Due to the large number of existing LtR approaches, we will however not evaluate all possible combinations with GNNs, but instead focus on the following two representatives:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">DirectRanker [6]:</head><p>A recently proposed generalization of the already mentioned pairwise RankNet architecture <ref type="bibr" target="#b3">[4]</ref>. It guarantees the reflexivity, antisymmetry, and transitivity of the learned preference relation and achieves state-of-the-art performance on multiple common LtR benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CmpNN [7]</head><p>: Unlike DirectRanker, this pair-wise architecture does not enforce transitivity. The authors suggest that this can, for example, be useful to model certain non-transitive voting criteria.</p><p>Formally, the DirectRanker architecture is defined as </p><formula xml:id="formula_3">f DR (o i , o j ) := ? w (h(x i ) ? h(x j )) ,<label>(1)</label></formula><formula xml:id="formula_4">where x i , x j ? R n</formula><formula xml:id="formula_5">(f DR u (x i ) ? f DR u (x j )), with f DR u (x) := w h(x)</formula><p>. DirectRanker therefore effectively learns an object utility</p><formula xml:id="formula_6">function f DR u and predicts o i o j iff f DR u (x i ) ? f DR u (x j )</formula><p>. Thus, the learned preference relation f DR directly inherits the reflexivity, antisymmetry and transitivity of the ? relation. The main difference between DirectRanker and a point-wise regression model is that DirectRanker learns f DR u indirectly from a set of object preference pairs. Consequently, DirectRanker is not penalized if it learns some order-preserving transformation of f DR u . We will come back to this point in Section 5.3.</p><p>Let us now look at the so-called Comparative Neural Network (CmpNN) architecture, which generalizes the DirectRanker approach. The main difference between both is that CmpNN does not implicitly assign a score f u (x i ) to each object o i . This allows it to learn non-transitive preferences. CmpNNs are defined as follows:</p><formula xml:id="formula_7">f Cmp (o i , o j ) := ?(z ? z ), with (2) z := ? (w 1 z 1 + w 2 z 2 + b ), z 1 := ? (W 1 x i + W 2 x j + b), z := ? (w 2 z 1 + w 1 z 2 + b ), z 2 := ? (W 2 x i + W 1 x j + b).</formula><p>Here, w 1 , w 2 ? R d and W 1 , W 2 ? R d?n are shared weight matrices, b, b bias terms, and ?, ? activation functions. Intuitively, z ? R and z ? R can be interpreted as weighted votes towards the predictions o i o j and o j o i , respectively. A CmpNN will simply choose the alternative with the largest weight. The key idea behind the definitions in (2) is that the pairs z , z and z 1 , z 2 will swap values when swapping the compared objects o i , o j . Consequently, f Cmp must be reflexive and antisymmetric [see 7]. If we set W 1 = w 1 = 0, the voting weights z , z ? R reduce to the predictions of a standard MLP h with the input o i and o j , respectively, i.e., z = h(x i ) and z = h(x j ). In this case, the CmpNN effectively becomes a DirectRanker model. By choosing non-zero weights for W 1 and w 1 , the model can however also learn nontransitive dependencies between objects. In fact, Rigutini et al. have shown that CmpNNs are able to approximate almost all useful pair-wise preference relations [7, Thm. 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Neural Networks</head><p>Over the recent years, GNNs have been successfully employed for a variety of graph ML tasks, with applications ranging from graph classification and regression to edge prediction and graph synthesis. Early GNN architectures were motivated by spectral graph theory and the idea of learning eigenvalue filters of graph Laplacians <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Those spectral GNNs take a graph G = (V, E) with vertex feature vectors x i ? R n as input and iteratively transform those vertex features by applying a filtered version of the Laplacian L of G. Formally, the filtered Laplacian is defined asL = U g(?)U , where L = U ?U is an eigendecomposition of L and g is a learned eigenvalue filter function that can amplify or attenuate the eigenvectors U . Intuitively, spectral GNNs learn which structural features of a graph are important and iteratively aggregate the feature vectors of the vertices that are part of a common important structural graph feature. Each of those aggregations is mathematically equivalent to a convolution operation. This is why they are referred to as (graph) convolution layers.</p><p>One important disadvantage of spectral convolutions is their computational complexity, making them especially unsuitable for large graphs. To overcome this limitation, Kipf and Welling proposed the so-called graph convolutional network (GCN) architecture <ref type="bibr" target="#b12">[13]</ref>, which restricts the eigenvalue filters g to be linear. As a consequence of this simplification, only adjacent vertices need to be aggregated in each convolution. Formally, the simplified GCN convolution can be expressed as follows:</p><formula xml:id="formula_8">x i = ? ? ? W ? ? ? ii x i + vj ??(vi) ? ij x j ? ? ? ?<label>(3)</label></formula><p>Here, x i , x i ? R d are the feature vectors of v i ? V before and after applying the convolution, ?(v i ) is the set of neighbors of v i , W ? R d?n is a learned linear operator representing the filter g, ? some activation function, and ? ii , ? ij ? [0, 1] normalization terms that will not be discussed here. After applying a series of such convolutions to the vertices of a graph, the resulting convolved vertex features can be used directly to solve vertex-level prediction tasks, e.g. vertex classification. To solve graph-level problems, such as graph classification or graph ranking, the vertex features must be combined into a single graph vector representation. This is typically achieved via a pooling layer, which could, for example, simply compute the componentwise mean or sum of all vertex features. More advanced graph pooling approaches use sorting or attention mechanisms in order to focus on the most informative vertices <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Xu et al. <ref type="bibr" target="#b15">[16]</ref> show that restricting the spectral filter g to be linear not only reduces the computational complexity but also the discriminative power of the GCN architecture. More precisely, they prove that any GNN using a vertex neighborhood aggregation scheme such as (3) can at most distinguish those graphs that are distinguishable via the so-called 1-dimensional Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b16">[17]</ref>. GCNs do, in fact, have a strictly lower discriminative power than 1-WL, i.e., there are 1-WL distinguishable graphs, which will always be mapped to the same graph feature vector by a GCN model. In addition to this bound, Xu et al. <ref type="bibr" target="#b15">[16]</ref> also propose the graph isomorphism network (GIN) architecture, which is able to distinguish all 1-WL distinguishable graphs. Recently, multiple approaches going beyond the 1-WL bound have been proposed. The so-called 2-WL-GNN architecture, for example, is directly based on the 2-dimensional (Folklore) WL test <ref type="bibr" target="#b17">[18]</ref>. Other current approaches use higher-order substructure counts <ref type="bibr" target="#b18">[19]</ref> or so-called k-order invariant networks <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Graph Ranking</head><p>To tackle the graph LtR problem, we propose the family of RankGNN models. A RankGNN is a combination of a GNN and one of the existing neural LtR methods. The GNN component is used to embed graphs into a feature space. The embedded graphs can then be used directly as the input for a comparator network, such as DirectRanker <ref type="bibr" target="#b5">[6]</ref> or CmpNN <ref type="bibr" target="#b6">[7]</ref>. Formally, a RankGNN is obtained by simply using a GNN to produce the feature vectors x i , x j in <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula">(2)</ref> for a given pair of graphs G i , G j . Since all components of such a combined model are differentiable, the proposed RankGNN architecture can be trained in an end-to-end fashion.</p><p>Despite the simplicity of this approach, there are a few details to consider when implementing it; these will be discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Efficient Batching for RankGNNs</head><p>In the existing neural LtR approaches for objects o i that are represented by features x i ? R n , efficient batch training is possible by encoding a batch of k relations {o ai o bi } k i=1 with two matrices</p><formula xml:id="formula_9">A := xa 1 . . . xa k ? R k?n , B := x b 1 . . . x b k ? R k?n and using Y := 1 . . . 1 ? R k</formula><p>as the target prediction of the model. However, this approach is suboptimal in the graph LtR setting. Given the relations {G 0 G 1 , G 1 G 2 }, the graph G 1 would for example have to be encoded twice. When dealing with datasets that consist of possibly large graphs, such redundant encodings quickly become infeasible due to the additional memory and runtime requirements incurred by the GNN having to embed the same graph multiple times. To prevent this redundancy, each graph occurring on the left or the right side of a relation should instead only be encoded once as part of a single graph batch. This graph batch can be fed directly into a GNN to produce a matrix Z of graph feature embeddings. The individual graph relation pairs G i G j can then be simply represented as pairs of indices (i, j) pointing to the corresponding feature vectors in the embedding matrix Z. Using those pointers, the graph vector representations for each pair can be looked up in Z. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sorting Graphs with RankGNNs</head><p>After training a RankGNN model using a set of graph relation pairs, the model can be used to compare arbitrary graph pairs. Following the approach of K?ppel et al. <ref type="bibr" target="#b5">[6]</ref> and Rigutini et al. <ref type="bibr" target="#b6">[7]</ref>, a set of graphs can then be ordered by using the RankGNN as the comparator function in a standard sorting algorithm. We propose a simple parallelized quicksort-based scheme to sort graphs. When implementing a RankGNN model on a parallel compute device, such as a GPU, there is a constant time overhead for each call to the model. To reduce the total cost of this call overhead, we suggest that all pivot comparison queries in one layer of the recursive quicksort call tree should be evaluated by the RankGNN in parallel. Using this parallel comparison approach, only one model invocation is required for each layer of the call tree, i.e., the asymptotic model call overhead for sorting n graphs is in O(log n). Additionally, a more efficient approach is available for DirectRanker-based models. There, the implicitly learned utility function f DR u can be computed directly for a set of graphs. A standard sorting algorithm can then be applied without any further calls to the model, which reduces the call overhead to O(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>To evaluate the family of RankGNNs described in Section 4, we choose six different combinations of GNNs and comparator networks. The evaluated graph embedding modules are GCN <ref type="bibr" target="#b12">[13]</ref>, GIN <ref type="bibr" target="#b15">[16]</ref>, and 2-WL-GNN <ref type="bibr" target="#b17">[18]</ref>. Those three GNN methods are combined with the previously described DirectRanker <ref type="bibr" target="#b5">[6]</ref> and the CmpNN <ref type="bibr" target="#b6">[7]</ref> comparator. Because there are currently no common graph ranking benchmark datasets, we instead convert a selection of graph regression benchmarks into ranking problems by interpreting the numeric regression targets as utility values, which are used to determine the target orderings. The following five graph regression datasets are used:</p><p>1. TRIANGLES: This is a synthetic dataset that we created. It consists of 778 randomly sampled graphs, each of which contains 3 to 85 unlabeled vertices. The regression target is to learn how many triangles, i.e. 3-cliques, a given graph contains. The triangle counts in the sampled graphs vary between 0 and 9. The sampled graphs are partitioned into 80%/10%/10% training/validation/test splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OGB-molesol, -mollipo and -molfreesolv:</head><p>These three datasets are provided as part of the Open Graph Benchmark (OGB) project <ref type="bibr" target="#b20">[21]</ref>. They contain 1128, 4200, and 642 molecular structure graphs, respectively. The regression task is to predict the solubility of a molecule in different substances. We use the dataset splits that are provided by OGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ZINC:</head><p>This dataset contains the molecular structures of 250k commercially available chemicals from the ZINC database <ref type="bibr" target="#b21">[22]</ref>. The regression task is to predict the so-called octanol-water partition coeffients. We use the preprocessed and presplit graphs from the TUDataset collection <ref type="bibr" target="#b22">[23]</ref>.</p><p>To train the proposed pair-wise graph ranking network architecture, a subset of graph pairs from the training split is sampled uniformly at random. The size of a training sample is M = ?N , where N is the number of graphs in the training split of a dataset and ? ? R + is a constant factor. We use a sampling factor of ? = 20 for all datasets except ZINC, where we use ? = 3 due to the large number of graphs in the training split (N ZINC = 220011, whereas e.g. N OGB-mollipo = 3360). This sampling strategy guarantees that each training graph occurs in at least one sampled pair with a probability of at least 1 ? e ?2? ; thus, for both ? = 20 and even ? = 3, all graphs are considered with high probability (&gt; 99.75%).</p><p>In addition to the six pair-wise RankGNN model variants, we also evaluate the ranking performance of standard point-wise GNN graph regression models, which are trained directly on graph utility values. We use two different target graph utilities: The original regression target y i ? R for each training graph G i , and the normalized graph rankr i ? [0, 1], i.e. the normalized ordinal index of each training graph G i when sorted by y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate the performance of the different RankGNN variants via Kendall's ? B rank correlation coefficient. Given two graph rankings r 1 : G ? N, r 2 : G ? N, this coefficient is defined as</p><formula xml:id="formula_10">? B := C ? D (C + D + T 1 )(C + D + T 2 ) ,</formula><p>where C is the number of concordant pairs</p><formula xml:id="formula_11">{{G i , G j } | i = j ? r 1 (G i ) &lt; r 1 (G j ) ? r 2 (G i ) &lt; r 2 (G j )} ,</formula><p>D is the number of discordant pairs</p><formula xml:id="formula_12">{{G i , G j } | i = j ? r 1 (G i ) &lt; r 1 (G j ) ? r 2 (G i ) &gt; r 2 (G j )} ,</formula><p>and T 1,2 are the numbers of tied graph pairs, which have the same rank in r 1 and r 2 , respectively. Kendall's ? B rank coefficient ranges between ?1 and +1, where ? B = +1 indicates that the two compared rankings are perfectly aligned, whereas ? B = ?1 means that one rankings is the reversal of the other.</p><p>Another commonly used metric in the LtR literature is the normalized discounted cummulative gain (NDCG), which penalizes rank differences at the beginning of a ranking more than differences at the end. This is motivated by the idea that typically only the top-k items in a ranking are of interest.We do not employ the NDCG metric because this motivation does not hold for the used target graph rankings. Since the target rankings are derived from regression targets, such as the water solubility of a molecule, both, the beginning and the end of a ranking are of interest and should therefore be weighted equally.</p><p>To train the evaluated point-and pair-wise models, we use the standard Adam optimizer <ref type="bibr" target="#b23">[24]</ref>. The mean squared error (MSE) loss is used for the point-wise regression models, while the pairwise variants of those GNNs are optimized via binary cross-entropy. All models were tuned via a simple hyperparameter grid search over the following configurations:</p><p>1. Layer widths: {32, 64}. The width of both, the convolutional layers, as well as the fully-connected MLP layers that are applied after graph pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Number of graph convolutions: {3, 5}.</head><p>A fixed number of two hidden layers was used for the MLP that is applied after the pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pooling layers:</head><p>{mean, sum, softmax}. Here, "mean" and "sum" refer to the standard arithmetic mean and sum operators, as described by Xu et al. <ref type="bibr" target="#b15">[16]</ref>, while "softmax" refers to the weighted mean operator described by Damke et al. <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning rates: {10</head><formula xml:id="formula_13">?2 , 10 ?3 , 10 ?4 }.</formula><p>We used standard sigmoid activations for all models and trained each hyperparameter configuration for up to 2000 epochs with early stopping if the validation loss did not improve by at least 10 ?4 for 100 epochs. The configuration with the highest ? B coefficient on the validation split was chosen for each model/dataset pair. To account for differences caused by random weight initialization, the training was repeated three times; 10 repeats were used for the TRIANGLES dataset due to its small size and fast training times. Note that, depending on the type of GNN, the pair-wise models can have between 3% and 10% more trainable weights than their point-wise counterparts, due to the added comparator network. All models were implemented in Tensorflow and trained using a single Nvidia GTX 1080Ti GPU. The code is available on GitHub 1 . <ref type="table" target="#tab_0">Table 1</ref> shows the ranking performance of the evaluated point-and pair-wise approaches on the test splits of the previously described benchmark datasets. Each group of rows corresponds to one of the three evaluated GNN variants. The first two rows in each group show the results for the point-wise models that are trained directly on the original regression targets and on the normalized ranks, respectively. The last two rows in each group hold the results for the pair-wise DirectRanker-and CmpNN-based models. Generally speaking, the pair-wise approaches either significantly outperform or at least match the performance of the point-wise regression models. The most significant performance delta between the point-and pair-wise approaches can be observed on the ZINC and OGB-mollipo datasets. Only on the OGB-molesol dataset, the point-wise models achieve a slightly higher average ? B value than the pair-wise models, which is however not significant when considering the standard deviations. Overall, we find that the pair-wise rank loss that directly penalizes inversions is much better suited for the evaluated graph ranking problems than the point-wise MSE loss.  0.972 ? 0.017 0.720 ? 0.019 0.332 ? 0.083 0.524 ? 0.020 0.810 ? 0.003 DirectRanker 1.000 ? 0.000 0.745 ? 0.009 0.505 ? 0.012 0.525 ? 0.010 0.894 ? 0.008 CmpNN 1.000 ? 0.000 0.718 ? 0.020 0.503 ? 0.010 0.527 ? 0.064 0.873 ? 0.002</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion of Results</head><p>Comparing the two evaluated variants of point-wise regression models, we find that the ones trained on normalized graph ranks generally either have a similar or significantly better ranking performance than the regression models with the original targets. We will come back to this difference in Section 5.3.</p><p>Looking at the results for the synthetic TRIANGLES dataset, we find that only the higher-order 2-WL-GNN is able to reliably rank graphs by their triangle counts. This is plausible, because architectures bounded by the 1-WL test, such as GCN and GIN, are unable to detect cycles in graphs <ref type="bibr" target="#b24">[25]</ref>. While both the point-and the pair-wise 2-WL-GNN models achieve perfect or near-perfect ? B scores on this task, the pair-wise approaches did perform more consistently, without a single inversion on the test graphs over 10 iterations of retraining.</p><p>Since the target graph rankings for all evaluated datasets are derived from regression values, all models have to learn a transitive preference relation. Consequently, the ability of CmpNN-based RankGNNs to learn non-transitive preferences is, in theory, not required to achieve optimal ranking performance. If the sample size of training graph pairs is too small, such that it contains few transitivity-indicating subsets, e.g. {G 1 G 2 , G 2 G 3 , G 1 G 3 }, the higher expressiveness of CmpNNs could even lead to overfitting and therefore worse generalization performance compared to DirectRanker. Nonetheless, with the used sampling factor of ? = 20 (and ? = 3 for ZINC), each graph is, in expectation, sampled 40 times (6 for ZINC). This appears to be sufficient to prevent overfitting. In fact, the CmpNN-based RankGNNs perform very similarly to their DirectRanker-based counterparts. However, since DirectRanker-based models allow for a more efficient sorting implementation than CmpNN-based ones (cf. Section 4.2), we suggest the use of DirectRanker for problems where transitivity can be assumed. For each dataset, we plot the predicted utilities of the GNN architecture that achieved the best point-wise ranking performance in <ref type="table" target="#tab_0">Table 1</ref>, i.e. 2-WL-GNN for OGB-molesol and -molfreesolv and GIN for OGB-mollipo. Each point along the horizontal axes corresponds to a graph in the training split of a dataset. The graphs are sorted in ascending order by the ground truth utility values (shown in black) from which the target rankings are derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of the Implicit Utilities of DirectRanker GNNs</head><p>As described in Section 2. As expected, the blue utility curves of the point-wise approaches align with the black target utility curves, while the gray curve more closely follows the 45?diagonal line on which the normalized graph ranks would lie. However, this alignment does not necessarily imply good ranking performance. For example, on the OGB-molfreesolv dataset, the blue utility curve of the point-wise 2-WL-GNN model fits the black target curve fairly well for the graphs in the middle of the ranking. However, near the low and the high graph ranks, the target curve abruptly falls/rises to its minimum and maximum values; the point-wise regression model that is trained on the original utilities ignores those outliers. By instead training a point-wise model on the normalized ranks, outliers in the original utility values are effectively smoothed out, as can be seen in the gray OGB-molfreesolv utility curve. Looking at <ref type="table" target="#tab_0">Table 1</ref>, we find that this corresponds to a significantly higher mean ? B coefficient and a lower variance on the OGB-molfreesolv dataset. The pair-wise DirectRanker-based approach solves the problem of outliers in a more general fashion. It uses a loss function that does not penalize for learning a monotonous, rank-preserving transformation of the target utility curve. This allows it to effectively "stretch" the target utilities into a linearly growing curve with fewer abrupt changes, which results in a similar performance to that of the regression model trained on normalized ranks.</p><p>The target utilities of the OGB-molesol dataset are distributed more smoothly, without any outliers. There the advantage of approaches that work well with outliers (e.g. pair-wise models) over the ones that do not is less pronounced. Lastly, looking at the OGB-mollipo dataset, we also do not find outliers in the target utility curve. However, there the pair-wise RankGNN models perform significantly better than the point-wise approaches. The reason for this performance difference is not yet fully understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we addressed the problem of learning to rank graph-structured data and proposed RankGNNs, a combination of neural pair-wise ranking models and GNNs. When compared with the na?ve approach of using a point-wise GNN regression model for ranking, we found that RankGNNs achieve a significantly higher or at least similar ranking performance on a variety of synthetic and real-world graph datasets. We therefore conclude that RankGNNs are a promising approach for solving graph ranking problems.</p><p>There are various directions for future research. First, due to the lack of graph ranking benchmark datasets, we had to use graph regression datasets in our evaluation instead. For a more thorough analysis of the practical applicability of graph ranking models, a collection of real-world graph ranking benchmarks should be created. One potential benchmark domain could, for example, be the drug screening problem we described in the introduction, where the training data consists of drug candidate pairs ranked by a human expert.</p><p>Second, list-wise graph ranking approaches could be evaluated in addition to the point-and pair-wise models considered in this paper. Such list-wise models can be useful to learn a human's individual preferences for structured objects, such as task schedules or organizational hierarchies, represented as directed acyclic graphs or trees, respectively. A list-wise ranking approach [e.g. 10] would be able to consider context-dependent preferences in such scenarios <ref type="bibr" target="#b25">[26]</ref>. Yet another interesting idea, motivated by the behavior we observed for the point-and pair-wise 2-WL-GNNbased models on the OGB-molfreesolv dataset (cf. <ref type="figure" target="#fig_3">Figure 2)</ref>, is a hybrid approach that combines regression and ranking, that is, point-wise and pair-wise learning <ref type="bibr" target="#b26">[27]</ref>.</p><p>Third, although graph neural networks are quite popular these days, the problem of graph ranking could also be tackled by well-established kernel-based methods. In the past, there has been a lot of work on graph kernels <ref type="bibr" target="#b27">[28]</ref>, making graph-structured data amenable to kernel-based learning methods. In principle, one may hence think of combining graph kernels with learning-to-rank methods such as RankSVM. However, our first experiences with an approach of that kind suggest that kernel-based approaches are computationally complex and do not scale sufficiently well, even for point-wise implementations -for larger data sets, the running time as well as the memory requirements are extremely high (which is also the reason why we excluded them from the experiments). Although they can be reduced using suitable approximation techniques, complexity clearly remains an issue. Besides, the ranking performance turned out to be rather poor. For pair-wise approaches, not only the complexity further increases, but the problem also becomes conceptually non-trivial. This is because the simple reduction of ranking to classification, on which RankSVM is based, no longer works (this reduction takes differences between feature vectors, an operation that cannot be applied to graphs). Instead, a (preference) kernel function on pairs of pairs of objects, i.e. on quadruples, has to be used <ref type="bibr" target="#b28">[29]</ref>. Nevertheless, this does of course not exclude the existence of more efficient (approximate) algorithms operating on kernel-representation for graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are feature vectors representing the compared objects o i , o j , the function h : R n ? R d being a standard MLP, w ? R d a learned weight vector and an activation function ? : R ? R such that ?(?x) = ??(x) and sign(x) = sign(?(x)) for all x ? R. One could, for example, use ? = tanh and interpret negative outputs of f DR (o i , o j ) as o j o i and positive outputs as o io j . This model can be trained in an end-to-end fashion using gradient descent with the standard binary cross-entropy loss. Note that f DR can be rewritten as ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>General architecture of the proposed family of RankGNNs. Here the common sparse adjacency representation for message-passing GNNs is shown; different types of graph batch encodings can of course also be used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>GCN</head><label></label><figDesc>Utility regr.0.273 ? 0.004 0.706 ? 0.001 0.232 ? 0.002 0.015 ? 0.242 0.547 ? 0.414 Rank regr.0.172 ? 0.040 0.702 ? 0.012 0.224 ? 0.006 0.446 ? 0.038 0.823 ? 0.005 DirectRanker 0.234 ? 0.002 0.714 ? 0.003 0.327 ? 0.006 0.483 ? 0.011 0.879 ? 0.002CmpNN 0.195 ? 0.007 0.632 ? 0.076 0.381 ? 0.008 0.351 ? 0.055 0.819 ? 0.002 GIN Utility regr. 0.469 ? 0.056 0.729 ? 0.006 0.353 ? 0.052 0.243 ? 0.125 0.790 ? 0.003 Rank regr. 0.481 ? 0.014 0.717 ? 0.011 0.310 ? 0.017 0.495 ? 0.021 0.827 ? 0.011 DirectRanker 0.502 ? 0.028 0.712 ? 0.007 0.429 ? 0.026 0.439 ? 0.065 0.894 ? 0.012 CmpNN 0.520 ? 0.070 0.710 ? 0.007 0.506 ? 0.013 0.518 ? 0.018 0.891 ? 0.006 2-WL Utility regr. 0.997 ? 0.006 0.747 ? 0.007 0.318 ? 0.017 0.379 ? 0.207 0.803 ? 0.006 Rank regr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Normalized learned utility values of the point-wise GNN regression model trained on the original utilities (in blue), the point-wise model trained on normalized ranks (in gray) and the pair-wise DirectRanker model (in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2, a DirectRanker model f DR : O ? O ? {0, 1} implicitly learns a utility function f DR u : O ? R from the set of pairs it sees during training. We will now take a closer look at this implicitly learned utility function f DR u and compare it to the explicitly learned utilities f util. u and f rank u of the point-wise GNN regression models. Figure 2 shows the values of all three, f DR u (in red), f util. u (in blue) and f rank u (in gray), normalized to the unit interval. Any monotonically increasing curve corresponds to a perfect ranking (? B = +1), while a monotonically decreasing curve would signify an inverse ranking (? B = ?1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean Kendall's ? B coefficients with standard deviations for the rankings produced by point-and pair-wise models on unseen test graphs.</figDesc><table><row><cell>TRIANGLES OGB-molesol</cell><cell>-mollipo</cell><cell>-molfreesolv</cell><cell>ZINC</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Cortys/rankgnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to rank on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5185-8</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="333" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When drug discovery meets web search: Learning to rank for ligand-based virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/775047.775067</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;02</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;02</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
		<idno type="DOI">10.1145/1102351.1102363</idno>
		<editor>I</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMART: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010-82</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>K?ppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Segner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wagener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Pensel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Karwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD 2019</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11908</biblScope>
			<biblScope unit="page" from="237" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SortNet: Learning to rank by a neural preference function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Papini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnn.2011.2160875</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1368" to="1380" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ranking distributions based on noisy sorting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Mesaoudi-Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?llermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML 2018, 35th International Conference on Machine Learning</title>
		<meeting>ICML 2018, 35th International Conference on Machine Learning<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3469" to="3477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to rank. In I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273513</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep architectures for learning context-dependent ranking functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karlson</forename><surname>Pfannschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritha</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>H?llermeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6661" to="6670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>F?rer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel higher-order Weisfeiler-Lehman graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Damke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitalik</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>H?llermeier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Asian Conference on Machine Learning (ACML 2020</title>
		<meeting>the 12th Asian Conference on Machine Learning (ACML 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">129</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ZINC 15 -ligand discovery for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.5b00559</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the Combinatorial Power of the Weisfeiler-Lehman Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>F?rer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Puto</surname></persName>
		</author>
		<idno type="DOI">10.1086/208899</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combined regression and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Kernel-based learning methods for preference aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Baets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boullart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="189" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
