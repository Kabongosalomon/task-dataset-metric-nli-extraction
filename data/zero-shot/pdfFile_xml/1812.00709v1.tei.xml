<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Shabat</forename><surname>Yizhak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engineering</forename><surname>Mechanical</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mechanical</forename><surname>Engineering</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a normal estimation method for unstructured 3D point clouds. This method, called Nesti-Net, builds on a new local point cloud representation which consists of multi-scale point statistics (MuPS), estimated on a local coarse Gaussian grid. This representation is a suitable input to a CNN architecture. The normals are estimated using a mixtureof-experts (MoE) architecture, which relies on a datadriven approach for selecting the optimal scale around each point and encourages sub-network specialization. Interesting insights into the network's resource distribution are provided. The scale prediction significantly improves robustness to different noise levels, point density variations and different levels of detail. We achieve state-of-the-art results on a benchmark synthetic dataset and present qualitative results on real scanned scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Commodity 3D sensors are rapidly becoming an integral part of autonomous systems. These sensors, e.g. RGB-D cameras or LiDAR, provide a 3D point cloud representing the geometry of the scanned objects and surroundings. This raw representation is challenging to process since it lacks connectivity information or structure, and is often incomplete, noisy and contains point density variations. In particular, processing it by means of the highly effective convolutional neural networks (CNNs) is problematic because CNNs require structured, grid-like data as input.</p><p>When available, additional local geometric information, such as the surface normals at each point, induces a partial local structure and improves performance of different tasks such as over-segmentation <ref type="bibr" target="#b2">[3]</ref>, classifica-tion <ref type="bibr" target="#b20">[21]</ref> and surface reconstruction <ref type="bibr" target="#b10">[11]</ref>.</p><p>Estimating the normals from a raw, point-only, cloud, is a challenging task due to difficulties associated with sampling density, noise, outliers, and detail level. The common approach is to specify a local neighborhood around a point and to fit a local basic geometric surface (e.g. a plane) to the points in this neighborhood. Then the normal is estimated from the fitted geometric entity. The chosen size (or scale) of the neighborhood introduces an unavoidable tradeoff between robustness to noise and accuracy of fine details. A large-scale neighborhood over-smoothes sharp corners and small details but is otherwise robust to noise. A small neighborhood, on the other hand, may reproduce the normals more accurately around small details but is more sensitive to noise. Thus it seems that an adaptive, data-driven scale may improve estimation performance.</p><p>We propose a normal estimation method for unstructured 3D point clouds. It features a mixtureof-experts network for scale prediction, which significantly increases its robustness to different noise levels, outliers, and varying levels of detail. In addition, this method overcomes the challenge of feeding point clouds into CNNs by extending the recently proposed 3D modified Fischer Vector (3DmFV) representation <ref type="bibr" target="#b3">[4]</ref> to encode local geometry on a coarse multi-scale grid. It outperforms state-of-the-art methods for normal vector estimation. The main contributions of this paper are:</p><p>? A new normal estimation method for unstructured 3D point clouds based on mixture of experts and scale prediction.</p><p>? A local point representation which can be used as input to a CNN. For each point in a given point cloud, we compute a multi-scale point statistics representation <ref type="bibr">(MuPS)</ref>. Then, a scale manager network is used to determine the optimal scale and uses the corresponding expert sub-network to estimate the normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related-work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep learning for unstructured 3D point clouds</head><p>The point cloud representation is challenging for deep learning methods because it is both unstructured and point-wise unordered. In addition, the number of points in the point cloud is usually not constant. Several methods were proposed to overcome these challenges. Voxel-based methods embed the point cloud into a voxel grid but suffer from several accuracycomplexity tradeoffs <ref type="bibr" target="#b15">[16]</ref>. The PointNet approach <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> applies a symmetric, order-insensitive, function on a high-dimensional representation of individual points. The Kd-Network <ref type="bibr" target="#b13">[14]</ref> imposes a kd-tree structure on the points and uses it to learn shared weights for nodes in the tree. The recently proposed 3DmFV <ref type="bibr" target="#b3">[4]</ref> represents the points by their deviation from a Gaussian Mixture Model (GMM) whose Gaussians are uniformly positioned on a coarse grid.</p><p>In this paper, we propose a point-wise and multiscale variation of 3DmFV. Instead of generating a structured representation for the entire point cloud, we represent each point and its neighbors within several scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Normal estimation</head><p>A classic method for normal estimation uses Principal Component Analysis (PCA) <ref type="bibr" target="#b11">[12]</ref>. It first specifies the neighbors within some scale, and then uses PCA regression to estimate a tangent plane. Variants fitting local spherical surfaces <ref type="bibr" target="#b9">[10]</ref> or jets <ref type="bibr" target="#b6">[7]</ref> (truncated Taylor expansion) were also proposed. To be robust to noise, these methods usually choose a large-scale neighborhood, leading them to smooth sharp features and to fail in estimating normals near edges. Computing the optimal neighborhood size can decrease the estimation error <ref type="bibr" target="#b17">[18]</ref> but requires the (usually unknown) noise standard deviation value and a costly iterative process to estimate the local curvature and additional density parameters.</p><p>Other approaches rely on using Voronoi cells of point clouds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. These methods are characterized by robustness to sharp features but are sensitive to noise. To overcome this challenge, Alliez et al. <ref type="bibr" target="#b0">[1]</ref> proposed PCA-Voronoi approach to create cell sets which group adjacent cells to provide some control over smoothness. While many of these methods hold theoretical guarantees on approximation and robustness, in practice they rely on a preprocessing stage in the presence of strong or unstructured noise in addition to a fine-tuned set of parameters.</p><p>A few deep learning approaches have been proposed to estimate normal vectors from unstructured point clouds. Boulch et al. proposed to transform local point cloud patches into a 2D Hough space accumulator by randomly selecting point triplets and voting for that plane's normal. Then, the normal is estimated from the accumulator by designing explicit criteria <ref type="bibr" target="#b4">[5]</ref> for bin selection or, more recently, by training a 2D CNN <ref type="bibr" target="#b5">[6]</ref> to estimate it continuously as a regression problem. This method does not fully utilize the 3D information since it loses information during the transformation stage. We reffer to this method as HoughCNN in the evaluation section. A more recent method, PCPNnet <ref type="bibr" target="#b10">[11]</ref>, uses a PointNet <ref type="bibr" target="#b19">[20]</ref> architecture on local point neighborhoods of multiple scales. It achieves good normal estimation performance and has been extended to estimating other surface properties. However, it processes the multi-scale point clouds jointly and does not select an optimal scale. This type of architecture tends to encourage averaging during training rather than specialization <ref type="bibr" target="#b12">[13]</ref>.</p><p>In this paper we propose a method that approximates the local normal vector using a point-wise, multiscale 3DmFV representation which serves as an input to a deep 3D CNN architecture. In addition, we learn the neighborhood size that minimizes the normal estimation error using a mixture of experts (MoE) <ref type="bibr" target="#b12">[13]</ref>, which encourages specialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Representing point clouds using 3DmFV</head><p>The 3DmFV representation for point clouds <ref type="bibr" target="#b3">[4]</ref> achieved good results for point cloud classification using a 3D CNN. See Section 3.1 for details. In this paper we propose the Multi-scale Point Statistics (MuPS) representation, which extends the 3DmFV and computes a point-wise multi-scale 3DmFV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The proposed method is outlined in <ref type="figure" target="#fig_0">Figure 1</ref>. It receives a 3D point cloud as input and consists of two main stages. In the first stage, we compute a multiscale point representation, denoted MuPS. In the second stage we feed it into a mixture-of-experts (MoE) CNN architecture and estimate the normal at each point as output. The stages are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MuPS -Multi-scale point statistics</head><p>MuPS is a local multi-scale representation which computes point statistics on a coarse Gaussian grid. It builds on the well-known Fisher Vector (FV) <ref type="bibr" target="#b21">[22]</ref>, and the recently proposed 3DmFV representation <ref type="bibr" target="#b3">[4]</ref>. Therefore, we first outline the FV and the 3DmFV, and then continue to the MuPS representation and its attractive properties. FV and 3DmFV for 3D point clouds: Given a set of T 3D points X = {p t ? R 3 , t = 1, ...T } and a set of parameters for a K component GMM, ? = {(w k , ? k , ? k ), k = 1, ...K}, where w k , ? k , ? k are the mixture weight, center, and covariance matrix of the k-th Gaussian. The likelihood of a single 3D point p associated with the k-th Gaussian density is</p><formula xml:id="formula_0">u k (p) = 1 (2?) D/2 |? k | 1/2 exp ? 1 2 (p ? ? k ) ? ?1 k (p ? ? k ) .<label>(1)</label></formula><p>Therefore, the likelihood of a single point associated with the GMM density is:</p><formula xml:id="formula_1">u ? (p) = K k=1 w k u k (p).<label>(2)</label></formula><p>The 3DmFV uses a uniform GMM on a coarse m ? m ? m 3D grid, where m is an adjustable parameter usually chosen to be from m = 3 to 8. The weights are set to be w k = 1 K , the standard deviation is set to be ? k = 1 m , and the covariance matrix to be ? k = ? k I. Although the parameters in GMMs are usually set using maximum likelihood estimation, here uniformity is crucial for shared weight filtering (convolutions).</p><p>The FV is expressed as the sum of normalized gradients for each point p t . The 3DmFV is specified similarly using additional symmetric functions, i.e. min and max. They are symmetric in the sense proposed in <ref type="bibr" target="#b19">[20]</ref> and are therefore adequate for representing the structureless and orderless set of points. Adding these functions makes the representation more informative and the associated classification more accurate <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_2">G X F V ? = T t=1 L ? ? ? log u ? (p t ),<label>(3)</label></formula><formula xml:id="formula_3">G X 3DmF V ? = ? ? ? T t=1 L ? ? ? log u ? (p t ) ?=?,?,? max t (L ? ? ? log u ? (p t )| ?=?,?,? min t (L ? ? ? log u ? (p t ))| ?=?,? ? ? ? (4)</formula><p>where L ? is the square root of the inverse Fisher Information Matrix, and the normalized gradients are:</p><formula xml:id="formula_4">G X ? k = 1 ? w k T t=1 (? t (k) ? w k ),<label>(5)</label></formula><formula xml:id="formula_5">G X ? k = 1 ? w k T t=1 ? t (k) p t ? ? k ? k ,<label>(6)</label></formula><formula xml:id="formula_6">G X ? k = 1 ? 2w k T t=1 ? t (k) (p t ? ? k ) 2 ? 2 k ? 1 .<label>(7)</label></formula><p>Here, we follow <ref type="bibr" target="#b14">[15]</ref> and ensure that u ? (x) is a valid distribution by changing the variable w k to ? k to simplify the gradient calculation using :</p><formula xml:id="formula_7">w k = exp(? k ) K j=1 exp(? j ) .<label>(8)</label></formula><p>In addition, ? t (k) expresses the soft assignment of point p t to Gaussian k, as obtained from the derivatives:</p><formula xml:id="formula_8">? t (k) = w k u k (p t ) K j=1 w j u j (p t ) .<label>(9)</label></formula><p>The FV and 3DmFV are normalized by the number of points in order to be sample-size independent <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_9">G X F V ? ? 1 T G X F V ? , G X 3DmF V ? ? 1 T G X 3DmF V ? .<label>(10)</label></formula><p>Note that these are global representations which are applied to the entire set, i.e., the entire point cloud.</p><p>MuPS definition : For each point p in point set X we first extract n point subsetsX i (r i )| i=1,...n ? X which contain T i (p, r i ) points and lie within a distance of radius r i from p. We refer to each of these subsets as a scale. Note that each scale may contain a different number of points. For scales with many points, we set a maximal point threshold, and sample a random subset of T max points for that scale. Here, r i and T max are design parameters. Next, the scales (subsets) are independently translated and uniformly scaled so that they fit into a zero-centered unit sphere with p mapped to the origin. Then, the 3DmFV representation is computed for each scale relative to a Gaussian grid positioned around the origin; see above.</p><p>Concatenating the 3DmFVs of all scales yields the MuPS representation:</p><formula xml:id="formula_10">G p M uP S = GX 1(r1 ) 3DmF V , ..., GX n(rn) 3DmF V .<label>(11)</label></formula><p>MuPS properties: The MuPS representation overcomes the main challenges associated with feeding point clouds into CNNs. The symmetric functions make it independent of the number and order of points within each scale. In addition, the GMM gives it its grid structure, necessary for the use of CNNs. Furthermore, the multi-scale representation incorporates description of fine detail as well as robustness to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Nesti-Net architecture</head><p>The deep network architecture is outlined in <ref type="figure" target="#fig_0">Figure 1</ref> (the green part). It is a mixture-of-experts architecture <ref type="bibr" target="#b12">[13]</ref> which consists of two modules: a scale manager network module, and an experts module. The MoE architecture was chosen in order to overcome the averaging effect of typical networks when solving a regression problem. Scale manager network : This module receives the MuPS representation as input and processes it using several 3D Inception inspired convolutions, and maxpool layers, followed by four fully connected layers, after which a softmax operator is applied. The architecture is specified in the top left part of <ref type="figure" target="#fig_1">Figure 2</ref>. The output is a vector of n scalars q i , which can be intuitively interpreted as the probability of expert i to estimate the normal correctly. Experts: The normal is estimated using n separate "expert" networks. Each is a multi-layered 3D Inception inspired CNN followed by four fully connected layers. The MuPS representation is distributed to the experts. This distribution is a design choice. We obtained the best results when feeding each scale to two different experts in addition to one expert which receives the entire MuPS representation as input. Specifically, Nesti-Net uses 7 experts: experts 1-2 receive the smallest scale (1%), 3-4 the medium scale (3%), 5-6 the large scale (5%), and expert 7 receives all the scales. The last layer of each expert outputs a three-element vec-</p><formula xml:id="formula_11">tor N i = (N x , N y , N z ) i . The final predicted normal (for point p) is N argmax(qi) , i.e.</formula><p>, the normal associated with the expert expected to give the best results. The architecture is specified in the top right of <ref type="figure" target="#fig_1">Figure 2</ref>. Loss function: We train the network to minimize the difference between a predicted normal N i and a ground truth normal N GT . This difference is quantified by the metric D N = sin ?, where the angle ? is the difference between the vectors, and D N is calculated as the magnitude of the cross product between these two vectors; see Eq. 12. In addition, to encourage specialization of each expert network, we follow <ref type="bibr" target="#b12">[13]</ref> and minimize the loss:</p><formula xml:id="formula_12">L = n i=1 q i ? D N = n i=1 q i N i ? N GT N i ? N GT .<label>(12)</label></formula><p>Using this loss, each expert is rewarded for specializing in a specific input type. Note that during training, all n normal vectors are predicted and used to compute the loss and derivatives. However, at test time, we compute only one normal, which is associated with the maximal q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For training and testing we used the PCPNet shape dataset <ref type="bibr" target="#b10">[11]</ref>. The trainset consists of 8 shapes: four CAD objects (fandisk, boxunion, flower, cup) and four high quality scans of figurines (bunny, armadillo, dragon and turtle). All shapes are given as triangle meshes and densely sampled with 100k points. The data is augmented by introducing Gaussian noise for each point's spacial location with a standard deviation of 0.012, 0.006, 0.00125 w.r.t the bounding box. This yields a set with 3.2M points and 3.2M corresponding training examples. The test set consists of 22 shapes, including figurines, CAD objects, and analytic shapes. For evaluation we use the same 5000 point subset per shape as in <ref type="bibr" target="#b10">[11]</ref>.</p><p>For qualitative testing on scanned data, we used the NYU Depth V2 dataset <ref type="bibr" target="#b18">[19]</ref> and the recent ScanNet dataset <ref type="bibr" target="#b7">[8]</ref>, which include RGB-D images of indoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>All variations of our method were trained using 32,768 (1024 samples?32 shapes) random subsets of the 3.2M training samples at each epoch. For each point, we extract 512 neighboring points enclosed within a sphere of radius r. For neighborhoods with more than 512 points, we perform random sampling, and for those with fewer points we use the maximum number of points available. For the MuPS representaiton we chose to use an m = 8 Gaussian grid. We used Tensorflow on a single NVIDIA Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Normal estimation performance</head><p>We use the RMS normal estimation error metric for comparing the proposed NestiNet to other deep learning based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref> and geometric methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>. We also analyze robustness for two types of data corruptions (augmentations):</p><p>? Gaussian noise -perturbing the points with three levels of noise specified by ?, given as a percentage of the bounding box.</p><p>? Density variation -selecting a subset of the points based on two sampling regimes: gradient, simulating effects of distance from the sensor, and stripes, simulating occlusions.</p><p>For the geometric methods, we show results for three different scales: small, medium and large, which correspond to 18, 112, 450 nearest neighbors. For the deep learning based methods we show the results for the single-scale (ss) and multi-scale (ms) versions. Additional evaluation results using other metrics are available in the supplemental material. <ref type="table">Table 1</ref> shows the unoriented normal estimation results for the methods detailed above. It can be seen that our method outperforms all other methods across all noise levels and most density variations. It also shows that both PCA and Jet perform well for specific noise-scale pairs. In addition, for PCPNet and HoughCNN, using a multi-scale approach only mildly improves performance. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates Nesti-Net's results on three point clouds. For visualization, the normal vector is mapped to the RGB cube. It shows that for complex shapes (pillar, liberty) with high noise levels, the general direction of the normal vector is predicted correctly, but, the fine details and exact normal vector are not obtained. For a basic shape (Boxysmooth) the added noise does not affect the results substantially. Most notably, Nesti-Net shows robustness to point density corruptions. The angular error in each point is visualized in <ref type="figure" target="#fig_3">Figure 4</ref> for the different methods using a heat map. For PCA and Jet we display the best result out of the three scales (small, medium, and large, specified above), and for PCPNet the best out of its single-scale and multi-scale options. For all methods, it can be seen that more errors occur near edges, corners and small regions with a lot of detail and high curvature. Nesti-Net suffers the least from this effect due to its scale manager, which allows it to adapt to the different local geometry types. <ref type="figure" target="#fig_4">Figure 5</ref> shows the performance of the scale manager network. A color is assigned to each expert and the chosen expert color is visualized over the point cloud. This provides some insight regarding each expert's specialization. For example, the figure shows that experts 1, 2 (small scale) specialize in points in regions with high curvatures (near corners). Experts 3 and 4 (medium scale) specialize in the complex cases where multiple surfaces are close to each other, or in the presence of noise. As for the large-scale experts, expert 5 specializes in planar surfaces with normal vec-  tors, which have a large component in the x direction, whereas expert 6 specializes in planar surfaces, which have a large component in the y direction. Expert 5 also specializes in very noisy planar surfaces with a large component in the z direction. Expert 7 (combined scales) plays multiple roles; it handles points on planar surfaces which have a large component in the z direction, complex geometry, and low to medium noise. <ref type="figure" target="#fig_5">Figure 6</ref> shows the number of points assigned to each expert for all points in the test set, and the average error per expert. It shows an inverse relation between the number of points assigned to an expert and its average error: the more points assigned to the expert, the lower the error. This is consistent with the definition of the cost function. Timing performance and visualization of additional results are provided in the supplemental material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Scale selection performance</head><p>We analyze the influence of scale selection on the normal estimation performance. We create several ablations of our method.</p><p>? ss -A single scale version which directly feeds a 3DmFV representation into a CNN architecture (a single-scale MuPS).</p><p>? ms -A multi-scale version which feeds the MuPS representation into a CNN architecture.</p><p>? ms-sw -A multi scale version which first tries to  estimate the noise level and then feeds the 3DmFV representation of the corresponding input scale into different sub-networks for a discrete number of noise levels (switching). Note that for this version, the noise level is provided during training.</p><p>? NestiNet -The method described in Section 3 which uses an MoE network to learn the scale.</p><p>Details of the architectures for the above methods are provided in the supplemental material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on scanned data</head><p>We show qualitative results on scanned point clouds from the ScanNet <ref type="bibr" target="#b7">[8]</ref> and NYU Depth V2 <ref type="bibr" target="#b18">[19]</ref> datasets in <ref type="figure" target="#fig_6">Figure 7</ref>. For visualization we project the normal vectors' color back to the depth image plane. column (c) shows the results for Nesti-Net, trained on synthetic data with Gaussian noise. The estimated normals reveal the nonsmoothness of the scanned data associated with the correlated, non-Gaussian, noise signature associated with the scanning process. Essentially it shows normal estimation of the raw data, rather than the desired normal of the underlying surface. The raw point clouds suffer from "bumps" which get bigger as the distance from the sensor increases. Further improvement may be obtained by training Nesti-Net on data corrupted with scanner noise and with ground truth normals, but such data is is currently not available and is difficult to manually label. Instead, we train Nesti-Net with normal vectors obtained from applying a Total Variation (TV) algorithm on the depth map, provided by Ladicky et al. <ref type="bibr" target="#b22">[23]</ref> for the NYU depth V2 dataset. Note that TV substantially smooths fine detail and uses the depth image rather than unstructured point clouds. Column (d) in <ref type="figure" target="#fig_6">Figure 7</ref> shows that after training on the TV data, the normal vector estimation of the underlying surface improves significantly. Column (b) shows the results of PCA with a medium scale for reference, for small radius, the result is significantly noisier and for large radius it over-smooths detail, see supplemental material. Note that Nesti-Net performs the estimation on the raw point cloud and does not use the depth image grid structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this work, we propose multi-scale point statistics, a new representation for 3D point clouds that encodes fine and coarse details while using a grid structure. The representation is effective processed by a CNN architecture (Nesti-Net) for provide accurate normal estimation, which can be used for various applications, e.g. surface reconstruction. The mixture-of-experts design of the architecture enables the prediction of an optimal local scale and provides insights into the network's resource distribution. The proposed representation and architecture achieve state-of-the-art results relative to all other methods and demonstrate robustness to noise and occlusion data corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledegment</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using</head><p>Convolutional Neural Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Scale selection methods: architecture details</p><p>In Section 4.4 we report the performance of several ablations of our method. Here we detail the architecture of the following ablations:</p><p>? ss -A single scale version which directly feeds a 3DmFV representation into a CNN architecture (a single-scale MuPS); see <ref type="table" target="#tab_3">Table 3</ref>.</p><p>? ms -A multi-scale version which feeds the full MuPS representation into a CNN architecture; see <ref type="table" target="#tab_3">Table 3</ref>.</p><p>? ms-sw -A multi scale version which first attempts to estimate the noise level and then feeds the 3DmFV representation of the corresponding input scale into two sub-networks for two noise levels (switching). Note that for this version, the noise level is provided during training and we use a predetermined threshold for the sub-network selection; see <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Normal estimation performance analysis</head><p>In Section 4.3 we report the RMS error metric results for comparison to other methods. The RMS error favors averaging methods. For example, near corners, it will reward a method that estimates an average normal direction rather than a method that estimates the normal of the wrong plane. Therefore, a complimentary metric is required to negate this effect. We use the ss ms 3D Inception(3,5,128) 3D Inception(3,5,128) 3D Inception(3,5,256) 3D Inception(3,5,256) 3D Inception(3,5,256) 3D Inception <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr">256)</ref> maxpool(2,2,2) maxpool(2,2,2) 3D Inception(3,5,512) 3D Inception <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">512)</ref> 3D Inception(3,5,512) 3D Inception <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">512)</ref> maxpool <ref type="formula">(</ref>  proportion of good points metric (PGP?), which computes the percentage of points with an error less than ?; e.g., PGP10 computes the percentage of points with angular error of less than 10 degrees. <ref type="table">Table 5</ref> reports the results of PGP10 and <ref type="table">Table 6</ref> the results of PGP5 for the baseline methods compared to Nesti-Net. We show here additional results from section 4.3. <ref type="figure" target="#fig_7">Figure 8</ref> shows normal vectors mapped to RGB color at each point. <ref type="figure" target="#fig_0">Figure 10</ref> shows the angular error mapped to a heatmap between 0-60. The number above each point cloud is its RMS error. <ref type="figure">Figure 9</ref> shows the expert (scale) prediction by assigning a color to each expert and visualizing the chosen expert color over the point cloud.</p><p>In Section 4.5 we report the normal estimation of Nesti-Net and compare it qualitatively to the PCA results with medium scale. For additional comparison, 11 shows results of PCA for small and large scale. It shows that a small scale produces a noisy output and a large scale over-smooths fine details and corners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time complexity and timing</head><p>We subdivide Nesti-Net's time complexity into its two main stages: MuPS computation and normal estimation. It was shown in <ref type="bibr" target="#b3">[4]</ref> that the time complexity of 3DmFV is O(KT ). Here K is the number of Gaussians and T is the number of points in the point cloud. MuPS computes 3DmFV of n scales (point subsets) containing a maximum of T max points. Therefore, its time complexity is O(nKT max ). The time complexity of the normal estimation network is conms-sw noise estimation net normal estimation net 3D Inception <ref type="bibr">(3,5,</ref>  stant and proportional to the number of operators in the network. Adding experts to the network increases training time but does not affect test time since only one expert is evaluated during test time. Adding additional scales, however, affects the scale manager network by introducing additional operators. Nevertheless, the normal estimation time is independent of the number of points. We report the time performance of our method and its ablations in <ref type="figure" target="#fig_0">Figure 12</ref>. It includes timing results for single-scale (ss), multi-scale (ms) and mixture-of-experts (Nesti-Net) using 8 3 Gaussians and 3 3 Gaussians in the 'light' versions. Timing is measured as a function of the number of points within each scale. <ref type="figure" target="#fig_0">Figure 12</ref> shows that choosing a lower number of Gaussians for the MuPS representation significantly improves speed but introduces a tradeoff with accuracy. For example, the average RMS error of 'Nesti-Net light' is 13.5, which is still superior to all other methods but by a smaller margin. We also report the timing results of different methods in <ref type="figure" target="#fig_0">Figure 13</ref> and compare to our 'light' version. Note that the methods were implemented using different frameworks; PCA and Jet were implemented as part of the CGAL library, PCP-Net uses pytorch, HoughCNN uses Cuda code directly, and Nesti-Net was implemented using TensorFlow. All measurements were performed on the same machine with a quad-core Intel i7-4770 CPU, 16GB RAM, and an Nvidia GTX 1080 GPU. The figure shows that PCA and Jet are the fastest methods (PCA is slightly faster) and that the learning-based approaches are comparable. All of the results are outside the range of real-time performance. The figure also shows that our method's timing is not as sensitive to the number of points within each scale as the other methods. Point clouds in rows marked with * were rotated for a better view angle. <ref type="figure">Figure 9</ref>. Nesti-Net predicted experts (scales). Each color represents the predicted expert for optimal normal estimation. Color coding is given at the bottom. Point clouds in rows marked with * were rotated for a better view angle. <ref type="figure" target="#fig_0">Figure 10</ref>. Normal estimation error results for Nesti-Net compared to other methods for different noise levels (columns 1-4) and density distortions (columns 5-6). The point colors correspond to angular difference, mapped to a heatmap between 0-60; see bottom color bar. The number above each point cloud is its RMS error. <ref type="figure" target="#fig_0">Figure 11</ref>. Normal estimation results on scanned data from the NYU Depth V2 <ref type="bibr" target="#b18">[19]</ref> dataset and the ScanNet <ref type="bibr" target="#b7">[8]</ref> dataset.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Nesti-Net pipeline for normal estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The mixture of experts and 3D Inception module architecture details. The scale manager and experts use several convolutional and maxpooling layers followed by fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Nesti-Net normal prediction results for different noise levels (columns 1-4), and density distortions (columns 5-6). The colors of the points are normal vectors mapped to RGB. This figure is best viewed digitally on a large screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Normal estimation error results for Nesti-Net compared to other methods for three types of point clouds with low noise level (? = 1%). The colors of the points correspond to angular difference, mapped to a heatmap ranging from 0-60 degrees; see bottom color bar. The number under each point cloud is its RMS error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Nesti-Net predicted experts (scales). Each color represents the predicted expert for optimal normal estimation. Color coding is given at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Nesti-Net expert (scale) prediction statistics. Number of points assigned to each expert (left), and average expert error (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Normal estimation results on scanned point clouds from the ScanNet [8] (top), and NYU Depth V2 dataset [19] (bottom). (a) RGB image, (b) PCA results using a medium scale, (c) Nesti-Net results trained on synthetic data (d) Nesti-net results trained on TV algorithm data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Nesti-Net normal prediction results for different noise levels (columns 1-4) and density distortions (columns 5-6). The point colors are normal vectors mapped to RGB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 . 3</head><label>123</label><figDesc>Timing results for our method and its ablations using 8 Gaussians and 3 3 Gaussians in the 'light' version. Ablations include single-scale (ss), multi-scale (ms) and mixture-of-experts (Nesti-Net). Time is measured in ms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Timing results for normal estimating methods measured in ms per point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>summarizes the results of the scale selection performance analysis. It shows that Nesti-Net's scale selection performs better than all other variations. This is due to the trained scale-manager network within the MoE. The single-scale version performs well for specific noise-scale pairs but inferior performance for an inadequate scale selection. The multi-scale variations show improvement; however, selecting the correct scale yields improved performance over concatenating multiple scales. The main advantage of Nesti-Net over the switching variation is that the scale prediction is .00125 11.31 13.36 12.98 10.46 10.29 10.11 ? = 0.006 36.5 18.37 21.06 18.43 18.45 17.63 ? = 0.012 55.24 23.14 26.03 22.59 22.25 22.28 Density Gradient 16.61 14.65 12.81 11.89 9.44 9.00 Stripes 14.5 14.57 12.97 10.06 9.65 8.47 average 23.91 16.14 16.11 13.55 12.97 12.41</figDesc><table><row><cell>Aug.</cell><cell>ss</cell><cell cols="4">ms ms-sw NestiNet</cell></row><row><cell>scale</cell><cell>0.01 0.05</cell><cell>0.01 0.05</cell><cell>0.01 0.05</cell><cell>0.01 0.05</cell><cell>0.01 0.03 0.05</cell></row><row><cell>None</cell><cell cols="3">9.32 12.73 10.83 7.88</cell><cell cols="2">7.76 6.99</cell></row><row><cell>Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">? = 0Table 2. Comparison of the RMS angle error for unoriented</cell></row><row><cell cols="6">normal vector estimation of our method using single-scale</cell></row><row><cell cols="6">(SS), multi-scale (MS), multi-scale with switching (MS-Sw</cell></row><row><cell cols="5">and multi-scale with mixture of experts (Nesti-Net)</cell><cell></cell></row><row><cell cols="6">unsupervised, i.e., does not need the additional noise</cell></row><row><cell cols="4">parameters as input during training.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Ablation architecture details for single-scale (ss) and multi-scale (ms).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Ablation architecture details for multi-scale with switching. First the noise is estimated and then the input is fed into the corresponding scale network according to a threshold. The network for both scales is constructed identically. Normal estimation results comparison using the PGP10 metric (higher is better).</figDesc><table><row><cell>Aug.</cell><cell cols="2">PCPNet [11]</cell><cell></cell><cell>Jet [7]</cell><cell></cell><cell></cell><cell>PCA [12]</cell><cell></cell><cell>NestiNet</cell></row><row><cell>Scale</cell><cell>ss</cell><cell>ms</cell><cell>small</cell><cell>med</cell><cell>large</cell><cell>small</cell><cell>med</cell><cell>large</cell><cell>MoE</cell></row><row><cell>None</cell><cell cols="8">0.8364 0.8404 0.8802 0.7509 0.6584 0.8686 0.7409 0.6606</cell><cell>0.9120</cell></row><row><cell>Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? = 0.00125</cell><cell cols="8">0.8013 0.8031 0.7346 0.7447 0.6575 0.7712 0.7378 0.6598</cell><cell>0.8384</cell></row><row><cell>? = 0.006</cell><cell cols="8">0.6667 0.6294 0.1006 0.6397 0.6311 0.1101 0.6402 0.6301</cell><cell>0.7164</cell></row><row><cell>? = 0.01</cell><cell cols="4">0.5546 0.5124 0.0377 0.3827</cell><cell>0.547</cell><cell cols="2">0.04063 0.394</cell><cell>0.5462</cell><cell>0.6123</cell></row><row><cell>Density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gradient</cell><cell cols="8">0.7801 0.8062 0.8848 0.7695 0.6401 0.8731 0.7624 0.6366</cell><cell>0.9003</cell></row><row><cell>Striped</cell><cell cols="8">0.7967 0.8076 0.8743 0.7504 0.6001 0.8609 0.7379 0.5879</cell><cell>0.8929</cell></row><row><cell>Average</cell><cell cols="8">0.7393 0.7332 0.5854 0.6730 0.6224 0.5874 0.6689 0.6202</cell><cell>0.8120</cell></row><row><cell>Aug.</cell><cell cols="2">PCPNet [11]</cell><cell></cell><cell>Jet [7]</cell><cell></cell><cell></cell><cell>PCA [12]</cell><cell></cell><cell>NestiNet</cell></row><row><cell>Scale</cell><cell>ss</cell><cell>ms</cell><cell>small</cell><cell>med</cell><cell>large</cell><cell>small</cell><cell>med</cell><cell>large</cell><cell>MoE</cell></row><row><cell>None</cell><cell cols="8">0.7078 0.6986 0.7905 0.6284 0.5395 0.7756 0.6192 0.5361</cell><cell>0.8057</cell></row><row><cell>Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? = 0.00125</cell><cell cols="8">0.6245 0.5932 0.4132 0.6237 0.5377 0.4758 0.6157 0.5335</cell><cell>0.6611</cell></row><row><cell>? = 0.006</cell><cell>0.4486</cell><cell>0.366</cell><cell>0.027</cell><cell cols="3">0.4152 0.4837 0.02998</cell><cell>0.42</cell><cell>0.4812</cell><cell>0.5618</cell></row><row><cell>? = 0.01</cell><cell cols="6">0.3156 0.2482 0.0099 0.1462 0.3715 0.0104</cell><cell>0.154</cell><cell>0.3719</cell><cell>0.399</cell></row><row><cell>Density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gradient</cell><cell cols="6">0.6065 0.6254 0.7883 0.6442 0.4976 0.7743</cell><cell>0.647</cell><cell>0.4894</cell><cell>0.7749</cell></row><row><cell>Striped</cell><cell cols="8">0.6126 0.6231 0.7753 0.6321 0.4598 0.7575 0.6174 0.4415</cell><cell>0.7676</cell></row><row><cell>Average</cell><cell cols="8">0.5526 0.5257 0.4674 0.5150 0.4816 0.4706 0.5122 0.4756</cell><cell>0.6617</cell></row></table><note>Table 6. Normal estimation results comparison using the PGP5 metric (higher is better).</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voronoi-based variational reconstruction of unoriented point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desbrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surface reconstruction by voronoi filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="504" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph based over-segmentation methods for 3d point clouds. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust normal estimation for point clouds with sharp features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1765" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for robust normal estimation in unstructured point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating differential quantities using polynomial fitting of osculating jets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cazals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="146" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Provable surface reconstruction from noisy samples. Computational Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goswami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="124" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algebraic point set surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surface reconstruction from unorganized points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duchampt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling spatial layout with fisher vectors for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1487" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Voronoibased curvature and feature estimation from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>M?rigot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating surface normals in noisy point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Annual Symposium on Computational geometry</title>
		<meeting>the Nineteenth Annual Symposium on Computational geometry</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Derek Hoiem and R. Fergus. Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
