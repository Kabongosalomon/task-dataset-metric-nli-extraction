<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feitong</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><forename type="middle">A I</forename><surname>Labs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Comparison between the state-of-the-art learning-based multi-view stereo approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and MVS-Net+Ours. (a)-(d): Reconstructed point clouds of MVSNet [52], R-MVSNet [53], Point-MVSNet [4] and MVSNet+Ours. (e) and (f): The relationship between reconstruction accuracy and GPU memory or run-time. The resolution of input images is 1152 ? 864.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the depth or disparity. These methods are limited with high-resolution outputs since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a memory and time efficient cost volume formulation complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the prediction from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner.</p><p>We apply the cascade cost volume to the representative MVS-Net, and obtain a 35.6% improvement on DTU benchmark (1st place), with 50.6% and 59.3% reduction in GPU memory and run-time. It is also rank first on Tanks and Temples benchmark of all deep models. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method. Our source code is available at https: //github.com/alibaba/cascade-stereo. * Equal contribution. ? This work was done during an internship at Alibaba A.I. Labs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Comparison between the state-of-the-art learning-based multi-view stereo approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and MVS-Net+Ours. (a)-(d): Reconstructed point clouds of MVSNet <ref type="bibr" target="#b51">[52]</ref>, R-MVSNet <ref type="bibr" target="#b52">[53]</ref>, Point-MVSNet <ref type="bibr" target="#b3">[4]</ref> and MVSNet+Ours. (e) and (f): The relationship between reconstruction accuracy and GPU memory or run-time. The resolution of input images is 1152 ? 864.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the depth or disparity. These methods are limited with high-resolution outputs since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a memory and time efficient cost volume formulation complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the prediction from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner.</p><p>We apply the cascade cost volume to the representative MVS-Net, and obtain a 35.6% improvement on DTU benchmark (1st place), with 50.6% and 59.3% reduction in GPU memory and run-time. It is also rank first on Tanks and Temples benchmark of all deep models. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method. Our source code is available at https: //github.com/alibaba/cascade-stereo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have been widely adopted in 3D reconstruction and broader computer vision tasks. State-of-the-art multi-view stereo <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and stereo matching algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56]</ref> often compute a 3D cost volume according to a set of hypothesized depth (or disparity) and warped features. 3D convolutions are applied to this cost volume to regularize and regress the final scene depth (or disparity).</p><p>Compared with the methods based on 2D CNNs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55]</ref>, the 3D cost volume can capture better geometry structures, perform photometric matching in 3D space, and alleviate the influence of image distortion caused by perspective transformation and occlusions <ref type="bibr" target="#b3">[4]</ref>. However, methods relying on 3D cost volumes are often limited to low-resolution input images (and results), because 3D CNNs are generally time and GPU memory consuming. Typically, these methods downsample the feature maps to formulate the cost volumes at a lower resolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>, and adopt upsampling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref> or post-refinement <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> to output the final high-resolution result.</p><p>Inspired by the previous coarse-to-fine learning-based stereo approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>, we present a novel cascade formulation of 3D cost volumes. We start from a feature pyramid to extract multi-scale features which are commonly used in standard multi-view stereo <ref type="bibr" target="#b51">[52]</ref> and stereo match-ing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> networks. In a coarse-to-fine manner, the cost volume at the early stages is built upon larger scale semantic 2D features with sparsely sampled depth hypotheses, which lead to a relatively lower volume resolution. Subsequently, the later stages use the estimated depth (or disparity) maps from the earlier stages to adaptively adjust the sampling range of depth (or disparity) hypotheses and construct new cost volumes where finer semantic features are applied. This adaptive depth sampling and adjustment of feature resolution ensures the computation and memory resources are spent on more meaningful regions. In this way, our cascade structure can remarkably decrease computation time and GPU memory consumption. The effectiveness of our method can be seen in <ref type="figure">Figure 1</ref>.</p><p>We validate our method on both multi-view stereo and stereo matching on various benchmark datasets. For multiview stereo, our cascade structure achieves the best performance on the DTU dataset <ref type="bibr" target="#b0">[1]</ref> at the submission time of this paper, when combined with MVSNet <ref type="bibr" target="#b51">[52]</ref>. It is also the state-of-the-art learning-based method on Tanks and Temples benchmark <ref type="bibr" target="#b23">[24]</ref>. For stereo matching, our method reduces the end-point-error (EPE) and GPU memory consumption of GwcNet <ref type="bibr" target="#b14">[15]</ref> by about 15.2% and 36.9% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Stereo Matching According to the survey by Scharstein et al. <ref type="bibr" target="#b37">[38]</ref>, a typical stereo matching algorithm contains four steps: matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement. Local methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref> aggregate matching costs with neighboring pixels and usually utilize the winner-take-all strategy to choose the optimal disparity. Global methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> construct an energy function and try to minimize it to find the optimal disparity. More specifically, works in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref> use belief propagation and semi-global matching <ref type="bibr" target="#b16">[17]</ref> to approximate the global optimization with dynamic programming.</p><p>In the context of deep neural networks, CNNs based stereo matching methods are first introduced by Zbontar and LeCun <ref type="bibr" target="#b53">[54]</ref>, in which a convolutional neural network is introduced to learn the similarity measure of small patch pairs. The introduction of the widely used 3D cost volume in stereo is first proposed in GCNet <ref type="bibr" target="#b21">[22]</ref>, in which the disparity regression step uses the soft argmin operation to figure out the best matching results. PSMNet <ref type="bibr" target="#b2">[3]</ref> further introduces pyramid spatial pooling and 3D hourglass networks for cost volume regularization and yields better results. GwcNet <ref type="bibr" target="#b14">[15]</ref> modifies the structure of 3D hourglass and introduces group wise correlation to form a group based 3D cost volume. HSM <ref type="bibr" target="#b47">[48]</ref> builds a light model for high-resolution images with a hierarchical design. EMCUA <ref type="bibr" target="#b32">[33]</ref> introduces an approach for multi-level context ultra-aggregation. GANet <ref type="bibr" target="#b55">[56]</ref> constructs several semi-global aggregation layers and local guided aggregation layers to further improve the accuracy. Deep-Pruner <ref type="bibr" target="#b4">[5]</ref> is a coarse to fine method which proposes a differentiable PatchMatch-based module to predict the pruned search range for each pixel.</p><p>Although methods based on 3D cost-volume remarkably boost the performance, they are limited to downsampled cost volumes and rely on interpolation operations to generate high-resolution disparity. Our cascade cost volumes can be combined with these methods to improve the disparity accuracy and GPU memory efficiency.</p><p>Multi-View Stereo According to the comprehensive survey <ref type="bibr" target="#b11">[12]</ref>, works in traditional muti-view stereo can be roughly categorised into volumetric methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>, which estimate the relationship between each voxel and surfaces; point cloud based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, which directly process 3D points to iteratively densify the results; and depth map reconstruction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref>, which use only one reference and a few source images for single depth map estimation. For large-scale Structurefrom-Motion, works in <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> use distributed methods based on distributed motion averaging and global camera consensus.</p><p>Recently, learning-based approaches also demonstrate superior performance on multi-view stereo. Multi-patch similarity <ref type="bibr" target="#b15">[16]</ref> introduces a learned cost metric. Sur-faceNet <ref type="bibr" target="#b19">[20]</ref> and DeepMVS <ref type="bibr" target="#b17">[18]</ref> pre-warp the multi-view images to 3D space and use deep networks for regularization and aggregation. Most recently, multi-view stereo based on 3D cost volumes have been proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. A 3D cost volume is built based on warped 2D image features from multiple views and 3D CNNs are applied for cost regularization and depth regression. Because the 3D CNNs require large GPU memory, these methods generally use downsampled cost volumes. Our cascade cost volume can be easily integrated into these methods to enable high-resolution cost volumes and further boosts accuracy, computational speed, and GPU memory efficiency.</p><p>High-Resolution Output in Stereo and MVS Recently, some learning-based methods try to reduce the memory requirement in order to generate high resolution outputs. Instead of using voxel grids, Point MVSNet <ref type="bibr" target="#b3">[4]</ref> proposes to use a small cost volume to generate the coarse depth and uses a point-based iterative refinement network to output the full resolution depth. In comparison, a standard MVS-Net combined with our cascade cost volume can output full resolution depth with superior accuracy using less runtime and GPU memory than Point MVSNet <ref type="bibr" target="#b3">[4]</ref>. Works in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref> partition advanced space to reduce memory consumption and construct a fixed cost volume representation which lacks flexibility. Works in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref> build extra refinement module by 2D CNNs and output a high resolution prediction. Notably, such refinement modules can be utilized jointly with our proposed cascade cost volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section describes the detailed architecture of the proposed cascade cost volume which is complementary to the existing 3D cost volume based methods in multi-view stereo and stereo matching. Here, we use the representative MVSNet <ref type="bibr" target="#b51">[52]</ref> and PSMNet <ref type="bibr" target="#b2">[3]</ref> as the backbone networks to demonstrate the application of the cascade cost volume in multi-view stereo and stereo matching tasks respectively. <ref type="figure" target="#fig_0">Figure 2</ref> shows the architecture of MVSNet+Ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cost volume Formulation</head><p>Learning-based multi-view stereo <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and stereo matching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> construct 3D cost volumes to measure the similarity between corresponding image patches and determine whether they are matched. Constructing 3D cost volume requires three major steps in both multi-view stereo and stereo matching. First, the discrete hypothesis depth (or disparity) planes are determined. Then, we warp the extracted 2D features of each view to the hypothesis planes and construct the feature volumes, which are finally fused together to build the 3D cost volume. Pixelwise cost calculation is generally ambiguous in inherently ill-posed regions such as occlusion areas, repeated patterns, textureless regions, and reflective surfaces. To solve this, 3D CNNs at multiple scales are generally introduced to aggregate contextual information and regularize the possibly noise-contaminated cost volumes. <ref type="bibr" target="#b51">[52]</ref> proposes to use fronto-parallel planes at different depth as hypothesis planes and the depth range is generally determined by the sparse reconstruction. The coordinate mapping is determined by the homography:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Cost Volumes in Multi-View Stereo MVSNet</head><formula xml:id="formula_0">H i (d) = K i ? R i ? (I ? (t 1 ? t i ) ? n 1 T d ) ? R 1 T ? K 1 ?1 (1)</formula><p>where H i (d) refers to the homography between the feature maps of the i th view and the reference feature maps at depth d. Moreover, K i , R i , t i refers to the camera intrinsics, rotations and translations of the i th view respectively, and n 1 denotes the principle axis of the reference camera. Then differentiable homography is used to warp 2D feature maps into hypothesis planes of the reference camera to form feature volumes. To aggregate multiple feature volumes to one cost volume, the variance-based cost metric is proposed to adapt an arbitrary number of input feature volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Cost Volumes in Stereo Matching</head><p>PSMNet <ref type="bibr" target="#b2">[3]</ref> uses disparity levels as hypothesis planes and the range of disparity is designed according to specific scenes. Since the left and right images have been rectified, the coordinate mapping is determined by the offset in the x-axis direction:</p><formula xml:id="formula_1">C r (d) = X l ? d<label>(2)</label></formula><p>where C r (d) refers to the transformed x-axis coordinate of the right view at disparity d, and X l is the source x-axis coordinate of the left view. To build feature volumes, we warp the feature maps of the right view to the left view using the translation along the x-axis. There are multiple ways to build the final cost volume. GCNet <ref type="bibr" target="#b21">[22]</ref> and PSM-Net <ref type="bibr" target="#b2">[3]</ref> concatenate the left feature volume and the right feature volume without decreasing the feature dimension. The work <ref type="bibr" target="#b54">[55]</ref> uses the sum of absolute differences to compute matching cost. DispNetC <ref type="bibr" target="#b29">[30]</ref> computes full correlation about the left feature volume and right feature volume Plane Num. Plane Interv. Spatial Res.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency Negative Positive Negative</head><p>Accuracy Positive Negative Positive <ref type="figure">Figure 3</ref>: Left: the standard cost volume. D is the number of hypothesis planes, W ? H is the spatial resolution and I is the plane interval. Right: The influence factors of efficiency (run-time and GPU memory) and accuracy. and produces only a single-channel correlation map for each disparity level. GwcNet <ref type="bibr" target="#b14">[15]</ref> proposes group-wise correlation by splitting the features into groups and computing correlation maps in each group. <ref type="figure">Figure 3</ref> shows a standard cost volume of a resolution of W ? H ? D ? F , where W ? H denotes the spatial resolution, D is the number of plane hypothesis, and F is the channel number of feature maps. As mentioned in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, an increased number of plane hypothesis D, a larger spatial resolution W ? H, and a finer plane interval are likely to improve the reconstruction accuracy. However, the GPU memory and run-time grow cubically as the resolution of the cost volume increases. As demonstrated in R-MVSNet <ref type="bibr" target="#b52">[53]</ref>, MVSNet <ref type="bibr" target="#b51">[52]</ref> is able to process a maximum cost volume of H ?W ?D?F = 1600?1184?256?32 on a 16 GB Tesla P100 GPU. To resolve the problems above, we propose a cascade cost volume formulation and predict the output in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cascade Cost Volume</head><p>Hypothesis Range As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, the depth (or disparity) range of the first stage denoted by R 1 covers the entire depth (or disparity) range of the input scene. In the following stages, we can base on the predicted output from the previous stage, and narrow the hypothesis range. Consequently, we have R k+1 = R k ? w k , where R k is the hypothesis range at the k th stage and w k &lt; 1 is the reducing factor of hypothesis range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Plane Interval</head><p>We also denote the depth (or disparity) interval at the first stage as I 1 . Compared with the commonly adopted single cost volume formulation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52]</ref>, the initial hypothesis plane interval is comparatively larger to generate a coarse depth (or disparity) estimation. In the following stages, finer hypothesis plane intervals are applied to recover more detailed outputs. Therefore, we have: I k+1 = I k ? p k , where I k is the hypothesis plane interval at the k th stage and p k &lt; 1 is the reducing factor of hypothesis plane interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Hypothesis Planes</head><p>At the k th stage, given the hypothesis range R k and hypothesis plane interval I k , the corresponding number of hypothesis planes D k is determined by the equation: D k = R k /I k . When the spatial resolution of a cost volume is fixed, a larger D k generates more hypothesis planes and correspondingly more accurate results while leads to increased GPU memory and run-time. Based on the cascade formulation, we can effectively reduce the total number of hypothesis planes since the hypothesis range is remarkably reduced stage by stage while still covering the entire output range.</p><p>Spatial Resolution Following the practices of Feature Pyramid Network <ref type="bibr" target="#b27">[28]</ref>, we double the spatial resolution of the cost volume at every stage along with the doubled resolution of the input feature maps. We define N as the total stage number of cascade cost volume, then the spatial resolution of cost volume at the k th stage is defined as</p><formula xml:id="formula_2">W 2 N ?k ? H 2 N ?k .</formula><p>We set N = 3 in multi-view stereo tasks and N = 2 in stereo matching tasks.</p><p>Warping Operation Applying the cascade cost volume formulation to multi-view stereo, we base on Equation 1 and rewrite the homography warping function at the (k + 1) th stage as:</p><formula xml:id="formula_3">H i (d m k +? m k+1 ) = K i ?R i ?(I? (t 1 ? t i ) ? n 1 T d m k + ? m k+1 )?R 1 T ?K 1 ?1 (3) where d m</formula><p>k denotes the predicted depth of the m th pixel at the k th stage, and ? m k+1 is the residual depth of the m th pixel to be learned at the k + 1 th stage.</p><p>Similarly in stereo matching, we reformulate Equation 2 based on our cascade cost volume. The m th pixel coordinate mapping at the k + 1 th stage is expressed as:</p><formula xml:id="formula_4">C r (d m k + ? m k+1 ) = X l ? (d m k + ? m k+1 )<label>(4)</label></formula><p>where d m k denotes the predicted disparity of the m th pixel at the k th stage, and ? m k+1 denotes the residual disparity of the m th pixel to be learned at the k + 1 th stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Pyramid</head><p>In order to obtain high-resolution depth (or disparity) maps, previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56]</ref> generally generate a comparatively low-resolution depth (or disparity) map using the standard cost volume and then upsample and refine (a) MVSNet <ref type="bibr" target="#b51">[52]</ref> (b) R-MVSNet <ref type="bibr" target="#b52">[53]</ref> (c) Point MVSNet <ref type="bibr" target="#b3">[4]</ref> (d) MVSNet+Ours (e) Ground Truth it with 2D CNNs. The standard cost volume is constructed using the top level feature maps which contains high-level semantic features but lacks low-level finer representations.</p><p>Here, we refer to Feature Pyramid Network <ref type="bibr" target="#b27">[28]</ref> and adopt its feature maps with increased spatial resolutions to build the cost volumes of higher resolutions. For example, when applying cascade cost volume to MVSNet <ref type="bibr" target="#b51">[52]</ref>, we build three cost volumes from the feature maps {P1, P2, P3} of Feature Pyramid Network <ref type="bibr" target="#b27">[28]</ref>. Their corresponding spatial resolutions are {1/16, 1/4, 1} of the input image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>The cascade cost volume with N stages produces N ? 1 intermediate outputs and a final prediction. We apply the supervision to all the outputs and the total loss is defined as:</p><formula xml:id="formula_5">Loss = N k=1 ? k ? L k<label>(5)</label></formula><p>where L k refers to the loss at the k th stage and ? k refers to its corresponding loss weight. We adopt the same loss function L k as the baseline networks in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed cascade cost volume on multiview stereo and stereo matching tasks. Implementation We apply the proposed cascade cost volume to the representative MVSNet <ref type="bibr" target="#b51">[52]</ref> and denote the network as MVSNet+Ours. During training, we set the number of input images to N =3 and image resolution to 640 ? 512. After balancing accuracy and efficiency, we adopt a three-stage cascade cost volume. From the first to the third stage, the number of depth hypothesis is 48, 32 and 8, and the corresponding depth interval is set to 4, 2 and 1 times as the interval of MVSNet <ref type="bibr" target="#b51">[52]</ref> respectively. Accordingly, the spatial resolution of feature maps gradually increases and is set to 1/16, 1/4 and 1 of the original input image size. We follow the same input view selection and data pre-processing strategies as MVSNet <ref type="bibr" target="#b51">[52]</ref> in both training and evaluation. During training, we use Adam optimizer with ? 1 = 0.9 and ? 2 = 0.999. The training is done for 16 epochs with an initial learning rate of 0.001, which is downscaled by a factor of 2 after 10, 12, and 14 epochs. We   , we calculate the accuracy and the completeness by the MATLAB code provided by DTU dataset <ref type="bibr" target="#b0">[1]</ref>. The percentage evaluation is implemented following MVSNet <ref type="bibr" target="#b51">[52]</ref>. The Fscore is used as the evaluation metric for Tanks and Temples dataset <ref type="bibr" target="#b23">[24]</ref> to measure the accuracy and completeness of the reconstructed point clouds. We use fusibile <ref type="bibr" target="#b35">[36]</ref> as our post-processing consisting of three steps: photometric filtering, geometric consistency filtering, and depth fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-view stereo</head><p>Benchmark Performance Quantitative results on DTU evaluation set <ref type="bibr" target="#b0">[1]</ref> are shown in <ref type="table">Table 1</ref>. We can see that MVSNet <ref type="bibr" target="#b51">[52]</ref> with cascade cost volume outperforms other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> in both completeness and overall quality and rank the 1st place on DTU dataset <ref type="bibr" target="#b0">[1]</ref>, with the improvement of 35.6%, and the decrease of memory, run-time reduction of 50.6% and 59.3%. The qualitative results are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. We can see that MVS-Net+Ours generates more complete point clouds with finer details. Besides, we demonstrate the generalization ability of our trained model by testing on Tanks and Temples dataset <ref type="bibr" target="#b23">[24]</ref>. The corresponding quantitative results are reported in <ref type="table" target="#tab_2">Table 2</ref>, and MVSNet+Ours achieves the state-ofthe-art performance among the learning-based multi-view stereo methods. The qualitative point cloud results of the intermediate set of Tanks and Temples benchmark <ref type="bibr" target="#b23">[24]</ref> are visualized in <ref type="figure">Figure 6</ref>. Note that, we get the results of above mentioned methods by running their provided pretrained model and code except R-MVSNet <ref type="bibr" target="#b52">[53]</ref> which provides point cloud results with their post-processing method.</p><p>To analyse the accuracy, GPU memory and run-time at each stage, we evaluate the MVSNet+Ours method on the DTU dataset <ref type="bibr" target="#b0">[1]</ref>. We provide comprehensive statistics in <ref type="table">Table 9</ref> and visualization results in <ref type="figure">Figure 7</ref>. In a coarse-tofine manner, the overall quality is improved from 0.602 to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stereo Matching</head><p>Datasets Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref> is a large scale-dataset containing 35,454 training and 4,370 testing stereo pairs of size 960 ? 540. It contains accurate ground truth disparity maps. We use the Finalpass of the Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref> since it contains more motion blur and defocus and is more like a real-world environment. KITTI 2015 <ref type="bibr" target="#b31">[32]</ref> is a realworld dataset with dynamic street views. It contains 200 training pairs and 200 testing pairs. Middlebury <ref type="bibr" target="#b36">[37]</ref> is the publicly available dataset for high-resolution stereo matching contains 60 pairs under imperfect calibration, different exposures, and different lighting conditions. Implementation In Scene Flow dataset, we extend PSM-Net <ref type="bibr" target="#b2">[3]</ref>, GwcNet <ref type="bibr" target="#b14">[15]</ref> and GANet11 <ref type="bibr" target="#b55">[56]</ref> with our proposed cascade cost volume and denote them as PSMNet+Ours, GwcNet+Ours and GANet11+Ours. Balancing the tradeoff between accuracy and efficiency, a two-stage cascade cost volume is applied, and the number of disparity hypothesis is 12. The corresponding disparity interval is set to 4 and 1 pixels respectively. The spatial resolution of feature maps increases from 1/16 to 1/4 of the original input image size. The maximum disparity is set to 192.</p><p>In KITTI 2015 benchmark <ref type="bibr" target="#b31">[32]</ref>, we mainly compare GwcNet <ref type="bibr" target="#b14">[15]</ref> and GwcNet+Ours. For a fair comparison, we follow the training details of the original networks. The evaluation metric in Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref> is end-pointerror (EPE), which is the mean absolute disparity error in pixels. For KITTI 2015 <ref type="bibr" target="#b31">[32]</ref>, the percentage of disparity outliers D1 is used to evaluate disparity error larger than     <ref type="table">Table 7</ref>: The quantitative comparison between MVSNet and MVSNet with different settings of the cascade cost volumes. Specifically, "cascade" denotes that the original cost volume is divided into three cascade cost volumes, "upsample" denotes cost volumes with increased spatial resolutions by bilinear upsampling corresponding feature maps, and feature pyramid denotes cost volumes with higher spatial resolutions built on pyramid feature maps. The statistics are evaluated on the DTU dataset. <ref type="table" target="#tab_6">Table 5</ref> shows the percentage of disparity outliers D1 evaluated for background, foreground, and all pixels. Compared with the original GwcNet <ref type="bibr" target="#b14">[15]</ref>, the rank of GwcNet+Ours rises from 29 th to 17 th (date: Nov.5, 2019). Several disparity estimation on KITTI 2015 test set <ref type="bibr" target="#b31">[32]</ref> is shown in <ref type="figure" target="#fig_3">Figure 8</ref>. In Middlebury benchmark, PSMNet+Ours ranks 37th for the avgerr metric(date: Feb.7, 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Extensive ablation studies are performed to validate the improved accuracy and efficiency of our approach. All results are obtained by the three-stage model on DTU validation set <ref type="bibr" target="#b0">[1]</ref> unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascade Stage Number</head><p>The quantitative results with different stage numbers are summarized in <ref type="table" target="#tab_7">Table 6</ref>. In our implementation, we use MVSNet <ref type="bibr" target="#b51">[52]</ref> with 192 depth hypothesis as the baseline model, and replace its cost volume with our cascade design which is also consisted of 192 depth hypothesis. Note that the spatial resolution of different stages are the same as that of the original MVSNet <ref type="bibr" target="#b51">[52]</ref>. This extended MVSNet is denoted as MVSNet-Cas i where i indicates the total stage number. We find that as the number of stages increases, the overall quality first remarkably increases and then stabilizes.</p><p>Spatial Resolution Then, we study how the spatial resolution of a cost volume W ? H affects the reconstruction performance. Here, we compare MVSNet-Cas 3 , which contains 3 stages and all the stages share the same spatial resolution, and MVSNet-Cas 3 -Ups where the spatial resolution increases from 1/16 to 1 of the original image size and bilinear interpolation is used to upsample feature maps. As shown in <ref type="table">Table 7</ref>, the overall quality of MVSNet+Ours is obviously superior to those of MVSNet-Cas 3 (0.453 vs. 0.355). Accordingly, a higher spatial resolution also leads to increased GPU memory (2373 vs. 5345 MB) and runtime (0.322 vs. 0.492 seconds).</p><p>Feature Pyramid As shown in <ref type="table">Table 7</ref>, the cost volume constructed from Feature Pyramid Network <ref type="bibr" target="#b27">[28]</ref> denoted by MVSNet+Ours can slightly improve the overall quality from 0.379 to 0.355. The GPU memory (6227 vs. 5345 MB) and run-time (0.676 vs. 0.492 seconds) are also decreased. Compared with the improvement between MVSNet-Cas 3 and MVSNet-Cas 3 -Ups, the increased spatial resolution is still more critical to the improvement of reconstruction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing in Cost Volume Regularization</head><p>We also analyze the effect of weight sharing in 3D cost volume regularization across all the stages. As is shown in Table 6, the shared parameters cascade cost volume denoted by MVSNet-Cas 3 -share achieves worse performance than MVSNet-Cas 3 . It indicates that separate parameter learning of the cascade cost volumes at different stages further improves the accuracy. <ref type="table">Table 1</ref> shows the comparison of GPU memory and runtime between MVSNet <ref type="bibr" target="#b51">[52]</ref> with and without cascade cost volume. Given the remarkable accuracy improvement, the GPU memory decreases from 10,823 to 5,345 MB, and the run-time drops from 1.210 to 0.492 seconds. In <ref type="table" target="#tab_5">Table 4</ref>, we compare the GPU memory between PSMNet <ref type="bibr" target="#b2">[3]</ref>, Gwc-Net <ref type="bibr" target="#b14">[15]</ref> and GANet11 <ref type="bibr" target="#b55">[56]</ref> with and without the proposed cascade cost volume. The GPU memory of PSMNet <ref type="bibr" target="#b2">[3]</ref>, GwcNet <ref type="bibr" target="#b14">[15]</ref> and GANet11 <ref type="bibr" target="#b55">[56]</ref> decreases by 39.97%, 36.99% and 24.11% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Run-time and GPU Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a both GPU memory and computationally efficient cascade cost volume formulation for high-resolution multi-view stereo and stereo matching. First, we decompose the single cost volume into a cascade formulation of multiple stages. Then, we can narrow the depth (or dispartiy) range of each stage and reduce the total number of hypothesis planes by utilizing the depth (or disparity) map from the previous stage. Next, we use the cost volumes of higher spatial resolution to generate the outputs with finer details. The proposed cost volume is complementary to existing 3D cost-volume-based multi-view stereo and stereo matching approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Discussion</head><p>Why Hypothesis Range is Remarkably Decreased? In <ref type="figure">Figure 9</ref>, we provide the statistics of the absolute depth errors which measure the distance between the predicted depth and its ground truth. Since there is no depth prediction at the first stage, we regard the ground-truth depth as the absolute depth errors of the first stage. As shown in <ref type="figure">Figure 9</ref>(a), the entire depth range of the first stage is approximately 500mm while the entire depth range at the second and the third stage shown in <ref type="figure">Figure 9</ref>(b) is narrowed to about 50mm which is reduced by 90% compared with the first stage. Accordingly, we can significantly reduce the hypothesis range at the second and third stages.</p><p>How to Set Hypothesis Range at Different Stages? In <ref type="figure">Figure 10</ref>, we calculate the percentage of the absolute errors less than a certain threshold (noted as inlier percentage). Hypothesis range should cover most erroneously predicted depth (or disparity) and correct them. As shown in <ref type="figure">Figure 10</ref>(a), the inlier percentage of MVSNet <ref type="bibr" target="#b51">[52]</ref> and that of MVSNet+Ours at the first stage intersects at 5.92mm and 86%. That means if we set a hypothesis range larger than 5.92mm, we can cover more possibly correct predictions than MVSNet, since our cascade cost volume is able to correct the erroneous prediction at the later stages. On multi-view stereo data-sets, we set the hypotheis range as 32 ? 2 ? 2.5 = 160mm which still has a large margin to be reduced.</p><p>Similarly in stereo matching, as shown in <ref type="figure">Figure 10</ref>(b), the disparity hypothesis range is set as 12 ? 2 = 24 pixel (the intersection is 19.60 pixel), which covers the range of more erroneous predictions at the first stage compared with the original single cost volume approach.</p><p>Why Cascade Cost Volume is Memory Efficient? In multi-view stereo, the hypothesis range is able to remarkably decrease since the entire depth range is narrowed by nearly 90% (500mm vs. 50mm) since the first stage. Therefore, we can use less hypothesis planes for cost volumes in later stage. In MVSNet+Ours, we set the number of hypothesis planes in first stage as 48 whereas MVSNet <ref type="bibr" target="#b51">[52]</ref> has 192 planes, leading to the GPU memory decrease from 10,823MB to 2,373MB. In order to improve the accuracy in subsequent stages, we increase the spatial resolution and the GPU memory increases from 2,373 MB to 4,093 MB and 5,345 MB. Although we increase the spatial resolution, the total GPU memory is deceased about 50.6% compared MVSNet and run-time is about 2 times faster shown in <ref type="figure">Figure 1</ref> in the main paper. Similarly, in stereo matching we also decrease the GPU memory from 3,827MB to 2,699MB using our two stage cost volume.  Moreover, we can balance between the time (or memory) efficiency and accuracy by adopting different cascade numbers, hypothesis range and spatial resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Multi-view Stereo</head><p>In this section, we demonstrate more multi-view stereo experimental results. As shown in <ref type="figure" target="#fig_1">Figure 14</ref>, we visualize the reconstructed point cloud of MVSNet+Ours on DTU dataset [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Stereo Matching</head><p>Qualitative Results on Scene Flow Dataset In this section, we show several reconstruction results of PSM-Net <ref type="bibr" target="#b2">[3]</ref>, GwcNet <ref type="bibr" target="#b14">[15]</ref>, GANet11 <ref type="bibr" target="#b55">[56]</ref> and the extended model PSMNet+Ours, GwcNet+Ours, GANet11+Ours on Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref>. As is shown in <ref type="figure" target="#fig_0">Figure 12</ref>, the visual quality is improved with the replacement of our cascade cost volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascade Stage Number in Stereo Matching</head><p>In this experiment, we replace the cost volume in GwcNet <ref type="bibr" target="#b14">[15]</ref> with our proposed cascade cost volume, namely GwcNet+Ours. Note that, the experiment setting in GwcNet <ref type="bibr" target="#b14">[15]</ref> is 64 channel concatenation volume, the spatial resolution of different stages are the same as that of the original GwcNet. The extended model with total i th stages is denoted as GwcNet-Cas i . As is shown in <ref type="table" target="#tab_9">Table 8</ref>, the accuracy of the extended model increases with stage increases. We can notice the details get cleaner as the stage increases in <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Resolution in Stereo Matching</head><p>We study how the spatial resolution of a cost volume affects the reconstruction accuracy and GPU memory in stereo matching. Similar to the experiment in multi-view stereo, we formulate a three-stage cost volume based on GwcNet with the spatial resolution gradually increases from 1/4 ? 1/4 to 1 of the original input image size. In a coarse-to fine manner, the end-point-error is improved from 0.972 to 0.619. Accordingly, the GPU memory increases from 1,545MB to 3,429MB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage2 Stage3</head><p>(b) Distribution of absolute errors at the 2 nd and 3 rd stages <ref type="figure">Figure 9</ref>: Distribution of absolute errors at different stages. We assume that the absolute errors at the 1 st stage are the groundtruth depth since there is no predicted depth at this stage. The statistical results are calculated on DTU evaluation dataset <ref type="bibr" target="#b0">[1]</ref> using MVSNet+Ours with a three-stage cost volume.   <ref type="figure">Figure 10</ref>: The percentage of the absolute errors between the prediction and the ground-truth less than a certain threshold. We demonstrate the results of MVSNet <ref type="bibr" target="#b51">[52]</ref>, GwcNet <ref type="bibr" target="#b14">[15]</ref>, and certain networks with cascade cost volume at different stages.  <ref type="table">Table 9</ref>: The statistical results of different stages in cascade cost volume. The statistics are collected on the Scene Flow evaluation set <ref type="bibr" target="#b29">[30]</ref> using GwcNet+Ours. The run-time is the sum of the current and previous stages and the original input size is 960?512.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of the proposed cascade cost volume on MVSNet<ref type="bibr" target="#b51">[52]</ref>, denoted as MVSNet+Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of hypothesis plane generation. R k and I k are respectively the hypothesis range and the hypothesis plane number at the k th stage. Pink lines are hypothesis planes. Yellow line indicates the predicted depth (or disparity) map from stage 1, which is used to determine the hypothesis range and hypothesis plane intervals at stage 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Multi-view stereo qualitative results of scan 10 on DTU dataset<ref type="bibr" target="#b0">[1]</ref>. Top row: Generated point clouds of different methods and ground truth point clouds. Bottom row: Zoomed local areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on the test set of KITTI2015<ref type="bibr" target="#b31">[32]</ref>. Top row: Input images, Second row: Results of PSMNet<ref type="bibr" target="#b2">[3]</ref>. Third row: Results of GwcNet<ref type="bibr" target="#b14">[15]</ref>. Bottom row: Results of GwcNet with cascade cost volume (GwcNet+Ours). 0.355. Accordingly, the GPU memory increases from 2,373 MB to 4,093 MB and 5,345 MB, and run-time increases from 0.081 s to 0.243 s and 0.492 s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Datasets DTU<ref type="bibr" target="#b0">[1]</ref> is a large-scale MVS dataset consisting of 124 different scenes scanned in 7 different lighting conditions at 49 or 64 positions. Tanks and Temples dataset<ref type="bibr" target="#b23">[24]</ref> contains realistic scenes with small depth ranges. More specifically, its intermediate set is consisted of 8 scenes including Family, Francis, Horse, Lighthouse, M60, Panther, Playground, and Train. Following the work<ref type="bibr" target="#b52">[53]</ref>, we</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s)</cell></row><row><cell>Camp [2]</cell><cell>0.835</cell><cell>0.554</cell><cell>0.695</cell><cell>-</cell><cell>-</cell></row><row><cell>Furu [13]</cell><cell>0.613</cell><cell>0.941</cell><cell>0.777</cell><cell>-</cell><cell>-</cell></row><row><cell>Tola [44]</cell><cell>0.342</cell><cell>1.190</cell><cell>0.766</cell><cell>-</cell><cell>-</cell></row><row><cell>Gipuma [14]</cell><cell>0.283</cell><cell>0.873</cell><cell>0.578</cell><cell>-</cell><cell>-</cell></row><row><cell>SurfaceNet [20]</cell><cell>0.450</cell><cell>1.040</cell><cell>0.745</cell><cell>-</cell><cell>-</cell></row><row><cell>R-MVSNet [53]</cell><cell>0.383</cell><cell>0.452</cell><cell>0.417</cell><cell>7577</cell><cell>1.28</cell></row><row><cell>P-MVSNet [29]</cell><cell>0.406</cell><cell>0.434</cell><cell>0.420</cell><cell>-</cell><cell>-</cell></row><row><cell>Point-MVSNet [4]</cell><cell>0.342</cell><cell>0.411</cell><cell>0.376</cell><cell>8731</cell><cell>3.35</cell></row><row><cell>MVSNet(D=192) [52]</cell><cell>0.456</cell><cell>0.646</cell><cell>0.551</cell><cell>10823</cell><cell>1.210</cell></row><row><cell>MVSNet+Ours</cell><cell>0.325</cell><cell>0.385</cell><cell>0.355</cell><cell>5345</cell><cell>0.492</cell></row><row><cell>Comp. with MVSNet</cell><cell>28.7%</cell><cell>40.4%</cell><cell>35.6%</cell><cell>50.6%</cell><cell>59.</cell></row></table><note>Methods Acc.(mm) Comp.(mm) Overall(mm) GPU Mem(MB) Run-time(3% Table 1: Multi-view stereo quantitative results of different methods on DTU dataset [1] (lower is better). We conduct this experiment using two resolution settings according to PointMVSNet [4] where MVSNet+Ours uses resolution of 1152 ? 864.use DTU training set [1] to train our method, and test on DTU evaluation set. To validate the generalization of our approach, we also test it on the intermediate set of Tanks and Temples dataset [24] using the model trained on DTU dataest without fine-tuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistical results on the Tanks and Temples dataset<ref type="bibr" target="#b23">[24]</ref> of state-of-the-art multi-view stereo and our methods.</figDesc><table><row><cell>s)</cell></row></table><note>Figure 6: Point cloud results of MVSNet+Ours on the intermediate set of Tanks and Temples dataset [24].Stages Resosution &gt;2mm(%) &gt;8mm(%) Overall (mm) GPU Mem. (MB) Run-time (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The statistical results of different stages in cascade cost volume. The statistics are collected on the DTU evaluation set<ref type="bibr" target="#b0">[1]</ref> using MVSNet+Ours. The run-time is the sum of the current and previous stages. The base of resolution of input images in this experiment is 1152 ? 864.</figDesc><table><row><cell>(a) GT&amp;Ref Img</cell><cell>(b) Stage 1</cell><cell>(c) Stage 2</cell><cell>(d) Stage 3</cell></row><row><cell cols="4">Figure 7: Reconstruction results of each stage. Top row:</cell></row><row><cell cols="4">Ground truth depth map and intermediate reconstructions.</cell></row><row><cell cols="4">Bottom row: Error maps of intermediate reconstructions.</cell></row><row><cell cols="4">train our method with 8 Nvidia GTX 1080Ti GPUs with 2</cell></row><row><cell cols="2">training samples on each GPU.</cell><cell></cell><cell></cell></row><row><cell cols="4">For quantitative evaluation on DTU dataset [1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results of different stereo matching methods with and without cascade cost volume on Scene Flow dataset<ref type="bibr" target="#b29">[30]</ref>. Accuracy, GPU memory consumption and run-time are included for comparisons.</figDesc><table><row><cell>Methods</cell><cell cols="6">All (%) D1-bg D1-fg D1-all D1-bg D1-fg D1-all Noc (%)</cell></row><row><cell>DispNetC [30]</cell><cell>4.32</cell><cell>4.41</cell><cell>4.34</cell><cell>4.11</cell><cell>3.72</cell><cell>4.05</cell></row><row><cell>GC-Net [22]</cell><cell>2.21</cell><cell>6.16</cell><cell>2.87</cell><cell>2.02</cell><cell>5.58</cell><cell>2.61</cell></row><row><cell>CRL [34]</cell><cell>2.48</cell><cell>3.59</cell><cell>2.67</cell><cell>2.32</cell><cell>3.12</cell><cell>2.45</cell></row><row><cell>iResNet-i2e2 [27]</cell><cell>2.14</cell><cell>3.45</cell><cell>2.36</cell><cell>1.94</cell><cell>3.20</cell><cell>2.15</cell></row><row><cell>SegStereo [49]</cell><cell>1.88</cell><cell>4.07</cell><cell>2.25</cell><cell>1.76</cell><cell>3.70</cell><cell>2.08</cell></row><row><cell>PSMNet [3]</cell><cell>1.86</cell><cell>4.62</cell><cell>2.32</cell><cell>1.71</cell><cell>4.31</cell><cell>2.14</cell></row><row><cell>GwcNet [15]</cell><cell>1.74</cell><cell>3.93</cell><cell>2.11</cell><cell>1.61</cell><cell>3.49</cell><cell>1.92</cell></row><row><cell>GwcNet+Ours</cell><cell>1.59</cell><cell>4.03</cell><cell>2.00</cell><cell>1.43</cell><cell>3.55</cell><cell>1.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different stereo matching methods on KITTI2015 benchmark<ref type="bibr" target="#b31">[32]</ref>. max(3px, 0.05d * ), where d * denotes the ground-truth disparity.</figDesc><table><row><cell>Benchmark Performance Quantitative results of differ-</cell></row><row><cell>ent stereo methods on Scene Flow dataset [30] is shown in</cell></row><row><cell>Table 4. By applying the cascade 3D cost volume, we boost</cell></row><row><cell>the accuracy in all the metrics and less memory is required</cell></row><row><cell>owing to the cascade design with smaller number of dispar-</cell></row><row><cell>ity hypothesis. Our method reduces the end-point-error by</cell></row><row><cell>0.166, 0.116 and 0.050 on PSMNet [3] (0.887 vs. 0.721),</cell></row><row><cell>GwcNet [15] (0.765 vs. 0.649) and GANet11 [56] (0.950</cell></row><row><cell>vs. 0.900) respectively. The obvious improvement on &gt;1px</cell></row><row><cell>indicates that small errors are suppressed with the introduc-</cell></row><row><cell>tion of high-resolution cost volumes. In KITTI 2015 [32],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="7">Comparisons between MVSNet [52] and MVS-</cell></row><row><cell cols="7">Net using our cascade cost volume with different setting of</cell></row><row><cell cols="7">depth hypothesis numbers and depth intervals. The statis-</cell></row><row><cell cols="5">tics are collected on DTU dataset [1].</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">cascade? upsample? feature pyramid? Acc. (mm) Comp. (mm) Overall (mm)</cell></row><row><cell>MVSNet</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.456</cell><cell>0.646</cell><cell>0.551</cell></row><row><cell>MVSNet-Cas3</cell><cell></cell><cell>?</cell><cell>?</cell><cell>0.450</cell><cell>0.455</cell><cell>0.453</cell></row><row><cell>MVSNet-Cas3-Ups</cell><cell></cell><cell></cell><cell>?</cell><cell>0.419</cell><cell>0.338</cell><cell>0.379</cell></row><row><cell>MVSNet+Ours</cell><cell></cell><cell>?</cell><cell></cell><cell>0.325</cell><cell>0.385</cell><cell>0.355</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparisons between GwcNet<ref type="bibr" target="#b14">[15]</ref> and Gwc-Net using our cascade cost volume with different setting of the numbers of hypothesis planes and depth intervals. The statistics are collected on the test set of Scene Flow dataset<ref type="bibr" target="#b29">[30]</ref> </figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Limitations and Future Works</head><p>The proposed cascade cost volume formulation benefits from decomposing the single cost volume into a cascade formulation of multiple stages. We have analyzed the effect of hypothesis range setting in Section 6.1. Although the cascade formulation is complementary to existing 3D cost-volume-based multi-view stereo and stereo matching approaches, some limitations still exist. As shown in <ref type="figure">Figure 13</ref>, GwcNet+Ours generates a biased result since the earlier stages output erroneous disparity and the hypothesis range in the next stage is not able to cover its corresponding ground truth value. Note that this case happens with little probability since the cascade cost volume formulation could correct almost erroneous predictions according to the analysis in Section 6.1 and the overall performance is also better than single cost volume models.</p><p>Currently, the hypothesis range of each pixel is identical. The future works include determine the hypothesis range for each region by incorporating semantic information but probably need a more flexible cost volume formulation.  <ref type="bibr" target="#b14">[15]</ref> (o) Error map of GwcNet <ref type="bibr" target="#b14">[15]</ref> (p) GT (q) GwcNet+Ours (r) Error map of GwcNet+Ours <ref type="figure">Figure 12</ref>: Qualitative results on the test set of Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref>. We show the results of several representative stereo CNNs and the extended models with the proposed cascade cost volume.</p><p>(a) Ref Img (b) GwcNet <ref type="bibr" target="#b14">[15]</ref> (c) Error map of GwcNet <ref type="bibr" target="#b14">[15]</ref> (d) GT (e) GwcNet+Ours (f) Error map of GwcNet+Ours <ref type="figure">Figure 13</ref>: A failed case of GwcNet+Ours on the test set of Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref>. Top row: Reference image, the prediction of GwcNet <ref type="bibr" target="#b14">[15]</ref> and the error map of GwcNet. Bottom row: Ground Truth, the prediction of GwcNet+Ours and the error map of GwcNet+Ours. The red arrow points out the wrong prediction region. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using multiple hypotheses to improve depth-maps for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duggal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4384" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view stereo by temporal nonparametric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2651" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tapa-mvs: Textureless-aware patchmatch multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romanoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10413" to="10422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tonioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5893" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning multi-view stereo with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4312" to="4321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-view stereo: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CGV</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="148" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learned multi-patch similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1586" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2821" to="2830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00538</idno>
		<title level="m">Stephen Lin, and In So Kweon. Dpsnet: end-to-end deep plane sweep stereo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segment-based stereo matching using belief propagation and a self-ddapting dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Karner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale ccene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiriakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A quasi-dense approach to surface reconstruction from uncalibrated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="433" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow sstimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segment-tree based cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-level context ultra-aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Yu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3283" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<ptr target="https://github.com/kysucix/fusibile/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structurefrom-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Photorealistic scene reconstruction by voxel coloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="35" />
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan-Ning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<title level="m">Stereo matching using belief propagation. TPAMI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="787" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient large-scale multi-view stereo for ultra high-resolution image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic stereo matching with pyramid cost volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-view stereo with asymmetric checkerboard propagation and multi-hypothesis joint view selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical deep stereo matching on highresolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page" from="636" to="651" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A non-local cost aggregation method for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relative camera refinement for accurate dense reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multiview stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5525" to="5534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-toend stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cross-based local stereo matching using orthogonal integral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gauthier</forename><surname>Lafruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1073" to="1079" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distributed very large scale bundle adjustment by global camera consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Very large-scale global sfm by distributed motion averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4568" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
