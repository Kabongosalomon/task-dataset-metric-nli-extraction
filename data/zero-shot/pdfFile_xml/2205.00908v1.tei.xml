<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MemSeg: A semi-supervised method for image surface defect detection using differences and commonalities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MemSeg: A semi-supervised method for image surface defect detection using differences and commonalities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Defect detection</term>
					<term>Semi-supervised learning</term>
					<term>U-Net</term>
					<term>Deep neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Under the semi-supervised framework, we propose an end-to-end memory-based segmentation network (MemSeg) to detect surface defects on industrial products. Considering the small intra-class variance of products in the same production line, from the perspective of differences and commonalities, MemSeg introduces artificially simulated abnormal samples and memory samples to assist the learning of the network. In the training phase, MemSeg explicitly learns the potential differences between normal and simulated abnormal images to obtain a robust classification hyperplane. At the same time, inspired by the mechanism of human memory, MemSeg uses a memory pool to store the general patterns of normal samples. By comparing the similarities and differences between input samples and memory samples in the memory pool to give effective guesses about abnormal regions; In the inference phase, MemSeg directly determines the abnormal regions of the input image in an end-to-end manner. Through experimental validation, MemSeg achieves the state-of-the-art (SOTA) performance on MVTec AD datasets with AUC scores of 99.56% and 98.84% at the image-level and pixel-level, respectively. In addition, MemSeg also has a significant advantage in inference speed benefiting from the end-to-end and straightforward network structure, which better meets the real-time requirement in industrial scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The detection of product surface anomalies in industrial scenarios is crucial to the development of industrial intelligence. Surface defect detection is a problem of locating abnormal regions in images, such as scratches and smudges. But in practical applications, anomaly detection by traditional supervised learning is more difficult due to the low probability of abnormal samples and the diverse forms of anomalies. Therefore, the methods based on semisupervised techniques for surface defect detection have more significant advantages in practical applications, which require only normal samples in the training phase.</p><p>Based on semi-supervised techniques, most image surface defect detection models attempt to explore the general patterns of normal samples efficiently. For example, reconstruction models based on autoencoder (AE) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or generative adversarial network (GAN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> aim to reconstruct normal images with minimal error and locate anomalies based on the reconstruction error. But due to the powerful generalization ability of CNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, abnormal regions may also be reconstructed correctly in the inference stage, which clearly violates the basic assumptions of the reconstruction models. Recently, the embedding-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> have shown better anomaly detection performance than reconstruction-based methods. The fundamental principle of the embedding-based methods is the feature matching between the test samples and the normal samples. Although such models require little time consumption in the training phase, they need to perform complex feature matching operations in the inference phase, which incurs excessive computational costs for the inference of the model. Besides, such models are not trained using anomaly-specific datasets and directly use pre-trained parameters for feature extraction and anomaly detection, which is not sufficiently adaptable to the anomaly detection task.</p><p>Given the shortcomings of existing methods, we propose an end-to-end memory-based segmentation network (MemSeg) in this paper to accomplish the defect detection of product surface. Instead of reconstructing the input images, our model determines the abnormal regions in the images end-to-end. Also, our model does not entirely rely on the pre-trained model for feature extraction, which alleviates the problem of inconsistent distribution between the source and target domains. The design of MemSeg is based on the observation of the small intra-class variance of products in the same industrial production line, we believe that artificially creating intra-class differences and preserving intra-class commonalities can help the model achieve better defect detection performance. These two types of information, differences and commonalities, can provide a more direct orientation to the model learning and help the model generalize normal patterns more comprehensively and distinguish non-normal patterns more precisely. Specifically, from the perspective of differences, similar to self-supervised learning, MemSeg introduces artificially simulated anomalies during the training phase to make the model consciously discriminate normal from non-normal while does not require the simulated anomalies are consistent with those in the real scenarios, which alleviates the deficiency that semi-supervised learning can only use normal samples and allows the model to obtain a more robust decision boundary. MemSeg uses normal and simulated abnormal images to finish the model training and directly judges the abnormal regions of the input images without any auxiliary tasks in the inference phase. <ref type="figure" target="#fig_0">Fig. 1</ref> shows our data usage during the training and inference phases.</p><p>Meanwhile, from the perspective of commonalities, MemSeg introduces a memory pool to record the general patterns of normal samples. In the training and inference phases of the model, we compare the similarities and differences between the input samples and the memory samples in the memory pool to provide more effective information for the localization of abnormal regions. In addition, in order to coordinate the information from the memory pool and the input image more effectively, MemSeg introduces a multi-scale feature fusion module and a novelty spatial attention module, which substantially improves the performance of the model.</p><p>With the target of precise localization of abnormal regions in images, MemSeg achieves state-of-the-art (SOTA) performance with 99.56% and 98.84% AUROC scores in image-level and pixel-level, respectively, on the MVTec AD dataset <ref type="bibr" target="#b0">[1]</ref>, which contains 75 different forms of anomalies in real scenarios. Meanwhile, MemSeg also has outstanding performances on other datasets. More precise but faster, MemSeg can process 31.34 images per second in the inference phase using an NVIDIA RTX 3090 GPU, which better meets the real-time requirements in industrial production.</p><p>To summarize, the main contributions of this paper are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a well-designed anomaly simulation strategy for self-supervised learning of the model, which integrates the three aspects of target foreground, textural and structural anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a memory module with a more efficient feature matching algorithm, and innovatively introduce the memory information of normal patterns in the U-Net structure to assist the model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Through the two points above, and combining the multi-scale feature fusion module and the spatial attention module, we effectively simplify semi-supervised anomaly detection into an end-to-end semantic segmentation task, making semi-supervised image surface defect detection more flexible.</p><p>? Through extensive experimental validation, MemSeg has high accuracy in surface defect detection and localization tasks while better meeting the real-time requirements of industrial scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Reconstruction-Based Methods</head><p>One of the traditional methods for image anomaly detection is reconstruction-based methods. Most reconstructionbased methods use autoencoder (AE) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or generative adversarial network (GAN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> to train a network to reconstruct the input images. The basic assumption of the reconstruction models is that the model can reconstruct normal images with a small error and reconstruct abnormal images with a large error. However, in practical applications, the learning ability of neural networks is too strong <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, so the abnormal regions in the image may also be reconstructed well, and anomaly discrimination based on the reconstruction error may be invalid. To reduce the influence of abnormal regions on the reconstruction models, RIAD <ref type="bibr" target="#b14">[15]</ref>, based on an autoencoder, performs a multiscale complementary mask operation on the original image and tries to cover the abnormal regions using the mask. Similarly, InTra <ref type="bibr" target="#b15">[16]</ref>, based on a transformer, uses the image with a masked patch as the input and completes a patch repair task. However, in any case, the reconstruction models are still affected by the abnormal regions in the inference phase because the exact locations of abnormal regions are not clear. MemSeg is still based on an AE but avoids the reconstruction process of the input image and completes the anomaly localization in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anomaly Simulation-Based Methods</head><p>The semi-supervised models of image surface defect detection use only normal samples as training data. For the models to explicitly learn the potential differences between normal and abnormal samples, some works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> attempted to generate artificially simulated abnormal samples during training. Specifically, DRAEM <ref type="bibr" target="#b16">[17]</ref> superimposes additional texture images as noise onto the normal images to generate abnormal regions, and this type of data augmentation method aims to create textural anomalies. CutPaste <ref type="bibr" target="#b17">[18]</ref> and AnoSeg <ref type="bibr" target="#b18">[19]</ref> use an augmentation method similar to copy and paste. This kind of method randomly copies a small rectangular area from the input image and randomly pastes it to the image to simulate abnormal samples. By pasting rectangular patches of different sizes, aspect ratios, and rotation angles to create structural anomalies. As a means of data processing, the existing anomaly simulation methods only consider structural anomalies or textural anomalies one-sidedly; at the same time, for some datasets, there is a problem of low simulation efficiency because the target foreground and background in images cannot distinguish well. However, the anomaly simulation strategy used by MemSeg solves these shortcomings.</p><p>In addition, despite the introduction of simulated abnormal samples, AnoSeg and DRAEM still need to complete the reconstruction process of the input image; CutPaste only completes the defect detection at the image level, and the defect localization at the pixel level is implemented by GradCAM or Gaussian density estimation. More directly, with the help of our well-designed anomaly simulation strategy, our model does not need the reconstruction of the input image as an auxiliary task for model learning and completes the defect localization at the pixel-level end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Embedding-Based Methods</head><p>Embedding-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> usually use a pre-trained network on ImageNet <ref type="bibr" target="#b19">[20]</ref> to extract the highlevel features of the original images, and the anomaly score is calculated through the distance between the test sample and the normal samples on the features to obtain the abnormal regions. FYD <ref type="bibr" target="#b9">[10]</ref> designs a two-stage coarse-to-fine feature alignment network that learns robust feature distributions of normal images; SPADE <ref type="bibr" target="#b11">[12]</ref> extends the KNN anomaly detection method to pixel-level and detects anomalies in the images through the pixel-level correspondence between the test image and normal images; PaDiM <ref type="bibr" target="#b12">[13]</ref> uses a pre-trained CNN to extract the patch embeddings of the input image and uses the multivariate Gaussian distribution to obtain the probability representation of the normal samples. Due to the simplicity and effectiveness, embedding-based models are widely used, but they usually require a complex features matching process in the inference phase, which greatly limits the inference speed of models. The memory module in our model is still based on the principle of embedding-based methods, but through the design of a more efficient feature matching algorithm, the memory module does not add too much computational cost to the model while guaranteeing the model precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we demonstrate our novel framework to detect and localize fine-grained anomalies. An overview of MemSeg is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. MemSeg uses U-Net <ref type="bibr" target="#b20">[21]</ref> as a framework to complete a semantic segmentation task with the help of simulated abnormal samples and memory information in the training phase, and localizes abnormal regions in images end-to-end in the inference phase. MemSeg consists of several essential parts, we will describe these parts in the following order: generation of abnormal samples by the way of artificial simulation (Subsection 3.1), generation of memory information and spatial attention maps (Subsection 3.2), multi-scale feature fusion module (MSFF Module) for the fusion of memory information and high-level features of images (Subsection 3.3), and loss functions (Subsection 3.4).  <ref type="bibr" target="#b13">[14]</ref> as an encoder. From the perspective of differences and commonalities, MemSeg introduces simulated abnormal samples and a memory module to assist the model learning in a more oriented way, and thus accomplishes the semi-supervised surface defect task in an end-to-end manner. At the same time, in order to fully fuse the memory information with the high-level features of the input image, MemSeg introduces a multi-scale feature fusion module (MSFF Module) and a novel spatial attention module, which greatly improves the model precision of anomaly localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Anomaly Simulation Strategy</head><p>In industrial scenarios, anomalies occur in various forms, and it is impossible to cover all of them when performing data collection, which limits the modeling with the supervised learning methods. However, in the semi-supervised framework, using only normal samples and no comparisons with non-normal samples is not sufficient for the model to learn what are the normal patterns. In this paper, inspired by DRAEM <ref type="bibr" target="#b16">[17]</ref>, we design a more effective strategy to simulate abnormal samples and introduce them during training to accomplish self-supervised learning. MemSeg summarizes the patterns of normal samples by comparing non-normal patterns to mitigate the drawbacks of semisupervised learning. As shown in <ref type="figure">Fig. 3</ref>, the anomaly simulation strategy proposed in this paper is mainly divided into three steps.</p><p>In the first step, a two-dimensional Perlin noise <ref type="bibr" target="#b21">[22]</ref> is generated, then is binarized by threshold T to obtain the mask . The Perlin noise has several random peaks, and generated by it can extract contiguous blocks of regions in the image. At the same time, considering that the proportion of the main body of some industrial components in the acquired image is small, if data augmentation is performed directly without processing, it is easy to generate noise in the background part of the image, which increases the differences between simulated abnormal samples and real abnormal samples on the data distribution, which is not conducive for the model to learn effective discriminative information, so we adopt a foreground enhancement strategy for this type of images. That is, the input image is binarized to obtain the mask , and the noise generated in the binarization process is removed using the open or close operation. After that, the final mask image is obtained by performing an element-wise product on the two obtained masks. <ref type="figure">Fig.3</ref>. The three steps of our anomaly simulation strategy. In the first step, the mask image is generated using Perlin noise and target foreground; in the second step, the ROI defined by in the noise image is extracted to generate the noise foreground image ; in the third step, the noise foreground image is superimposed on the original image to obtain the simulated abnormal image .</p><p>In the second step, the mask image and the noise image perform element-wise product to get the region of interest (ROI) in defined by . Consistent with DRAEM <ref type="bibr" target="#b16">[17]</ref>, we introduce a transparency factor in this process to balance the fusion of the original image and the noisy image, so that the patterns of simulated anomalies are closer to the real anomalies. Therefore, the noisy foreground image is generated using the following equation:</p><formula xml:id="formula_0">= ( ? ) + (1 ? )( ? )<label>(1)</label></formula><p>For the noisy image , we want its maximum transparency to be higher to increase the difficulty of model learning and thus improve the robustness of the model. So for in Eq. 1, we will randomly and uniformly sample from [0.15, 1].</p><p>In the third step, the mask image is inverted to obtain , then the element-wise product is performed on and the original image to obtain image , and according to</p><formula xml:id="formula_1">= ? +<label>(2)</label></formula><p>the data-augmented image , namely, the simulated abnormal image, is obtained. takes the original input image as the background and the ROI in the noise image extracted by mask image as the foreground.</p><p>Among them, the noisy image comes from two parts, one part from the DTD texture dataset <ref type="bibr" target="#b22">[23]</ref>, which aims to simulate textural anomalies, the other part from the input image, which aims to simulate structural anomalies. For the simulation of structural anomalies, we first perform random adjustment of mirror symmetry, rotation, brightness, saturation, and hue on the input image . Then the preliminary processed image is uniformly divided into a 4?8 grid and randomly arranged to obtain the disordered image .</p><p>With the above anomaly simulation strategy, we obtain simulated abnormal samples from both textural and structural perspectives, and most abnormal regions are generated on the target foreground, which maximizes the similarity between the simulated abnormal samples and the real abnormal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Module and Spatial Attention Maps</head><p>Memory Module. For humans, our identification of anomalies is predicated on knowing what is normal, and the abnormal regions are obtained by comparing the test image with the normal images in our memory. Inspired by the human learning process and embedding-based methods, we use a small number of normal samples as memory samples and extract high-level features of the memory samples as memory information using a pre-trained encoder (ResNet18 <ref type="bibr" target="#b13">[14]</ref>) to assist the learning of MemSeg.</p><p>To obtain the memory information, we first randomly select normal images from the training data as memory samples and input them to the encoder to get features of dimensions N?64?64?64, N?128?32?32, and N?256?16?16 from block 1, block 2, and block 3 of ResNet18, respectively. These features with different resolutions together constitute the memory information . It needs to be emphasized that in order to ensure the unification of the memory information and the high-level features of the input images, we always freeze the model parameters of block 1, block 2, and block 3 in ResNet18, but the rest of the model is still trainable.</p><p>Given an input image in the training or inference phase, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the encoder also extracts high-level features of the input image to obtain features with dimensions of 64?64?64, 128?32?32, and 256?16?16. These features with different resolutions together constitute the information of the input image . After that, the L2 distance between the and all the memory information is calculated, so difference information between the input image and the memory samples is obtained:</p><formula xml:id="formula_2">= ? ? ? (3)</formula><p>where is the number of memory samples. For difference information, take the minimum sum of all elements in each as the standard to obtain the best difference information * between and ; that is, * = argmin ? ?</p><p>where ? [1, ]. The best difference information * contains the differences between the input sample and its most similar memory sample, the larger the difference value at a position, the higher the probability that the region of the input image corresponding to that position is abnormal. Subsequently, the best difference information * completes the concatenation operation with the high-level features of the input image in the channel dimension to obtain the concatenated information , and . Finally, the concatenated information will go through the multi-scale feature fusion module for feature fusion, and the fused features flow to the decoder through the skip connection of U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Attention Maps.</head><p>It is evident from specific observations and experiments (Subsection 4.6) that the best difference information * has an important influence on the localization of abnormal regions. To make full use of the difference information, we extract three spatial attention maps using * , which are used to reinforce the guesses of best difference information on the abnormal regions.</p><p>For the features with three different dimensions in * , the mean values are calculated in the channel dimension, and three feature maps of size 16?16, 32?32 and 64?64 are obtained, respectively. The 16?16 feature map is directly used as the spatial attention map . After is up-sampled, perform the element-wise product operation with the 32?32 feature map to obtain ; and after is up-sampled, perform the element-wise product operation with the 64?64 feature map to obtain . As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, spatial attention map , and weighted the information which obtained after , and are processed by the MSFF, respectively. Mathematically, the formulas for solving , and are given as follows:</p><formula xml:id="formula_4">= 1 * (5) = 1 * ? (6) = 1 * ? (7)</formula><p>where denotes the number of channels of * ; * denotes the feature map of channel in * ; and denote the feature maps obtained after up-sampling and , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Scale Feature Fusion Module</head><p>With the help of the memory module, we obtain the concatenated information composed of the input image information and the best difference information * . The direct use of has the problem of feature redundancy on the one hand; on the other hand, it increases the computational scale of the model and causes a decrease in the inference speed. Given the success of multi-scale feature fusion in target detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, an intuitive idea is to fully fuse the visual information and semantic information in the concatenated information with the help of the channel attention mechanism and multi-scale feature fusion strategy. <ref type="figure">Fig.4</ref>. The multi-scale feature fusion module used by MemSeg. Considering that is a concatenation of two kinds of information in the channel dimension, and comes from different locations of the encoder with different semantic information and visual information, so we use channel attention CA-Block and multi-scale strategy for feature fusion.</p><p>Our proposed multi-scale feature fusion module is shown in <ref type="figure">Fig. 4</ref>: the concatenated information ( = 1,2,3) is initially fused by a 3?3 convolutional layer that maintains the number of channels. Meanwhile, considering that is a simple concatenation of two kinds of information in the channel dimension, we use coordinate attention (CA) <ref type="bibr" target="#b25">[26]</ref> to capture the information relationship between channels of . Then, for the features with different dimensions weighted by coordinate attention, we continue perform multi-scale information fusion: the feature maps of different dimensions are firstly aligned in resolution using up-sampling, then aligned in the number of channels using convolution, finally the operation of element-wise add is executed to achieve multi-scale feature fusion. The fused features are weighted by the spatial attention maps ( = 1,2,3) obtained in Subsection 3.2 and then fed to the final decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Constraints</head><p>To ensure that the predicted value of the MemSeg is close to its ground truth, we use L1 loss and focal loss <ref type="bibr" target="#b26">[27]</ref> to guarantee the similarity of all pixels in the image space. The segmentation images predicted under the L1 loss constraint retain more edge information compared to the L2 loss. Meanwhile, focal loss alleviates the problem of area imbalance between normal and abnormal regions in images and makes the model focus more on the segmentation of difficult samples to improve the accuracy of abnormal segmentation.</p><p>Specifically, we minimize the L1 loss and the focal loss between the ground truth of the abnormal regions in the simulated image and the predicted value of the model using <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_5">(9)</ref>, respectively.</p><formula xml:id="formula_5">= ? (8) = ? (1 ? ) ( )<label>(9)</label></formula><p>where is equal to the predicted probability of the pixel category when the ground truth of the corresponding pixel in S is 1, and = 1 ? when the ground truth of the pixel in is 0, and as hyperparameters to control the degree of weighting. Finally, we combine all these constraints into an objective function, and arrive at the following objective function:</p><formula xml:id="formula_6">= +<label>(10)</label></formula><p>where and are the balancing hyper-parameters. In the training process, our optimization goal is to minimize the objective function defined by Eq. (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the performance of MemSeg as well as the functionalities of different components on the semi-supervised anomaly detection datasets: MVTec AD dataset <ref type="bibr" target="#b0">[1]</ref>, BeanTech AD dataset <ref type="bibr" target="#b27">[28]</ref>, and a toy dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metric</head><p>The MVTec AD dataset <ref type="bibr" target="#b0">[1]</ref> is mainly aimed to the task of semi-supervised surface anomaly detection. The MVTec AD dataset comprises 15 categories, including 5 different texture categories and 10 different object categories, each category includes about 60 to 400 normal samples for training and a mixture of normal and abnormal images for testing, and the test set contains a variety of realistic anomalies with different textures and scales. The BeanTech dataset <ref type="bibr" target="#b27">[28]</ref> has 3 categories of 2540 images, which also only contain normal images in the training set.</p><p>For the evaluation metric on the image surface, following the works in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, we leverage imagelevel and pixel-level ROC-AUC for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>MemSeg is based on the AE model, it uses ResNet18 <ref type="bibr" target="#b13">[14]</ref> as the encoder. And for the decoder part, corresponding to <ref type="figure" target="#fig_1">Fig. 2</ref>, the up-sampling layer contains a bilinear interpolation layer and a basic convolution block consisting of a convolution layer, batch-normalization, and a ReLU activation function. The Conv Layer contains two stacked basic convolution blocks; only the last Conv Layer contains one basic convolution block and a 2-channel convolution layer. The training process of MemSeg is carried out with 2700 iterations, the size of the input image is set to 256?256, and the batch size is set to 8, which contains 4 normal samples and 4 simulated abnormal samples. When performing anomaly simulation, most categories have an equal probability of using textural anomaly simulation and structural anomaly simulation. We use a grid search to set hyper-parameters: the learning rate used is set to 0.04; in the focal loss is set to <ref type="bibr">4;</ref> and in the objective function is set to 0.6 and 0.4, respectively. For most of the categories in both datasets, we randomly select 30 memory samples in the training set to generate the memory information, but since the orientation of the screws in the MVTec AD dataset is randomly arranged, we increased the number of their memory samples for better feature matching; the sample size of toothbrushes in the training set is too small, so we use only 10 memory samples while ensuring adequate training samples. MemSeg obtains the anomaly score for each pixel in the image in an end-to-end manner, and the mean of the scores of the top 100 most abnormal pixel points in the image is used as the anomaly score at the image-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Methods</head><p>In this subsection, we compare MemSeg with different methods. The AUC scores of different methods are listed in Tables 1 and 2. We can see that our method outperforms most existing methods, which demonstrates the effectiveness of our method. For the MVTec AD dataset, at the image-level, the category with the worst anomaly detection effect is the screw. On the one hand, the reason is that our model relies more on the alignment of detection targets in space, but the orientation of screws in the dataset is randomly arranged, which makes it difficult to generate effective difference information; on the other hand, the reason is that some abnormal regions of screws in the test set are small and difficult to distinguish, and the model is prone to misclassification, which is also reflected in other models. At the pixel-level, the category with the worst anomaly detection effect is the transistor, part of the reason is that when the transistor in the circuit is missing or the direction is shifted, it is difficult for our model to give a precise anomaly localization.</p><p>For the BeanTech AD dataset, although the anomaly localization of MemSeg at the pixel-level is not as good as PaDiM (97.1% vs 97.3%), the AUC score of MemSeg is better than all the models in the experiment at the imagelevel, which indicates that our model still has an accurate anomaly detection capability for other datasets. <ref type="figure">Fig.5</ref>. Comparison of MemSeg with PaDiM <ref type="bibr" target="#b12">[13]</ref> and SPADE <ref type="bibr" target="#b11">[12]</ref> for anomaly localization on the MVTec AD dataset (before thresholding). Our model has a more precise judgment of the abnormal regions.</p><p>Meanwhile, as shown in <ref type="figure">Fig. 5</ref>, the anomaly localization of MemSeg at the pixel-level is closer to the ground truth, and the boundary between the normal and abnormal regions is more precise, which benefits from the end-to-end learning approach adopted by MemSeg and the training of the model is directly guided by the pixel-level ground truth of simulated abnormal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of Anomaly Simulation Strategy</head><p>To evaluate the effectiveness of the proposed anomaly simulation strategy for image defect detection, we remove textural anomaly simulation, structural anomaly simulation, and foreground enhancement strategy in training, respectively, and compare the three cases with our complete strategy. We report the AUC scores at the image-level and pixel-level for each of the above four experiments in <ref type="table" target="#tab_1">Table 3</ref>. The AUC scores decrease when either component of the anomaly simulation strategy is removed, which verifies that our anomaly simulation strategy is not only theoretically interpretable, but also has an excellent performance in experimental validation. Now, to evaluate the role of simulated abnormal samples more fully, we also want to know the data distribution of simulated and real abnormal samples after the training of the model, so we visualize the output of the encoder of simulated abnormal samples, the real abnormal samples in the test set and the normal samples using t-SNE <ref type="bibr" target="#b30">[31]</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 6</ref>, for most categories, there is some overlapping in the spatial distribution of simulated abnormal samples and the real abnormal samples, while the abnormal samples are separated from the normal samples, which proves the validity of our anomaly simulation strategy. It is important to emphasize that the features we visualized are generated only at the bottleneck structure of U-Net. Although the separability of some features in some categories is not strong in two-dimensional space, our model can still be corrected in the decoder part using information from the skip connection. Besides, MemSeg does not completely require the distribution of simulated abnormal samples to be the same as real abnormal samples. MemSeg is based on the semi-supervised learning framework, the reason we introduce the simulated abnormal samples during training is simply to make the model explicitly learn the difference between normal and non-normal, so the model can better generalize the general patterns of normal samples, and then treats samples outside the normal patterns as abnormal samples. As shown in <ref type="figure" target="#fig_2">Fig. 6(d)</ref>, for leather, although the distribution of real and simulated abnormal and normal samples in two-dimensional space is not ideal, MemSeg can finally complete the accurate defect localization at the pixel-level with an AUC score of 99.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of Different Losses</head><p>In MemSeg, we use L1 loss and focal loss as the loss functions. In <ref type="table" target="#tab_2">Table 4</ref>, we report the AUC scores of MemSeg for anomaly localization when using different loss functions.  Since the ratio of normal samples to simulated abnormal samples is 1:1 in the training phase, and the proportion of abnormal regions in the simulated abnormal samples to the image is small, there exists data imbalance in the training samples at the pixel-level, so the model is difficult to train using only L1 loss. Therefore, focal loss needs to be introduced to make the model focus more on the abnormal regions in the training samples. As shown in <ref type="figure" target="#fig_3">Fig. 7</ref>, the simultaneous use of L1 loss and focal loss helps the model to learn better anomaly discrimination while converging more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Impact of Module Components</head><p>This subsection mainly discusses the effects of the memory module, multi-scale strategy, spatial attention, and CA [26] on the model. As shown in <ref type="table" target="#tab_3">Table 5</ref>, memory information has a significant effect on abnormal localization. To further explore the effect of memory sample size on anomaly localization, we report the changes of AUC scores when the number of memory samples is 1, 15, 30, and 70 in <ref type="figure" target="#fig_4">Fig. 8</ref>. Within a certain range, as the number of memory samples increases, the model locates the abnormal regions more accurately, but when the number of memory samples is too large, it causes insufficient training samples and leads to a slight decrease on AUC scores, so an appropriate number of memory samples is crucial for MemSeg.  As shown in <ref type="table" target="#tab_3">Table 5</ref>, the multi-scale strategy, spatial attention, and CA contribute significantly to the anomaly detection performance, and the multi-scale strategy has the greatest impact. In <ref type="figure" target="#fig_5">Fig. 9</ref>, we visualize the generation process of spatial attention maps , and following the steps in Subsection 3.2. As shown in the figure, the best difference information * ( = 1,2,3 ) with different scales, which is generated relying on the memory information, already contains a blurred guess of the abnormal region in the input image. In the process of generating the spatial attention maps from * , after multi-scale fusion, the noise in the heat map becomes less and the guess of the abnormal regions in the image becomes more certain. Although the final generated spatial attention map is still different from the final prediction of MemSeg to some extent, it is close to the ground truth of the abnormal regions, which visually demonstrates the effectiveness of memory information, multi-scale fusion, and spatial attention. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Evaluation with A Toy Dataset</head><p>The noise generated by MemSeg using the anomaly simulation strategy is irregular. To verify the ability of MemSeg to generalize regular noise, we generate a toy dataset using normal samples in the test set of MVTec AD dataset. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, the shapes of the generated noise are rectangle, triangle, lightning bolt, star, heart, and circle; and the size, color, position, angle, and aspect ratio of the noise are random. For the abnormal samples in the toy dataset, it is never seen in the training phase of MemSeg. We apply the trained model directly to the toy dataset and compare the performance with three models. The AUC scores of the four models are shown in <ref type="table" target="#tab_4">Table 6</ref>. MemSeg achieves precise localization of the abnormal regions with an AUC score close to 100%, which further demonstrates the strong generalization ability of our model to localize unknown anomalies.  <ref type="figure" target="#fig_0">Fig.10</ref>. The toy dataset is generated using six regular shapes and regular colors to verify the generalization ability of MemSeg to regular noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Inference Speed</head><p>Compared to reconstruction-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, the embedding-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> achieve better performance in semi-supervised defect detection of images, but this kind of models needs to perform complex feature matching in the inference phase, which is difficult to be applied in industrial scenarios with high real-time requirements. Therefore, we are also interested in the inference speed of MemSeg. Our experiments are carried out on a PC with an NVIDIA RTX 3090 GPU. We calculate the time consumption of PaDiM <ref type="bibr" target="#b12">[13]</ref> and SPADE <ref type="bibr" target="#b11">[12]</ref> in the inference phase, and the time to process one image is 0.319s and 0.339s for these two models, respectively, while the time to process one image is 0.0319s for MemSeg, which is a 10-fold improvement in the inference speed. Meanwhile, compared with the reconstruction-based models, MemSeg avoids the conventional way of reconstructing the input samples and achieves the segmentation of abnormal regions in an end-to-end manner, which also has a competitive advantage in the inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Considering the small intra-class variation of products in the same production line, MemSeg simplifies the semisupervised image surface defect detection into a simple and straightforward semantic segmentation task by introducing well-designed anomaly simulation strategy and memory information from the perspective of difference and commonality. Simple but high performance, MemSeg achieves SOTA performance while meeting the real-time requirements of industrial scenarios.</p><p>However, consistent with most embedding-based methods, the introduction of memory information makes MemSeg have some requirements on the spatial location of the detection targets in the images. When the location or direction of the detection targets is randomly distributed, such as screws in the MVTec AD dataset, MemSeg requires more memory samples to memorize more normal patterns, which places higher demands on the quality and quantity of the datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The data usage during the training and testing phases. The images are taken from the MVTec AD dataset<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of MemSeg. MemSeg is based on the U-Net architecture and uses a pre-trained ResNet18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Separability display of normal samples, simulated abnormal samples and real abnormal samples in the MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>The effect of L1 loss on anomaly localization. When L1 loss and focal loss are used simultaneously, the edges of the segmented images obtained by MemSeg are more precise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Effects of the different number of memory samples on AUC scores. The vertical coordinates report the mean AUC scores of the 13 categories in the MVTec AD dataset, excluding screw and toothbrush.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>The generation process of spatial attention maps 1 , 2 and 3 . This process visually demonstrates the effectiveness of memory module, multi-scale strategy, and spatial attention for image defect localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .level, Pixel-level).Table 2 .Pixel-level).</head><label>12</label><figDesc>The comparison between our method and different methods on the MVTec AD dataset in terms of ROC-AUC % with the format of (Image-The comparison between our method and different methods on the BeanTech AD dataset in terms of ROC-AUC % with the format of (Image-level,</figDesc><table><row><cell cols="2">Category</cell><cell>SPADE [12]</cell><cell cols="4">PaDiM [13] DRAEM [17] CutPaste [18]</cell><cell cols="2">P-SVDD [29] P-SVDD-C [30]</cell><cell>Ours</cell></row><row><cell></cell><cell>carpet</cell><cell>(-,97.5)</cell><cell>(-,98.9)</cell><cell></cell><cell>(97.0,95.5)</cell><cell>(92.9,92.6)</cell><cell>(93.1,98.3)</cell><cell>(94.4,92.9)</cell><cell>(99.6,99.2)</cell></row><row><cell></cell><cell>grid</cell><cell>(-,93.7)</cell><cell>(-,94.9)</cell><cell></cell><cell>(99.9,99.7)</cell><cell>(94.6,96.2)</cell><cell>(99.9,97.5)</cell><cell>(95.6,97.2)</cell><cell>(100,99.3)</cell></row><row><cell>Texture</cell><cell>leather</cell><cell>(-,97.6)</cell><cell>(-,99.1)</cell><cell></cell><cell>(100,98.6)</cell><cell>(90.9,97.4)</cell><cell>(100,99.5)</cell><cell>(96.1,98.2)</cell><cell>(100,99.7)</cell></row><row><cell></cell><cell>tile</cell><cell>(-,87.4)</cell><cell>(-,91.2)</cell><cell></cell><cell>(99.6,99.2)</cell><cell>(97.8,91.4)</cell><cell>(93.4,90.5)</cell><cell>(93.5,91.9)</cell><cell>(100,99.5)</cell></row><row><cell></cell><cell>wood</cell><cell>(-,85.5)</cell><cell>(-,93.6)</cell><cell></cell><cell>(99.1,96.4)</cell><cell>(96.5,90.8)</cell><cell>(98.6,95.5)</cell><cell>(98.0,92.1)</cell><cell>(99.6,98.0)</cell></row><row><cell></cell><cell>average</cell><cell>(-,92.3)</cell><cell>(-,95.6)</cell><cell></cell><cell>(99.1,97.9)</cell><cell>(94.5,93.7)</cell><cell>(97.0,96.3)</cell><cell>(95.5,94.5)</cell><cell>(99.8,99.1)</cell></row><row><cell></cell><cell>bottle</cell><cell>(-,98.4)</cell><cell>(-,98.1)</cell><cell></cell><cell>(99.2,99.1)</cell><cell>(98.6,98.1)</cell><cell>(98.3,97.6)</cell><cell>(99.5,98.6)</cell><cell>(100,99.3)</cell></row><row><cell></cell><cell>cable</cell><cell>(-,97.2)</cell><cell>(-,95.8)</cell><cell></cell><cell>(91.8,94.7)</cell><cell>(90.3,96.8)</cell><cell>(80.6,90)</cell><cell>(97.8,97.6)</cell><cell>(98.2,97.4)</cell></row><row><cell></cell><cell>capsule</cell><cell>(-,99.0)</cell><cell>(-,98.3)</cell><cell></cell><cell>(98.5,94.3)</cell><cell>(76.7,95.8)</cell><cell>(96.2,97.4)</cell><cell>(88.7,96.3)</cell><cell>(100,99.3)</cell></row><row><cell></cell><cell>hazelnut</cell><cell>(-,99.1)</cell><cell>(-,97.7)</cell><cell></cell><cell>(100,99.7)</cell><cell>(92.0,97.5)</cell><cell>(97.3,97.3)</cell><cell>(97.9,98.2)</cell><cell>(100,98.8)</cell></row><row><cell></cell><cell>metal nut</cell><cell>(-,98.1)</cell><cell>(-,96.7)</cell><cell></cell><cell>(98.7,99.5)</cell><cell>(94.0,98.0)</cell><cell>(99.3,93.1)</cell><cell>(96.5,98.1)</cell><cell>(100,99.3)</cell></row><row><cell>Object</cell><cell>pill</cell><cell>(-,96.5)</cell><cell>(-,94.7)</cell><cell></cell><cell>(98.9,97.6)</cell><cell>(86.1,95.1)</cell><cell>(92.4,95.7)</cell><cell>(91.9,92.4)</cell><cell>(99.0,99.5)</cell></row><row><cell></cell><cell>screw</cell><cell>(-,98.9)</cell><cell>(-,97.4)</cell><cell></cell><cell>(93.9,97.6)</cell><cell>(81.3,95.7)</cell><cell>(86.3,96.7)</cell><cell>(83.3,95.3)</cell><cell>(97.8,98.0)</cell></row><row><cell></cell><cell>toothbrush</cell><cell>(-,97.9)</cell><cell>(-,98.7)</cell><cell></cell><cell>(100,98.1)</cell><cell>(100,98.1)</cell><cell>(98.3,98.1)</cell><cell>(95.6,96.0)</cell><cell>(100,99.4)</cell></row><row><cell></cell><cell>transistor</cell><cell>(-,94.1)</cell><cell>(-,97.2)</cell><cell></cell><cell>(93.1,90.9)</cell><cell>(91.5,97)</cell><cell>(95.5,93.0)</cell><cell>(92.1,93.5)</cell><cell>(99.2,97.3)</cell></row><row><cell></cell><cell>zipper</cell><cell>(-,96.5)</cell><cell>(-,98.2)</cell><cell></cell><cell>(100,98.8)</cell><cell>(97.9,95.1)</cell><cell>(99.4,99.3)</cell><cell>(95.9,96.0)</cell><cell>(100,98.8)</cell></row><row><cell></cell><cell>average</cell><cell>(-,97.57)</cell><cell>(-,97.3)</cell><cell></cell><cell>(97.4,97.0)</cell><cell>(90.8,96.7)</cell><cell>(94.3,95.8)</cell><cell>(93.9,96.2)</cell><cell>(99.4,98.7)</cell></row><row><cell cols="2">Average</cell><cell>(85.5,96.0)</cell><cell cols="2">(95.3,96.7)</cell><cell>(98.0,97.3)</cell><cell>(95.2,96.0)</cell><cell>(92.1,95.7)</cell><cell>(94.4,95.6)</cell><cell>(99.56,98.84)</cell></row><row><cell></cell><cell>Category</cell><cell cols="2">PatchCore [11]</cell><cell cols="2">SPADE [12]</cell><cell>PaDiM [13]</cell><cell>P-SVDD [29]</cell><cell>Ours</cell></row><row><cell></cell><cell>01</cell><cell cols="2">(90.9,95.5)</cell><cell cols="2">(91.4,97.3)</cell><cell>(99.8,97.0)</cell><cell>(95.7,91.6)</cell><cell>(98.7,98.9)</cell></row><row><cell></cell><cell>02</cell><cell cols="2">(79.3,94.7)</cell><cell cols="2">(71.4,94.4)</cell><cell>(82.0,96.0)</cell><cell>(72.1,93.6)</cell><cell>(87.0,96.2)</cell></row><row><cell></cell><cell>03</cell><cell cols="2">(99.8,99.3)</cell><cell cols="2">(99.9,99.1)</cell><cell>(99.4,98.8)</cell><cell>(82.1,91.0)</cell><cell>(99.4,96.3)</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">(90.0,96.5)</cell><cell cols="2">(87.6,96.9)</cell><cell>(93.7,97.3)</cell><cell>(83.3,92.1)</cell><cell>(95.0,97.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Evaluating the components of our anomaly simulation strategy on the MVTec AD dataset. The AUC scores are reported for different strategies.</figDesc><table><row><cell></cell><cell>w/o Texture</cell><cell>w/o Structure</cell><cell>w/o Foreground</cell><cell>All</cell></row><row><cell>Image-level</cell><cell>98.80</cell><cell>98.77</cell><cell>99.34</cell><cell>99.56</cell></row><row><cell>Pixel-level</cell><cell>97.31</cell><cell>98.09</cell><cell>98.40</cell><cell>98.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The AUC scores of MemSeg on MVTec AD dataset when using different loss functions</figDesc><table><row><cell>L1 Loss</cell><cell>Focal Loss [27]</cell><cell>Image-level</cell><cell>Pixel-level</cell></row><row><cell>?</cell><cell></cell><cell>84.82</cell><cell>73.38</cell></row><row><cell></cell><cell>?</cell><cell>98.92</cell><cell>98.64</cell></row><row><cell>?</cell><cell>?</cell><cell>99.56</cell><cell>98.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The AUC scores of MemSeg on the MVTec AD dataset when using different module components.</figDesc><table><row><cell cols="2">Memory Multi-</cell><cell>Spatial</cell><cell>Coordinate</cell><cell cols="2">Image-level Pixel-level</cell></row><row><cell></cell><cell>scale</cell><cell>Attention</cell><cell>Attention</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>96.42</cell><cell>96.08</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>98.41</cell><cell>98.27</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>99.08</cell><cell>98.60</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>99.26</cell><cell>98.67</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>98.96</cell><cell>98.44</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>99.56</cell><cell>98.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>AUC scores of PatchCore, SPADE, PaDiM, and MemSeg on the toy dataset.</figDesc><table><row><cell></cell><cell>PatchCore [11]</cell><cell>SPADE [12]</cell><cell>PaDiM [13]</cell><cell>Ours</cell></row><row><cell>Image-level</cell><cell>99.75</cell><cell>95.75</cell><cell>99.30</cell><cell>99.83</cell></row><row><cell>Pixel-level</cell><cell>99.33</cell><cell>98.72</cell><cell>98.70</cell><cell>99.77</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MVTec AD-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection neural network with dual auto-encoders GAN and its industrial inspection applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3336</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient gan-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06222</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seebock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A semantically driven self-supervised algorithm for detecting anomalies in image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Karimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Focus your distribution: coarse-to-fine non-contrastive learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards total recall in industrial anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08265</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inpainting transformer for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pirnay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13897</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DRAEM-a discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vitjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skocaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CutPaste: self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">AnoSeg: anomaly segmentation network using self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03396</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SnipeDet: Attention-guided pyramidal prediction kernels for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="302" to="310" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13713" to="13722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">VT-ADL: a vision transformer network for image anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10036</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Patch svdd: patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Application of optimal clustering and metric learning to patch-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
