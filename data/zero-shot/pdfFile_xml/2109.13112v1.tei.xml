<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recall and Learn: A Memory-augmented Solver for Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Huang</surname></persName>
							<email>huangshifeng@cvte.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
							<email>wangjiawei0531@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvte</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Xu</surname></persName>
							<email>xujiao@cvte.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvte</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<email>yangming@cvte.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvte</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CVTE Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hunan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recall and Learn: A Memory-augmented Solver for Math Word Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we tackle the math word problem, namely, automatically answering a mathematical problem according to its textual description.</p><p>Although recent methods have demonstrated their promising results, most of these methods are based on template-based generation scheme which results in limited generalization capability. To this end, we propose a novel human-like analogical learning method in a recall and learn manner. Our proposed framework is composed of modules of memory, representation, analogy, and reasoning, which are designed to make a new exercise by referring to the exercises learned in the past. Specifically, given a math word problem, the model first retrieves similar questions by a memory module and then encodes the unsolved problem and each retrieved question using a representation module. Moreover, to solve the problem in a way of analogy, an analogy module and a reasoning module with a copy mechanism are proposed to model the interrelationship between the problem and each retrieved question. Extensive experiments on two wellknown datasets show the superiority of our proposed algorithm as compared to other state-ofthe-art competitors from both overall performance comparison and micro-scope studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Math Word Problem (MWP) aims at automatically solving a mathematical question according to its textual description. Given a problem description, a model needs to understand the relevant quantities and reason the corresponding expression, which is a difficult task because it requires the model to learn mathematics knowledge from the labeled problem and generalize the knowledge to the unseen problems.</p><p>In fact, great efforts have been made to address the MWPs in the research community. Boosted * Both authors contributed equally to this research. ? Corresponding author. by the proliferation of deep learning techniques, Seq2Seq-based models have been developed to solve MWPs.  presented a large-scale MWP dataset Math23K and proposed an RNN-based framework with a number mapping technique, which aims to generate a math template first, and then fill the extracted number from the problem into the slots of the generated template to obtain an expression. This two-stage method is widely used as a baseline by the latest papers, such as Math-EN , GTS <ref type="bibr" target="#b21">(Xie and Sun, 2019)</ref>, Graph2Tree <ref type="bibr" target="#b23">(Zhang et al., 2020b)</ref>, Ape  and so on <ref type="bibr" target="#b19">(Wang et al., 2019b;</ref><ref type="bibr" target="#b4">Li et al., 2019)</ref>.</p><p>Despite its value and significance, the math word problem has not been well addressed due to the following challenges: 1) Although promising results have been reported, the aforementioned models all use the template-based framework to solve MWPs, such a two-stage process may introduce systematic cumulative errors. In light of this, how to solve MWPs properly without using the template is a non-trivial task. 2) Furthermore, instead of learning through a single training example, the way human learn often rely on the so-called analogical learning method, which is able to explore the inherent laws between various cases and generalize them to new examples <ref type="bibr" target="#b14">(Schwartz et al., 2016;</ref><ref type="bibr" target="#b6">Hope et al., 2017)</ref>.</p><p>Therefore, how to combine the analogical learning method in a unified framework is worth exploring.</p><p>To address the aforementioned issues, as revealed in <ref type="figure" target="#fig_0">Figure 1</ref>, we design a novel memory-augmented model named REAL (short for "REcall And Learn") to solve the MWP task in an end-to-end manner. REAL is able to recall some familiar questions that have been solved when solving a new problem, and learns to generate a similar solution in an analogical way. Specifically, REAL model first initializes a memory module by a dataset formed with questions and their expressions. When solving a problem, the memory module is utilized to retrieve the most similar questions as references according to the unsolved problem. Next, a representation module is proposed to extract item memories of the unsolved problem and the retrieved question. Thereafter, we employ an analogy module to construct relational memory based on the item memories. Finally, a reasoning module is applied to generate the expression of the unsolved problem by combining the generation and copy mechanisms. Extensive experiments show that we have achieved competitive performance on MWP task. Moreover, our proposed model is able to improve the performance by retrieving more questions, which shows the model has the ability to learn by analogy.</p><p>The main contributions of this work are summarized as follows:</p><p>? To the best of our knowledge, this is the first model that learns to solve math word problems using human-like analogical learning way.</p><p>? We develop a novel memory-augmented framework combined with the copy mechanism, REAL, to solve MWPs in a recall and learn manner, in which the model is composed of modules of memory, representation, analogy and reasoning.</p><p>? Extensive experiments are conducted on two well-known datasets, and the results showed that the REAL model not only achieves competitive performance on MWP task, but also demonstrates the unique ability of learning by analogy. Meanwhile, we have released the code to facilitate the research community. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review some literatures that are tightly related to our work, namely, math 1 https://github.com/sfeng-m/REAL4MWP word problems and memory-augmented generative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Math Word Problems</head><p>In the MWP task, the algorithms are designed to calculate a mathematical expression based on the textual description of mathematical problems. Therefore, the methods of natural language processing can be widely used in MWP task. Most of existing models adopt an encoder-decoder framework, where the encoder is designed as a bidirectional RNN and the decoder is designed as a unidirectional RNN. For example,  constructed a large dataset and proposed a Seq2Seq model that shows the superiority over previous works.  proposed an equation normalization technique to solve the order-duplicated problem and bracket-duplicated problem. <ref type="bibr" target="#b19">Wang et al. (2019b)</ref> designed a tree-structure model to predict the suffix expression of MWPs, which reduces the target space of the problem. <ref type="bibr" target="#b21">Xie and Sun (2019)</ref> proposed a tree-structured gated recurrent unit as decoder, which passes the information through the expression tree in both top-down and bottom-up manners. <ref type="bibr" target="#b23">Zhang et al. (2020b)</ref> proposed a graph encoder to enrich the quantity representations in the problem, and decode the expression by a tree structure decoder.  presented a new largescale and template-rich MWP dataset Ape210K and proposed a strong Seq2Seq model, which achieves state-of-the-art performance on both the Math23K and Ape210K datasets. However, these models highly rely on a method that extracting numbers from the question, and then mapping numbers to the slots of the generated templates. Such a twostage process will introduce some systematic errors to the model. Therefore, we consider exploring the pipeline of generating expression directly instead of utilizing the template as an intermediate process, in which the model may gain more information from the question description and benefit from the end-toend training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory-augmented Generative Methods</head><p>In the text generation task, there are mainly two types of models, one is based on retrieval <ref type="bibr" target="#b29">(Zhou et al., 2016</ref><ref type="bibr" target="#b1">Chen et al., 2019b;</ref><ref type="bibr" target="#b17">Wang et al., 2019a)</ref>, and the other is based on generation <ref type="bibr" target="#b13">(Qian et al., 2018;</ref><ref type="bibr" target="#b28">Zhou and Wang, 2018;</ref><ref type="bibr" target="#b4">Dong et al., 2019;</ref><ref type="bibr" target="#b5">Han et al., 2019)</ref>. The retrieval algorithm can solve a particular task by <ref type="figure">Figure 2</ref>: The illustration of our proposed REAL framework, which is composed of modules of memory, representation, analogy and reasoning. For an unsolved problem X, we first use maximum inner product search to find a similar question Z from the memory module. And then the solution is generated with the copy mechanism in an analogical manner. constructing a knowledge base, which has high scalability. However, the retrieval-based approach cannot generate arbitrary results, which restricts the generation space of the model. In addition, the generative framework is able to store the knowledge in the model with the form of parameters, which has a certain generalization ability. However, in the knowledge-intensive task, it is difficult to remember all the knowledge in the parameters of generative model. To this end, many researchers attempted to combine retrieval and generation methods for text generation task <ref type="bibr" target="#b24">(Zhang et al., 2017;</ref><ref type="bibr" target="#b31">Zhu et al., 2019;</ref><ref type="bibr">Lewis et al., 2020;</ref><ref type="bibr" target="#b9">Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b27">Zhou et al., 2020)</ref>. In particular, <ref type="bibr" target="#b24">Zhang et al. (2017)</ref> proposed a memory-augmented neural model for Chinese poetry generation, which investigates the contribution of memory. <ref type="bibr" target="#b31">Zhu et al. (2019)</ref> have demonstrated a retrieval-enhanced response generation approach for a dialogue system, which makes use of informative content in retrieved results to generate new responses. <ref type="bibr">Lewis et al. (2020)</ref> proposed a retrieval-augmented generation method where the parametric memory is a pre-trained Seq2Seq model and the non-parametric memory is a pre-trained neural retriever.</p><p>Our work is inspired by the success of incorporating memory into the generative model, showing memory-augmented model is capable of achieving strong performance in MWPs. Moreover, with the help of the memory module, our proposed model is able to solve the MWPs by analogy, which opens up a new research direction on MWP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The framework of REAL is presented in <ref type="figure">Figure 2</ref>. In general, our proposed framework is composed of four key components: 1) Memory Module is constructed with a pre-trained model and is able to return top-K similar questions given a math word problem. 2) Representation Module is used to represent each token of the problem and the question in an inductive manner. 3) Analogy Module is utilized to aggregate the information of the problem and the retrieved question for better generating the correct expression. 4) Reasoning Module is combined with a copy mechanism that acts as a decoder to generate each token of the expression based on the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We denote a problem as X = {X q , X e }, where the subscript q and e indicate the question description and mathematical expression respectively. X q is a sequence of word tokens</p><formula xml:id="formula_0">X q = {x 1 q , x 2 q , ? ? ? , x L q }, where L is the length of the question descrip- tion. We let its K retrieved similar questions Z = {Z 1 , Z 2 , ? ? ? , Z K } where Z i = {Z i q , Z i e }.</formula><p>For each unsolved problem, the goal is to predict the token of X e at each time step t, namely y t ? V ?X q , where V is a generated vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Module</head><p>Aiming at solving a math word problem based on its similar retrieved questions, we employ a memory module to acquire external knowledge for enhanc-ing the learning ability of the unsolved problem. The memory module is a non-parameter retriever, which is defined as the following formulation:</p><formula xml:id="formula_1">p(Z|X) ? p(Z q |X q ) = e f (Xq) T f (Zq) Zq e f (Xq) T f (Zq) , (1)</formula><p>where f (?) is a Word2Vec <ref type="bibr" target="#b12">(Mikolov et al., 2013)</ref> model followed by a mean pooling technique that can represent a question description as a dense vector. In order to retrieve the similar question Z q given an unsolved problem X q , we first normalize each vector and perform the MIPS (maximum inner product search) algorithm, which is implemented similar to the FAISS library <ref type="bibr" target="#b7">(Johnson et al., 2017)</ref>. Note that we utilize p(Z q |X q ) to approximate p(Z|X) because only the problem description is provided in the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representation Module</head><p>The representation module is leveraged to summarize the representation of the problem and each retrieved question, which is called item memory. The module is constructed by the Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> block with a casual mask that similar to the settings of UniLM <ref type="bibr" target="#b4">(Dong et al., 2019)</ref>, which can learn a bidirectional encoder and a unidirectional decoder simultaneously. Specifically, we perform a causal masking mechanism to allow each position in the expression to attend to previous positions, which preserve the auto-regressive property during decoding. In addition, we realize the representation of each token by summing the token, segment and position embeddings, which is similar to the approach of BERT model <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. Next, follows the settings of UniLM <ref type="bibr" target="#b4">(Dong et al., 2019)</ref>, to avoid the information-leakage problem during training, we use causal masks to ensure that the representation of each token in expression is only related to the previous states, as shown in <ref type="figure">Figure 2</ref>. Therefore, in the training stage, given a problem {X q , X e } with its corresponding retrieved questions {Z q , Z e }, the representation module is employed to acquire the item memories X q , X e , Z q and Z e with the same dimension of 768 respectively, which efficiently learns the representations of the problem and each retrieved question in an inductive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analogy Module</head><p>In order to achieve the way of analogical learning, the model needs to aggregate contextual informa- tion from the item memories of the unsolved problem and the retrieved questions. Therefore, we first concatenate item memories to form input features {Z q , Z e , X q , X e } and preprocess the input features using the mechanisms of position encoding and segment encoding. Thereafter, based on the length of the input sequences Z q , Z e , X q and X e , a casual mask can be constructed similar to the approach of representation module, as shown in <ref type="figure">Figure 2</ref>. The purpose of the casual mask is to enhance the analogical learning capability by focusing the attention of unsolved problem on the retrieved questions. In addition, the expression part in a casual mask is designed to only attend to the previous token, which avoids the information-leakage problem. Lastly, we utilize a Transformer network that similar to the representation module for learning relational memories by analogy. Thereinto, the output states of the analogy module are denoted as relational memorie? Z q ,? e ,X q andX e respectively. Note that? q and Z e are the outputs of the last layer, and theX q and X e are the outputs of penultimate layers.</p><p>In order to extract the knowledge from the retrieved question {Z q , Z e }, we further employ a classifier C ? R 768?|V | to solve the question description of Z q and propose an auxiliary loss L inductive to navigate the learning direction of the analogy module, which is formulated as follows:</p><formula xml:id="formula_2">L inductive = ? N t logp ?a (z t e |Z q , z 1:t?1 e ),<label>(2)</label></formula><p>where z t e indicates the t th token of Z e , and ? a represents the parameters of analogy module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Reasoning Module</head><p>Taking the structure of the word math problem into account, we know that the operands of an expression are likely to come from the problem description X q . Therefore, we design a reasoning module with a copy mechanism <ref type="bibr" target="#b15">(See et al., 2017)</ref>, which is build based on the last layer of analogy module. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, given a decoder statex t e , the vocabulary distribution p g (y t |X q ) and the copy distribution p c (y t |X q ) are formulated as follows:</p><formula xml:id="formula_3">p g (y t |X q ) = e ?g(yt) y?V e ?g(y) ,<label>(3)</label></formula><formula xml:id="formula_4">p c (y t |X q ) = 1 h j?h i:x i =yt ? j x (x i ) x k ?Xq ? j x (x k ) ,<label>(4)</label></formula><p>where the generated probability p g (y t |X q ) is implemented as a fully-connected layer ? g followed by the analogy module with weights W g . And ? j x (x i ) indicates the j th head attention value <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> of token x i , h is the total number of the attention head.</p><p>To combine the vocabulary distribution p g (y t |X q ) and the copy distribution p c (y t |X q ), we use a learnable value p gen to calculate the aggregated distribution p(y t |X q ) as follows:</p><formula xml:id="formula_5">p gen p g (y t |X q ) + (1 ? p gen )p c (y t |X q ),<label>(5)</label></formula><p>where probability p gen is computed by a fullyconnected layer followed by the analogy module with weights W p . Therefore, the reasoning module can decide whether to copy the number in the problem description according to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Learning Details</head><p>Suppose the length of expression of a problem is N , the goal of our model is to generate a token probability distribution p ? (y t |X q , Z, y 1:t?1 ) based on the problem and its retrieved question, where t ? N and ? is the parameters of the model. Next, we marginalize the token distribution to generate the t th output distribution p ? (y t |X q , y 1:t?1 ) based on the top-K retrieved questions Z. Finally, generating each token y t sequentially is able to form a complete expression X e of problem X q . Formally, the framework p ? (y|X q ) can be defined as follows:</p><formula xml:id="formula_6">N t E Z?top?K(p(Z|X)) p ? (y t |X q , Z, y 1:t?1 ),<label>(6)</label></formula><p>where top?K(p(Z|X)) is a probability model that instantiated as a memory module to retrieve K similar questions. The loss function can be defined as the negative marginal log-likelihood as follows:</p><formula xml:id="formula_7">L analogical = ?log(p ? (y|X q )),<label>(7)</label></formula><p>where p ? (y|X q ) is a probability model of REAL illustrated in Eqn. <ref type="formula" target="#formula_6">(6)</ref>. In order to facilitate the inductive learning of model, we further employ an auxiliary loss illustrated in Eqn.</p><p>(2). Therefore, the total loss function is defined as a weighted sum of analogical loss and inductive loss. Formally, our training goal is formulated as follows:</p><formula xml:id="formula_8">L = L analogical + ?L inductive ,<label>(8)</label></formula><p>which ? is a hyperparameter for balancing the weights between L analogical and L inductive . We simply set ? equal to 1 and found it works well in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experiments on two well-known datasets to answer the following five research questions:</p><p>RQ1 How does our proposed REAL framework perform as compared to other state-of-the-art competitors?</p><p>RQ2 Are memory and copy mechanisms equally important? How does REAL model perform if one mechanism is removed?</p><p>RQ3 How does REAL perform with respect to various number of retrieved questions?</p><p>RQ4 How does REAL perform when solving problems of varying expression lengths (difficulties)?</p><p>RQ5 Can we visualize the solving process for MWP task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We evaluate our framework on two datasets, Math23K 2  and Ape210K 3 . The Math23K dataset labeled with equations and answers contains 22,162 questions in the training set and 1,000 questions in the testing set. Since most of the state-of-the-art results were experimented via 5-fold cross-validation and a published testing dataset, we evaluate REAL on both settings. Ape210K is a relatively large-scale dataset containing 210,488 math word problems, which are split into training, validation and testing subsets. Both validation and testing subsets have 5,000 samples and we leave the rest of 200,488 as the training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>To justify the effectiveness of our method, we compare it to state-of-the-art baselines:</p><p>? DNS . This is a vanilla Seq2Seq model that jointly utilizes a number mapping technique and an equation template technique to generate the expression of problems.</p><p>? Math-EN . A preprocessed technique that called equation normalization is proposed to significantly reduce the template space.</p><p>? T-RNN <ref type="bibr" target="#b19">(Wang et al., 2019b)</ref>. This method applies a tree-structure Seq2Seq model to predict suffix expression, with inferred numbers as leaf nodes and unknown operators as inner nodes.</p><p>? StackDecoder (Chiang and . This method proposes a stack-based decoding process to model semantic meanings of operands and operations of MWPs.</p><p>? GTS <ref type="bibr" target="#b21">(Xie and Sun, 2019)</ref>. This is a goaldriven tree-structured model to decode the expression in both top-down and bottom-up manners.</p><p>? TSN-MD <ref type="bibr" target="#b22">(Zhang et al., 2020a)</ref>. This method proposes a teacher-student networks with multiple decoders to improve the diversity of generated expressions.</p><p>? Graph2Tree <ref type="bibr" target="#b23">(Zhang et al., 2020b)</ref>. This method designs a graph network to enrich quantity representations and decodes the expression using a tree-based decoder like GTS.</p><p>? Ape . This paper proposes a feature-enriched and copy-augmented Seq2Seq model, which achieves competitive performance on both Math23K and Ape210K datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>Our model is implemented based on the PyTorch 4 framework on a server equipped with 2 NVIDIA 1080Ti GPU. In the REAL model, the representation module and analogy module are both constructed by 6 layers Transformer block <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. To initialize the hidden layers in Transformer, we set their parameters with a pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. The equation normalization technique  is applied in the training stage, which follows the previous works for fair comparison. Our model is trained for 80 epochs where the mini-batch size is set to 12. In each mini-batch, problems with their corresponding retrieved questions are randomly sampled from the training set. For optimizer, we use ADAM optimization algorithm <ref type="bibr" target="#b8">(Kingma and Ba, 2015)</ref> with the learning rate of 5e-4, ? 1 = 0.9 and ? 2 = 0.99. In addition, the learning rate is halved per 5 epochs when the total epoch is greater than 40 and we also set beam size to 5 in beam search during decoding. Lastly, we treat the predicted expression as correct if its calculated value equals to the answer, and we use the answer accuracy as the evaluation metric which follows previous works .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison (RQ1)</head><p>To demonstrate the effectiveness of our proposed REAL solution, we compare it to several state-ofthe-art approaches: 1) DNS; 2) Math-EN; 3) T-RNN; 4) StackDecoder; 5) GTS; 6) TSN-MD; 7) Graph2Tree; and 8) Ape.   <ref type="table">Table 1</ref> shows the comparison results on Math23K and Ape210K datasets among different methods, we have the following observations: 1) Our proposed REAL method shows the best performance on all benchmark datasets as compared to other methods. To verify the statistical significance of our improvement, we further conduct one-sample t-test on Math23K* experiments compared to the accuracy of Ape model and acquire a p-value about 4e-4, which unveils the superiority of our algorithm. 2) Jointly observing the experimental results on Math23K and Ape210K, we can see that our proposed model has better improvement on Ape210K dataset as compared to the improvement on Math23K. This is probably because our model is more effective on the large-scale dataset. 3) We do not perform any handcraft preprocessing steps to reduce the difficulty of model training, such as number mapping  and relation extraction <ref type="bibr" target="#b23">(Zhang et al., 2020b)</ref>, and still achieves great performance, which manifests the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study (RQ2)</head><p>To evaluate the effectiveness of our proposed analogical learning method, especially the design of equation normalization technique, memory component and copy mechanism, we conduct ablation study on these components. In particular, we employ EN to denote equation normalization technique, Copy to denote the copy mechanism and w/o Memory to denote the model trained by inductive loss without using the memory module.</p><p>The performance of the three-component ablation study is shown in <ref type="table" target="#tab_2">Table 2</ref>. We have the following observations: 1) By comparing the results of REAL and w/o EN, the performance of model is benefited from the equation normalization technique, which reveals its effectiveness in MWP task. 2) Jointly observing the performance of w/o Copy and w/o Memory models, we can infer that the Memory component is more important than the Copy component. This is mainly because the memory component is the key to perform analogical learning, which can learn the intrinsic relationships among unsolved problem and similar questions. Meanwhile, the copy component is reasonable due to it takes the structure of MWP task into account, which also results in better performance. 3) By comparing the results of w/o All with the other experiments, we find the accuracy drops significantly, proving that the three components have positive impacts on the model's performance consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Retrieved Questions (RQ3)</head><p>Although REAL is trained with only a retrieved question, we still have the flexibility to adjust the number of retrieved questions at the testing stage, which can affect the model's performance. In order to show that REAL is able to solve MWPs by analogy, we test the model according to various number of retrieved questions on Math23K* and Ape210K datasets.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we have the following observations: 1) With the increased number of the retrieved questions, the model's performance is monotonically improved. This clearly shows that REAL model is able to master the knowledge in an analogical way, which manifests the rationality of our proposed framework. 2) It is obviously observed that when K increases from 0 to 1, the model's performance achieves significant improvement. This is mainly because the training method of the model is changed from an inductive way to an analogical way, showing the effectiveness of the memory components.</p><p>3) The performance on both datasets are relatively stable and reach their maximum values when K = 4. It indicates that with the increase of K, the marginal benefits of improving the model's performance will gradually diminish. We consider the noise introduced by the retrieved questions may affect the performance. Because the more questions retrieved by the memory module, the lower the similarity of the corresponding questions. 4) The experimental results demonstrate REAL's flexibility in balancing the performance and efficiency, which is an advantage of performing our memoryaugmented framework in practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Length (Difficulty) (RQ4)</head><p>To further evaluate REAL's analogy ability based on MWPs with different difficulty, we split every fold of Math23K dataset into 4 subsets according to the length of expression. Specifically, we deem that the longer the length of expression, the more difficult the corresponding problem is, and vice versa. Therefore, we sort the problems of the testing set according to the length of the expression in ascending order, and split it into 4 subsets, which are categorized as different difficulty levels of easy, medium, upper and hard. According to this, we conduct 20 experiments to consider how the retrieved number of questions will affect the performance under different difficulty problems. Note that each experimental result is obtained by averaging the results of the 5-fold subsets.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we have the following observations: 1) With the increase of difficulty, the performance of REAL gradually decreases, which is reasonable because the longer the length of expression, the more difficult for the model to predict. 2) In the "Medium", "Upper" and "Hard" experiments, the analogical results of K ? 1 are noticeably superior to the inductive results of K = 0, which manifests the rationality of analogical learning method. Furthermore, as the number of retrieved memories increases, the model's performance is consistently improved. It demonstrates the effectiveness of our proposed analogical learning method. 3) In contrast, the experimental results are unstable when the difficulty of the problem is "Easy". We consider the reasons behind are: a) The solutions of simple problems with shorter expressions are easy to master by the model, so the model is far more likely to rely on the inductive method and can not benifit from more analogies. b) When solving relatively easy problems, the performance of the inductive-preferred model may be harmed due to the noise introduced by the increased retrieved questions. This indicates the quality of retrieved questions should be carefully considered and we leave it for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study (RQ5)</head><p>To better understand how the analogical learning method work in MWP task, we exploited some macro-level case studies. Specifically, we first trained a REAL model with Top-2 settings in the Math23K dataset, and selected two hard problems from the testing set that can not be solved in inductive mode but solve correctly by the analogical one.</p><p>As shown in <ref type="table">Table 3</ref>, the case 1 describes a problem about surfacing a swimming pool by cement. The prediction is wrong when the model try to solve the problem in an inductive manner. It seems that the model is lack of common sense about the formula of cube area and misunderstands the concept of depth. To this end, we attempt to solve the problem using analogical method, which results in a correct solution. From the descriptions of problem and the retrieved questions, we can see that the REAL model is able to discover the common structure among the problem and the retrieved questions, and solve the problem through the expression template of the retrieved questions in an analogical manner. Case 2 describes a counting problem that the quantitative relationship is very complicated, in which an ingenious and complex reasoning process is required for solving the problem correctly. As shown in <ref type="table">Table 3</ref>, it is as expected that our model fail to solve this complex problem in an inductive manner, because the existing deep learning models are still difficult to have human-like reasoning ability. In constrast, the analogical one can generate a correct solution by refering to the similar questions, which demonstrates that our proposed framework is able to learn by analogy.</p><p>The above two cases qualitatively show that the memory-augmented component is an effective structure in REAL framework, which introduces an Case 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>To build a swimming pool with a length of 18 meters, a width of 10 meters, and a depth of 2 meters. We need to surface the walls and bottom of the swimming pool with cement, how many square meters of cement should be applied? Inductive Prediction ( 18 ? 10 + 10 ? 2 + 2 ? 2 ) ? 18 ? 10 Retrieved Questions 1) Question: A rectangular swimming pool is 60 meters long, 40 meters wide, and 2 meters deep. Now we need to put cement on the walls and bottom. What is the area of the cement? Equation: ( 60 ? 40 + 40 ? 2 + 60 ? 2 ) ? 2 ? 60 ? 40 2) Question: A rectangular water pool, 20 meters long, 10 meters wide, and 2 meters high. We need to surface the walls and bottom of the pool with cement. How many square meters of cement do we need to apply? Equation: 20 ? 10 + 20 ? 2 ? 2 + 10 ? 2 ? 2 Analogical Prediction ( 18 ? 10 + 10 ? 2 + 18 ? 2) ? 2 ? 18 ? 10 Case 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>A class held a math competition with a total of 20 questions. It is stipulated that 5 points will be given for one correct answer, and 2 points will be deducted for one wrong answer. Xiao Ming got 86 points. How many questions did he answer correctly? Inductive Prediction ( 20 ? 5 ? 86 ) ? ( 5 + 2 ) Retrieved Questions 1) Question: There are 20 questions in total. 7 points will be given for one correct answer, and 4 points will be deducted for one wrong answer. Wang Lei scored 74 points. How many questions did he answer correctly? Equation: 20 ? ( 20 ? 7 ? 74 ) ? ( 7 + 4 ) 2) Question: In the knowledge competition, there are 10 judgment questions. The scoring rules are: 2 points for each correct answer, and 1 point will be deducted for wrong answer. Xiao Ming only got 14 points. How many questions did he answer correctly? Equation: ( 14 + 10 ? 1 ) ? ( 2 + 1 ) Analogical Prediction 20 ? ( 20 ? 5 ? 86 ) ? ( 5 + 2 ) <ref type="table">Table 3</ref>: Two cases of REAL solving MWPs using inductive mode and analogical mode. <ref type="bibr">(Section 4.6)</ref> novel analogical approach for MWP task and opens a new possibility for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion And Future Work</head><p>In this work, we propose a memory-augmented solver called REAL for MWPs. Under the REAL framework, there are four key components: 1) Memory module; 2) Representation module; 3) Analogy module; 4) Reasoning module, which are proposed to perform analogical learning schema based on the retrieved similar questions. In addition, to enhance the generation performance, a copy mechanism is designed to properly aggregate the information of operands from the problem description. The experimental results show that REAL achieves state-of-the-art performance for MWP task. Extensive micro-scope studies demonstrate the ability of REAL in learning by analogy.</p><p>In the future, we plan to extend our work in the following two directions. First, the model's performance can be further improved if the memory module of REAL model is jointly trained with the whole framework. Second, we will consider designing a more meaningful analogy module that can take the structure of question and expression into account, thus providing more information for the reasoning module to generate the problem solution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our proposed framework for solving math word problems in a recall and learn manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overview of the reasoning module with a copy mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The performance of REAL w.r.t. various number of retrieved questions. (Section 4.4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The top-K performance of REAL w.r.t. varying difficulties of unsolved problems. (Section 4.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Performance comparisons of various compo-</cell></row><row><cell>nents on Math23K* and Ape210K datasets. (Section</cell></row><row><cell>4.3)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/SumbeeLei/Math_EN/ tree/master/data 3 https://github.com/Chenny0808/ape210k</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.pytorch.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards knowledge-based personalized product description generation in e-commerce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3040" to="3050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bidirectional attentive memory networks for question answering over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2913" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantically-aligned equation generation for solving and reasoning math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2656" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep generative approach to search extrapolation and recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating innovation through analogy mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafna</forename><surname>Shahaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Tim Rockt?schel</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>et al. 2020.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling intra-relation in math word problems with different functional multi-head attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jierui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6162" to="6167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assigning personality/profile to a chatting machine for coherent conversation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/595</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4279" to="4285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The ABCs of how we learn: 26 scientifically proven approaches, how they work, and when to use them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><forename type="middle">P</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiturn response selection in retrieval-based chatbots with iterated attentive convolution matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management</title>
		<meeting>the ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Translating a math word problem to a expression tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Template-based math word problem solvers with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="7144" to="7151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Teacher-student networks with multiple decoders for solving math word problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee</forename><forename type="middle">Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graphto-tree learning for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flexible and creative chinese poetry generation using neural memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1364" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling multiturn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11506</idno>
		<title level="m">Ape210k: A large-scale and template-rich dataset of math word problems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving conversational recommender systems via knowledge graph based semantic fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mojitalk: Generating emotional responses at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1128" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retrieval-enhanced adversarial training for neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3763" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
