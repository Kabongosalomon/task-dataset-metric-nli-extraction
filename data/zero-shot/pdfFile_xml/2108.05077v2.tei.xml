<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining the Benefits of Two-stage and One-stage HOI Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Miao Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mining the Benefits of Two-stage and One-stage HOI Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-stage methods have dominated Human-Object Interaction (HOI) detection for several years. Recently, one-stage HOI detection methods have become popular. In this paper, we aim to explore the essential pros and cons of two-stage and one-stage methods. With this as the goal, we find that conventional two-stage methods mainly suffer from positioning positive interactive human-object pairs, while one-stage methods are challenging to make an appropriate trade-off on multi-task learning, i.e., object detection, and interaction classification. Therefore, a core problem is how to take the essence and discard the dregs from the conventional two types of methods. To this end, we propose a novel one-stage framework with disentangling human-object detection and interaction classification in a cascade manner. In detail, we first design a human-object pair generator based on a state-of-the-art one-stage HOI detector by removing the interaction classification module or head and then design a relatively isolated interaction classifier to classify each human-object pair. Two cascade decoders in our proposed framework can focus on one specific task, detection or interaction classification. In terms of the specific implementation, we adopt a transformer-based HOI detector as our base model. The newly introduced disentangling paradigm outperforms existing methods by a large margin, with a significant relative mAP gain of 9.32% on HICO-Det. The source codes are available at https://github.com/YueLiao/CDN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Human-Object Interaction (HOI) detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3]</ref> is to make a machine detailedly understand human activities from a static image. Human activities in this task are abstracted as a set of &lt;human, object, action&gt; HOI triplets. Thus, an HOI detector is required to locate humanobject pairs and classify their corresponding action simultaneously. Based on this definition, we can summarize conventional HOI detection methods into two paradigms, i.e., two-stage methods, and one-stage methods. These two paradigms have made significant progress with the development of deep learning, but both paradigms still have their shortcomings due to their structural design. This paper aims to present a detailed analysis of methods under these two paradigms and propose a solution to mine the benefits of two-stage and one-stage methods.</p><p>We first take a closer look at the conventional two-stage and one-stage HOI detectors. Conventional two-stage methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref> are mostly with a serial architecture. As shown in <ref type="figure">Figure 1 (a)</ref>, two-stage methods detect humans and objects first and then feeds the human-object pairs, which are generated by matching humans and objects one by one, into an interaction classifier. The serial architecture suffers from locating the interactive human-object pairs under the interference of a large number of negative pairs only based on local region features. Otherwise, the efficiency of Instance Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Classification</head><p>One by One (a). Two-stage framework (b). One-stage end-to-end framework "throw" "no" "no"</p><p>"throw"</p><p>Detect HOI Triplets with a Multi-task Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Detection</head><p>"throw" object offset human offset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Classification &amp; Association</head><p>(c). One-stage framework with parallel architecture Human-object Pair Generator "throw"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Classification</head><p>Gradient Truncation (d). Our one-stage framework with cascade disentangling decoder "frisbee" "frisbee" "frisbee" "frisbee" <ref type="figure">Figure 1</ref>: (a) Two-stage framework, (b) one-stage end-to-end framework, (c) one-stage framework with parallel architecture, and (d) our one-stage framework with a cascade disentangling head.</p><p>two-stage methods is also limited by the serial architecture. To alleviate these problems, one-stage methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref> are proposed to detect the HOI triplets directly and break HOI detection as multi-task learning, i.e., human-object detection and interaction classification, which is shown in <ref type="figure">Figure 1(b)</ref>. Therefore, one-stage methods can easily focus on the interactive human-object pairs and effectively extract corresponding features in an end-to-end manner. However, it is difficult for a single model to make a good trade-off on multi-task learning since human-object detection and interaction classification are two very different tasks, which requires the model to focus on different visual features. As shown in <ref type="figure">Figure 1</ref>(c), though some previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref> design two parallel branches to detect instances and predict interaction respectively, the interaction classification branch still needs to regress additional offsets to associate humans and objects. Thus the interaction branch is also required to make a trade-off between interaction classification and human and object positioning.</p><p>Therefore, the intuitive idea is to take the essence and discard the dregs from the two paradigms. To attain this, we propose a novel end-to-end one-stage framework with disentangling human-object detection and interaction classification in a cascade manner, namely Cascade Disentangling Network (CDN). The original intention of our framework is to keep the advantages of conventional onestage methods, directly and accurately locating the interactive human-object pairs, and bring the advantages of two-stage methods into our one-stage framework, disentangling human-object detection and interaction classification. As shown in <ref type="figure">Figure 1</ref>(d), in our proposed framework, we design a human-object pair decoder based on the one-stage paradigm by removing the interaction classification function, namely HO-PD, and following an isolated interaction classifier. To instantiate our idea with an end-to-end manner, we design the HO-PD based on the previous state-of-the-art one-stage transformer-based HOI detector, HOI-Trans <ref type="bibr" target="#b38">[39]</ref> and QPIC <ref type="bibr" target="#b27">[28]</ref>, where we remove the interaction classification head for each query and make it focus on human-object pairs detection. Otherwise, we design an independent HOI decoder for interaction classification to make the interaction classification unaffected by human-object detection. Therefore, there exists a core problem, i.e., how to link the human-object pair and the corresponding action class. To address this problem, we initialize the query embedding of the HOI decoder with the output of the last layer of the HO-PD. In this case, the HOI decoder is able to learn the corresponding action category under the guidance of the query embedding and free out from the human-object detection task. Moreover, we design a decoupling dynamic re-weighting manner to handle the long-tailed problems in HOI detection.</p><p>Our contributions can be summarized threefold: <ref type="bibr" target="#b0">(1)</ref> We conduct a detailed analysis of two conventional HOI detection paradigms, i.e., two-stage and one-stage. <ref type="bibr" target="#b1">(2)</ref> We propose a novel one-stage framework with a cascade disentangling decoder to combine the advantages of two-stage and onestage methods. (3) Our method outperforms previous state-of-the-art methods by a large margin on the HOI detection task, especially achieves a 25.35% performance gain on rare classes of HICO-Det.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analysis of Two-stage and One-stage HOI detectors</head><p>We first introduce a unified formulation for the HOI detection problem. Given a human-centric image I, the model T (?) is required to predict a set of HOI triplets</p><formula xml:id="formula_0">S = {(b h i , b o i , a i ), i ? {1, 2, ? ? ? , K}}, where b h i , b o i</formula><p>and a i denotes a human bounding-box, an object bounding-box and their corresponding action category, respectively.</p><p>Two-stage HOI detector. Two-stage detectors can be regarded as an instance-driven manner, detecting instances first and predicting interaction based on the detected instances. The two-stage detector divides T (?) into two stages, i.e., detection T d (?) and interaction classification T c (?). In the first stage, we suppose that T d (?) produces M human bounding-boxes and N object bounding-boxes. Here the 'object' is a universal object which includes human as one class. Therefore, T d (?) generates M ? N human-object pairs. In general, the number of true-positive interactive human-object pairs, denoted as K , is much smaller than M ? N . However, in the second stage, T c (?) needs to scan all M ? N pairs one by one and predict an action category with its corresponding confidence score. In this case, T c (?) is required to inference M ? N times to find K interactive pairs from M ? N pairs. We argue that this manner causes three problems. Firstly, these models produce a more additional computational cost, whose time complexity is O(M ? N ) O(K ). Secondly, the imbalance between positive and negative samples makes the model easily overfit to negative samples. Thus the model tends to assign a 'no-interaction' class for human-object pairs with very high confidence, suppressing the true-positive samples. Thirdly, the accuracy of interaction classification is influenced by the non-end-to-end pipeline. Because the interaction classification is mostly based on the region features extracted by T d (?), while the core of T d (?) is to regress bounding-boxes and its extracted features pay more attention to the edge of regions, thereby such features are not good options for interaction classification, which needs more context. However, it is an excellent property for two-stage methods that disentangling detection and interaction classification makes each stage focus on its task and produce good results in each stage.</p><p>One-stage HOI detector. As for one-stage methods, they detect all HOI triplets S directly and simultaneously with an end-to-end framework. Such paradigm has greatly relieved the three problems of two-stage methods, especially for efficiency, where the time complexity is reduced to O(K ). Most one-stage methods are interaction-driven, which directly locate the interaction point <ref type="bibr" target="#b19">[20]</ref> or interactive human-object pairs <ref type="bibr" target="#b38">[39]</ref>, thereby reducing negative sample interference. However, coupling human-object detection and interaction classification limit their performance because it is hard to generate a unified feature representation for two very different tasks. Though the parallel one-stage methods break HOI detection into two parallel branches, their interaction branch still suffers from multi-task learning. Specifically, the optimization target of interaction branch is P(e h , e o , a|V ), where e h and e o are associative embeddings, e.g., offset, to match interaction with human and object respectively. Therefore, even if detection is organized as an independent branch, the interaction branch must position humans and objects for the association. The set-based detectors couple detection and interaction completely, whose optimization function is P(b h , b o , a|V ).</p><p>Next, we introduce a simple one-stage framework with disentangling human-object detection and interaction classification, namely CDN, to mine the benefits of two-stage and one-stage HOI detectors. Our CDN disentangles the original set-based one-stage optimization function into two cascade decoders. Firstly, we predict human-object pair by P(b h , b o |V ). Secondly, we apply an isolated decoder to predict the action category by P(a|V , b h , b o ). More details are in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we will present a detailed introduction to the pipeline of our proposed CDN. In section 3.1, we present an overview of our framework and briefly introduce the pipeline. In section 3.2, we introduce the visual feature extractor. The cascade disentangling HOI decoder is introduced in section 3.3. Section 3.4 introduces a novel dynamic re-weighting mechanism that mitigates the long-tailed problem. The detailed training process and post-processing are discussed in section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The architecture of our proposed CDN is illustrated in <ref type="figure">Figure 2</ref>. Our CDN is organized in a cascade manner with a visual feature extractor. Given an image, we first follow transformer-based detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref> to apply a CNN followed by a transformer encoder architecture to extract visual features into a sequence. Then we detect HOI triplets in two cascade decoders. Firstly, we apply the Human-Object Pair Decoder (HO-PD) to predict a set of human-object bounding-boxes pairs based on a set of learnable queries. Next, taking the output of the last layer of HO-PD as queries, an isolated interaction decoder is utilized to predict the action category for each query. Finally, the HOI triplets are formed by the output of the above two cascade decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Feature Extractor</head><p>We define the visual feature extractor by combining a CNN and a transformer encoder. Fed with an input image I with shape (H, W, C), the CNN generates a feature map of shape (H , W , D b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head><p>Flatten Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-Object Pair Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Decoder</head><p>Human-object Queries Interaction Queries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Feature Extractor Interaction Decoder Human-Object Pair Decoder (HO-PD)</head><formula xml:id="formula_1">N d Positional Encoding D c D c N d N d N d ? ? ! " !"# # # $%&amp; ' frisbee frisbee throw</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human box</head><p>Object box/class Interactive score Action category " <ref type="figure">Figure 2</ref>: The framework of our CDN. It is comprised of three components:Visual Feature Extractor, Human-Object Pair Decoder (HO-PD) and Interaction Decoder. We first apply a CNN-transformer combined architecture to extract sequenced visual features X s . Then, we divide HOI detection into two cascade transformer-based decoders. Firstly, we regress the human-object bounding-box pairs based on X s and a set of random-initialized queries Q d by HO-PD. The interactive score is from a binary classification to determine whether the human-object pair is an interactive pair or not. Secondly, we predict one or many action categories for each predicted human-object pairs, where we take the output of HO-PD Q out d to initialize the interaction queries Q c and aggregate information with X s . Finally, the HOI triplets are formed by the output of the cascade decoders.</p><p>Then, D b is reduced to D c by a projection convolution layer with a kernel size 1 ? 1. Next, a flatten operator is used to generating the flatten feature X v ? R (H ?W )?Dc by collapsing the spatial dimensions into one dimension. This flatten feature is then fed into a transformer encoder and the position encoding E pos ? R (H ?W )?Dc , which distinguishes the relative position in the sequence X s ? R (H ?W )?Dc . Thanks to the multi-head self-attention mechanism, the transformer encoder produces a feature map with richer contextual information by summarizing global information. The output of the encoder is denoted as global memory with a dimension of D c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cascade Disentangling HOI Decoder</head><p>The cascade disentangling HOI decoder consists of two decoders: Human-Object Pair Decoder (HO-PD) and interaction decoder. Both decoders share the same architecture, a transformer-based decoder, with independent weights. In this subsection, we first introduce the general architecture of the decoder and then elaborate on the two decoders in detail, respectively.</p><p>Transformer-based decoder. We follow the transformer-based object detector DETR <ref type="bibr" target="#b0">[1]</ref> to design the basic architecture in our cascade disentangling HOI decoder. We apply N transformer decoder layers for each decoder and equip each decoder layer with several FFN heads for intermediate supervision. Specifically, each decoder layer is comprised of a self-attention module and a multi-head co-attention module. During feed-forward, fed into a set of learnable queries Q ? R Nq?Cq , each decoder layer first applies a self-attention module on all queries and then conducts a multi-head co-attention operation between queries and the sequenced visual features, and outputs a set of updated queries. For the FFN heads, each head is comprised of one or several MLP branches, and each branch is for a specific task, e.g., regression, or classification. All queries share the same FFN heads. Therefore, each decoder can be simply represented as:</p><formula xml:id="formula_2">P = f (Q, X s , E pos ).<label>(1)</label></formula><p>Besides, the number of queries N q is determined by the number of positive samples of an image.</p><p>HO-PD. Firstly, we design the HO-PD to predict a set of human-object pairs from the sequenced visual features. To this end, we first randomly initialize a set of learnable queries Q d ? R N d ?Cq as HO queries. Then we apply a transformer-based decoder, which takes HO queries Q d and sequenced visual features as input and applies three FFN heads for each query to predict human bounding-box, object bounding-box, and object class, which form a human-object pair. We also utilize an additional interactive score head to simply determine whether the human-object pair is an interactive pair or not by a binary classification. In this case, P is instantiated as P ho , which is consist of a set of</p><formula xml:id="formula_3">human-object pairs {(b h i , b o i ), i ? {1, 2, ? ? ? , N d }}.</formula><p>Thus, HO-PD can be denoted as:</p><formula xml:id="formula_4">P ho = f d (Q d , X s , E pos ).<label>(2)</label></formula><p>In addition, we keep the output queries of the last layer of HO-PD as Q out d for the following step.</p><p>Interaction Decoder. Secondly, we propose the interaction decoder to classify the human-object queries and assign one or several action categories for each human-object query. To classify each human-object query one-to-one, we initialize Q c with the output of HO-PD Q out d . In this way, Q out d can provide prior knowledge to guide Q c to learn the corresponding action categories for each human-object query. The other components and inputs are the same as HO-PD, which conducts self-attention among queries and co-attention with X s and E pos . The final output is a set of action categories P cls : {a i , i ? {1, 2, ? ? ? , N d }}. Therefore, this process can be formulated as:</p><formula xml:id="formula_5">P cls = f cls (Q out d , X s , E pos ).<label>(3)</label></formula><p>In our proposed cascade disentangling HOI decoder, the task of HOI detection is disentangled into two relatively independent steps: human-object pairs detection and interaction classification. Therefore, each step can aggregate more related features to concentrate on its corresponding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoupling Dynamic Re-weighting</head><p>The HOI datasets usually have long-tail class distribution for both object class and action class. To alleviate the long-tail problem, we design a dynamic re-weighting mechanism for further improvements with a decoupling training strategy. In detail, we first train the whole model with regular losses. Then, we freeze the parameters of the visual feature extractor and only train the cascade disentangling decoders with a relatively small learning rate and the designed dynamic re-weighted losses.</p><p>During decoupling training, at each iteration, we apply two similar queues to accumulate number of each object class or action class. The queues are used as memory banks to accumulate training samples and truncate the accumulation with length L Q as sliding windows. In detail, Q o with length L o Q to accumulate object number N o i for each object class i ? {1, 2, ? ? ? , C o }, and Q a with length L a Q to accumulate interaction number N a i for each action category i ? {1, 2, ? ? ? , C a }. The dynamic re-weighting coefficients w dynamic are presented as follow:</p><formula xml:id="formula_6">w a i i?{1,2,??? ,Ca} = Ca i=1 N i N i pa , w a bg = Ca i=1 N i N a bg pa ,<label>(4)</label></formula><formula xml:id="formula_7">w o i i?{1,2,??? ,Co} = Co i=1 N i N i po , w o bg = Co i=1 N i N o bg po ,<label>(5)</label></formula><p>where N i is the number of accumulated positive samples of category i by the queues Q o and Q a , N bg is the number of accumulated background samples, C is the number of categories, and exponent p is a hyper-parameter that adapts the magnitude of mitigation. Specifically, the weight of background class, w bg , is designed to balance the positives and negatives. For the stability of the dynamic re-weighted training, the weight coefficients are initialized as w static with those calculated by 4 and 5 using the static number of object and action categories. The final dynamic weights are given as w = ?w static + (1 ? ?)w dynamic , where ? is a smooth factor, given as min(0.999 L Q , 0.9). The factor ? transits w from w static to w dynamic with the increasing of L Q . Finally, the weights are used to the classification losses in a traditional way by multiplying each coefficient to each class and then calculating the summation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Post-processing</head><p>In this section, we introduce the training and inference processes in detail. Especially, we will introduce a novel Pair-wise Non-Maximal Suppression (PNMS) strategy in the inference process.</p><p>Training. Following the set-based training process of HOI-Trans <ref type="bibr" target="#b38">[39]</ref> and QPIC <ref type="bibr" target="#b27">[28]</ref>, we first match each ground-truth with its best-matching prediction by the bipartite matching with the Hungarian algorithm. Then the loss is produced between the matched predictions and the corresponding ground truths for the final back-propagation. During matching, we consider the predictions of two cascade decoders together. The loss of CDN follows QPIC which is composed by five parts: the box regression loss L b , the intersection-over-union loss L GIoU <ref type="bibr" target="#b25">[26]</ref>, the interactive score loss L p , the object class loss L o c , and the action category loss L a c . The target loss is the weighted sum of these parts as:</p><formula xml:id="formula_8">L = k?(h,o) (? b L k b + ? GIoU L k GIoU ) + ? p L p + ? o L o c + ? a L a c ,<label>(6)</label></formula><p>where ? b , ? GIoU , ? p , ? o and ? a are the hyper-parameters for adjusting the weights of each loss.</p><p>Inference. The inference process is to composite the output of instance-related FFNs and the interaction-related FFN to form HOI triplets. By our cascade disentangling decoder architecture, the instance queries and the interaction queries are one-to-one corresponding, therefore, the five components &lt;human bounding box, object bounding box, object class, interactive score, action class&gt; can be homologous in each of the N d dimensions per FFN head. Formally, we generate the i-th output prediction as &lt;b PNMS. After sorting c hoi i in descending order and generating the top K HOI triplets, we design a pair-wise non-maximal suppression (PNMS) method to further filter out human-object pairs from pair-wise bounding boxes overlapping perspective. For two HOI triplets m and n, the pair-wise overlap PIoU is calculated as:</p><formula xml:id="formula_9">P IoU (m, n) = I(b h m , b h n ) U (b h m , b h n ) ? I(b o m , b o n ) U (b o m , b o n ) ? ,<label>(7)</label></formula><p>where the operators I and U compute the intersection and union areas between the two boxes of m and n, respectively. ? and ? are the balancing parameters between humans and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct comprehensive experiments to demonstrate the superiority of our designed CDN. In section 4.1, we briefly introduce the experimental benchmarks. Section 4.2 presents implementation details. Next, It is a detailed experimental comparison and analysis of two-stage and one-stage methods in section 4.3. In section 4.4, we compare our methods with the previous state-of-the-art methods. The ablation studies and components analysis are included in 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets. We carry out experiments on two widely-used HOI detection benchmarks: HICO-Det <ref type="bibr" target="#b1">[2]</ref> and V-COCO <ref type="bibr" target="#b7">[8]</ref>. We follow the standard evaluation scheme. HICO-Det consists of 47, 776 Creative Common images from Flickr (38, 118 for training and 9, 658 for test) with more than 150K humanobject pairs. It contains the same 80 object categories as MS-COCO <ref type="bibr" target="#b20">[21]</ref> and 117 action categories. The objects and actions form 600 classes of HOI triplets. V-COCO is derived from MS-COCO dataset, which consists of 5, 400 images in the trainval subset and 4, 946 images in the test subset. It has 29 action categories (25 HOIs and 4 body motions) and 80 object categories. For both datasets, one person can interact with multiple objects in different ways at the same time.</p><p>Evaluation Metrics. Following the standard evaluation <ref type="bibr" target="#b1">[2]</ref>, we use the mean average precision (mAP) as the evaluation metric. For one positively predicted HOI triplet &lt;human, object, action&gt;, it needs to contain accurate human and object locations (box IoU with reference to GT box is greater than 0.5) and correct object and action categories. Specifically, for HICO-Det, besides the full set of 600 HOI classes, we also report the mAP over a rare set of 138 HOI classes that have less than 10 training instances and a non-rare set of the other 462 HOI classes. For V-COCO, we report the role mAP for two scenarios: scenario 1 includes the cases even without any objects (for the four action categories of body motions), and scenario 2 ignores these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We During training, we initialize the network with the parameters of DETR <ref type="bibr" target="#b0">[1]</ref> trained with the MS-COCO dataset. We set the weight coefficients ? b , ? GIoU , ? p , ? o and ? a to 2. <ref type="figure">5, 1, 1, 1 and 1</ref>, respectively, which are exactly same with QPIC <ref type="bibr" target="#b27">[28]</ref>. We optimize the network by AdamW <ref type="bibr" target="#b22">[23]</ref> with the weight decay 10 ?4 . We first train the whole model for 90 epochs with a learning rate of 10 ?4 decreased by 10 times at the 60th epoch. Then, during the decoupling training process, we fine-tune the cascade disentangling decoders together with the box, object, and action FFNs for 10 epochs with a learning rate of 10 ?5 . We use both object and action dynamic re-weighting for HICO-Det and only action dynamic re-weighting for V-COCO. The re-weighting parameter p is set to 0.7 for both object and action. The length L Q of training sample queue Q for both object and action is set to 2 ? N s , where N s is the sample number of the training set. All experiments are conducted on the 8 Tesla V100 GPUs and CUDA10.2, with a batch size of 16.</p><p>For validation, we select 100 detection results with the highest scores and then adopt PNMS to further filter results. The threshold, ?, and ? of PNMS are set to 0.7, 1, and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Analysis of Two-stage and One-stage Methods</head><p>In this part, we introduce a detailed experimental analysis of conventional two-stage and one-stage methods and our proposed CDN from the following three aspects.</p><p>Human-object Pair Generation. We first explore the quality of the human-object pairs generation between two-stage and one-stage methods. To attain this, we conduct a detailed experiment based on a representative two-stage method iCAN <ref type="bibr" target="#b5">[6]</ref>. We first implement a PyTorch version iCAN as the baseline model, denoted as iCAN * , which only applies human and object appearance with a COCO-pretrained Faster-RCNN detector <ref type="bibr" target="#b24">[25]</ref>. For a fair comparison, we first fine-tune DETR on HICO-Det for 100 epochs only with the instance detection annotation based on COCO-pretrained weights. Then we combine the detected human and object bounding-boxes, whose confidences are greater than a threshold, one by one to generate human-object pairs denoted as iCAN ? in <ref type="table" target="#tab_1">Table 1</ref>. We train our CDN only with HO-PD for 100 epochs and get the human-object pairs from the output directly. Then, we graft the human-object pairs to the baseline model to extract box features and utilize the same interaction classifier in the second stage of iCAN * . In this way, we degrade the number of pairs from M ? N to K , which means time complexity is reduced from O(M ? N ) to O(K ). Primarily, HO-PD significantly promotes mAP from 15.37 to 24.05, as shown in <ref type="table" target="#tab_1">Table 1</ref>. This indicates that one-stage methods are much superior in human-object pair generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full</head><p>Rare Non-Rare   Interaction Classification. We aim to study the interaction classification between conventional multi-task one-stage methods and our disentangled one-stage detector. We can regard QPIC <ref type="bibr" target="#b27">[28]</ref>   <ref type="table">Table 2</ref>: Performance comparison on the HICO-Det test set. The 'P', 'T' represent human pose information and the language feature, respectively.</p><p>multi-task version of our CDN. <ref type="table" target="#tab_1">Table 1</ref> shows that our 'CDN-S base' (w/o re-weighting and PNMS strategies) has achieved mAP 30.96 with 6.50% relative mAP gain compared to QPIC. Especially, our 'CDN-S base' significantly outperforms QPIC for rare classes with a 23.66% improvement. The performance of rare classes can partly reflect the accuracy of interaction classification.</p><p>Feature Learning. This part discusses the differences in feature learning between the conventional one-stage method, QPIC, and our CDN from a qualitative view. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we visualized the feature maps extracted from the last layer of the decoder of QPIC, the HO-PD, and the interaction decoder in our CDN. We can see that HO-PD and QPIC attend very similar regions, e.g., the boundaries of humans and objects and the human-object contact areas, which are beneficial for locating the interactive human-object pairs. However, the interaction decoder concentrates on humanpose and the regions that contribute to understanding human actions. As for the specific case, for example, for 'hold cake', HO-PD in CDN attends to the boundaries of the cake while the interaction decoder in CDN concentrates on the interaction context, i.e., the human's hands holding the cake. Thus, it shows that CDN disentangles the human-object detection and interaction classification. For 'ride horse', HO-PD in CDN emphasizes the overall feature of the human and the horse, and the highlight parts are the edges of the human and horse. For the interaction decoder in CDN, the highlighted part emphasizes the interaction context, i.e., the human carries the rope when riding a horse. Finally, QPIC somehow combines the two highlights, but both are not obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State-of-the-Art</head><p>We conduct experiments on HICO-Det and V-COCO benchmarks to verify the effectiveness of our proposed methods. For HICO-Det dataset as shown in <ref type="table">Table 2</ref>, comparing to the previous state-of-theart two-stage method FCMNet <ref type="bibr" target="#b21">[22]</ref> with ResNet-50 as backbone, our CDN-B significantly promotes mAP from 20.41 to 31.78, with a relative gain of 55.71%. Even compared with PD-Net <ref type="bibr" target="#b35">[36]</ref> which adopts extra language feature and DJ-RN <ref type="bibr" target="#b15">[16]</ref> which utilizes extra human pose features, CDN-B achieves 52.71% and 48.92% relative mAP gains, respectively. When comparing to the one-stage method AS-Net <ref type="bibr" target="#b2">[3]</ref> and QPIC <ref type="bibr" target="#b27">[28]</ref> which also adopt transformer-based detector architecture, CDN-B attains 10.08% and 9.32% point relative mAP gains, respectively. <ref type="table" target="#tab_4">Table 3</ref> shows the comparisons on V-COCO dataset. CDN-B achieves 62.29 AP role on Scenario 1 and 64.42 AP role on Scenario 2, which significantly outperform previous state-of-the-art method QPIC with relative 5.94% and 5.61%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Extra AP S1 role AP S2 role Two-stage Method: InteractNet <ref type="bibr" target="#b6">[7]</ref> 40.0 -GPNN <ref type="bibr" target="#b23">[24]</ref> 44.0 -iCAN <ref type="bibr" target="#b5">[6]</ref> 45.3 52.4 TIN <ref type="bibr" target="#b17">[18]</ref> 47.8 54.2 VCL <ref type="bibr" target="#b11">[12]</ref> 48.3 -DRG <ref type="bibr" target="#b4">[5]</ref> T 51.0 -IP-Net <ref type="bibr" target="#b31">[32]</ref> 51.0 -VSGNet <ref type="bibr" target="#b28">[29]</ref> 51.8 57.0 PMFNet <ref type="bibr" target="#b29">[30]</ref> P 52.0 -PD-Net <ref type="bibr" target="#b34">[35]</ref> T 52.6 -CHGNet <ref type="bibr" target="#b30">[31]</ref> 52.7 -FCMNet <ref type="bibr" target="#b21">[22]</ref> 53.1 -ACP <ref type="bibr" target="#b14">[15]</ref> T 53.23 -IDN <ref type="bibr" target="#b16">[17]</ref> 53.3 60.3 One-stage Method:</p><p>UnionDet <ref type="bibr" target="#b12">[13]</ref> 47.5 56.2 HOI-Trans <ref type="bibr" target="#b38">[39]</ref> 52.9 -AS-Net <ref type="bibr" target="#b2">[3]</ref> 53.9 -GG-Net <ref type="bibr" target="#b36">[37]</ref> 54.7 -HOTR <ref type="bibr" target="#b13">[14]</ref> 55.2 64.4 DIRV <ref type="bibr" target="#b3">[4]</ref> 56.1 -QPIC <ref type="bibr" target="#b27">[28]</ref> 58   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this subsection, we analyse the effectiveness of the proposed strategies and components in detail. All experiments are eventuated on the HICO-Det dataset. The performance of each strategy is evaluated in <ref type="table" target="#tab_5">Table 4a</ref>. The five hyper-parameters of the training loss in 7 follow QPIC <ref type="bibr" target="#b27">[28]</ref>. The ablation study of the two hyper-parameters in the re-weighting is shown in <ref type="table" target="#tab_5">Table 4b</ref>, and that of the three hyper-parameters in the PNMS is shown in <ref type="table" target="#tab_5">Table 4c</ref>. We carry out all experiments based on the model CDN-B with ResNet-50 as backbone. Dynamic re-weighting. In this part, we conduct experiments to evaluate the components in the dynamic re-weighted training strategy based on the base model as shown in <ref type="table" target="#tab_5">Table 4b</ref>. If we only decouple training without re-weighting, the model achieves mAP 30.90, which is lower than the base model. Therefore, it shows that the performance gain does not come from a longer training process. Adding static weights w static to losses promotes mAP to 31.25. The dynamic re-weighting method improves the re-weighting effect since it captures the real-time weight of each class for each real-time sample during training. Thus it can sufficiently dig information from every single sample to achieve the best overall performance. Our method obtains best result mAP 31.38 when L Q = 2 ? N s and p = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategies. As shown</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PNMS.</head><p>On the basis of the model after re-weighted training, we compare the variance by different parameter settings of the PNMS strategy, which is shown in <ref type="table" target="#tab_5">Table 4c</ref>. We fix the human box balance factor ? to 1. Then we tune the object box balance factor ? and the threshold of the PIoU to filter pair-wise boxes. We achieve best performance mAP 31.78 when ? = 0.5 and thres = 0.7. The fact that ? is smaller than ?, indicates that the overall performance is more sensitive to human boxes compared with object boxes in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Two-stage Methods. Most previous HOI detectors are with a two-stage paradigm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>. Firstly, a fine-tuned detector <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref> is applied to detect the instances. Secondly, generating the human-object pairs by matching detected human and object one by one, and then feeding them into an interaction classifier. To improve the interaction classification, some extra features were applied, such as human pose <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>, human parts <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>, and language features <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref>. Besides, some two-stage methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref> applied graph neural networks to model the interactions.</p><p>One-stage Methods. One-stage methods detect HOI triplets directly <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref>. In detail, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref> proposed a point-based interaction detection method which performs inference at each interaction key point. <ref type="bibr" target="#b12">[13]</ref> proposed an anchor-based method to predict the interactions for each human-object union box. Recently, set-based detection approach has been proposed to handle HOI detection as a set prediction problem <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28]</ref> designed a transformer encoder-decoder architecture to directly predict HOI detection results in an end-to-end manner, while <ref type="bibr" target="#b2">[3]</ref> utilized parallel instance and interaction decoder branches to adaptively aggregate the HOI triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explore the essential pros and cons of two-stage and one-stage HOI detection in detail. We propose a novel one-stage framework with disentangling human-object detection and interaction classification in a cascade manner. Our CDN can keep the advantage of one-stage methods, directly and accurately locating the interactive human-object pairs, and bring the benefit of two-stage methods, disentangling detection and interaction classification. Our novel paradigm has outperformed previous methods by margins. However, we only implement a specific version to mine the benefits of two-stage and one-stage methods. In the future, we plan to apply our idea with more general one-stage methods and introduce more advantages of two-stage methods into the one-stage framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential Negative Societal Impacts</head><p>Similar to many other AI technologies, our proposed CDN itself is harmless. However, someone might utilize it for military purposes or apply it to any other malicious human activities detection, which might negatively impact society. Therefore, we encourage well-intentioned consideration before adopting our technique.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>is given by c hoi i = c a i c o i c p i , where c a i and c o i are the</head><label></label><figDesc>h i , b o i , argmax k c hoi i (k)&gt;.The HOI triplet score c hoi i scores of interaction and object classification, respectively, and c p i is the interactive score from the interactive FFN head for the query vector being an human-object pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>implement three variant architectures of CDN: CDN-S, CDN-B, and CDN-L, where 'S', 'B', and 'L' denote small, base, and large, respectively. For CDN-S and CDN-B, we adopt ResNet-50 with a 6-layer transformer encoder as the visual feature extractor. For the cascade decoders, CDN-S is equipped with both 3-layer transformers, while CDN-B has a 6-layer transformer for each decoder. CDN-L only replaces the ResNet-50 with ResNet-101 in CDN-B. The reduced dimension size D c is set to 256. The number of queries N d is set to 64 for HICO-Det and 100 for V-COCO since the average number of positives for variant human-object pairs per image of HICO-Det is smaller than V-COCO. The human and object box FFNs have 3 linear layers with ReLU, while the object and action category FFNs have one linear layer. The code is provided in supplemental material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of Feature Maps for Queries. Visual attended features for query with top-1 score extracted from the last layer of the decoder of (a) QPIC, (b) HO-PD in CDN, and (c) interaction decoder in CDN. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Analysis of Two-stage and Onestage Methods.</figDesc><table><row><cell></cell><cell>hold cake walk dog</cell><cell>ride bicycle</cell><cell>row boat</cell><cell>ride horse</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>denotes our implemented</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PyTorch version iCAN [6] baseline model.  ?</cell><cell>(c)</cell><cell></cell><cell></cell></row><row><cell>denotes replacing instance detection boxes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>given by a HICO-Det fine-tuned DETR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>detector to extract box features. 'HO-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PD+iCAN  *  ' denotes replacing original one-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>by-one generated human-object pairs with</cell><cell></cell><cell></cell><cell></cell></row><row><cell>our HO-PD generated. 'CDN-S base' de-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>notes CDN-S w/o re-weighting and PNMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>strategies.</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on the V-COCO test set. The 'P', 'T' represent the human pose information and the language feature, respectively. Strategies: Analysis of improvements by various training strategies. Dynamic re-weighting: Analysis of decouple training with dynamic re-weighted losses, i.e., different queue length LQ, coefficient p and dynamic or static. The effects of different settings of PNMS coefficients, i.e., ?, ?, and thres denotes threshold.</figDesc><table><row><cell cols="3">Strategy</cell><cell>Full</cell><cell cols="2">Rare</cell><cell>Non-Rare</cell></row><row><cell cols="3">QPIC [28]</cell><cell>29.07</cell><cell cols="2">21.85</cell><cell>31.23</cell></row><row><cell cols="2">base</cell><cell></cell><cell>31.06</cell><cell cols="2">26.68</cell><cell>32.36</cell></row><row><cell cols="4">+ re-weighting 31.38</cell><cell cols="2">27.36</cell><cell>32.58</cell></row><row><cell cols="3">+ PNMS</cell><cell>31.78</cell><cell cols="2">27.55</cell><cell>33.05</cell></row><row><cell cols="2">(a) Strategy</cell><cell>LQ</cell><cell>p</cell><cell>Full</cell><cell>Rare</cell><cell>Non-Rare</cell></row><row><cell>base</cell><cell></cell><cell>-</cell><cell>-</cell><cell>31.06</cell><cell>26.68</cell><cell>32.36</cell></row><row><cell cols="2">decouple</cell><cell>-</cell><cell>-</cell><cell>30.90</cell><cell>26.09</cell><cell>32.33</cell></row><row><cell>static</cell><cell></cell><cell>-</cell><cell>0.7</cell><cell>31.25</cell><cell>27.12</cell><cell>32.49</cell></row><row><cell cols="3">dynamic 2 ? Ns</cell><cell>0.8</cell><cell>31.33</cell><cell>27.45</cell><cell>32.49</cell></row><row><cell cols="3">dynamic 1 ? Ns</cell><cell>0.7</cell><cell>31.34</cell><cell>27.48</cell><cell>32.49</cell></row><row><cell cols="3">dynamic 2 ? Ns</cell><cell>0.7</cell><cell>31.38</cell><cell>27.36</cell><cell>32.58</cell></row><row><cell>(b) ?</cell><cell>?</cell><cell>thres</cell><cell cols="2">Full</cell><cell>Rare</cell><cell>Non-Rare</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">31.38</cell><cell>27.36</cell><cell>32.58</cell></row><row><cell>1</cell><cell>1</cell><cell>0.8</cell><cell cols="2">31.66</cell><cell>27.46</cell><cell>32.91</cell></row><row><cell>1</cell><cell>1</cell><cell>0.7</cell><cell cols="2">31.75</cell><cell>27.50</cell><cell>33.03</cell></row><row><cell>1</cell><cell>0.7</cell><cell>0.7</cell><cell cols="2">31.77</cell><cell>27.54</cell><cell>33.03</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.8</cell><cell cols="2">31.75</cell><cell>27.51</cell><cell>33.02</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.7</cell><cell cols="2">31.78</cell><cell>27.55</cell><cell>33.05</cell></row><row><cell cols="2">(c) PNMS:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of our proposed method on the HICO-Det test set. We carry out all experiments based on the base model (CDN-B). gains, respectively. As for efficiency analysis, CDN-S has almost the same number of parameters and flops compared to QPIC, but CDN-S achieves mAP 31.44 on HICO-Det, 8.15% higher than QPIC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4a</head><label>4a</label><figDesc>, our pure model without any additional post-processing operation, namely base model, achieves mAP 31.06, promoting 1.99 compared with QPIC<ref type="bibr" target="#b27">[28]</ref>. Especially, the base model significantly promotes mAP for rare classes from 21.85 to 26.68 compared to QPIC. It indicates the superiority of the architecture of disentangling human-object detection and interaction classification. The re-weighted training further promotes mAP to 31.38, with a gain of 0.32, and the gain mainly lies in rare classes. Finally, the PNMS further improves mAP to 31.78.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dirv: Dense interaction region voting for end-to-end human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">No-frills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Affordance transfer learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Baosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hotr: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferable interactiveness prior for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A graph-based interactive reasoning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Polysemy deciphering network for robust human-object interaction detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glance and gaze: Inferring action-aware points for one-stage human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Changxing Ding, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
