<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All About Knowledge Graphs for Actions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallabi</forename><surname>Ghosh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirat</forename><surname>Saini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
						</author>
						<title level="a" type="main">All About Knowledge Graphs for Actions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Zero-shot/Few-shot action recognition ? Knowledge graphs ? Graph Convolution Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current action recognition systems require large amounts of training data for recognizing an action. Recent works have explored the paradigm of zeroshot and few-shot learning to learn classifiers for unseen categories or categories with few labels. Following similar paradigms in object recognition, these approaches utilize external sources of knowledge (eg. knowledge graphs from language domains). However, unlike objects, it is unclear what is the best knowledge representation for actions. In this paper, we intend to gain a better understanding of knowledge graphs (KGs) that can be utilized for zero-shot and few-shot action recognition. In particular, we study three different construction mechanisms for KGs: action embeddings, actionobject embeddings, visual embeddings. We present extensive analysis of the impact of different KGs in different experimental setups. Finally, to enable a systematic study of zero-shot and few-shot approaches, we propose an improved evaluation paradigm based on UCF101, HMDB51, and Charades datasets for knowledge transfer from models trained on Kinetics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>not easy to train an action classifier for a new category. A potential solution is to leverage the knowledge from seen or familiar categories to recognize unseen or unfamiliar categories. This is the zero-shot learning paradigm, where we transfer or adapt classifiers of related, known, or seen categories to classify unseen ones. Similarly, for few-shot action recognition, instead of testing on completely unseen classes, we have only a few labeled samples from the test classes, which help in learning about the rest of the test samples.</p><p>Both zero-shot and few-shot learning methods have been studied widely for image classification. One of the recent technique involves building a knowledge graph (KG) representing relationships between seen and unseen classes and then training a graph convolutional network (GCN) on this KG to transfer classifier knowledge from seen to unseen classes . Using the same technique for action recognition is hard since, unlike objects, it is unclear what is the best knowledge representation for actions. One of the reasons as observed in <ref type="bibr" target="#b17">[Gentner, 1981]</ref> is that verbs have a broader definition and conflicting meaning.</p><p>In this work, we study the performance improvements by using different types of KGs for zero-shot and few-shot action recognition <ref type="figure">(Fig.1)</ref>. The primary step in building a KG is generating a good implicit representation for action classes. In image classification, standard word embeddings (word2vec, GloVe, ConceptNet, etc.) capture the semantic knowledge associated with welldefined class names. However, for action classification, class names vary from single words <ref type="bibr">(sit, stand, etc.)</ref> to phrases (shooting ball (not playing baseball)) and there are multiple definitions of the same (or similar) action class(es); like, apply eye makeup or put on eyeliner. Such diversity is less pronounced in image classification tasks due to the simplicity of labels. Our first contribution is studying different implicit representa-  <ref type="figure">Fig. 1</ref> We experiment with different Knowledge Graphs (KGs), using word and visual-feature based embeddings, for zero-shot learning and few-shot learning of actions. For zero-shot learning of actions, we construct a KG using action class names (i.e. KG1) (a) and a KG using the associated verb and nouns (i.e. KG2) (b). For few-shot learning, in addition, we use a KG with visual features (i.e. KG3) from a few examples from the test classes (c). tion for action classes and showing the advantages of a sentence2vector model in capturing the semantics of word sequences for zero/few-shot action recognition.</p><p>Our second contribution is building an explicit relationship map from these implicit representations of action classes. In image classification, the explicit representations for transferring knowledge from seen to unseen categories are using attributes or external KGs. Several datasets provide labeled class-attribute pairs (e.g., AwA <ref type="bibr" target="#b37">[Lampert et al., 2009]</ref> , aYahoo <ref type="bibr" target="#b13">[Farhadi et al., 2009]</ref>, COCO-Attributes <ref type="bibr" target="#b50">[Patterson and Hays, 2016]</ref>, MITstates <ref type="bibr" target="#b24">[Isola et al., 2015]</ref>, etc.). Similarly, many KGs have nodes that correspond to image classification classes (e.g., WordNet <ref type="bibr" target="#b45">[Miller, 1995]</ref>, NELL, and NEIL <ref type="bibr" target="#b3">[Carlson et al., 2010</ref><ref type="bibr" target="#b6">, Chen et al., 2013</ref>). In contrast, such sources are scarce for action classes. Wordnet contains verbs, therefore, it can be used to construct a KG for verbs, but we cannot have a KG with nodes representing the entire phrase (eg., "playing(verb) guitar(noun)") for an action class. Instead, there will be separate nodes for verbs and objects with defined inter-relationships. Concept-Net <ref type="bibr" target="#b59">[Speer et al., 2017]</ref> has some phrases, but the list is not exhaustive and a lot of label names in our datasets are not present in ConceptNet. On the other hand, we build a KG with an explicit relationship of the multiword action phrases in any dataset. We append dataset with action classes from other datasets and construct two KGs, one for noun, and other for verb either by splitting the action phrase in cases like "playing(verb) guitar(noun)" or using WordNet to get the nearest noun in cases like "cake"(noun) for action class named "baking"(verb). Further, we build a KG for few-shot learning using mean features of training data-points per class. We append this KG with the two KGs defined previously and observe performance improvement.</p><p>Finally, most previous work on zero-shot action recognition uses image-based learned models to estimate actions in videos. Recent advances in action recognition lead to the use of a network trained on video dataset as the feature extractor. So it requires an improved evaluation paradigm, since the action classes in the training set cannot be in the test set. We manually check for commonalities between the training datasets (Kinetics) and testing datasets (UCF101, HMDB51, Charades), but could not resolve problems within Kinetics which is a huge dataset and can have videos common across multiple classes. So we keep all Kinetics classes in training set and remove common classes from Kinetics with UCF101, HMDB51 and Charades from the test set. Hence, our third contribution is the creation of this evaluation paradigm using UCF101, HMDB51, Charades, and Kinetics datasets.</p><p>In summary our main three contributions are:</p><p>-Better implicit representation of action phrases (which are word sequences) using sentence2vec -Comparative study of different KGs for action zeroshot/few-shot learning -Develop an improved evaluation paradigm for zeroshot/few-shot action recognition using networks trained on video datasets as feature extractors These 3 contributions together builds an integrated approach for both zero-shot and few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Action Recognition: Significant performance boost in state-of-the-art action recognition was observed with improved dense trajectories <ref type="bibr" target="#b68">[Wang and Schmid, 2013]</ref> and 3D ConvNets <ref type="bibr" target="#b28">[Ji et al., 2013]</ref> which capture deep spatio-temporal features instead of handcrafted ones. Thereafter, multiple ideas like single stream networks <ref type="bibr" target="#b29">[Karpathy et al., 2014]</ref>, two-stream networks <ref type="bibr" target="#b56">[Simonyan and Zisserman, 2014]</ref>, end-to-end encoderdecoder based architectures <ref type="bibr" target="#b10">[Donahue et al., 2015</ref><ref type="bibr" target="#b65">, Tran et al., 2015</ref><ref type="bibr" target="#b78">, Yao et al., 2015</ref> and combining different streams with convolutional networks <ref type="bibr" target="#b14">[Feichtenhofer et al., 2016</ref> evolved. Recent studies include <ref type="bibr" target="#b4">[Carreira and Zisserman, 2017</ref><ref type="bibr" target="#b9">, Diba et al., 2017</ref><ref type="bibr" target="#b51">, Qiu et al., 2017</ref><ref type="bibr" target="#b79">, Zhang et al., 2017a</ref>. We use I3D model pre-trained on Kinetics described in <ref type="bibr" target="#b4">[Carreira and Zisserman, 2017]</ref>, to extract and learn features of the input videos.</p><p>Zero-Shot Action Recognition: Zero-shot learning (ZSL) refers to the task of learning to predict on classes that are excluded from the training set <ref type="bibr" target="#b49">[Palatucci et al., 2009]</ref>. Various studies do ZSL for image classification and object detection <ref type="bibr" target="#b5">[Changpinyo et al., 2016</ref><ref type="bibr" target="#b34">, Kodirov et al., 2015</ref><ref type="bibr" target="#b38">, Lampert et al., 2014</ref><ref type="bibr" target="#b61">, Sung et al., 2018a</ref>, and action recognition <ref type="bibr" target="#b0">[Alexiou et al., 2016</ref><ref type="bibr" target="#b15">, Gan et al., 2016</ref><ref type="bibr" target="#b20">, Hahn et al., 2019</ref><ref type="bibr" target="#b25">, Jain et al., 2015a</ref><ref type="bibr" target="#b26">, Jain et al., 2015b</ref><ref type="bibr" target="#b41">, Mettes and Snoek, 2017</ref><ref type="bibr" target="#b72">, Xu et al., 2015</ref><ref type="bibr" target="#b74">, Xu et al., 2016</ref><ref type="bibr" target="#b73">, Xu et al., 2017</ref>. The other zero-shot action papers, to the best of our knowledge, mostly are not GCN based, which has been proven to do better than traditional zero-shot techniques for image classification . While <ref type="bibr" target="#b16">[Gao et al., 2019]</ref> is GCN based, their KG is very different from the one we use. They construct a single KG with actions and objects using ConceptNet <ref type="bibr" target="#b59">[Speer et al., 2017]</ref>, where nodes are connected based on word embedding. They use visual object features as a second channel interconnected with the same edge weights to improve zero-shot learning. The number of objects in their graph is not dependent on the number of action classes. They show their best result when selecting 2000 most common visible objects in their dataset to get their object nodes, meaning they need access to the unlabelled test data (transductive). We use separate KGs for action, verb and noun and fuse them at the end with a fusion layer. Our verbs and nouns are dependent only on the action label and uses no visual information (inductive). We compare our results with <ref type="bibr" target="#b16">[Gao et al., 2019]</ref>, <ref type="bibr" target="#b54">[Romera-Paredes and Torr, 2015]</ref> and <ref type="bibr" target="#b80">[Zhang et al., 2017b]</ref>, where <ref type="bibr" target="#b54">[Romera-Paredes and Torr, 2015]</ref> uses a two linear layers network for learning relationships between features, attributes, and classes; while <ref type="bibr" target="#b80">[Zhang et al., 2017b]</ref> uses the image feature space to map the language embedding, instead of an intermediate space.</p><p>Few-Shot Action Recognition: Few-shot for image classification has been explored using meta-learning for learning distance of samples and decision boundary in the embedding space <ref type="bibr" target="#b53">[Ren et al., 2018</ref><ref type="bibr" target="#b57">, Snell et al., 2017</ref><ref type="bibr" target="#b62">, Sung et al., 2018b</ref>, or by learning the optimization algorithm which can be generalized over different datasets <ref type="bibr">[Mishra et al., 2018b, Ravi and</ref><ref type="bibr" target="#b52">Larochelle, 2017]</ref>. A benchmark for few shot image classification is created in <ref type="bibr" target="#b22">[Hariharan and Girshick, 2017]</ref>. For action recognition, studies propose embedding a video as a matrix <ref type="bibr" target="#b83">[Yang et al., 2018, Zhu and</ref>, using deep networks <ref type="bibr" target="#b46">[Mishra et al., 2018a]</ref> or generative models <ref type="bibr" target="#b36">[Kumar Dwivedi et al., 2019</ref><ref type="bibr" target="#b46">, Mishra et al., 2018a</ref> and using human-object interaction <ref type="bibr" target="#b30">[Kato et al., 2018]</ref>. We tried GCN based few-shot learning for action recognition, but our approach cannot be compared to many of these approaches due to two reasons -1) Each paper uses a different dataset split, and our splits are different as well because we use a pre-trained network from Kinetics in our pipeline; 2) We do not evaluate the episodic learning formulation like several other papers. Our aim is to improve few-shot using the KG constructed for the zero-shot setting (relationship of class names, etc.) thereby building an unified zeroshot and few-shot learning framework, which to the best of our knowledge, is not explored in the past.</p><p>Knowledge Graphs and Graph Convolution Networks: KGs are used to improve performance for different visual applications <ref type="bibr" target="#b41">[Marino et al., 2016</ref><ref type="bibr" target="#b12">, Fang et al., 2017</ref>. Automatic construction of a large KG and relationship learning has captured a lot of attention in the past <ref type="bibr" target="#b2">[Bordes and Gabrilovich, 2014</ref><ref type="bibr" target="#b7">, Choudhury et al., 2017</ref><ref type="bibr" target="#b16">, Gao et al., 2019</ref><ref type="bibr" target="#b39">, Lin et al., 2015</ref>.</p><p>We focus on construction of a KG to depict interrelationships of action categories. <ref type="bibr" target="#b17">[Gentner, 1981]</ref> shows how verbs and nouns have different levels of complexities and usually an action phrase comprises of both or just the verb. We explore different KGs, including one with verbs and nouns only, to understand how these knowledge graphs improve performance for action recognition in zero-shot and few-shot learning setup.</p><p>To process graphs using deep learning algorithms, graph convolution networks(GCN) have been used for a number of different applications including action recognition <ref type="bibr" target="#b75">[Yan et al., 2018</ref><ref type="bibr" target="#b18">, Ghosh et al., 2020</ref> We use knowledge graphs based on word embeddings (action class names, and associated verbs and nouns) and visual features for action recognition. With the word embeddings based knowledge graph, we propose a zero-shot learning approach and with visual features based knowledge graph we propose a few-shot learning approach.</p><p>2016, <ref type="bibr" target="#b11">Duvenaud et al., 2015</ref><ref type="bibr" target="#b23">, Henaff et al., 2015</ref>. Concept of spectral graph theory was introduced in [Hammond et al., 2011] and extended in <ref type="bibr" target="#b8">[Defferrard et al., 2016]</ref>, to a formal version of GCN. <ref type="bibr" target="#b32">[Kipf and Welling, 2017]</ref> simplified GCNs through localized first order approximations of spectral GCN. We use the GCN version developed by <ref type="bibr" target="#b32">[Kipf and Welling, 2017]</ref>. Graph attention networks <ref type="bibr" target="#b67">[Veli?kovi? et al., 2018]</ref>, <ref type="bibr">GraphFlow [Ji et al., 2020]</ref> and Long Tail relation extraction  are some of the recent developments in this field.  uses Graph Convolution network on KG for zero-shot image classification. The knowledge graph was formed with NELL(Never Ending Language Learning) <ref type="bibr" target="#b3">[Carlson et al., 2010]</ref>, NEIL(Never Ending Image Learning) <ref type="bibr" target="#b6">[Chen et al., 2013]</ref> and Word-Net <ref type="bibr" target="#b45">[Miller, 1995]</ref>. We use a similar model based on GCN for actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of GCN</head><p>The implementation technique of Graph Convolutional Networks in <ref type="bibr" target="#b32">[Kipf and Welling, 2017</ref>] is used to train our KG to transfer classifier layer weights from trained classes to unseen test classes. The GCN operation can be described by the equation</p><formula xml:id="formula_0">H l+1 = g(H l , A) = ?(D ?1/2?D?1/2 H l W l )</formula><p>where? = I +A and A is the adjacency matrix consisting of edge weights between nodes,D is the node degree matrix of?, H l and W l are the N ? d l input matrix of the l th layer and d l ?d l+1 weight matrix respectively. N is the number of nodes in the graph, d l is the dimension of the l th layer and ? represents a non-linear activation function (e.g., ReLU).</p><p>Zero-shot/few-shot action recognition using GCN <ref type="figure">(Fig.2</ref>) follows a similar technique as . It consists of training and testing phases as described next.</p><p>Training: Initially, a model pre-trained on Kinetics is fine-tuned using training classes of UCF101, HMDB51, or Charades, followed by the extraction of the final classifier layer weights to be used for training the GCN. The constructed KG, along with the adjacency matrix, are inputs to the GCN. The output of each node of the GCN has the same dimensions as the trained classifier layer filter size (1024 in our case). The GCN is trained such that its output for the training classes matches the classifier layer weights of the trained I3D model. The loss used is the mean squared error (MSE) loss.</p><p>So if there are C train number of training classes, C test number of test classes and the output feature dimension of each class is d, then the output of the GCN, </p><formula xml:id="formula_1">W GCN , is of size (C train + C test ) ? d. From W GCN ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Knowledge Graphs for Actions</head><p>In this section, we describe the construction of different KG for actions. We follow similar pipeline as  (also described in Section3) which requires a KG as input.  use Wordnet embeddings to construct the KG for ZSL on image classification. Compared to , our action label classes are sentences or phrases instead of words, which is why using wordnet or word2vec doesn't provide distributive and coherent embeddings for action labels. Moreover, getting semantically correlated embedding space for words and visual features for a good KG is another challenge. We describe these challenges and how we tackle them while constructing three different versions of KGs for actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG1:</head><p>The first KG is based on word descriptors of action class names. Since our action classes are composed of multiple words like a sentence or phrase, averaging word2vec embedding for all words in the sentence does not provide a cohesive embedding space. We discuss the experimental results for word2vec embeddings in Section7. To overcome this challenge, we use the sen-tence2vec model described in <ref type="bibr" target="#b48">[Pagliardini et al., 2018]</ref>, which is an unsupervised learning method to learn embeddings for whole sentences. We use the unigrams model trained on Wikipedia to generate our sentence embeddings. The node features in KG1 are the sentence embeddings. The nodes from Kinetics action classes are added in KG1 corresponding to each dataset (UCF101, HMDB51, and Charades). This is inspired by <ref type="bibr" target="#b74">[Xu et al., 2016</ref><ref type="bibr" target="#b72">, Xu et al., 2015</ref>, where they show distinct advantages of adding classes and images from other datasets in zero-shot learning. Although we cannot directly add images due to the way our model is constructed, we add new activity classes from the Kinetics dataset to increase the size of our KGs. Appending 400 Kinetics classes to UCF101 results in a total of 501 nodes in the KG1 for UCF. Similarly appending the nodes to HBDB51 and Charades results in a total of 451 nodes and 557 nodes respectively. We show more results on performance comparison with and without adding Kinetics nodes in Section 7.</p><p>With the sentence2vector node features, we construct the KG1 where node i is connected to another node j in the combined dataset based on edge weights A ij from cosine similarity of node features. Here, A is the adjacency matrix for KG1. We sort the edges weights in descending order to get the top N closest neighbors per node. N is a hyperparameter that is determined experimentally and is dependent on the dataset. It is 5 for HMDB51 and UCF101 and 20 for Charades. j being one of the top N neighbors of i does not mean that the vice versa is true as well. To make the adjacency matrix symmetric, we fill A ji with the same value as A ij , so the number of connections to each node &gt;= N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG2:</head><p>The second graph, KG2, is constructed with verbs and nouns associated with each action class. This graph is inspired by multiple works on zero-shot action using human object interaction where the detected objects in the scene are used to draw the relationships between seen and unseen action classes <ref type="bibr" target="#b16">[Gao et al., 2019</ref><ref type="bibr" target="#b25">, Jain et al., 2015a</ref>. In <ref type="bibr" target="#b16">[Gao et al., 2019]</ref> object detection is carried out in the visual domain as well and then mapped to word domain for zero-shot learning. We do not do mapping for objects features from visual to word. Instead, we just take the output of verb and noun graphs (KG2), and pass it through the fusion layer to get the visual action (noun+verb) classifier weights.</p><p>To construct KG2, we use a standard language lemmatizer <ref type="bibr" target="#b1">[Bird and Klein, 2009</ref>] to break up a phrase describing an action and convert the word to its root form. Then, we use a part-of-speech (pos) <ref type="bibr" target="#b64">[Toutanova et al., 2003]</ref> tagger to label the word as a noun or a verb. Still, a lot of action class names do not have a noun in the phrase, for example "beatboxing". For such classes the pos tagger gives a noun label of "unknown" and if Wordnet can return a noun that is related to that word, we replace the "unknown" by the noun. For action classes like "archery", which does not have a specific verb associated with it, we replace the verb with "doing". For node features, we compute sentence2vec embeddings as above for verbs and nouns. Hence, we get a set of graphs with only verbs and only nouns. These also have same number of nodes as KG1. Moreover, these graphs are used and categorized together as KG2, since they provide partial information about action class (either verb or noun). KG1 and KG2 can be used to define ZSL setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG3:</head><p>The third graph is developed to see relative performance improvements by incorporating only a few labelled images per test class. We use averaged visual features as nodes in KG3. In the visual feature space, we see implicit clustering of similar actions, which is sometimes not captured in word embedding space. For example, "pommel horse" and "horse walking" are considered similar in word embedding space, but these are very different activities which is captured in visual embedding space shown for dataset UCF101 in <ref type="figure" target="#fig_0">Fig. 3</ref>. We randomly pick 5 videos from each test class and use I3D to generate video features as described in Section 5.2. Then taking the mean of these features, we get the graph node descriptors and take their cosine similarity to generate the adjacency matrix as we do for KG1 and KG2. This generates a graph based on visual features. KG3 is used to replicate few-shot learning setup using KGs, since we use 5 visual samples for each test class to construct the nodes. In few-shot setting, we can combine KG3 with KG1 and KG2 to improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use following four datasets, where Kinetics is just for pre-training the model, and rest are used for experiments:</p><p>Kinetics <ref type="bibr" target="#b31">[Kay et al., 2017]</ref>: Kinetics is a large dataset with 400 classes and about 3 * 10 6 videos. We do not actually need access to Kinetics videos, but the class names and an I3D model pre-trained on Kinetics available in <ref type="bibr" target="#b4">[Carreira and Zisserman, 2017</ref>]. Since we use Kinetics for pre-training I3D and data augmentation while training the GCN, we cannot keep common classes between Kinetics and UCF101 or HMDB51 or Charades in the test set while doing zero-shot learning. So, we use classes in UCF101, HMDB51 and Charades that are also present in Kinetics, as training set.</p><p>UCF101 <ref type="bibr" target="#b58">[Soomro et al., 2012]</ref>: UCF101 has 13320 videos from 101 classes. After removing common classes with Kinetics, we get 23 classes with 3004 videos in test set for UCF101 and the remaining 78 classes are used for training. Some test class labels do not have semantically correlated neighbors. So, we appended these class names with extra words, for example "front crawl" in UCF101 becomes "front crawl swimming". We discuss class-wise accuracy for test classes in <ref type="figure" target="#fig_2">Fig.5</ref>.</p><p>HMDB51 <ref type="bibr" target="#b35">[Kuehne et al., 2013]</ref>: HMDB51 has 6849 videos from 51 classes. Similar to UCF101, we remove common classes with Kinetics, and get 12 classes with 1541 videos for HMDB51's test set and remaining 39 classes for training. Additionally, to encourage correlation with action classes in Kinetics, we convert the class labels to continuous tenses. For example, classes like "eat", uses sentence2vector embedding corresponding to "eating". Charades <ref type="bibr" target="#b55">[Sigurdsson et al., 2016]</ref>: Charades has 9848 videos from 157 classes and is also a multilabel dataset, meaning each video can have multiple action labels. Charades has noun and verb labels associated with each action class, which we use directly without labelling ourselves. After removing all videos which have at least one common label with Kinetics, we are left with 111 possible test classes. Each video can have both training and test labels in Charades. We cannot separate the training and test videos but just the classes. We split the classes into 50-50 train-test split meaning there are 79 and 78 train and test classes respectively. The 78 test classes are from the 111 classes not in common with Kinetics. All videos with at least one training class are kept in training set and we remove test class labels from them. The rest of the videos are test videos and training class labels are removed from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Extraction</head><p>To extract video features, we use initial model of I3D trained on Kinetics data and fine-tune the last layer on the training classes of either UCF101 or HMDB51. For Charades, just fine-tuning the last layer did not yield good classification performance, so we fine-tune the whole network. This means while training, we cannot compute loss on the Kinetics nodes in the KG for Charades. Even after fine-tuning the complete network for Charades we did not achieve significant performance for zero-shot learning; so we use inverse cross-correlation  <ref type="bibr" target="#b54">-Paredes and Torr, 2015]</ref>, to train GCN. We visualize the video feature space distribution of the UCF101 classes in <ref type="figure" target="#fig_0">Fig.3</ref> with some example images for the test classes. As we can see in <ref type="figure" target="#fig_0">Fig.3</ref>, similar classes are grouped together forming clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Our Pipeline</head><p>Our To fuse the outputs of the different KGs, we concatenate along the channel dimension and then pass them through a GCN layer. For zero-shot this fusion GCN layer uses Adjacency matrix of KG1 and for few-shot it uses the adjacency matrix of KG3. For KG1+KG2 in UCF101, the above fusion technique did not give good performance. So, we use the weighted sum of the outputs of KG1 and KG2 with weights of 0.9 for KG1 and 0.05 each for the verb and noun from KG2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results for zero-shot learning on all 23 test classes for UCF101, 12 test classes for HMDB51 and 78 test classes for Charades are in Table1. These results are based on KGs KG1 and KG2 and combination of both. The combination of KG1 and KG2 graph is done by passing it through the fusion layer (for HMDB51 and Charades) or weighted summation of output (for UCF101). Since all datasets have many action classes without any nouns, only KG2 did not give good performance, but the combination of KG1+KG2 works well.</p><p>We also provide the comparison with state-of-theart in <ref type="table" target="#tab_5">Table 2</ref>. For our data split, we have compared our results with three previous works carried out under similar zero-shot learning settings, ESZSL <ref type="bibr" target="#b54">[Romera-Paredes and Torr, 2015]</ref>, DEM <ref type="bibr" target="#b80">[Zhang et al., 2017b]</ref> and TS-GCN <ref type="bibr" target="#b16">[Gao et al., 2019]</ref>. We could not apply DEM baseline results for Charades, since it is a multilabel dataset. Also, TS-GCN only released code for the transductive setup for UCF101. We have implemented the inductive version and compared to it. We have also added some of the recent results for zero-shot learning. Either their splits are different, or they do not provide code, or an essential part of their framework is missing. However, note that recent work of <ref type="bibr" target="#b16">[Gao et al., 2019]</ref> outperforms these other approaches on their splits and we outperform <ref type="bibr" target="#b16">[Gao et al., 2019]</ref> on our splits.</p><p>We report results for combining KG3 with KG1 and KG2 in <ref type="table">Table 3</ref>. Since we are using KG3, these experiments can be considered as few shot learning setup. To create a baseline, we used the nearest neighbor search to get the class label for test videos. Based on the 5 labelled videos provided, we calculate the mean or center feature for each class and then we use cosine distances between the rest of the test videos and these class centers to sort them into corresponding classes. Our results along with the baselines are in <ref type="table">Table 3</ref>. We use the same train-test splits for UCF101 and HMDB51. For both UCF101 and HMDB51, we get best results if we use all 3 KGs. We do not conduct this experiment for Charades since each video has multiple labels, hence each video data point will update multiple class centers resulting in overlapping class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Word embeddings for action labels: For constructing node features from action labels, we used the word2-vec embeddings trained on Google News <ref type="bibr" target="#b42">[Mikolov et al., 2013a</ref><ref type="bibr" target="#b43">, Mikolov et al., 2013b</ref><ref type="bibr" target="#b44">, Mikolov et al., 2013c</ref>. For all words in each class name, the word2vec embeddings were averaged to give a resultant embedding for the whole phrase, which serves as features of the nodes in the KG. In <ref type="figure" target="#fig_1">Fig.4(b)</ref>, we show the word2vec embedding space of node "Pommel Horse" and its nearest neighbor class nodes.  Averaging word2Vec embedding for all words in action class label phrase works in some cases, but it cannot always capture the meaning or correct relationships between the action classes. Hence, for a class like "riding or walking with horse" in Kinetics dataset, the embedding for each word is located far apart from each other as displayed in <ref type="figure" target="#fig_1">Fig.4(b)</ref>. The mean of these individual words does not lie close to related words in the embedding space and hence does not capture meaningful information.</p><p>To solve this problem we use sentence2vec model from <ref type="bibr" target="#b48">[Pagliardini et al., 2018]</ref>, which captures the semantic meaning of sequences of words. Using this embedding space, the closest word match to a class like "uneven bars" is "gymnastics tumbling". The word em- <ref type="table">Table 4</ref> Performance comparison between word2vec embedding and sentence2vec embedding based models. Both the models are trained on graphs consisting of class nodes from Kinetics and UCF101 with losses on both. Performance metric used is mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean Accuracy Word2Vec 38.02 Sentence2vec 49.14 bedding space for all the classes in UCF101 and Kinetics are displayed in <ref type="figure" target="#fig_1">Fig.4(a)</ref>. The word "Uneven bars" along with its neighbors are emphasized. We run experiments with both word2vec embeddings trained on Google News <ref type="bibr" target="#b42">[Mikolov et al., 2013a</ref><ref type="bibr" target="#b43">, Mikolov et al., 2013b</ref><ref type="bibr" target="#b44">, Mikolov et al., 2013c</ref> and Sentence2Vec embeddings based on unigram model trained on Wikipedia <ref type="bibr" target="#b48">[Pagliardini et al., 2018]</ref>. The results on UCF101 are shown in <ref type="table">Table 4</ref>. These results show significant improvement by using setence2vec over word2vec for KG1.</p><p>Appending Knowledge Graphs with more action classes: We augment the UCF101 and HMDB51 KGs with Kinetics class labels in three different ways. In the first configuration, either only the UCF101 nodes or HMDB51 nodes are used in the KG (101/51 nodes) out of which, 78 and 39 are training nodes respectively. The loss is computed by comparing the output of the GCN on these classes to the weights in the final classifier layer of the fine-tuned I3D network. The second configuration uses the same KG as KG1 explained in section 4. The loss is computed by comparing the output of only the UCF101 or HMDB51 training nodes (78/39 nodes) to the final classifier layer of the fine-tuned I3D network.</p><p>In the third configuration, again KG1 is used. Although, now the loss is computed by summing the 2 MSE losses: (a) Loss 1 by comparing the output of only the UCF101 or HMDB training nodes(78/39 nodes) to the final classifier layer of the fine-tuned I3D network. (b) Loss 2 by comparing the output of the Kinetics nodes (400 nodes) to the classifier layer weight of I3D pre-trained on Kinetics. The results of these three experiments are shown in <ref type="table" target="#tab_6">Table 5</ref>. For UCF101 and HMDB51, third configuration works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Types of connections in Knowledge Graphs:</head><p>While constructing the KG with both UCF101 or HMDB51 and Kinetics dataset, we used two types of graph connections. In fully-connected graphs all nodes can be connected to all other nodes, out of which we select top 5 connections. In bipartite, for every node in UCF101 or HMDB51 dataset, we find the top 5 connections to the Kinetics dataset nodes and vice versa. The fully connected(FC) graph works better than the bipartite graph <ref type="table">(Table 6)</ref>.</p><p>Analysis of Class-wise Accuracy using different Knowledge Graphs: To understand the impact of using KG1, KG2 and KG3 for learning each test class, we plot the class-wise accuracy for UCF101 and HMDB51 in <ref type="figure" target="#fig_2">Fig. 5</ref>. Each color of the bar represents a different KG: blue is for word based KG1, orange is for visual feature based KG3 and grey is the combination of KG1, KG2 and KG3.</p><p>As observed in <ref type="figure" target="#fig_2">Fig. 5</ref>, for few classes such as "billiards", "talk", "playing tabla", KG1 performs the best. These classes innately have many neighbors in the word embeddings space, which help in learning them from given training classes. Few other classes, such as "front crawl swimming", "pommel horse gymnastics", "chew food" and "pour liquid" perform well with just KG1 as well, since we add the extra word "swimming", "gymnastics", "food" and "liquid" respectively, to enforce good neighbors in language domain. Intuitively, KG3 does well for "uneven bars", "fall floor", "smile" and "shoot gun", since these have distinct visual features. The combination KG works well for "still rings", "parallel bars", "jumping jack", "playing dhol", "climb stairs", "talk" and "wave".</p><p>Ablation for Network Architecture: We experiment with different number of layers of the GCN (2,4 6, 8 and 10) to explore influence of GCN depth on performance for both UCF101 and HMDB51. The increase in the number of layers of the GCN increases smoothing and decrease in number of layers causes less information propagation. We found that 6 layers gives us the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Usefulness of GCN vs a linear combination of training class weights:</head><p>To show the performance improvement due to GCN compared to just linear combinations, we perform an ablation study. For each test class, we find the top 4 neighbors in the training set. Then using the adjacency edge connection weights, the classifier layer . We added few words for better word embeddings in the labels (such as "pommel horse" becomes "pommel horse gymnastics"), which improves performance for only word based KG (i.e. KG1), as shown here. Each color for bar represents a KG, blue is word based KG, orange is visual feature based KG and grey is combination of all three KGs (KG1,KG2 and KG3). weight for the test class is a weighted average of the classifier layer weights for its neighbors. The performance is in <ref type="table" target="#tab_7">Table 7</ref>.</p><p>Use encoder decoder before GCN: We run another set of experiments where a 2 layered encoder decoder network is added before GCN, for improving encoding of sentence embedding features. The results do not show any promise as seen in <ref type="table">Table 8</ref>.</p><p>Random test train splits: Some of the experiments are done on a random sub-sample of the test-set classes. For UCF101, we choose 10 out of 23 classes 5 times; so that for each random sample of 10 test classes, the rest of the 91 classes forms the training set. The mean accuracy score is calculated after each run and the result of all 5 runs are averaged to get the final mean accuracy score. The results for each of these splits is in <ref type="table" target="#tab_8">Table 9</ref>.</p><p>Learning classifier for unknown classes from related classes in Knowledge Graph: The heatmaps in <ref type="figure" target="#fig_3">Figure 6</ref> depicts the test nodes learning from the interconnections to the train nodes in the KG. They are based on CAM <ref type="bibr" target="#b82">[Zhou et al., 2016]</ref>. Considering the test class "playing sitar in UCF101, one of the top 5 nearest train classes in UCF101 is playing guitar and one of the random classes that have no relation is biking. Now among the five sub-figures in <ref type="figure" target="#fig_3">Figure 6, (a)</ref> is the display of the activation from the "playing sitar class on a "playing sitar video, (b) is the display of the activation from the "playing guitar class on a "playing guitar video, (c) is the display of the activation from is the display of the activation from the "playing sitar" class on a"playing sitar" video, (b) is the display of the activation from the "playing guitar" class on a"playing guitar" video, (c) is the display of the activation from the "playing sitar" class on a"playing guitar" video, (d) is the display of the activation from the "biking" class on a"biking" video and (e) is the display of the activation from the "playing sitar" class on a"biking" video. These heatmaps show that test class "playing sitar" is correctly learning from training class "playing guitar" instead of training class "biking"</p><p>the "playing sitar class on aplaying guitar video, (d) is the display of the activation from the "biking class on a "biking video and (e) is the display of the activation from the "playing sitar class on a "biking video. What we show here is that "playing sitar classifier is similar to the "playing guitar classifier and hence the heat maps from both are similar. This is not the case between "playing sitar and "biking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work we investigate different combinations of knowledge graphs (KG) for actions that give better performance for zero and few shot action recognition. We show significant improvement on zero shot learning by using a network that models a sequence of words instead of traditional single word based models. Moreover, extending KG using other action classes leads to better results. We observe that combining word based knowledge graphs with visual knowledge graphs help in few shot learning. Also combining verbs and noun based KG, improves both zero and few shot learning. Work on dynamically learning the graph weights can be explored in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>t-SNE visualization showing feature distribution of UCF101 video dataset. Sample images are added for our test classes. (Best viewed in digital format)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>(a) Sentence2Vec embedding space for Kinetics and UCF101 classes. The class "uneven bars" and its neighbors are highlighted. (b) Class "Pommel horse" and its neighboring classes in Kinetics dataset using word2vec embedding. The embeddings of each individual word forming the phrase is also displayed. (Best viewed in digital format)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>This figure shows class-wise accuracy for different KGs and combination of KGs for UCF101 and HMDB51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Heatmaps showing activations of various classes' classifier layers on various class videos. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2008.12432v1 [cs.CV] 28 Aug 2020</figDesc><table><row><cell cols="4">Action Names KG</cell><cell></cell><cell></cell><cell cols="2">Verb-Noun KG</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Visual KG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tabla</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Soccer Juggling</cell><cell>Playing Soccer</cell><cell cols="2">Playing Drums Playing Tabla</cell><cell>Swinging</cell><cell>Juggling Playing</cell><cell>Violin Dhol</cell><cell>Drums</cell><cell>Guitar</cell><cell>Soccer Juggling</cell><cell></cell><cell>Playing Soccer</cell><cell>Playing Tabla Drums Playing</cell></row><row><cell>Tennis Swinging</cell><cell>Playing Ice Hockey</cell><cell>Playing Dhol Playing Hockey Field</cell><cell>Playing Violin</cell><cell>Playing Guitar</cell><cell></cell><cell>Tennis</cell><cell>Soccer Ice Hockey Field Hockey</cell><cell></cell><cell></cell><cell>Tennis Swinging</cell><cell>Playing Ice Hockey</cell><cell>Playing Dhol Playing Field Hockey</cell><cell>Playing Violin</cell><cell>Playing Guitar</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row><row><cell cols="2">ZSL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell></row><row><cell cols="2">FSL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. For GCNs, some of the initial works include [Atwood and Towsley,</figDesc><table><row><cell></cell><cell></cell><cell>Zero-Shot Learning Setup</cell><cell></cell><cell></cell></row><row><cell>Word Embedding</cell><cell>Knowledge Graph</cell><cell>Graph Convolution Network</cell><cell>Output Graph</cell><cell>I3D weight comparison</cell></row><row><cell>Playing piano,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Playing soccer,</cell><cell></cell><cell></cell><cell></cell><cell>Seen Class Classifier</cell></row><row><cell>Playing tabla,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Playing ice hockey, Playing guitar, Tennis</cell><cell></cell><cell>?</cell><cell></cell><cell>Unseen Class Classifier</cell></row><row><cell>swinging ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Few-Shot Learning Setup</cell><cell></cell><cell></cell></row><row><cell>Visual Features</cell><cell>Knowledge Graph</cell><cell>Graph Convolution Network</cell><cell>Output Graph</cell><cell>I3D weight comparison</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen Class Classifier</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unseen Class Classifier</cell></row></table><note>? Fig. 2 System overview:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the output dimensions corresponding to the training nodes are selected, denoted by W GCNTrain with size C train ? d. This feature is of the same dimension as the weights of the I3D classifier layer trained or fine-tuned on the training classes of the dataset, W cls . The MSE loss that is back-propagated is given by W GCNTrain ? W cls 2 . Testing: During test time, the penultimate layer of the I3D model is used to extract the features of the test images f test with dimensions N ?d. The output of the test nodes of the GCN with dimension C test ? d is extracted from W GCN , denoted by by W GCNTest . The output class probabilities for the test images (P test ) are obtained as P test = f test W T GCNTest .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Zero-shot learning results for all 3 datasets where we compare performances of KG1, KG2 and a combination of the two. KG1+KG2 always does the best. For UCF101 and HMDB51, the results are in mean accuracy whereas for Charades, we report mean average precision (mAP)</figDesc><table><row><cell>Dataset</cell><cell>KG1</cell><cell>KG2</cell><cell>KG1+KG2</cell></row><row><cell>UCF101</cell><cell cols="2">49.14 45.47</cell><cell>50.13</cell></row><row><cell cols="3">HMDB51 38.01 31.57</cell><cell>40.77</cell></row><row><cell cols="3">Charades 15.81 12.48</cell><cell>18.21</cell></row><row><cell cols="4">of training features multiplied with itself as last layer</cell></row><row><cell cols="2">weight inspired by [Romera</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>Zero-shot learning results for all 3 datasets. The baselines are ESZSL, DEM, Objects2Action, UR, CEWGAN and TS-GCN. For UCF101 and HMDB51, the results are in mean accuracy whereas for Charades, we report mean average precision (mAP) since it is multi-label dataset. Few-shot learning results for the UCF101 and HMDB51 datasets. The baseline is nearest neighbor, given 5 videos for each test set. The combination of KG1, KG2 and KG3 does the best in both cases.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>UCF101</cell><cell></cell><cell cols="2">HMDB51</cell><cell>Charades</cell></row><row><cell></cell><cell></cell><cell cols="5">23-78 split 50-51 split 12-39 split 25-26 split 78-79 split</cell></row><row><cell cols="2">ESZSL [Romera-Paredes and Torr, 2015]</cell><cell cols="2">35.27</cell><cell>15.0</cell><cell>34.16</cell><cell>18.5</cell><cell>17.21</cell></row><row><cell>DEM [Zhang et al., 2017b]</cell><cell></cell><cell cols="2">34.26</cell><cell>-</cell><cell>35.26</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Objects2Action [Jain et al., 2015a]</cell><cell>-</cell><cell></cell><cell>30.3</cell><cell>-</cell><cell>15.6</cell><cell>-</cell></row><row><cell>UR [Zhu et al., 2018]</cell><cell></cell><cell>-</cell><cell></cell><cell>17.5</cell><cell>-</cell><cell>24.4</cell><cell>-</cell></row><row><cell cols="2">CEWGAN [Mandal et al., 2019]</cell><cell>-</cell><cell></cell><cell>26.9</cell><cell>-</cell><cell>30.2</cell><cell>-</cell></row><row><cell>TS-GCN [Gao et al., 2019]</cell><cell></cell><cell>44.5</cell><cell></cell><cell>34.2</cell><cell>-</cell><cell>23.2</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">50.13</cell><cell>-</cell><cell>40.19</cell><cell>-</cell><cell>18.21</cell></row><row><cell>Table 3 Dataset</cell><cell>Baseline</cell><cell>KG3</cell><cell cols="4">KG3+KG1 K3+KG2 KG3+KG1+KG2</cell></row><row><cell>UCF101</cell><cell>52.7</cell><cell>57.04</cell><cell>62.10</cell><cell>59.92</cell><cell>64.24</cell></row><row><cell>HMDB</cell><cell>30.2</cell><cell>45.07</cell><cell>45.67</cell><cell>47.61</cell><cell>47.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Experiments with 3 different knowledge graph constructions. The variations are due to using only UCF101/HMDB51 classes for the knowledge graph or appending it with Kinetics classes and training loss being calculated on UCF101/HMDB51 nodes only or both UCF101/HMDB51 and Kinetics nodes in the knowledge graphs. Performance metric used is mean accuracy.</figDesc><table><row><cell>Knowledge</cell><cell cols="2">Nodes for Loss</cell><cell>Mean</cell></row><row><cell>Graph</cell><cell cols="2">Computation</cell><cell>Accuracy</cell></row><row><cell>UCF only</cell><cell>UCF</cell><cell></cell><cell>27.72</cell></row><row><cell>UCF+Kinetics</cell><cell>UCF</cell><cell></cell><cell>32.85</cell></row><row><cell>UCF+Kinetics</cell><cell cols="2">UCF+Kinetics</cell><cell>49.14</cell></row><row><cell>HMDB only</cell><cell>HMDB</cell><cell></cell><cell>31.09</cell></row><row><cell>HMDB+Kinetics</cell><cell>HMDB</cell><cell></cell><cell>29.22</cell></row><row><cell cols="3">HMDB+Kinetics HMDB+Kinetics</cell><cell>38.01</cell></row><row><cell cols="4">Table 6 Performance comparison for fully connected(FC)</cell></row><row><cell cols="4">and bipartite graphs constructed with UCF101 or HMDB51</cell></row><row><cell cols="4">with Kinetics dataset nodes. Both the models are trained on</cell></row><row><cell cols="4">graphs consisting of class nodes from two datasets (UCF101</cell></row><row><cell cols="4">and Kinetics or HMDB51 and Kinetics) with losses on both.</cell></row><row><cell cols="3">Performance metric used is mean accuracy.</cell></row><row><cell>Method</cell><cell cols="3">Mean-accuracy Mean-accuracy</cell></row><row><cell></cell><cell>for UCF</cell><cell cols="2">for HMDB</cell></row><row><cell>FC</cell><cell>49.14</cell><cell>38.01</cell></row><row><cell>Bipartite</cell><cell>33.11</cell><cell>28.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Performance comparison of using GCN vs a linear combination (using the adjacency matrix edge weights) of the top 4 closest training class weights to the test classes. Performance metric used is mean accuracy.</figDesc><table><row><cell>Method</cell><cell>Mean accuracy</cell></row><row><cell>GCN</cell><cell>49.14</cell></row><row><cell>Linear Combination</cell><cell>42.57</cell></row><row><cell cols="2">Table 8 Performance comparison of using an encoder-</cell></row><row><cell cols="2">decode layer before the GCN layers on UCF101 dataset vs</cell></row><row><cell cols="2">not using one. Performance metric used is mean accuracy.</cell></row><row><cell>Method</cell><cell>Mean accuracy</cell></row><row><cell>without encoder-decoder</cell><cell>49.14</cell></row><row><cell>with encoder-decoder</cell><cell>47.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Results on UCF101 with 10 randomly selected test classes leaving 91 classes to be used for training I3D and GCN. Mean accuracy is used for evaluation. The experiments are carried out 5 times and the final column provides the mean accuracy scores. We compare our results to two previous work with similar settings.</figDesc><table><row><cell cols="8">Method Nodes for Loss Computation Split 1 Split 2 Split 3 Split 4 Split 5 Mean</cell></row><row><cell>ESZSL</cell><cell>-</cell><cell>61.25</cell><cell>60.30</cell><cell>53.68</cell><cell>64.81</cell><cell>60.56</cell><cell>60.12</cell></row><row><cell>DEM</cell><cell>-</cell><cell>60.87</cell><cell>65.88</cell><cell>41.89</cell><cell>61.90</cell><cell>52.11</cell><cell>56.53</cell></row><row><cell>Ours</cell><cell>UCF101</cell><cell>59.68</cell><cell>48.51</cell><cell>42.18</cell><cell>49.86</cell><cell>43.12</cell><cell>48.67</cell></row><row><cell>Ours</cell><cell>UCF101+Kinetics</cell><cell>83.62</cell><cell>72.60</cell><cell>71.57</cell><cell>70.85</cell><cell>49.39</cell><cell>69.61</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the Air Force, via Small Business Technology Transfer (STTR) Phase I (FA8650-19-P-6014) and Phase II (FA864920C0010), and Defense Advanced Research Projects Agency (DARPA) via ARO contract number W911NF2020009.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring synonyms as context in zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexiou</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems</title>
		<editor>Towsley, 2016. Atwood, J. and Towsley, D.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
	<note>2016 IEEE International Conference on Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein ;</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Natural language processing with python</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructing and mining web-scale knowledge graphs: Kdd 2014 tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrilovich ;</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1967" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Changpinyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nous: Construction and querying of dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 33rd International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1563" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Defferrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08200</idno>
		<title level="m">Temporal 3d convnets: New architecture and transfer learning for video classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection meets knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1661" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="77" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8303" to="8311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Some interesting differences between verbs and nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gentner ; Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognition and brain theory</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked spatio-temporal graph convolutional networks for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="576" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick ;</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henaff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1383" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Objects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4588" to="4596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What do 15, 000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<title level="m">Gfcn: A new graph convolutional network based on parallel flows</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3332" to="3336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karpathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Welling ; Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kodirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuehne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Protogan: Towards few shot learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Dwivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zeroshot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9985" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4453" to="4462" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The more you know: Using knowledge graphs for image classification</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies</title>
		<meeting>the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller ; Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pagliardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018-Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palatucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Coco attributes: Attributes for people, animals, and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hays ; Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larochelle</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Conference on Learning Representations ICLR</title>
		<meeting>6th International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr ; Romera-Paredes</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigurdsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">510526</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman ;</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soomro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Speer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">5: An open multilingual graph of general knowledge</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">44444451</biblScope>
		</imprint>
	</monogr>
	<note>Conceptnet 5</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>NAACL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmid ;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">S3d: Stacking segmental p3d for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Semantic embedding space for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="333" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">One-shot action localization by learning sequence matching network</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">;</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9436" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
