<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEASEL: A TRANSFORMER-BASED SPEECH-PREFIXED LANGUAGE MODEL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Arjmand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tehran</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Javad</forename><surname>Dousti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tehran</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Moradi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tehran</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TEASEL: A TRANSFORMER-BASED SPEECH-PREFIXED LANGUAGE MODEL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal language analysis is a burgeoning field of NLP that aims to simultaneously model a speaker's words, acoustical annotations, and facial expressions. In this area, lexicon features usually outperform other modalities because they are pre-trained on large corpora via Transformer-based models. Despite their strong performance, training a new self-supervised learning (SSL) Transformer on any modality is not usually attainable due to insufficient data, which is the case in multimodal language learning. This work proposes a Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the mentioned constraints without training a complete Transformer model. TEASEL model includes speech modality as a dynamic prefix besides the textual modality compared to a conventional language model. This method exploits a conventional pre-trained language model as a cross-modal Transformer model. We evaluated TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset. Extensive experiments show that our model outperforms unimodal baseline language models by 4% and outperforms the current multimodal state-of-the-art (SoTA) model by 1% in F1-score. Additionally, our proposed method is 72% smaller than the SoTA model. arXiv:2109.05522v1 [cs.CL] 12 Sep 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Human language is not limited to communicating with words; it employs acoustical annotations, body and head movements, and facial expressions besides using words to reinforce our intentions. For instance, people may strengthen their opposing viewpoints by emphasizing words or changing their vocal pitch. With the growth of social media data and the essential role of additional modalities in expressing an opinion, a new research area in Natural Language Processing (NLP) called multimodal language analysis has emerged <ref type="bibr" target="#b0">[1]</ref>. This area includes multimodal sentiment analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and multimodal emotion recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In multimodal language analysis, different methods aim to combine heterogeneous sources of unimodal features. These methods leverage a spectrum of learning tasks, such as supervised <ref type="bibr" target="#b5">[6]</ref>, multitask learning <ref type="bibr" target="#b6">[7]</ref>, reinforcement learnings <ref type="bibr" target="#b7">[8]</ref>, and self-supervised <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> approaches. However, in most cases, the textual modality outperforms other modalities; This superiority is mainly because the textual modalities  <ref type="figure">Fig. 1</ref>: Process of fine-tuning TEASEL model on multimodal sentiment analysis task. TEASEL receives raw speech and text modalities as input and feed the speech prefixes and textual tokens to a pre-trained BERT-style language model. The</p><p>[CLS] output token will be used for fine-tuning.</p><p>are mainly from large language models pre-trained on huge corpora, while speech and visual features mainly use feature engineering-based approaches. Recent large language models use Self-Supervised Learning (SSL) and Transformer-based <ref type="bibr" target="#b9">[10]</ref> methods utilizing large number of data points for pre-training in textual <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, speech <ref type="bibr" target="#b14">[15]</ref>, vision <ref type="bibr" target="#b15">[16]</ref> and multimodal visualtextual settings <ref type="bibr" target="#b16">[17]</ref>. Collecting large amount of data is not easily attainable for most multimodal contexts, particularly multimodal language. Also, adding a new modality would require new data collection and SSL pre-training, which is not commonly feasible.</p><p>This work proposes a Transformer-Based Speech-Prefixed Language Model (TEASEL) as an approach to generalize a pre-trained language model as a cross-modal attention module. TEASEL introduces speech as a low-resource modality to a pre-trained Transformer-based language model without the need of pre-training the whole Transformer. In this approach, we pre-train a lightweight spatial encoder for speech to represent the speech modality as dynamic prefixes to a pretrained language model and fine-tune this module besides the Transformer encoder layers in a downstream task. With this approach, TEASEL model would exploit the pre-trained language model as a cross-modal multi-head attention during the pre-training and fine-tuning to a downstream task. TEASEL does not require any data alignment. This is a key benefit in multimodal language analysis, where data alignment results in losing crucial intramodal information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>We evaluate TEASEL for the multimodal sentiment analy-sis task on CMU-MOSI <ref type="bibr" target="#b17">[18]</ref> dataset. TEASEL shows 4% percent improvement over the unimodal baseline and state-ofthe-art (SoTA) performance gain of 1% compared to the current multimodal SoTA using extensive experiments. TEASEL does not need a fully-trained modality-specific Transformer and achieves better performance than late fusing of two fullytrained Transformers in a low-resource setting such as multimodal sentiment analysis. Moreover, TEASEL uses approximately 72% fewer parameters compared to late fusing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Multimodal Language Analysis</head><p>Multimodal language analysis is a modern field of study in NLP which intends to model language interactions through different parallel simultaneous modalities. These multimodal studies typically contain text, audio, and visual features. Multimodal human language analysis applications include, but are not limited to, sentiment analysis <ref type="bibr" target="#b17">[18]</ref>, emotion recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, humor detection <ref type="bibr" target="#b19">[20]</ref>, and sarcasm detection <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b21">[22]</ref> has defined representations, translation, alignment, fusion, and co-learning as five core challenges in multimodal machine learning. Among these challenges, <ref type="bibr" target="#b21">[22]</ref> have described fusion as the concept of integrating information from multiple modalities to predict an outcome measure and classifies the fusion methods as early fusion, hybrid fusions, and late fusion.</p><p>With the success of large Transformer-based language models, there are three common options to utilize these language models in multimodal language settings as described next.</p><p>First, certain multimodal language approaches freeze a Transformer and aim to fuse all modalities using tensor outer-products <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, Canonical Correlation Analysis based methods <ref type="bibr" target="#b24">[25]</ref>, Attentive LSTM based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19]</ref>, sequence to sequence based methods <ref type="bibr" target="#b28">[29]</ref>, cross-modal Transformer-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, graph-based method <ref type="bibr" target="#b31">[32]</ref>, and multi-task learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>Second, as fine-tuning large pre-trained Transformerbased language models improve their performances on the downstream tasks significantly, recently, some approaches aim to employ this advantage in multimodal settings. <ref type="bibr" target="#b32">[33]</ref> have proposed a method to fuse other modalities in the middle layer of a pre-trained Transformer-based language model in an aligned manner using a Multimodal Adaption Gate (MAG) module. Later, with the popularity of Transformerbased models in Speech, <ref type="bibr" target="#b33">[34]</ref> has examined jointly finetuning lexicon and speech Transformer on the multimodal language task. They implemented Co-Attention fusions and Shallow-Fusion using an attentive and a straightforward late fusion of two BERT-style <ref type="bibr" target="#b10">[11]</ref> Transformers, respectively. Third, <ref type="bibr" target="#b8">[9]</ref> have proposed a BERT-style pre-training scheme for multimodal language settings, using text, speech, and visual features. They pre-trained their proposed model using one of the most extensive multimodal language datasets, consisting of around 1 million video segments <ref type="bibr" target="#b34">[35]</ref>. Nevertheless, this amount of data is much smaller than the textual corpora size and limits their model's performance.</p><p>Our method is a variation of the second approach, i.e., it fine-tunes a Transformer-based language model in a multimodal setting. In contrast to previous work, we utilize a pretrained Transformer language model as cross-modal attention without needing an entirely different Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformers and BERT-style Models</head><p>A Transformer network is a model which utilizes the selfattention mechanism to encode sequential feature representations. <ref type="bibr" target="#b9">[10]</ref> have introduced the Transformer encoder-decoder network and attained a significant gain in Neural Machine Translations (NMT) task. Regarding this considerable improvement, <ref type="bibr" target="#b10">[11]</ref> have proposed Bidirectional Encoder Representations from Transformers (BERT) model, which uses 12 layers of Transformer encoder. They pre-trained the model with Masked Language Model (MaskedLM) and Next Sentence Prediction (NSP) tasks in a self-supervised manner. BERT has significantly improved after fine-tuning on the GLUE benchmark <ref type="bibr" target="#b35">[36]</ref>, consisting of nine Natural Language Understanding (NLU) tasks, including sentiment analysis and question answering. We refer the reader to <ref type="bibr" target="#b10">[11]</ref> for more details on the pre-training process. Moreover, <ref type="bibr" target="#b11">[12]</ref> introduced Robustly optimized BERT approach (RoBERTa) model, which pre-trained BERT Transformer solely on MaskedLM task using larger corpora, and more effective training parameters.</p><p>With regards to this notable success of BERT-style models in NLP, <ref type="bibr" target="#b14">[15]</ref> introduced wav2vec 2.0, which contains a convolution layer as a temporal encoder, a quantization method, and a BERT-style Transformer as the backbone of the spatial encoder for speech encoding. They pre-trained the model on LIBRISPEECH dataset <ref type="bibr" target="#b36">[37]</ref> containing 960 hours of speech audio. Subsequently, they achieved a significant enhancement on Automatic Speech Recognition (ASR) task.</p><p>The key point of training these BERT-style models is the tremendous number of training data points. Accessing this massive number of data points is not always feasible for a multimodal language setting, and additionally, pre-training a new Transformer is a challenging task. This work employs a low-resource approach to include other modalities to a pre-trained BERT-style Transformer. Specifically, we use a method to introduce another modality as prefixes into a pre-trained BERT-style Transformer in the fine-tuning process. As such, there is no need to train a new task-specific Transformer. [CLS]</p><p>Ligthweight Attentive Aggregation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positional Embiddings</head><p>Fixed RoBERTa Fixed Encoder Layer <ref type="bibr">[mask]</ref> ? Z L CA <ref type="figure">Fig. 2</ref>: The architecture of TEASEL during the pre-training process. TEASEL takes ? as a raw speech and L as corresponding text and calculates the MaskedLM task on the textual tokens to train LAA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generalization of Pre-Trained Language Model</head><p>With the accomplishment of Transformers as language models on several textual tasks, <ref type="bibr" target="#b37">[38]</ref> have examined the generalizations of fine-tuning these language models on various non-verbal modalities downstream tasks, such as image classifications and protein folding. They introduced Frozen Pre-trained Transformer (FPT), using the GPT-2 <ref type="bibr" target="#b12">[13]</ref> language model as the backbone, and re-trained a small part of the Transformer layers. Their exhaustive set of evaluations demonstrate that it is feasible to achieve comparable results to a fully trained Transformer by only fine-tuning a small part of the pre-trained model on the downstream task. Furthermore, following the success of the GPT-3 <ref type="bibr" target="#b13">[14]</ref> auto-regressive Transformer in few-shot learnings, <ref type="bibr" target="#b38">[39]</ref> have introduced an image encoder that can generate prefix tokens for large frozen auto-regressive Transformer. They have achieved a method that performs better than baselines on several multimodal visual tasks, containing few-shot image classifications and the visual question answering. Inspired by <ref type="bibr" target="#b38">[39]</ref> approach, we propose a method to train Lightweight Attentive Aggregation (LAA) module, which generates speech prefixes for a pre-trained language model. However, in contrast to <ref type="bibr" target="#b38">[39]</ref>, we fine-tune LAA module and the pre-tranined language model on the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>This section describes our proposed TEASEL model , <ref type="figure">Figure 2</ref>, that exploits conventional pre-trained language models as a cross-modal attention module. Fundamentally, TEASEL model focus on representing speech features as prefixes to a pre-trained BERT-style language model (i.e., RoBERTa). We choose the speech modality as our additional modal among the speech and visual (facial expressions) modalities for three main reasons. First, speech datasets are more comprehensive. Second, working on raw speech data is less computationally intensive. Finally, previous works have shown that the speech modality serves more information in multimodal language settings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Our method contains two steps, pre-training and finetuning. As for the pre-training step, we learn a representation to insert an additional modality to a fixed pre-trained language model using LAA module with relatively few training steps (8,000 total steps). Afterward, we fixed most part of LAA module and fine-tuned the pre-trained Transformer as a cross-modal Transformer on CMU-MOSI dataset for the multimodal sentiment analysis downstream task. We discuss the parts to fine-tune and parts to keep frozen in the ablation studies section.</p><p>In the rest of this section, we present the methodology of BERT-style Transformers, which serve as the backbone of our method. Respectively, we describe the speech feature extraction and LAA module. Finally, we describe TEASEL model's pre-training and fine-tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BERT-style Language Models</head><p>We choose RoBERTa model <ref type="bibr" target="#b11">[12]</ref> for our conventional BERTstyle encoder, because it has trained more robustly on much larger datasets. RoBERTa tokenizer decomposes sentence L as</p><formula xml:id="formula_0">tokenizer(L) = {[CLS], l 1 , l 2 , . . . , l T L , [SEP ]},<label>(1)</label></formula><p>where l i ? R d represents a byte-level token, T L denotes the number of time-steps in the textual modality, and [CLS] and [SEP ] designate the beginning and the end of the sequence symbols, respectively. The standard published RoBERTa models consist of a base and a large model containing 12 and 24 layers, respectively, both of which are exclusively trained on the MaskedLM task <ref type="bibr" target="#b10">[11]</ref>. MaskedLM task intends to predict randomly masked tokens using the entire unmasked words of the sentence. MaskedLM loss masks 15% of the tokens using a variety of masking methods. The standard MaskedLM <ref type="bibr" target="#b10">[11]</ref> uses the [MASK] token 80% of the time, a random token 10% of the time, and the unchanged token 10% of the time; forcing the language model to predict the output token confidently. We use standard MaskedLM for our pre-training process. We refer the reader to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> for additional specifications of the pre-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Speech Module</head><p>As discussed, we need a unimodal pre-trained speech feature extractor model and then learn a relatively small spatial head at the top of representations to encode speech signals as a word token.  <ref type="figure">Fig. 3</ref>: The architecture of Lightweight Attentive Aggregation (LAA) to aggregate the speech latent representation as prefixes with bidirectional attentive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-GRU</head><formula xml:id="formula_1">? Z ? 2 2 ? 2 T A ? 1 2 ? 1 1 C A ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Speech Temporal Encoder</head><p>wav2vec 2.0 model <ref type="bibr" target="#b14">[15]</ref> utilizes five layers of Convolutional Neural Networks (CNN) as a temporal feature encoder and a BERT-style Transformer as the contextualized encoder. In this work, we select wav2vec 2.0 pre-trained fixed-parameter CNN as an audio feature encoder.</p><formula xml:id="formula_2">CNN ? W (?) = {z 1 , z 2 , . . . , z T A ; z i ? R d A },<label>(2)</label></formula><p>In Equation 2 the CNN ? W would take ? as a raw speech vector and represents z i as a speech latent feature for T A timesteps, and ? W denotes pre-trained parameters from wav2vec 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Lightweight Attentive Aggregation (LAA)</head><p>Although wav2vec 2.0 contains a well-defined Transformer network as a contextual encoder, for the sake of generalization, we train a small contextual encoder at the top of fixed features to simulate the effect of not having a fully trained Transformer for low-resources modalities. Essentially, the speech LAA aims to encode Z as prefix tokens for RoBERTa encoder. As <ref type="figure">Figure 3</ref> illustrates, this module performs a Bi-directional Gated Recurrent Unit (Bi-GRU) at the top of speech fixed features to capture the information in a bidirectional behavior. More specifically, two output sequences of the BiGRU (?) are defined as follows.</p><formula xml:id="formula_3">Z = LayerN orm(Z)<label>(3)</label></formula><formula xml:id="formula_4">? = BiGRU (W 1? + b 1 ),<label>(4)</label></formula><formula xml:id="formula_5">? = {{? 1,1 , . . . , ? 1,T A }, {? 2,1 , . . . , ? 2,T A }},<label>(5)</label></formula><p>where W 1 ? R d A ?d A and b 1 ? R d A respectively denotes projection weight and bias and ? ? A 2?T A ?d A denotes the BiGRU output sequences. Next, we need an aggregation module to interpret the final prefixed tokens. Accordingly, we design LAA module based on the encoder used in the encoder-decoder models in the NMT field <ref type="bibr" target="#b39">[40]</ref>. Our LAA module calculates a dynamic weighted sum over ? as,</p><formula xml:id="formula_6">u k,i = ?(W Agg1 ? k,i + b Agg1 ), k ? {1, 2},<label>(6)</label></formula><formula xml:id="formula_7">? k = Sof tmax(W Agg2 u k + b Agg2 ), ? k ? [0, 1],<label>(7)</label></formula><formula xml:id="formula_8">C A = T i=1 ? k,i ? k,i ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">W Agg1 ? R d A ?d A , b Agg1 ? R d A , W Agg2 ? R d A , b</formula><p>Agg2 ? R, are feed forward's weights and biases, ? is activation function, and C A ? R 2?da are two RoBERTa-ready speech-prefixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Pre-Training Process</head><p>Similar to the training process mentioned in <ref type="bibr" target="#b38">[39]</ref>, we used a fixed pre-trained language model as our Transformer encoder. We feed the sequence</p><formula xml:id="formula_10">{[CLS], C A , l 1 , l 2 , . . . , [M ASK], . . . , l T L , [SEP ]}</formula><p>to a pre-trained RoBERTa model. In the mentioned sequence, [M ASK] token is the MaskedLM masked token, and it only applies to lexicon tokens. We only calculate the loss function on the verbal output tokens, and its gradient only affects LAA module. In particular, the MaskedLM aims to predict [M ASK] tokens using other words and speech prefixes. In contrast to <ref type="bibr" target="#b38">[39]</ref>, we used a BERT-style language model, absolute position embedding, and MaskedLM loss to train our LAA module. Moreover, in the ablation studies section, we study the importance of pre-training on the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Fine-tuning on Downstream Task</head><p>As <ref type="figure">Figure 1</ref> illustrates, we feed the speech prefixes and textual tokens to the pre-trained model and fine-tune it using an additional head at the top of the [CLS] token. In contrast to <ref type="bibr" target="#b38">[39]</ref> which Freezes their model on both pre-training and downstream tasks, we fine-tune a subset of our model besides the language model encoder on the downstream task. In the </p><formula xml:id="formula_11">? i ? Attention ? A (? i ) 7: C A ? T i=1 ? i ? i 8: [CLS; H A ; H L ] ? RoBERT a ? LM ([C A ; L L ]) 9: ?? A ? backpropagate L M LM (H L ) 10:</formula><p>? A ? ? A ? ? p ?? A 11: end for 12: // Pre-training process 13: // Fine-tune ? LM and ? A on the downstream task. <ref type="bibr">14:</ref> for iteration = 1, 2, ..., epoch g do <ref type="bibr" target="#b14">15</ref>: </p><formula xml:id="formula_12">? A ? BiGRU ? A (Z A ) 16: ? i,k ? Attention ? A (? i,k ) 17: C A ? T i=1 ? k,i ? k,i 18: [CLS; H A ; H L ] ? RoBERT a ? LM ([C A ; L L ]) 19:? ? Classif icationHead ? h (CLS) 20: ?? A,LM,h ? backpropagate L M SE (?, y) 21: ? A,LM,h ? ? A,LM,h ? ? f ?? A,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Methods</head><p>We evaluated TEASEL on CMU-MOSI <ref type="bibr" target="#b17">[18]</ref> for the multimodal sentiment analysis task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">CMU-MOSI</head><p>Multimodal Opinion-level Sentiment Intensity dataset (CMU-MOSI) <ref type="bibr" target="#b17">[18]</ref> contains 2199 video segments. Each has a sentiment intensity label with intensity in the range of [?3, +3]; +3 denotes the solid positive sentiment, and ?3 indicates the solid negative sentiment. To be consistent with prior work, we use the authors' published train/validation/test sets on CMU-Multimodal SDK <ref type="bibr" target="#b25">[26]</ref>.</p><p>We followed the evaluation approach presented in <ref type="bibr" target="#b5">[6]</ref> using binary accuracy and F1-scores on the non-zero human-annotated labels. More concretely, [?3, 0) labels expresses negative and (0, +3] predictions indicating positive sentiments. Additionally, we report the 7-class accuracy (Acc 7 ), Mean Absolute Error (MAE), and the correlation of the predicted labels with target labels to be consistent with the prior work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline</head><p>We consider a variety of SoTA unimodal and multimodal Transformers and fusion methods. Experimental comparisons are reported in three sections:</p><p>First, we compare TEASEL against unimodal (text only) large textual Transformers,</p><p>? RoBERTa <ref type="bibr" target="#b11">[12]</ref> is a robustly pre-trained version of BERT Transformer. <ref type="bibr" target="#b11">[12]</ref> have published a base and large version of RoBERTa, using 12 and 24 layers, respectively. We used the base and large version of RoBERTa as a text-only baseline.</p><p>Second, we compare TEASEL against popular multimodal fusion methods which use a Transformer as a feature extractor. These methods are as follows.</p><p>? Tensor Fusion Network (TFN) utilizes tensor outerproduct to combine three modalities in unimodal, bimodal, and trimodal fashion. Furthermore, it concatenates all six flattened vectors as a multimodal representation <ref type="bibr" target="#b22">[23]</ref> .</p><p>? Low-rank Multimodal Fusion (LFM) optimizes the TFN time complexity from exponential to linear using modality-specific low-rank factors <ref type="bibr" target="#b23">[24]</ref>.</p><p>? Memory Fusion Network (MFM) trains modalityspecific generative factorized representations and joint multimodal features across all modalities for multimodal tasks <ref type="bibr" target="#b40">[41]</ref>.</p><p>? Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis (MISA) factorizes modalities into modality-invariant and modalityspecific features and then fuses them to predict a label <ref type="bibr" target="#b41">[42]</ref>.</p><p>? Interaction Canonical Correlation Network (ICCN) applies Deep CCA to dissolve hidden relationships between BERT sentence embeddings, audio sequence, and facial features <ref type="bibr" target="#b24">[25]</ref>.</p><p>Finally, we compare TEASEL performance against multimodal fusion Transformer-based methods.</p><p>? Multimodal Adaptation Gate-Transformer (MAG-Transformer) is a module to generate a shift to the internal representation of Transformer models using visual and acoustic modalities. Combining the MAG module with a pre-trained Transformer would let them integrate speech and visual modalities to the middle level of the pre-trained lexicon Transformer <ref type="bibr" target="#b32">[33]</ref>.</p><p>? Multimodal Transformer (MulT) performs directional cross-modal attention mechanism followed by self-attention for every two modalities to combine the heterogeneous multimodal signals using supervised settings <ref type="bibr" target="#b5">[6]</ref>.</p><p>? Self-Supervised Multi-task Multimodal (Self-MM) employs multi-task learning and self-supervised learning to generate unimodal labels. Later, training the multimodal and unimodal representations to learn both variations and agreements among the modalities <ref type="bibr" target="#b6">[7]</ref>.</p><p>? Shallow-Fusion method jointly fine-tunes robertalarge and vq-wav2vec models on downstream tasks using a late fusion method <ref type="bibr" target="#b33">[34]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>In this subsection, we review the implementation specification of both pre-training and fine-tuning steps. We implemented TEASEL using PyTorch <ref type="bibr" target="#b42">[43]</ref> and HuggingFace frameworks <ref type="bibr" target="#b43">[44]</ref> with a fixed random seed to guarantee reproducibility. We applied grid search for hyperparameter tuning during fine-tuning with learning rates in range of {1e-4, 5e-5, 2e-5, 1e-5}, LAA dropout in the range of {0.0, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5}, batch sizes in the range of {8, 16, 32}, and warm-up steps using {0%, 10%, 20%, 30%, 40%} of total step. We used the validation set of CMU-MOSI to find the best hyperparameters. The best setting was 2e-5 as learning rate, 0.1 as LAA dropout, and warmup step of 10% of total fine-tuning steps. Also, we used a batch size of 32 and 16 during pre-training and fine-tuning, respectively. We pre-trained and fine-tuned TEASEL using AdamW optimizer <ref type="bibr" target="#b44">[45]</ref> , GELU activation function <ref type="bibr" target="#b45">[46]</ref>, cosine scheduler with afrosaid warmup steps. We pre-trained TEASEL using 100 hours of LIBRISPEECH in 8,000 cumulative steps (9 epochs) and fine-tuned TEASEL on CMU-MOSI with 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION OF EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Analysis</head><p>In <ref type="table" target="#tab_3">Table 1</ref>, we present the results of experiments on CMU-MOSI dataset on several benchmarks. Benchmark results are from their corresponding paper discussed in the Baseline section, except TFN, LMF, and MFM models, which <ref type="bibr" target="#b41">[42]</ref> reproduced using BERT sentence representations as a textual modality. First, we evaluate TEASEL against the text-only popular Transformers. TEASEL has performed better than text-only RoBERTa base and large models. The p-value of the student l indicates lower is better. ? Models are proposed by <ref type="bibr" target="#b41">[42]</ref>. ? Models are suggested by <ref type="bibr" target="#b32">[33]</ref>. ? ? Models are produced using the same environment.</p><p>t-test between TEASEL and RoBERTa base is p &lt; 0.05, demonstrating significant improvement over RoBERTa base model.</p><p>Second, in our experiments, we significantly improve over methods that apply the frozen Transformer-based features, principally because fine-tuning pre-trained Transformers for the downstream task would significantly improve the performance of Transformers.</p><p>Eventually, we examine TEASEL against networks which fine-tune a Transformer for the downstream task. Our method has outperformed Self-MM <ref type="bibr" target="#b6">[7]</ref>, MAG-BERT, MAG-XLNet <ref type="bibr" target="#b32">[33]</ref>, and Shallow-fusion <ref type="bibr" target="#b33">[34]</ref> methods in most metrics. Unlike MAG-Transformer methods, TEASEL does not require an aligned feature and explicitly feeds speech representations to the Transformer. As for Shallow Fusion, TEASEL outperforms their best setup in F1-score, Acc 2 metrics, but their method has a superior result for the Acc 7 and MAE utilizing a fully trained speech BERT-style Transformer. Moreover, TEASEL uses 133.4M parameters while Shallow-Fusion uses approximately 483.6M parameters, which indicated TEASEL uses 72.4% fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>We study the following subjects to examine the usefulness of some decisions in designing TEASEL .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Question 1: Does pre-training effects performance of TEASEL ?</head><p>We examined the effect of pre-training process on the multimodal sentiment analysis downstream task. We trained LAA module using 100 hours of LIBRISPEECH dataset (around 28k data points) for 8,000 training steps. Subsequently, we saved the model every 2,000 steps and fine-tuned the saved checkpoints for the multimodal sentiment analysis on CMU-MOSI dataset. As <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates, pre-training LAA module improves TEASEL.   We trained the new classification head along with some parts of pre-trained TEASEL model. We have done exhaustive experiments on which parts of LAA modules should be fixed for the fine-tuning part. As <ref type="table" target="#tab_4">Table 2</ref> represents, we obtained the best result on solely fine-tuning RoBERTa encoder and LAA besides the classification head. We Believe this is primarily because CMU-MOSI does not contain many data points <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">199)</ref>, and fine-tuning BiGRU and Projection hurt the model's performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Question 3: Why we need an aggregation for the speech prefixes tokens?</head><p>The Transformer's attention complexity is O(n 2 ), where n is the length of a sequence <ref type="bibr" target="#b10">[11]</ref>. Letting the pre-trained Trans-former use additional sequences might improve the entire process, but that would significantly increase the computation complexity. As we aggregate speech features before feeding them to a pre-trained Transformer, this would not increase the Transformer's complexity quadratically. Moreover, our attention module uses linear computation complexity with respect to sequence length in order to attend BiGRU's output sequences. We illustrate and discuss the effectiveness of speech prefixes in the qualitative analysis section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Analysis</head><p>We illustrate TEASEL's random attention activation layer to investigate whether the fine-tuned Transformer encoder has learned to attend the speech prefixes. <ref type="figure" target="#fig_3">Figure 4</ref> exhibits a random datapoint from CMU-MOSI test set, "lXPQBPVc5Cw 16", in which the speaker says "uh I will I will actually say this right now it was a good waste of like 12 (dollars)" with a non-negative voice tone, which caused the label to be weakly positive (i.e., +0.2). In <ref type="figure" target="#fig_3">Figure 4</ref>, RoBERTa attention masks demonstrate that different word tokens attend to speech prefixes that are from an attentive module. While in BERT-style Transformers, there is excellent attention between each token and [CLS]/[SEP ] tokens, we removed the [CLS] and [SEP ] tokens for better illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We proposed a Transformer-Based Speech-Prefixed Language Model called TEASEL to model multimodal sentiment analysis without training a new cross-modal Transformer. We implemented a Lightweight Attentive Aggregation module to create an efficient spatial encoding, which creates speech prefixes for a pre-trained Transformer. TEASEL successfully utilized a pre-trained Transformer as a cross-modal attention module. Comprehensive analyses validated that TEASEL outperforms Transformer based text-only language model by 4% and achieves an approximately 1% superior result than the current multimodal SoTA. Extensive experiments verify the effectiveness of various steps of TEASEL model. We hope this work can provide a new perspective on pre-trained Transformers' generalization for multimodal language environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>LM,h 22: end for 23: return ? A,LM,h as TEASEL ablation studies section, we demonstrate the effect of freezing each part of the proposed model on the downstream task. Furthermore, Algorithm 1 shows a pseudocode for the whole proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s p e e c hp r e f i xl e f t s p e e c hp r e f i xr i g h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The attention activation layer of a random data point in the test set. Left plots are LAA speech attention values and the right figure demonstrates the attention activation layer in the attention encoder. As the cross-modal attention demonstrates, different word tokens are attending to speech prefixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Results of fine-tuning different checkpoints of pretrained TEASEL using the same condition. Performance of each fine-tuned model on validation set is evaluated. For each checkpoint, F1-score of different pre-trained checkpoints is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Projection LayerNorm Projection LayerNorm LayerNorm Projection Attention Module LayerNorm LayerNorm</head><label></label><figDesc></figDesc><table><row><cell>Shared</cell></row><row><cell>parameters</cell></row><row><cell>FeedForward</cell></row><row><cell>Path</cell></row><row><cell>Attention Module</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1: TEASEL Model Input: speech ?, sentences as L, number of iterations epoch t , epoch f , learning rate? t , ? f Parameter: Initialize ? LM with pre-trained RoBERTa, ? W with pre-trained wav2vec 2.0, and ? A , ? h randomly</figDesc><table><row><cell cols="2">1: Z A ? CNN ? W (?) 2: X L ? tokenizer(L) 3: // Pre-training process</cell></row><row><cell cols="2">4: for iteration = 1, 2, ..., epoch t do</cell></row><row><cell>5: 6:</cell><cell>? A ? BiGRU ? A (Z A ) // dropped Eq. 3 for brevity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results for multimodal sentiment analysis on CMU-MOSI. (B) and (R) indicate sentence representations are from BERT and RoBERTa, respectively.</figDesc><table><row><cell></cell><cell cols="2">CMU-MOSI</cell><cell></cell></row><row><cell>TFN  ? (B)</cell><cell cols="4">MAE l Corr h Acc h 2 0.901 0.698 80.82 80.77 34.94 F1 h Acc h 7</cell></row><row><cell>LMF  ? (B)</cell><cell cols="4">0.917 0.695 82.47 82.47 33.23</cell></row><row><cell>MFM  ? (B)</cell><cell cols="4">0.877 0.706 81.72 81.64 35.42</cell></row><row><cell>ICCN(B)</cell><cell cols="4">0.860 0.710 83.00 83.00 39.00</cell></row><row><cell>MulT</cell><cell cols="4">0.871 0.698 83.00 82.80 40.00</cell></row><row><cell>BERT  ?</cell><cell cols="3">0.739 0.782 85.20 85.20</cell><cell>-</cell></row><row><cell>Self-MM(B)</cell><cell cols="3">0.713 0.798 85.98 85.95</cell><cell>-</cell></row><row><cell>MAG-BERT(B)</cell><cell cols="3">0.739 0.796 86.10 86.00</cell><cell>-</cell></row><row><cell>MAG-XLNet</cell><cell cols="3">0.675 0.821 87.90 87.90</cell><cell>-</cell></row><row><cell>Shallow-Fusion(R)</cell><cell>0.577</cell><cell>-</cell><cell cols="2">88.27 88.57 48.92</cell></row><row><cell>roberta-base  ? ?</cell><cell cols="4">0.704 0.807 85.31 85.37 46.36</cell></row><row><cell>roberta-large  ? ?</cell><cell cols="4">0.687 0.835 86.89 86.91 46.65</cell></row><row><cell>TEASEL  ? ? (R) (ours)</cell><cell cols="4">0.644 0.842 89.33 89.31 47.52</cell></row></table><note>h indicates higher is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of fixing or fine-tuning different subsections of Lightweight Attentive Aggregation. + in each row indicates that we fine-tuned that module and all of the above ones together. For instance, in the second row we fine-tuned robertaencoder and Attention module and keep the rest parts fixed. Also, We understand fine-tuning the embedding layer would hurt the model's performance.</figDesc><table><row><cell cols="5">Frozen or Fine-tuning Parts of TEASEL</cell></row><row><cell></cell><cell>MAE l</cell><cell>Corr h</cell><cell>Acc h 2</cell><cell>F1 h</cell><cell>Acc h 7</cell></row><row><cell cols="6">robert-encoder 0.6861 0.8228 87.80 87.80 47.81</cell></row><row><cell>+ Attention</cell><cell cols="5">0.6443 0.8423 89.33 89.31 47.52</cell></row><row><cell>+ BiGRU</cell><cell cols="5">0.6531 0.8334 88.41 88.40 48.98</cell></row><row><cell>+ Projection</cell><cell cols="5">0.7142 0.8192 85.37 85.28 47.08</cell></row><row><cell cols="6">All Parameters 0.6556 0.8360 88.57 88.57 48.69</cell></row><row><cell>roberta-base</cell><cell cols="5">0.7042 0.8072 85.31 85.37 46.36</cell></row><row><cell>roberta-large</cell><cell cols="5">0.6870 0.8347 86.89 86.91 46.65</cell></row><row><cell cols="6">5.2.2. Question 2: Does the whole model needs fine-tuning?</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Second grand-challenge and workshop on multimodal language (challenge-hml)</title>
		<imprint>
			<date type="published" when="2020-07" />
			<publisher>Association for Computational Linguistics</publisher>
			<pubPlace>Seattle, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Multimodal Sentiment Analysis: Harvesting Opinions from The Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interfaces</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
	<note>ICMI 2011</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ziqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Jiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised learning with cross-modal transformers for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiva</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for selfsupervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ur-funny: A multimodal language dataset for understanding humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><forename type="middle">Iftekhar</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06618</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards multimodal sarcasm detection (an obviously perfect paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ver?nica</forename><surname>P?rez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01815</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathusha</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8992" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiattention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Memory fusion network for multi-view sequential learning</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
	<note>Louis-Philippe Morency, and Barnab?s P?czos</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Factorized multimodal transformer for multimodal sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09826</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Humor knowledge enriched transformer for understanding multimodal humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12972" to="12980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mtgat: Multimodal temporal graph attention networks for unaligned human multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruitao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azaan</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics. Meeting. NIH Public Access</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2359</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Jointly fine-tuning&quot; bert-like&quot; self supervised models to improve multimodal speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamane</forename><surname>Siriwardhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivindu</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suranga</forename><surname>Nanayakkara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06682</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pretrained transformers as universal computation engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05247</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pieter Abbeel, and Igor Mordatch</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13884</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning factorized multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06176</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Misa: Modality-invariant and-specific representations for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-ofthe-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
