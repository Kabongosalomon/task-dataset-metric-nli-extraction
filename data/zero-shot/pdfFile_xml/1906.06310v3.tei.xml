<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PSEUDO-LIDAR++: ACCURATE DEPTH FOR 3D OBJECT DETECTION IN AUTONOMOUS DRIVING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PSEUDO-LIDAR++: ACCURATE DEPTH FOR 3D OBJECT DETECTION IN AUTONOMOUS DRIVING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects -currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection -outperforming the previous state-of-the-art detection accuracy for faraway objects by 40%. Our code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head> <ref type="figure">Figure 1</ref><p>: An illustration of our proposed depth estimation and correction method. The green box is the ground truth location of the car in the KITTI dataset. The red points are obtained with a stereo disparity network. Purple points, obtained with our stereo depth network (SDN), are much closer to the truth. After depth propagation (blue points) with a few (yellow) LiDAR measurements the car is squarely inside the green box. (One floor square is 1m?1m.) Safe driving in autonomous cars requires accurate 3D detection and localization of cars, pedestrians and other objects. This in turn requires accurate depth information, which can be obtained from LiDAR (Light Detection And Ranging) sensors. Although highly precise and reliable, LiDAR sensors are notoriously expensive: a 64-beam model can cost around $75,000 (USD) <ref type="bibr" target="#b33">1</ref> . The alternative is to measure depth through inexpensive commodity cameras. However, in spite of recent dramatic progress in stereo-based 3D object detection brought by pseudo-LiDAR <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>, a significant performance gap remains especially for faraway objects (which we want to detect early to allow time for reaction). The trade-off between affordability and safety creates an ethical dilemma.</p><p>In this paper we propose a possible solution to this remaining challenge that combines insights from both perspectives. We observe that the higher 3D object localization error of stereo-based systems, compared to LiDAR-based ones, stems entirely from the higher error in depth estimation (after the 3D point cloud is obtained the two approaches are identical <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>). Importantly, this error is not random but systematic: we observe that stereo methods do indeed detect objects with high reliability, yet they estimate the depth of the entire object as either too far or too close. See <ref type="figure">Figure 1</ref> for an illustration: the red stereo points capture the car but are shifted by about 2m completely outside the ground-truth location (green box). If we can de-bias these depth estimates it should be possible to obtain accurate 3D localization even for distant objects without exorbitant costs.</p><p>We start by revisiting the depth estimation routine embedded at the heart of state-of-the-art stereobased 3D detection approach <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>. A major contributor to the systematic depth bias comes from the fact that depth is typically not computed directly. Instead, one first estimates the disparity -the horizontal shift of a pixel between the left and right images -and then inverts it to obtain pixel-wise depth. While the use of deep neural networks has largely improved disparity estimation <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018;</ref><ref type="bibr" target="#b6">Cheng et al., 2018;</ref><ref type="bibr">Mayer et al., 2016;</ref><ref type="bibr" target="#b20">Wang et al., 2019b)</ref>, designing and learning the networks to optimize the accuracy of disparity estimation simply overemphasizes nearby objects due to the reciprocal transformation. For instance, a unit disparity error (in pixels) for a 5-meter-away object means a 10cm error in depth: the length of a side mirror. The same disparity error for a 50-meter-away object, however, becomes a 5.8m error in depth: the length of an entire car. Penalizing both errors equally means that the network spends more time correcting subtle errors on nearby objects than gross errors on faraway objects, resulting in degraded depth estimates and ultimately poor detection and localization for faraway objects. We thus propose to adapt the stereo network architecture and loss function for direct depth estimation. Concretely, the cost volume that fuses the left-right images and the subsequent 3D convolutions are the key components in stereo networks. Taking the central assumption of convolutions -all neighborhoods can be operated in an identical manner -we propose to construct the cost volume on the grid of depth rather than disparity, enabling 3D convolutions and the loss function to perform exactly on the right scale for depth estimation. We refer to our network as stereo depth network (SDN). See <ref type="figure">Figure 1</ref> for a comparison of 3D points obtained with SDN (purple) and disparity estimation (red).</p><p>Although our SDN improves the depth estimates significantly, stereo images are still inherently 2D and it is unclear if they can ever match the accuracy and reliability of a true 3D LiDAR sensor. Although LiDAR sensors with 32 or 64 beams are expensive, LiDAR sensors with only 4 beams are two orders of magnitude cheaper 2 and thus easily affordable. The 4 laser beams are very sparse and ill-suited to capture 3D object shapes by themselves, but if paired with stereo images they become the ideal tool to de-bias our dense stereo depth estimates: a single high-precision laser beam may inform us how to correct the depth of an entire car or pedestrian in its path. To this end, we present a novel depth-propagation algorithm, inspired by graph-based manifold learning <ref type="bibr" target="#b21">(Weinberger et al., 2005;</ref><ref type="bibr" target="#b12">Roweis &amp; Saul, 2000;</ref><ref type="bibr" target="#b24">Xiaojin &amp; Zoubin, 2002)</ref>. In a nutshell, we connect our estimated 3D stereo point cloud locally by a nearest neighbor graph, such that points corresponding to the same object will share many local paths with each other. We match the few but exact LiDAR measurements first with pixels (irrespective of depth) and then with their corresponding 3D points to obtain accurate depth estimates for several nodes in the graph. Finally, we propagate this exact depth information along the graph using a label diffusion mechanism -resulting in a dense and accurate depth map at negligible cost. In <ref type="figure">Figure 1</ref> we see that the few (yellow) LiDAR measurements are sufficient to position almost all final (blue) points of the entire car within the green ground truth box.</p><p>We conduct extensive empirical studies of our approaches on the KITTI object detection benchmark <ref type="bibr">(Geiger et al., 2012;</ref><ref type="bibr">2013)</ref> and achieve remarkable results. With solely stereo images, we outperform the previous state of the art <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref> by 10%. Further adding a cheap 4-beam LiDAR brings another 27% relative improvement -on some metrics, our approach is nearly on par with those based on a 64-beam LiDAR but can potentially save 95% in cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>3D object detection. Most work on 3D object detection operates on 3D point clouds from LiDAR as input <ref type="bibr" target="#b4">(Li, 2017;</ref><ref type="bibr">Li et al., 2016;</ref><ref type="bibr">Meyer et al., 2019b;</ref><ref type="bibr" target="#b29">Yang et al., 2018a;</ref><ref type="bibr">Du et al., 2018;</ref><ref type="bibr" target="#b14">Shi et al., 2019;</ref><ref type="bibr">Engelcke et al., 2017;</ref><ref type="bibr" target="#b28">Yan et al., 2018;</ref><ref type="bibr">Lang et al., 2019</ref><ref type="bibr">). Frustum PointNet (Qi et al., 2018</ref> applies <ref type="bibr">PointNet (Qi et al., 2017a;</ref><ref type="bibr">b)</ref> to the points directly, while Voxelnet <ref type="bibr" target="#b32">(Zhou &amp; Tuzel, 2018)</ref> quantizes them into 3D grids. For street scenes, several work finds that processing points from the bird's-eye view can already capture object contours and locations <ref type="bibr" target="#b4">(Chen et al., 2017;</ref><ref type="bibr" target="#b30">Yang et al., 2018b;</ref><ref type="bibr">Ku et al., 2018)</ref>. Images have also been used, but mainly to supplement LiDAR <ref type="bibr">(Meyer et al., 2019a;</ref><ref type="bibr">Liang et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr">Ku et al., 2018)</ref>. Early work based solely on images -mostly built on the 2D frontal-view detection pipeline <ref type="bibr" target="#b11">(Ren et al., 2015;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr" target="#b23">Lin et al., 2017</ref>) -fell far behind in localizing objects in <ref type="bibr">3D (Li et al., 2019a;</ref><ref type="bibr" target="#b22">Xiang et al., 2015;</ref><ref type="bibr" target="#b0">Chabot et al., 2017;</ref><ref type="bibr">Mousavian et al., 2017;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b25">Xu &amp; Chen, 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2016;</ref><ref type="bibr" target="#b7">Pham &amp; Jeon, 2017;</ref>  <ref type="bibr" target="#b34">3</ref> .</p><p>Pseudo-LiDAR. This gap has been reduced significantly recently with the introduction of the pseudo-LiDAR framework proposed in <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>. This framework applies a drastically different approach from previous image-based 3D object detectors. Instead of directly detecting the 3D bounding boxes from the frontal view of a scene, pseudo-LiDAR begins with image-based depth estimation, predicting the depth Z(u, v) of each image pixel <ref type="bibr">(u, v)</ref>. The resulting depth map Z is then back-projected into a 3D point cloud: a pixel (u, v) will be transformed to <ref type="bibr">(x, y, z)</ref> </p><formula xml:id="formula_0">in 3D by z = Z(u, v), x = (u ? c U ) ? z f U , y = (v ? c V ) ? z f V ,<label>(1)</label></formula><p>where (c U , c V ) is the camera center and f U and f V are the horizontal and vertical focal length. The 3D point cloud is then treated exactly as LiDAR signal -any LiDAR-based 3D detector can be applied seamlessly. By taking the state-of-the-art algorithms from both ends <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018;</ref><ref type="bibr">Ku et al., 2018;</ref><ref type="bibr" target="#b9">Qi et al., 2018)</ref>, pseudo-LiDAR obtains the highest image-based performance on the KITTI object detection benchmark <ref type="bibr">(Geiger et al., 2012;</ref><ref type="bibr">2013)</ref>. Our work builds upon this framework.</p><p>Stereo disparity estimation. Pseudo-LiDAR relies heavily on the quality of depth estimation. Essentially, if the estimated pixel depths match those provided by LiDAR, pseudo-LiDAR with any LiDAR-based detector should be able to achieve the same performance as that obtained by applying the same detector to the LiDAR signal. According to <ref type="bibr" target="#b19">(Wang et al., 2019a</ref>), depth estimation from stereo pairs of images <ref type="bibr">(Mayer et al., 2016;</ref><ref type="bibr" target="#b27">Yamaguchi et al., 2014;</ref><ref type="bibr" target="#b1">Chang &amp; Chen, 2018)</ref> are more accurate than that from monocular (i.e., single) images <ref type="bibr" target="#b18">(Fu et al., 2018;</ref><ref type="bibr">Godard et al., 2017)</ref> for 3D object detection. We therefore focus on stereo depth estimation, which is routinely obtained from estimating disparity between images.</p><p>A disparity estimation algorithm takes a pair of left-right images I l and I r as input, captured from a pair of cameras with a horizontal offset (i.e., baseline) b. Without loss of generality, we assume that the algorithm treats the left image, I l , as reference and outputs a disparity map D recording the horizontal disparity to I r for each pixel (u, v). Ideally, I l (u, v) and I r (u, v + D(u, v)) will picture the same 3D location. We can therefore derive the depth map Z via the following transform,</p><formula xml:id="formula_1">Z(u, v) = f U ? b D(u, v) (f U : horizontal focal length).<label>(2)</label></formula><p>A common pipeline of disparity estimation is to first construct a 4D disparity cost volume C disp , in which C disp <ref type="bibr">(u, v, d, :</ref>) is a feature vector that captures the pixel difference between I l (u, v) and I r (u, v + d). It then estimates the disparity D(u, v) for each pixel (u, v) according to the cost volume C disp . One basic algorithm is to build a 3D cost volume with C disp (u, v, d) = I l (u, v)?I r (u, v+d) 2 and determine D(u, v) as arg min d C disp <ref type="bibr">(u, v, d)</ref>. Advanced algorithms exploit more robust features in constructing C disp and perform structured prediction for D. In what follows, we give an introduction of PSMNet <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018)</ref>, a state-of-the-art algorithm used in <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>.</p><p>PSMNet begins with extracting deep feature maps h l and h r from I l and I r , respectively. It then constructs C disp <ref type="bibr">(u, v, d, :)</ref> by concatenating features of h l (u, v) and h r (u, v + d), followed by layers  of 3D convolutions. The resulting 3D tensor S disp , with the feature channel size ending up being one, is then used to derive the pixel disparity via the following weighted combination,</p><formula xml:id="formula_2">D(u, v) = d softmax(?S disp (u, v, d)) ? d,<label>(3)</label></formula><p>where softmax is performed along the 3 rd dimension of S disp . PSMNet can be learned end-to-end, including the image feature extractor and 3D convolution kernels, to minimize the disparity error</p><formula xml:id="formula_3">(u,v)?A (D(u, v) ? D (u, v)),<label>(4)</label></formula><p>where is the smooth L1 loss, D is the ground truth map, and A contains pixels with ground truths. A stereo network designed and learned to minimize the disparity error (cf. Equation 4) may over-emphasize nearby objects with smaller depths and therefore perform poorly in estimating depths for faraway objects. To see this, note that Equation 2 implies that for a given error in disparity ?D, the error in depth ?Z increases quadratically with depth:</p><formula xml:id="formula_4">Z ? 1 D ? ?Z ? 1 D 2 ?D ? ?Z ? Z 2 ?D.<label>(5)</label></formula><p>The middle term is obtained by differentiating Z(D) w.r.t. D. In particular, using the settings on the KITTI dataset <ref type="bibr">(Geiger et al., 2012;</ref><ref type="bibr">2013)</ref>, a single pixel error in disparity implies only a 0.1m error in depth at a depth of 5 meters, but a 5.8m error at a depth of 50 meters. See <ref type="figure" target="#fig_1">Figure 2</ref> for a mapping from disparity to depth.</p><p>Depth Loss. We propose two changes to adapt stereo networks for direct depth estimation. First, we learn stereo networks to directly optimize the depth loss</p><formula xml:id="formula_5">(u,v)?A (Z(u, v) ? Z (u, v)).<label>(6)</label></formula><p>Z and Z can be obtained from D and D using Equation 2. The change from the disparity loss to the depth loss corrects the disproportionally strong emphasis on tiny depth errors of nearby objectsa necessary but still insufficient change to overcome the problems of disparity estimation.</p><p>Depth Cost Volume. To facilitate accurate depth learning (rather than disparity) we need to further address the internals of the depth estimation pipeline. A crucial source of error is the 3D convolutions within the 4D disparity cost volume, where the same kernels are applied for the entire cost volume. This is highly problematic as it implicitly assumes that the effect of a convolution is homogeneous    throughout -which is clearly violated by the reciprocal depth to disparity relation ( <ref type="figure" target="#fig_1">Figure 2</ref>). For example, it may be completely appropriate to locally smooth two neighboring pixels with disparity 85 and 86 (changing the depth by a few cm to smooth out a surface), whereas applying the same kernel for two pixels with disparity 5 and 6 could easily move the 3D points by 10m or more.</p><formula xml:id="formula_6">E 0 Z w / k Q R c z F L d / / s R U = " &gt; A A A B 6 n i c b V D L S g N B E O z x G R M f M R 6 9 D E Y h p 7 A b D 3 q S g B e 9 R T Q P S J Y w O 5 l N h s z O L j O z Q l j i F + j F g x K 8 + k X e / B s n j 4 M m F j Q U V d 1 0 d / m x 4 N o 4 z j d a W 9 / Y 3 N r O 7 G R z u 3 v 7 B / n D Q k N H i a K s T i M R q Z Z P N B N c s r r h R r B W r B g J f c G a / v B 6 6 j c f m d I 8 k g 9 m F D M v J H 3 J A 0 6 J s d L 9 b V d 1 8 0 W n 7 M y A V 4 m 7 I M V q Y f L 8 V D r N 1 b r 5 r 0 4 v o k n I p K G C a N 1 2 n d h 4 K V G G U 8 H G 2 U 6 i W U z o k P R Z 2 1 J J Q q a 9 d H b q G J 9 Z p Y e D S N m S B s / U 3 x M p C b U e h b 7 t D I k Z 6 G V v K v 7 n t R M T X H o p l 3 F i m K T z R U E i s I n w 9 G / c 4 4 p R I 0 a W E K q 4 v R X T A V G E G p t O 1 o b g L r + 8 S h q V s</formula><formula xml:id="formula_7">V Q = " &gt; A A A B 6 n i c b V D L T g J B E O z F F 4 I P x K O X i W j C i e z i Q U + G x I t H j P J I Y E N m h 1 m Y M D u z m Z k 1 I R v 8 A r 1 4 0 B C v f p E 3 / 8 b h c V C w k k 4 q V d 3 p 7 g p i z r R x 3 W 8 n s 7 G 5 t b 2 T 3 c 3 l 9 / Y P D g t H x a a W i S K 0 Q S S X q h 1 g T T k T t G G Y 4 b Q d K 4 q j g N N W M L q Z + a 1 H q j S T 4 s G M Y + p H e C B Y y A g 2 V r o f 9 n i v U H I r 7 h x o n X h L U q o V p 8 9 P 5 b N 8 v V f 4 6 v Y l S S I q D O F Y 6 4 7 n x s Z P s T K M c D r J d R N N Y 0 x G e E A 7 l g o c U e 2 n 8 1 M n 6 N w q f R R K Z U s Y N F d / T 6 Q 4 0 n o c B b Y z w m a o V 7 2 Z + J / X S U x 4 5 a d M x I m h g i w W h Q l H R q L Z 3 6 j P F C W G j y 3 B R D F 7 K y J D r D A x N p 2 c D c F b f X m d N K s V 7 6 J S v b N p X M M C W T i B U y i D B 5 d Q g 1 u o Q w M I D O A F 3 u D d 4 c 6 r M 3 U + F q 0 Z Z z l z D H / g f P 4 A Z E a Q E w = = &lt; / l a t e x i t &gt; hr &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + h 3 H G U o m t K m M B q c b N G j X 0 u z F + H k = " &gt; A A A B 6 n i c b V D L T g J B E O z F F 4 I P x K O X i W j C i e z i Q U + G x I t H j P J I Y E N m h 1 m Y M D u z m Z k 1 I R v 8 A r 1 4 0 B C v f p E 3 / 8 b h c V C w k k 4 q V d 3 p 7 g p i z r R x 3 W 8 n s 7 G 5 t b 2 T 3 c 3 l 9 / Y P D g t H x a a W i S K 0 Q S S X q h 1 g T T k T t G G Y 4 b Q d K 4 q j g N N W M L q Z + a 1 H q j S T 4 s G M Y + p H e C B Y y A g 2 V r o f 9 l S v U H I r 7 h x o n X h L U q o V p 8 9 P 5 b N 8 v V f 4 6 v Y l S S I q D O F Y 6 4 7 n x s Z P s T K M c D r J d R N N Y 0 x G e E A 7 l g o c U e 2 n 8 1 M n 6 N w q f R R K Z U s Y N F d / T 6 Q 4 0 n o c B b Y z w m a o V 7 2 Z + J / X S U x 4 5 a d M x I m h g i w W h Q l H R q L Z 3 6 j P F C W G j y 3 B R D F 7 K y J D r D A x N p 2 c D c F b f X m d N K s V 7 6 J S v b N p X M M C W T i B U y i D B 5 d Q g 1 u o Q w M I D O A F 3 u D d 4 c 6 r M 3 U + F q 0 Z Z z l z D H / g f P 4 A b V 6 Q G Q = = &lt; / l a t e x i t &gt; Cdisp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 U T O j v A X M E J k s x / h V A k O 9 / W j h J o = " &gt; A A A B 9 H i c b V A 7 S w N B E N 7 z G e M r a m m z G g S r c B c L r S S Q x j K C e U B y h L 2 9 S b J k 7 + H u X D Q c K a y t x c Z C E V t / j J 3 / x s 2 j 0 M Q P B j 6 + b 4 a Z + b x Y C o 2 2 / W 0 t L a + s r q 1 n N r K b W 9 s 7 u 7 m 9 / Z q O E s W h y i M Z q Y b H N E g R Q h U F S m j E C l j g S a h 7 / f L Y r w 9 A a R G F N z i M w Q 1 Y N x Q d w R k</formula><p>Taking this insight and the central assumption of convolutions -all neighborhoods can be operated upon in an identical manner -into account, we propose to instead construct the depth cost volume C depth , in which C depth (u, v, z, :) will encode features describing how likely the depth Z(u, v) of pixel (u, v) is z. The subsequent 3D convolutions will then operate on the grid of depth, rather than disparity, affecting neighboring depths identically, independent of their location. The resulting 3D tensor S depth is then used to predict the pixel depth similar to Equation 3</p><formula xml:id="formula_8">Z(u, v) = z softmax(?S depth (u, v, z)) ? z.</formula><p>We construct the new depth volume, C depth , based on the intuition that C depth <ref type="bibr">(u, v, z, :)</ref> and</p><formula xml:id="formula_9">C disp u, v, f U ? b z ,</formula><p>: should lead to equivalent "cost". To this end, we apply a bilinear interpolation to construct C depth from C disp using the depth-to-disparity transform in Equation 2. Specifically, we consider disparity in the range of <ref type="bibr">[0,</ref><ref type="bibr">191]</ref> following PSMNet <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018)</ref>, and consider depth in the range of [1m, 80m] and set the grid of depth in C depth to be 1m. <ref type="figure" target="#fig_4">Figure 5</ref> (top) depicts our stereo depth network (SDN) pipeline. Crucially, all convolution operations are operated on C depth exclusively. <ref type="figure">Figure 4</ref> compares the median values of absolute depth estimation errors using the disparity cost volume (i.e., PSMNet) and the depth cost volume (SDN) (see subsection D.5 for detailed numbers). As expected, for faraway depth, SDN leads to drastically smaller errors with only marginal increases in the very near range (which disparity based methods over-optimize). See the appendix for the detailed setup and more discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEPTH CORRECTION</head><p>Our SDN significantly improves depth estimation and more precisely renders the object contours (see <ref type="figure" target="#fig_0">Figure 3</ref>). However, there is a fundamental limitation in stereo because of the discrete nature of pixels: the disparity, being the difference in the horizontal coordinate between corresponding pixels, has to be quantized at the level of individual pixels while the depth is continuous. Although the quantization error can be alleviated with higher resolution images, the computational depth prediction cost scales cubically with resolution-pushing the limits of GPUs in autonomous vehicles.</p><p>We therefore explore a hybrid approach by leveraging a cheap LiDAR with extremely sparse (e.g., 4 beams) but accurate depth measurements to correct this bias. We note that such sensors are too sparse to capture object shapes and cannot be used alone for detection. However, by projecting the LiDAR points into the image plane we obtain exact depths on a small portion of "landmark" pixels.</p><p>We present a graph-based depth correction (GDC) algorithm that effectively combines the dense stereo depth that has rendered object shapes and the sparse accurate LiDAR measurements. Conceptually, we expect the corrected depth map to have the following properties: globally, landmark pixels associated with LiDAR points should possess the exact depths; locally, object shapes captured by neighboring 3D points, back-projected from the input depth map (cf. Equation 1), should be preserved. <ref type="figure" target="#fig_4">Figure 5 (bottom)</ref> illustrates the algorithm.</p><p>Input Matching. We take as input the two point clouds from LiDAR (L) and Pseudo-LiDAR (PL) by stereo depth estimation. The latter is obtained by converting pixels (u, v) with depth z to 3D points (x u , y v , z). First, we characterize the local shapes by the directed K-nearest-neighbor (KNN) graph in the PL point cloud (using accelerated KD-Trees <ref type="bibr" target="#b13">(Shevtsov et al., 2007)</ref>) that connects each 3D point to its KNNs with appropriate weights. Similarly, we can project the 3D LiDAR points onto pixel locations (u, v) and match them to corresponding 3D stereo points. Without loss of generality, we assume that we are given "ground truth" LiDAR depth for the first n points and no ground truth for the remaining m points. We refer to the 3D stereo depth estimates as Z ? R n+m and the LiDAR depth ground-truth as G ? R n .</p><p>Edge weights. To construct the KNN graph in 3D we ignore the LiDAR information on the first n points and only use their predicted stereo depth in Z. Let N i denote the set of k neighbors of the i th point. Further, let W ? R (n+m)?(n+m) denote the weight matrix, where W ij denotes the edge-weight between points i and j. Inspired by prior work in manifold learning <ref type="bibr" target="#b12">(Roweis &amp; Saul, 2000;</ref><ref type="bibr" target="#b21">Weinberger et al., 2005)</ref> we choose the weights to be the coefficients that reconstruct the depth of any point from the depths of its neighbors in N i . We can solve for these weights with the following constrained quadratic optimization problem:</p><formula xml:id="formula_10">W = arg min W Z ? W Z 2 2 , s.t. W 1 = 1 and W ij = 0 if j / ? N i .<label>(7)</label></formula><p>Here 1 ? R n+m denotes the all-ones vector. As long as we pick k &gt; 3 and the points are in general position there are infinitely many solutions that satisfy Z = W Z, and we pick the solution with the minimum L 2 norm (obtained with slight L 2 regularization).</p><p>Depth Correction. Let us denote the corrected depth values as Z ? R n+m , with Z = [Z L ; Z P L ] and Z L ? R n and Z P L ? R m , where Z L are the depth values of points with LiDAR ground-truth and Z P L otherwise. For the n points with LiDAR measurements we update the depth to the (ground truth) values Z L = G. We then solve for Z P L given G and the weighted KNN graph encoded in W . Concretely, we update the remaining depths Z P L such that the depth of any point i can still be be reconstructed with high fidelity as a weighted sum of its KNNs' depths using the learned weights W ; i.e. if point i : 1 ? i ? n is moved to its new depth G i , then its neighbors in N i must also be corrected such that G i ? j?Ni W ij Z j . Further, the neighbors' neighbors must be corrected and the depth of the few n points propagates across the entire graph. We can solve for the final Z directly with another quadratic optimization:</p><formula xml:id="formula_11">Z = arg min Z Z ? W Z 2 , s.t. Z 1:n = G.<label>(8)</label></formula><p>To illustrate the correction process, imagine the simplest case where the depth of only a single point (n = 1) is updated to G 1 = Z 1 + ?. A new optimal depth for Equation 8 is to move all the remaining points similarly, i.e. Z = Z + 1?: as Z = W Z and W 1 = 1 we must have W (Z + 1?) = Z + 1?.</p><p>In the setting with n &gt; 1, the least-squares loss ensures a soft diffusion between the different LiDAR depth estimates. Both optimization problems in Equation 7 and Equation 8 can be solved exactly and efficiently with sparse matrix solvers. We summarize the procedure as an algorithm in the appendix.</p><p>From the view of graph-based manifold learning, our GDC algorithm is reminiscent of locally linear embeddings <ref type="bibr" target="#b12">(Roweis &amp; Saul, 2000)</ref> with landmarks to guide the final solution <ref type="bibr" target="#b21">(Weinberger et al., 2005)</ref>. <ref type="figure">Figure 1</ref> illustrates vividly how the initial 3D point cloud from SDN (purple) of a car in the KITTI dataset is corrected with a few sparse LiDAR measurements (yellow). The resulting points (blue) are right inside the ground-truth box and clearly show the contour of the car. <ref type="figure">Figure 4</ref> shows the additional improvement from the GDC (blue) over the pure SDN depth estimates (see subsection D.5 for detailed numbers). The error (calculated only on non-landmark pixels) is corrected over the entire image where many regions have no LiDAR measurements. This is because that the pseudo-LiDAR point cloud is sufficiently dense and we choose k to be large enough (in practice, we use k = 10) such that the KNN graph is typically connected (or consists of few large connected components). See subsection D.6 for more analysis. For objects such as cars the improvements through GDC are far more pronounced, as these typically are touched by the four LiDAR beams and can be corrected effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SETUP</head><p>We refer to our combined method (SDN and GDC) for 3D object detection as PSEUDO-LIDAR++ (PL++ in short). To analyze the contribution of each component, we evaluate SDN and GDC independently and jointly across several settings. For GDC we set k = 10 and consider adding signal from a (simulated) 4-beam LiDAR, unless stated otherwise.</p><p>Dataset, Metrics, and Baselines. We evaluate on the KITTI dataset <ref type="bibr">(Geiger et al., 2013;</ref><ref type="bibr">2012)</ref>, which contains 7,481 and 7,518 images for training and testing. We follow <ref type="bibr" target="#b2">(Chen et al., 2015)</ref> to separate the 7,481 images into 3,712 for training and 3,769 validation. For each (left) image, KITTI provides the corresponding right image, the 64-beam Velodyne LiDAR point cloud, the camera calibration matrices, and the bounding boxes. We focus on 3D object detection and bird's-eye-view (BEV) localization and report results on the validation set. Specifically, we focus on the "car" category, following <ref type="bibr" target="#b4">Chen et al. (2017)</ref> and . We report average precision (AP) with IoU (Intersection over Union) thresholds at 0.5 and 0.7. We denote AP for the 3D and BEV tasks by AP 3D and AP BEV . KITTI defines the easy, moderate, and hard settings, in which objects with 2D box heights smaller than or occlusion/truncation levels larger than certain thresholds are disregarded. We compare to four stereo-based detectors: PSEUDO-LIDAR (PL in short) <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>, 3DOP <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>, S- <ref type="bibr">RCNN (Li et al., 2019b)</ref>, and MLF-STEREO <ref type="bibr" target="#b25">(Xu &amp; Chen, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo depth network (SDN).</head><p>We use PSMNET <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018)</ref> as the backbone for our stereo depth estimation network (SDN). We follow <ref type="bibr" target="#b19">Wang et al. (2019a)</ref> to pre-train SDN on the synthetic Scene Flow dataset <ref type="bibr">(Mayer et al., 2016)</ref> and fine-tune it on the 3,712 training images of KITTI. We obtain the depth ground truth by projecting the corresponding LiDAR points onto images. We also train a PSMNET in the same way for comparison, which minimizes disparity error.</p><p>3D object detection. We apply three algorithms: AVOD <ref type="bibr">(Ku et al., 2018)</ref>, PIXOR <ref type="bibr" target="#b30">(Yang et al., 2018b)</ref>, and P-RCNN <ref type="bibr" target="#b14">(Shi et al., 2019)</ref>. All utilize information from LiDAR and/or monocular images. We use the released implementations of AVOD ( specifically, AVOD-FPN) and P-RCNN. We implement PIXOR ourselves with a slight modification to include visual information (denoted    as PIXOR ). We train all models on the 3,712 training data from scratch by replacing the LiDAR points with pseudo-LiDAR data generated from stereo depth estimation. See the appendix for details.</p><p>Sparser LiDAR. We simulate sparser LiDAR signal with fewer beams by first projecting the 64-beam LiDAR points onto a 2D plane of horizontal and vertical angles. We quantize the vertical angles into 64 levels with an interval of 0.4 ? , which is close to the SPEC of the 64-beam LiDAR. We keep points fallen into a subset of beams to mimic the sparser signal. See the appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPERIMENTAL RESULTS</head><p>Results on the KITTI val set. We summarize the main results on KITTI object detection in <ref type="table" target="#tab_2">Table 1</ref>. Several important trends can be observed: 1) Our PL++ with enhanced depth estimations by SDN and GDC yields consistent improvement over PL across all settings; 2) PL++ with GDC refinement of 4-beam LiDAR (Input: L# + S) performs significantly better than PL++ with only stereo inputs (Input: S); 3) PL experiences a substantial drop in accuracy from IoU at 0.5 to 0.7 for the hard setting. This suggests that while PL detects faraway objects, it mislocalizes them, likely placing them at the wrong depth. This causes the object to be considered a missed detection at higher overlap thresholds. Interestingly, here is where we experience the largest gain -from PL: P-RCNN (AP BEV = 52.7) to PL++: P-RCNN (AP BEV = 73.4) with input as L# + S. Note that the majority of the gain comes from GDC, as PL++ with the stereo-only version only improving the score to 57.3 AP BEV . 4) The gap between PL++ and LiDAR is at most 13% AP BEV , even at the hard setting under IoU at 0.7. 5) For IoU at 0.5, with the aid of only 4 LiDAR beams, PL++ (SDN + GDC) achieves results comparable to models with 64-beam LiDAR signals.</p><p>Results on the KITTI test set. <ref type="table" target="#tab_3">Table 2</ref> summarizes results on the car category on the KITTI test set. We see a similar gap between our methods and LiDAR as on the validation set, suggesting that our improvement is not particular to the validation data. Our approach without LiDAR refinement (pure SDN) is placed at the top position among all the image-based algorithms on the KITTI leaderboard.</p><p>In the following, we conduct a series of experiments to analyze the performance gain by our approaches and discuss several key observations. We mainly experiment with P-RCNN: we find that the results with AVOD and PIXOR follow similar trends and thus include them in the appendix.</p><p>Depth loss and depth cost volume. To turn a disparity network (e.g., PSMNET) into SDN, there are two changes: 1) change the disparity loss into the depth loss; 2) change the disparity cost volume into the depth cost volume. In <ref type="table" target="#tab_4">Table 3</ref>, we uncover the effect of these two changes separately. On the AP BEV /AP 3D (moderate) metric, the depth loss gives us a 6%/2% improvement and the depth cost volume brings another 2 ? 3% gain 4 . Impact of sparse LiDAR beams. We leverage 4-beam LiDAR to correct stereo depth using GDC. However, it is possible that gains in 3D object detection come entirely from the new LiDAR sensor and that the stereo estimates are immaterial. In <ref type="table" target="#tab_5">Table 4</ref>, we study this question by comparing the detection results against those of models using 1) sole 4-beam LiDAR point clouds and 2) pseudo-LiDAR point clouds with depths of landmark pixels replaced by 4-beam LiDAR: i.e., in depth correction, we only correct depths of the landmark pixels without propagation. It can be seen that 4-beam LiDAR itself performs fairly well on locating faraway objects but cannot capture nearby objects precisely, while simply replacing pseudo-LiDAR with LiDAR at the landmark pixels prevents the model from detecting faraway object accurately. In contrast, our proposed GDC method effectively combines the merits of the two signals, achieving superior performance than using them alone.</p><p>Pedestrian and cyclist detection. For a fair comparison to <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>, we apply F-POINTNET <ref type="bibr" target="#b9">(Qi et al., 2018)</ref> for detecting pedestrians and cyclists. <ref type="table" target="#tab_6">Table 5</ref> shows the results: our methods significantly boosts the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiDAR</head><p>Pseudo-LiDAR Pseudo-LiDAR++ (SDN) Pseudo-LiDAR++ (SDN + GDC) <ref type="figure">Figure 6</ref>: Qualitative Comparison. We show the detection results on a KITTI validation scene by P-RCNN with different input point clouds. We visualize them from both frontal-view images and bird's-eye view (BEV) point maps. Ground-truth boxes are in green and predicted bounding boxes are in red. The observer is at the left-hand side of the BEV map looking to the right. In other words, ground truth boxes on the right are more faraway (i.e., deeper) from the observer, and hence hard to localize. Best viewed in color.</p><p>Qualitative visualization. In <ref type="figure">Figure 6</ref>, we show an qualitative comparison of detection results on a randomly chosen scene in the KITTI object validation set, using P-RCNN (with confidence &gt; 0.95) with different input signals. Specifically, we show the results from the frontal-view images and the bird's-eye view (BEV) point clouds. In the BEV map, the observer is on the left-hand side looking to the right. It can be seen that the point clouds generated by PSEUDO-LIDAR ++ (SDN alone or SDN than the depth loss (for PIXOR and AVOD). Nevertheless, <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_7">Table 6</ref> both suggest the compatibility of the two approaches: combining them leads to the best performance. +GDC) align better with LiDAR than that generated by PSEUDO-LIDAR (PSMNET). For nearby objects (i.e., bounding boxes close to the left in the BEV map), we see that P-RCNN with any point cloud performs fairly well in localization. However, for faraway objects (i.e., bounding boxes close to the right), PSEUDO-LIDAR with depth estimated from PSMNET predicts objects (red boxes) that are deviated from the ground truths (green boxes). Moreover, the noisy PSMNET points also leads to false negatives. In contrast, the detected boxes by our PSEUDO-LIDAR ++, either with SDN alone or with SDN +GDC, align pretty well with the ground truth boxes, justifying our targeted improvement in estimating faraway depths.</p><p>Additional results, analyses, qualitative visualization and discussions. We provide results of PSEUDO-LIDAR ++ with fewer LiDAR beams, comparisons to depth completion methods, analysis on depth quality and detection accuracy, run time, failure cases, and more qualitative results in the appendix. With simple optimizations, GDC runs in 90 ms/frame using a single GPU (7.7 ms for KD-tree construction and search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper we made two contributions to improve the 3D object detection in autonomous vehicles without expensive LiDAR. First, we identify the disparity estimation as a main source of error for stereo-based systems and propose a novel approach to learn depth directly end-to-end instead of through disparity estimates. Second, we advocate that one should not use expensive LiDAR sensors to learn the local structure and depth of objects. Instead one can use commodity stereo cameras for the former and a cheap sparse LiDAR to correct the systematic bias in the resulting depth estimates. We provide a novel graph propagation algorithm that integrates the two data modalities and propagates the sparse yet accurate depth estimates using two sparse matrix solvers. The resulting system, PSEUDO-LIDAR ++ (SDN + GDC), performs almost on par with 64-beam LiDAR systems for $75,000 but only requires 4 beams and two commodity cameras, which could be obtained with a total cost of less than $1,000.</p><p>scene (in LiDAR coordinate system (x: front, y: left, z: up, and (0, 0, 0) is the location of the LiDAR sensor)), we compute the elevation angle ? i to the LiDAR sensor as</p><formula xml:id="formula_12">? i = arg cos x 2 i + y 2 i x 2 i + y 2 i + z 2 i .</formula><p>We order the points by their elevation angles and slice them into separate lines by step 0.4 ? , starting from ?23.6 ? (close to the Velodyne 64-beam LiDAR SPEC). We select LiDAR points whose elevation angles fall within</p><formula xml:id="formula_13">[?2.4 ? , ?2.0 ? ) ? [?0.8 ? , ?0.4 ? ) to be the 2-beam LiDAR signal, and similarly [?2.4 ? , ?2.0 ? ) ? [?1.6 ? , ?1.2 ? ) ? [?0.8 ? , ?0.4 ? ) ? [0.0 ? , 0.4 ? )</formula><p>to be the 4-beam LiDAR signal. We choose them in such a way that consecutive lines has a 0.8 ? interval, following the SPEC of the "cheap" 4-beam LiDAR ScaLa. We visualize these sparsified LiDAR point clouds from the bird's-eye view on one example scene in <ref type="figure" target="#fig_5">Figure 7</ref>. In this section, we provide more details about the way we train 3D object detection models on pseudo-LiDAR point clouds. For AVOD, we use the same model as in <ref type="bibr" target="#b19">(Wang et al., 2019a)</ref>. For P-RCNN, we use the implementation provided by the authors. Since the P-RCNN model exploits the sparse nature of LiDAR point clouds, when training it with pseudo-LiDAR input, we will first sparsify the point clouds into 64 beams using the method described in subsection C.1. For PIXOR , we implement the same base model structure and data augmentation specified by <ref type="bibr" target="#b30">Yang et al. (2018b)</ref>, but without the "decode fine-tune" step and focal loss. Inspired by the trick in <ref type="bibr">(Liang et al., 2018)</ref>, we add another image feature <ref type="bibr">(ResNet-18 by He et al. (2016)</ref>) branch along the LiDAR branch, and concatenate the corresponding image features onto the LiDAR branch at each stage. We train PIXOR using RMSProp with momentum 0.9, learning rate 10 ?5 (decay by 10 after 50 and 80 epochs) for 90 epochs. The BEV evaluation results are similar to the reported results (see <ref type="table" target="#tab_2">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS, ANALYSES, AND DISCUSSIONS D.1 ABLATION STUDY</head><p>In <ref type="table" target="#tab_7">Table 6</ref> and <ref type="table" target="#tab_8">Table 7</ref> we provide more experimental results aligned with experiments in subsection 5.2 of the main paper. We conduct the same experiments on two other models, AVOD and PIXOR , and observe similar trends of improvements brought by learning with the depth loss (from PSMNET to PSMNET +DL), constructing the depth cost volume (from PSMNET +DL to SDN), and applying GDC to correct the bias in stereo depth estimation (comparing SDN +GDC with SDN).</p><p>We note that, in <ref type="table" target="#tab_8">Table 7</ref>, results of AVOD (or PIXOR ) with SDN + L# are worse than those with L# at the moderate and hard settings. This observation is different from that in <ref type="table" target="#tab_5">Table 4</ref>, where P-RCNN with SDN + L# outperforms P-RCNN with L# in 5 out of 6 comparisons. We hypothesize that this is because P-RCNN takes sparsified inputs (see subsection C.2) while AVOD and PIXOR take dense inputs. In the later case, the four replaced LiDAR beams in SDN + L# will be dominated by the dense stereo depths so that SDN + L# is worse than L#.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 USING FEWER LIDAR BEAMS</head><p>In PL++ (i.e., SDN + GDC), we use 4-beam LiDAR to correct the predicted point cloud. In <ref type="table" target="#tab_9">Table 8</ref>, we investigate using fewer (and also potentially cheaper) LiDAR beams for depth correction. We observe that even with 2 beams, GDC can already manage to combine the two signals and yield a better performance than using 2-beam LiDAR or pseudo-LiDAR alone.    We compare our GDC algorithm for depth correction to depth completion algorithms, which aim to "densify" LiDAR data beyond the beam lines <ref type="bibr" target="#b16">Tomasello et al., 2018;</ref><ref type="bibr">Ma et al., 2019;</ref><ref type="bibr" target="#b31">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Cheng et al., 2018;</ref><ref type="bibr" target="#b17">Torres-Mendez &amp; Dudek, 2004)</ref>  <ref type="bibr">6</ref> . We note that most depth completion approaches take as input a 64-beam LiDAR and a single image, while our focus is on fusing a much sparser 4-beam LiDAR and stereo depths. As such, the two problems are not   commensurate. Also, our GDC algorithm is a general, simple, inference-time approach that requires no training, unlike prior learning-based approaches to depth completion.</p><p>Here we empirically compare to PNP , a recently proposed depth completion algorithm compatible with any (even stereo) depth estimation network, similar to GDC. We use SDN for initial depth estimation, and evaluate GDC and PNP by randomly selecting a fraction of LiDAR points as provided ground truths and calculating the median absolute depth errors on the remaining LiDAR points. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, GDC outperforms PNP by a large margin. <ref type="table" target="#tab_10">Table 9</ref> shows a further comparison to PNP on 3D object detection. We apply PNP and GDC respectively to correct the depth estimates obtained from SDN, train a P-RCNN or PIXOR using the resulting pseudo-LiDAR points on the KITTI training set, and compare the detection results on the KITTI validation set. In either case, SDN + GDC outperforms SDN + PNP by a notable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 RUN TIME</head><p>With the following optimizations for implementation,  For both tables, we divide pixels into beams according to their truth depths, and evaluate on pixels not on the 4-beam LiDAR. The improvement of SDN (+ GDC) over PSMNET becomes larger as we consider pixels farther away. <ref type="table" target="#tab_2">Table 13</ref> further demonstrates the relationship between depth quality and detection accuracy: SDN (+ GDC) significantly outperforms PSMNET for detecting faraway cars. We note that, for very faraway cars (i.e., 50-70 m), the number of training object instances are extremely small, which suggests that the very poor performance might partially cause by over-fitting.</p><p>Further, we apply the same evaluation procedure but group the errors by the shortest distance between each PSEUDO-LIDAR point and the 4-beam LiDAR points in <ref type="figure" target="#fig_7">Figure 9</ref>. We can see that the closer the PSEUDO-LIDAR points are to the 4-beam LiDAR points, the bigger improvement GDC can bring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 CONNECTED COMPONENTS IN KNN GRAPHS OF PSEUDO-LIDAR POINTS BY SDN</head><p>Here, we provide empirical analysis on the relationship between the k we choose in building the Knearest-neighbor graph of PSEUDO-LIDAR points by SDN and the number of connected components of that graph. We show the results on KITTI validation set in <ref type="figure" target="#fig_9">Figure 11</ref>. It can be seen that with k ? 9, the average number of connected components in the graph is smaller than 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 FAILURE CASES AND WEAKNESS</head><p>There is still a gap between our approach and LiDAR for faraway objects (see <ref type="table" target="#tab_2">Table 13</ref>). We further analyze AP BEV at different IoU in <ref type="figure" target="#fig_8">Figure 10</ref>. For low IoU (0.2-0.5), SDN (+GDC) is on par with LiDAR, but the gap increases significantly at high IoU thresholds. This suggests that the predominant gap between our approach and LiDAR is because of mislocalization, perhaps due to residual inaccuracies in depth.    <ref type="table" target="#tab_2">13</ref>: 3D object detection at various depth ranges. We compare different input signals. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set, using P-RCNN for detection. In the last two rows we show the number of car objects in KITTI object train and validation sets within different ranges. In <ref type="figure">Figure 6</ref>,12,13 and <ref type="figure">Figure 14</ref>, we show detection results using P-RCNN (with confidence &gt; 0.95) with different input signals on four randomly chosen scenes in the KITTI object validation set. Specifically, we show the results from the frontal-view images and the bird's-eye view (BEV) point clouds. In the BEV map, the observer is on the left-hand side looking to the right. It can be seen that the point clouds generated by PSEUDO-LIDAR ++ (SDN alone or SDN +GDC) align better with LiDAR than those generated by PSEUDO-LIDAR (PSMNET). For nearby objects (i.e., bounding boxes close to the left in the BEV map), we see that P-RCNN with any point cloud performs fairly well in localization. However, for faraway objects (i.e., bounding boxes close to the right), PSEUDO-LIDAR with depth estimated from PSMNET predicts objects (red boxes) deviated from the ground truths (green boxes). Moreover, the noisy PSMNET points also leads to several false positives or negatives. In contrast, the detected boxes by our PSEUDO-LIDAR ++, either with SDN alone or with SDN +GDC, align pretty well with the ground truth boxes, justifying our targeted improvement in estimating faraway depths. In <ref type="figure" target="#fig_1">Figure 12</ref>, we see one failure case for both PSEUDO-LIDAR and PSEUDO-LIDAR ++: the most faraway car is missed, while LiDAR signal can still detect it, suggesting that for very faraway objects stereo-based methods may still have limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiDAR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-LiDAR</head><p>Pseudo-LiDAR++ (SDN) Pseudo-LiDAR++ (SDN + GDC) <ref type="figure" target="#fig_1">Figure 12</ref>: Qualitative Comparison. We show the detection results on a KITTI validation scene by P-RCNN with different input point clouds. We visualize them from both frontal-view images and bird's-eye view (BEV) point maps. Ground-truth boxes are in green and predicted bounding boxes are in red. The observer is at the left-hand side of the BEV map looking to the right. In other words, ground truth boxes on the right are more faraway (i.e., deeper) from the observer, and hence hard to localize. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Disparity cost volume (left) vs. depth cost volume (right). The figure shows the 3D points obtained from LiDAR (yellow) and stereo (purple) corresponding to a car in KITTI, seen from the bird'seye view (BEV). Points from the disparity cost volume are stretched out and noisy; while points from the depth cost volume capture the car contour faithfully.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The disparity-todepth transform. We set fU = 721 (in pixels) and b = 0.54 (in meters) in Equation 2, which are the typical values used in the KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " a c 3 5 s P p n o b X h L F I 3 v I X t M m + 1 I n U = " &gt; A A A B 6 n i c b V D L S g N B E O z x G R M f M R 6 9 D E Y h p 7 A b D 3 q S g B e 9 R T Q P S J Y w O 5 l N h s z O L j O z Q l j i F + j F g x K 8 + k X e / B s n j 4 M m F j Q U V d 1 0 d / m x 4 N o 4 z j d a W 9 / Y 3 N r O 7 G R z u 3 v 7 B / n D Q k N H i a K s T i M R q Z Z P N B N c s r r h R r B W r B g J f c G a / v B 6 6 j c f m d I 8 k g 9 m F D M v J H 3 J A 0 6 J s d L 9 b V d 0 8 0 W n 7 M y A V 4 m 7 I M V q Y f L 8 V D r N 1 b r 5 r 0 4 v o k n I p K G C a N 1 2 n d h 4 K V G G U 8 H G 2 U 6 i W U z o k P R Z 2 1 J J Q q a 9 d H b q G J 9 Z p Y e D S N m S B s / U 3 x M p C b U e h b 7 t D I k Z 6 G V v K v 7 n t R M T X H o p l 3 F i m K T z R U E i s I n w 9 G / c 4 4 p R I 0 a W E K q 4 v R X T A V G E G p t O 1 o b g L r + 8 S h q V s n t e r t z Z N K 5 g j g w c w w m U w I U L q M I N 1 K A O F P r w A m / w j g R 6 R R P 0 M W 9 d Q 4 u Z I / g D 9 P k D N Q y P 9 A = = &lt; / l a t e x i t &gt; Right Image Ir &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e m B O e 2 6 k l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>n t e r t z Z N K 5 g j g w c w w m U w I U L q M I N 1 K A O F P r w A m / w j g R 6 R R P 0 M W 9 d Q 4 u Z I / g D 9 P k D P i S P + g = = &lt; / l a t e x i t &gt; hl &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N d 9 w G q A n y D k p E B h 0 V A a 7 g M / R 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>a y S 2 3 W w j 3 m P p C x 6 N 2 L m 8 X 7 A n o I n F m J F 8 6 u p O P 5 a e H S j v 3 1 f I j n g Q Q I p d M 6 6 Z j x + i m T K H g E k b Z V q I h Z r z P u t A 0 N G Q B a D e d H D 2 i J 0 b x a S d S p k K k E / X 3 R M o C r Y e B Z z o D h j 0 9 7 4 3 F / 7 x m g p 0 L N x V h n C C E f L q o k 0 i K E R 0 n Q H 2 h g K M c G s K 4 E u Z W y n t M M Y 4 m p 6 w J w Z l / e Z H U i g X n r F C 8 N m l c k i k y 5 J A c k 1 P i k H N S I l e k Q q q E k 1 v y T F 7 J m z W w X q x 3 6 2 P a u m T N Z g 7 I H 1 i f P 3 2 H l Y s = &lt; / l a t e x i t &gt; Cdepth &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G k K r 4 l O i s y H R n K H 9 2 X r P a m M 9 3 W k = " &gt; A A A B 9 X i c b V A 9 S w N B E N 3 z M 8 a v q K X N a R C s w l 0 s t J J A G s s I 5 g O S M + z t T Z I l e 3 v H 7 p w x H C m s r Q U b C 0 V s / S 9 2 / h s 3 H 4 U m P h h 4 v D f D z D w / F l y j 4 3 x b S 8 s r q 2 v r m Y 3 s 5 t b 2 z m 5 u b 7 + m o 0 Q x q L J I R K r h U w 2 C S 6 g i R w G N W A E N f Q F 1 v 1 8 e + / U 7 U J p H 8 g a H M X g h 7 U r e 4 Y y i k W 7 L 7 R b C P a Y B x N g b t X N 5 p + B M Y C 8 S d 0 b y p a O B e C w / P V T a u a 9 W E L E k B I l M U K 2 b r h O j l 1 K F n A k Y Z V u J h p i y P u 1 C 0 1 B J Q 9 B e O r l 6 Z J 8 Y J b A 7 k T I l 0 Z 6 o v y d S G m o 9 D H 3 T G V L s 6 X l v L P 7 n N R P s X H g p l 3 G C I N l 0 U S c R N k b 2 O A I 7 4 A o Y i q E h l C l u b r V Z j y r K 0 A S V N S G 4 8 y 8 v k l q x 4 J 4 V i t c m j U s y R Y Y c k m N y S l x y T k r k i l R I l T C i y D N 5 J W / W w H q x 3 q 2 P a e u S N Z s 5 I H 9 g f f 4 A Q O u V + g = = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O s 4 s B q y R M Z j o l 0 W Z Q q J O U R H 2 0 t 0 = " &gt; A A A B 6 H i c d V D L S g N B E J y N r x h f U Y 9 e B o P g a d m N h y Q n A 3 r w m I B 5 Y L K E 2 U k n G T M 7 u 8 z M C m H J F 3 j x o E g 8 + j d e v f k 3 z i Y K P g s a i q p u u r r 9 i D O l H e f N y i w t r 6 y u Z d d z G 5 t b 2 z v 5 3 b 2 m C m N J o U F D H s q 2 T x R w J q C h m e b Q j i S Q w O f Q 8 s d n q d + 6 A a l Y K C 7 1 J A I v I E P B B o w S b a T 6 V S 9 f c O z K S c U t l / B v 4 t r O H I X T l 1 m K p 1 o v / 9 r t h z Q O Q G j K i V I d 1 4 m 0 l x C p G e U w z X V j B R G h Y z K E j q G C B K C 8 Z B 5 0 i o + M 0 s e D U J o S G s / V r x M J C Z S a B L 7 p D I g e q Z 9 e K v 7 l d W I 9 K H s J E 1 G s Q d D F o k H M s Q 5 x e j X u M w l U 8 4 k h h E p m s m I 6 I p J Q b X 6 T M 0 / 4 v B T / T 5 p F 2 z 2 x i 3 W n U D 1 H C 2 T R A T p E x 8 h F J V R F F 6 i G G o g i Q L f o H j 1 Y 1 9 a d 9 W j N F q 0 Z 6 2 N m H 3 2 D 9 f w O k a 2 R 7 g = = &lt; / l a t e x i t &gt; The whole pipeline of improved stereo depth estimation: (top) the stereo depth network (SDN) constructs a depth cost volume from left-right images and is optimized for direct depth estimation; (bottom) the graph-based depth correction algorithm (GDC) refines the depth map by leveraging sparser LiDAR signal. The gray arrows indicates the observer's view point. We superimpose the (green) ground-truth 3D box of a car, the same one in Figure 1. The corrected points (blue; bottom right) are perfectly located inside the ground truth box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Bird's-eye views of sparsified LiDAR on an example scene. The observer is on the bottom side looking up. We filter out points invisible from the left image. (One floor square is 10m ? 10m.) C.2 3D OBJECT DETECTION ALGORITHMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of GDC and PNP for depth correction. We report the median of absolute errors on the KITTI validation set. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Median depth estimation errors w.r.t. the shortest distances to 4-beam LiDAR points on KITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>IoU vs. AP BEV on KITTI validation set on the car category (moderate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>k vs. average number of connected components in KNN graphs of PSEUDO-LIDAR points by SDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>3D object detection results on KITTI validation. We report APBEV / AP3D (in %) of the car category, corresponding to average precision of the bird's-eye view and 3D object detection. We arrange methods according to the input signals: M for monocular images, S for stereo images, L for 64-beam LiDAR, and L# for sparse 4-beam LiDAR. PL stands for PSEUDO-LIDAR. Our PSEUDO-LIDAR ++ (PL++) with enhanced depth estimation -SDN and GDC-are in blue. Methods with 64-beam LiDAR are in gray. Best viewed in color.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>IoU = 0.5</cell><cell></cell><cell></cell><cell>IoU = 0.7</cell><cell></cell></row><row><cell cols="2">Detection algo Input</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>3DOP</cell><cell>S</cell><cell cols="4">55.0 / 46.0 41.3 / 34.6 34.6 / 30.1 12.6 / 6.6</cell><cell>9.5 / 5.1</cell><cell>7.6 / 4.1</cell></row><row><cell>MLF-STEREO</cell><cell>S</cell><cell>-</cell><cell>53.7 / 47.4</cell><cell>-</cell><cell>-</cell><cell>19.5 / 9.8</cell><cell>-</cell></row><row><cell>S-RCNN</cell><cell>S</cell><cell cols="6">87.1 / 85.8 74.1 / 66.3 58.9 / 57.2 68.5 / 54.1 48.3 / 36.7 41.5 / 31.1</cell></row><row><cell>PL: AVOD</cell><cell>S</cell><cell cols="6">89.0 / 88.5 77.5 / 76.4 68.7 / 61.2 74.9 / 61.9 56.8 / 45.3 49.0 / 39.0</cell></row><row><cell>PL: PIXOR</cell><cell>S</cell><cell>89.0 / -</cell><cell>75.2 / -</cell><cell>67.3 / -</cell><cell>73.9 / -</cell><cell>54.0 / -</cell><cell>46.9 / -</cell></row><row><cell>PL: P-RCNN</cell><cell>S</cell><cell cols="6">88.4 / 88.0 76.6 / 73.7 69.0 / 67.8 73.4 / 62.3 56.0 / 44.9 52.7 / 41.6</cell></row><row><cell>PL++: AVOD</cell><cell>S</cell><cell cols="6">89.4 / 89.0 79.0 / 77.8 70.1 / 69.1 77.0 / 63.2 63.7 / 46.8 56.0 / 39.8</cell></row><row><cell>PL++: PIXOR</cell><cell>S</cell><cell>89.9 / -</cell><cell>78.4 / -</cell><cell>74.7 / -</cell><cell>79.7 / -</cell><cell>61.1 / -</cell><cell>54.5 / -</cell></row><row><cell>PL++: P-RCNN</cell><cell>S</cell><cell cols="6">89.8 / 89.7 83.8 / 78.6 77.5 / 75.1 82.0 / 67.9 64.0 / 50.1 57.3 / 45.3</cell></row><row><cell>PL++: AVOD</cell><cell cols="7">L# + S 90.2 / 90.1 87.7 / 86.9 79.8 / 79.2 86.8 / 70.7 76.6 / 56.2 68.7 / 53.4</cell></row><row><cell cols="3">PL++: PIXOR L# + S 95.1 / -</cell><cell>85.1 / -</cell><cell>78.3 / -</cell><cell>84.0 / -</cell><cell>71.0 / -</cell><cell>65.2 / -</cell></row><row><cell cols="8">PL++: P-RCNN L# + S 90.3 / 90.3 87.7 / 86.9 84.6 / 84.2 88.2 / 75.1 76.9 / 63.8 73.4 / 57.4</cell></row><row><cell>AVOD</cell><cell cols="7">L + M 90.5 / 90.5 89.4 / 89.2 88.5 / 88.2 89.4 / 82.8 86.5 / 73.5 79.3 / 67.1</cell></row><row><cell>PIXOR</cell><cell cols="2">L + M 94.2 / -</cell><cell>86.7 / -</cell><cell>86.1 / -</cell><cell>85.2 / -</cell><cell>81.2 / -</cell><cell>76.1 / -</cell></row><row><cell>P-RCNN</cell><cell>L</cell><cell cols="6">97.3 / 97.3 89.9 / 89.8 89.4 / 89.3 90.2 / 89.2 87.9 / 78.9 85.5 / 77.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the car category on the test set. We compare PL++ (blue) and 64-beam LiDAR (gray), using P-RCNN, and report APBEV / AP3D at IoU=0.7.</figDesc><table><row><cell>Input signal</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>PL++ (SDN)</cell><cell cols="3">75.5 / 60.4 57.2 / 44.6 53.4 / 38.5</cell></row><row><cell cols="4">PL++ (SDN + GDC) 83.8 / 68.5 73.5 / 54.7 66.5 / 51.2</cell></row><row><cell>LiDAR</cell><cell cols="3">89.5 / 85.9 85.7 / 75.8 79.1 / 68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on depth estimation. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on KITTI validation. DL: depth loss.</figDesc><table><row><cell>Stereo depth</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>PSMNET</cell><cell cols="3">73.4 / 62.3 56.0 / 44.9 52.7 / 41.6</cell></row><row><cell cols="4">PSMNET + DL 80.1 / 65.5 61.9 / 46.8 56.0 / 43.0</cell></row><row><cell>SDN</cell><cell cols="3">82.0 / 67.9 64.0 / 50.1 57.3 / 45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on leveraging sparse Li-DAR.</figDesc><table><row><cell cols="4">We report APBEV / AP3D (in %) of the car cate-</cell></row><row><cell cols="4">gory at IoU= 0.7 on KITTI validation. L#: 4-beam</cell></row><row><cell cols="4">LiDAR signal alone. SDN + L#: pseudo-LiDAR with</cell></row><row><cell cols="4">depths of landmark pixels replaced by 4-beam LiDAR.</cell></row><row><cell cols="4">The best result of each column is in bold font.</cell></row><row><cell>Stereo depth</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>SDN</cell><cell cols="3">82.0 / 67.9 64.0 / 50.1 57.3 / 45.3</cell></row><row><cell>L#</cell><cell cols="3">73.2 / 56.1 71.3 / 53.1 70.5 / 51.5</cell></row><row><cell>SDN + L#</cell><cell cols="3">86.3 / 72.0 73.0 / 56.1 67.4 / 54.1</cell></row><row><cell cols="4">SDN + GDC 88.2 / 75.1 76.9 / 63.8 73.4 / 57.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of pedestrians (top) and cyclists (bottom) on KITTI validation. We apply F-POINTNET Qi et al. (2018) and report APBEV / AP3D (in %) at IoU= 0.5, following Wang et al. (2019a).</figDesc><table><row><cell>Stereo depth</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>PSMNET</cell><cell cols="3">41.3 / 33.8 34.9 / 27.4 30.1 / 24.0</cell></row><row><cell>SDN</cell><cell cols="3">48.7 / 40.9 40.4 / 32.9 34.9 / 28.8</cell></row><row><cell cols="4">SDN + GDC 63.7 / 53.6 53.8 / 44.4 46.8 / 38.1</cell></row><row><cell>PSMNET</cell><cell cols="3">47.6 / 41.3 29.9 / 25.2 27.0 / 24.9</cell></row><row><cell>SDN</cell><cell cols="3">49.3 / 44.6 30.4 / 28.7 28.6 / 26.4</cell></row><row><cell cols="4">SDN + GDC 65.7 / 60.8 45.8 / 40.8 42.8 / 38.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on stereo depth estimation. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set. DL stands for depth loss.</figDesc><table><row><cell>Depth Estimation</cell><cell>Easy</cell><cell>PIXOR Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AVOD Moderate</cell><cell>Hard</cell></row><row><cell>PSMNET</cell><cell>73.9 / -</cell><cell>54.0 / -</cell><cell>46.9 / -</cell><cell cols="3">74.9 / 61.9 56.8 / 45.3 49.0 / 39.0</cell></row><row><cell cols="2">PSMNET + DL 75.8 / -</cell><cell>56.2 / -</cell><cell>51.9 / -</cell><cell cols="3">75.7 / 60.5 57.1 / 44.8 49.2 / 38.4</cell></row><row><cell>SDN</cell><cell>79.7 / -</cell><cell>61.1 / -</cell><cell>54.5 / -</cell><cell cols="3">77.0 / 63.2 63.7 / 46.8 56.0 / 39.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on leveraging sparse LiDAR. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set. L# stands for 4-beam LiDAR signal. SDN +L# means we replace the depth of a portion of pseudo-LiDAR points (i.e., landmark pixels) by L#.</figDesc><table><row><cell>Depth Estimation</cell><cell>Easy</cell><cell>PIXOR Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AVOD Moderate</cell><cell>Hard</cell></row><row><cell>SDN</cell><cell>79.7 / -</cell><cell>61.1 / -</cell><cell>54.5 / -</cell><cell cols="3">77.0 / 63.2 63.7 / 46.8 56.0 / 39.8</cell></row><row><cell>L#</cell><cell>72.0 / -</cell><cell>64.7 / -</cell><cell>63.6 / -</cell><cell cols="3">77.0 / 62.1 68.8 / 54.7 67.1 / 53.0</cell></row><row><cell>SDN + L#</cell><cell>75.6 / -</cell><cell>59.4 / -</cell><cell>53.2 / -</cell><cell cols="3">84.1 / 66.0 67.0 / 53.1 58.8 / 46.4</cell></row><row><cell>SDN + GDC</cell><cell>84.0 / -</cell><cell>71.0 / -</cell><cell>65.2 / -</cell><cell cols="3">86.8 / 70.7 76.6 / 56.2 68.7 / 53.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the sparsity of LiDAR. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set. L# stands for using sparse LiDAR signal alone. The number in brackets indicates the number of beams in use.</figDesc><table><row><cell>Depth Estimation</cell><cell>Easy</cell><cell>P-RCNN Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>PIXOR Moderate</cell><cell>Hard</cell></row><row><cell>SDN</cell><cell cols="4">82.0 / 67.9 64.0 / 50.1 57.3 / 45.3 79.7 / -</cell><cell>61.1 / -</cell><cell>54.5 / -</cell></row><row><cell>L# (2)</cell><cell cols="4">69.2 / 46.3 62.8 / 41.9 61.3 / 40.0 66.8 / -</cell><cell>55.5 / -</cell><cell>53.3 / -</cell></row><row><cell>L# (4)</cell><cell cols="4">73.2 / 56.1 71.3 / 53.1 70.5 / 51.5 72.0 / -</cell><cell>64.7 / -</cell><cell>63.6 / -</cell></row><row><cell cols="5">SDN + GDC (2) 87.2 / 73.3 72.0 / 56.6 67.1 / 54.1 82.0 / -</cell><cell>65.3 / -</cell><cell>61.7 / -</cell></row><row><cell cols="5">SDN + GDC (4) 88.2 / 75.1 76.9 / 63.8 73.4 / 57.4 84.0 / -</cell><cell>71.0 / -</cell><cell>65.2 / -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of GDC and PNP for 3D object detection. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set, using SDN + PNP or SDN + GDC for depth estimation and P-RCNN or PIXOR for detection.</figDesc><table><row><cell>Input signal</cell><cell>Easy</cell><cell>P-RCNN Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>PIXOR Moderate</cell><cell>Hard</cell></row><row><cell cols="5">SDN + PNP 86.3 / 72.1 73.3 / 58.9 67.2 / 54.2 79.1 / -</cell><cell>64.2 / -</cell><cell>54.0 / -</cell></row><row><cell cols="5">SDN + GDC 88.2 / 75.1 76.9 / 63.8 73.4 / 57.4 84.0 / -</cell><cell>71.0 / -</cell><cell>65.2 / -</cell></row><row><cell cols="4">D.3 DEPTH CORRECTION VS. DEPTH COMPLETION</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of 3D object detection using the naive and optimized implementation of GDC. We report APBEV / AP3D (in %) of the car category at IoU= 0.7 on the KITTI validation set, using P-RCNN for detection.</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>Naive</cell><cell cols="3">88.2 / 75.1 76.9 / 63.8 73.4 / 57.4</cell></row><row><cell cols="4">Optimized 87.6 / 75.0 76.3 / 63.4 73.1 / 57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Median depth estimation errors over various depth ranges (numerical values ofFigure 4).</figDesc><table><row><cell>Signal</cell><cell cols="7">range (m) 0-10 10-20 20-30 30-40 40-50 50-60 60-70</cell></row><row><cell>PSMNet</cell><cell>0.04</cell><cell>0.11</cell><cell>0.36</cell><cell>0.83</cell><cell>1.24</cell><cell>1.98</cell><cell>2.43</cell></row><row><cell>SDN</cell><cell>0.07</cell><cell>0.12</cell><cell>0.30</cell><cell>0.60</cell><cell>0.89</cell><cell>1.31</cell><cell>1.73</cell></row><row><cell cols="2">SDN + GDC 0.07</cell><cell>0.12</cell><cell>0.27</cell><cell>0.51</cell><cell>0.74</cell><cell>1.03</cell><cell>1.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Mean depth estimation errors (with standard deviation) over various depth ranges. 21?0.89 0.35?1.16 0.87?2.31 1.80?4.22 2.67?6.00 4.27?8.78 5.82?11.23 SDN + GDC 0.21?0.90 0.35?1.17 0.84?2.34 1.74?4.27 2.59?6.06 4.14?8.85 5.72?11.29</figDesc><table><row><cell>Signal</cell><cell>0-10</cell><cell>10-20</cell><cell>20-30</cell><cell>range (m) 30-40</cell><cell>40-50</cell><cell>50-60</cell><cell>60-70</cell></row><row><cell>PSMNet</cell><cell cols="7">0.18?0.93 0.36?1.20 0.97?2.32 2.02?4.05 2.94?5.64 4.61?8.03 6.03?10.32</cell></row><row><cell>SDN</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The Ibeo Wide Angle Scanning (ScaLa) sensor with 4 beams costs $600 (USD). In this paper we simulate the 4-beam LiDAR signal on KITTI benchmark(Geiger et al., 2012)  by sparsifying the original 64-beam signal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Recently,<ref type="bibr" target="#b15">Srivastava et al. (2019)</ref> proposed to lift 2D monocular images to 3D representations (e.g., bird's-eye view (BEV) images) and achieved promising monocular-based 3D object detection results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We note that, the degree of improvement brought by the depth loss and depth cost volume depends on the 3D detector in use.Table 3suggests that the depth loss provides more gain than the depth cost volume (for P-RCNN). InTable 6, we, however, see that the depth cost volume provides comparable or even bigger gain</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">These two problems yield identical solutions but we found the second one is easier to solve in practice. We note that, Equation 7 is an under-constrained problem, with infinitely many solutions. To identify a single solution, we add a small L2 regularization term to the objective (as mentioned in the main text).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6"><ref type="bibr" target="#b17">Torres-Mendez &amp; Dudek (2004)</ref> use MRFs and may thus require less (or even no) training data compared to deep learning algorithms: a property shared by GDC.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by grants from the National Science Foundation NSF (III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822), the Office of Naval Research DOD (N00014-17-1-2175), the Bill and Melinda Gates Foundation, and the Cornell Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875). We are thankful for generous support by Zillow and SAP America Inc. We thank Gao Huang for helpful discussion.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide details omitted in the main text.</p><p>? Appendix A: details on constructing the depth cost volume (section 3 of the main paper).</p><p>? Appendix B: detailed implementation of the GDC algorithm (section 4 of the main paper).</p><p>? Appendix C: additional details of experimental setups (subsection 5.1 of the main paper).</p><p>? Appendix D: additional results, analyses, and discussions (subsection 5.2 of the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DEPTH COST VOLUME</head><p>With Equation 2, we know where each grid <ref type="bibr">(u, v, z)</ref> in C depth corresponds to in C disp (may not be on a grid). We can then obtain features for each grid in C depth (i.e., C depth (u, v, z, :)) by bilinear interpolation over features on grids of C disp around the non-grid location (i.e., u, v, f U ? b z ). We applied the "grid_sample" function in PyTorch for bilinear interpolation.</p><p>We use PSMNET <ref type="bibr" target="#b1">(Chang &amp; Chen, 2018)</ref> as the backbone for our stereo depth estimation network (SDN). The only change is to construct the depth cost volume before performing 3D convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GRAPH-BASED DEPTH CORRECTION (GDC) ALGORITHM</head><p>Here we present the GDC algorithm in detail (see algorithm 1). The two steps described in the main paper can be easily turned into two (sparse) linear systems and then solved by using Lagrange multipliers. For the first step (i.e., Equation 7), we solve the same problem as in the main text but we switch the objective to minimizing the L 2 -norm of W and set Z ? W Z = 0 as a constraint 5 . For the second step (i.e., Equation 8), we use the Conjugate Gradient (CG) to iteratively solve the sparse linear system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>Graph-based depth correction (GDC). ";" stands for column-wise concatenation.</p><p>Input: Stereo depth map Z ? R (n+m)?1 , the corresponding pseudo-LiDAR (PL) point cloud P ? R (n+m)?3 , and LiDAR depths G ? R n?1 on the first the n pixels. Output: Corrected depth map Z ? R (n+m)?1 function GDC(Z, P, G, K)</p><p>Solve: W = arg min W ?R (n+m)?(n+m) W 2 s.t. Z ? W ? Z = 0, W ij = 0 if j / ? N i (i.e., the set of neighbors of the i th point) according to P ,</p><p>In this section, we explain how we generate sparser LiDAR with fewer beams from a 64-beam LiDAR point cloud from KITTI dataset in detail. For every point (x i , y i , z i ) ? R 3 of the point cloud in one</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust object proposals re-ranking for object detection in autonomous driving using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Wook</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="110" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highly parallel fast kd-tree construction for interactive ray tracing of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Shevtsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Soupikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kapustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 2d to 3d lifting for object detection in 3d for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Sidhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anting</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobie</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gataryi</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Phadte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dscnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07070</idno>
		<title level="m">Replicating lidar point clouds with deep sensor cloning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical inference and synthesis in the image domain for mobile robot environment modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz Abril Torres-Mendez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Plugand-play: Improve depth estimation via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08350</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by semidefinite programming and kernel matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani</forename><surname>Zoubin</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sub-sampling pseudo-LiDAR points: keeping at most one point within a cubic of size 0.1m 3 2. Limiting the pseudo-LiDAR points for depth correction: keeping only those whose elevation angles are within</title>
		<imprint/>
	</monogr>
	<note>?3.0 ? , 0.4 ? ) (the range of 4-beam LiDAR plus 0.6 ? ; see subsection C.1 for details</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">After performing GDC for depth correction, combining the corrected pseudo-LiDAR points with those outsides the elevation angles of</title>
		<imprint/>
	</monogr>
	<note>?3.0 ? , 0.4 ?</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">7ms for KD-tree construction and search, 46.5ms for solving W , and 26.9ms for solving Z P L ) with negligible performance difference (see Table 10). For consistency, all results reported in the main paper are based on the naive implementation</title>
	</analytic>
	<monogr>
		<title level="m">GDC runs in 90 ms/frame using a single GPU</title>
		<imprint/>
	</monogr>
	<note>Further speedups can be achieved by CUDA programming for GPUs</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">In Table 12 we further show mean errors with standard deviation (the large standard deviation likely results from outliers such as occluded pixels around object boundaries)</title>
	</analytic>
	<monogr>
		<title level="m">LiDAR Pseudo-LiDAR Pseudo-LiDAR++ (SDN) Pseudo-LiDAR++</title>
		<imprint>
			<publisher>SDN + GDC</publisher>
		</imprint>
	</monogr>
	<note>DETECTION We quantitatively evaluate the stereo depths by median errors in Figure 4 of the main text (numerical values are listed in Table 11)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Qualitative Comparison -another example. The same setup as in Figure 12 LiDAR Pseudo-LiDAR Pseudo-LiDAR++ (SDN) Pseudo-LiDAR++</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<publisher>SDN + GDC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Qualitative Comparison -another example. The same setup as in Figure 12</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
