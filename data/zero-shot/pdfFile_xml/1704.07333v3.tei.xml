<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting human, verb, object triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -their pose, clothing, action -is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual recognition of individual instances, e.g., detecting objects <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> and estimating human actions/poses <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref>, has witnessed significant improvements thanks to deep learning visual representations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>. However, recognizing individual objects is just a first step for machines to comprehend the visual world. To understand what is happening in images, it is necessary to also recognize relationships between individual instances. In this work, we focus on human-object interactions.</p><p>The task of recognizing human-object interactions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref> can be represented as detecting human, verb, object triplets and is of particular interest in applications and in research. From a practical perspective, photos containing people contribute a considerable portion of daily uploads to internet and social networking sites, and thus human-centric understanding has significant demand in practice. From a research perspective, the person category involves a rich set of actions/verbs, most of which are rarely taken by other subjects (e.g., to talk, throw, work). The fine granularity of human actions and their interactions with a wide array of object types presents a new challenge compared to recognition of entry-level object categories.</p><p>In this paper, we present a human-centric model for recognizing human-object interaction. Our central observation is that a person's appearance, which reveals their action and pose, is highly informative for inferring where the target object of the interaction may be located <ref type="figure" target="#fig_0">(Figure 1(b)</ref>). The search space for the target object can thus be narrowed by conditioning on this estimation. Although there are often many objects detected <ref type="figure" target="#fig_0">(Figure 1(a)</ref>), the inferred target location can help the model to quickly pick the correct object associated with a specific action <ref type="figure" target="#fig_0">(Figure 1(c)</ref>).</p><p>We implement this idea as a human-centric recognition branch in the Faster R-CNN framework <ref type="bibr" target="#b25">[26]</ref>. Specifically, on a region of interest (RoI) associated with a person, this branch performs action classification and density estimation for the action's target object location. The density estimator predicts a 4-d Gaussian distribution, for each action type, that models the likely relative position of the target object to the person. The prediction is based purely on the human appearance. This human-centric recognition branch, along with a standard object detection branch <ref type="bibr" target="#b8">[9]</ref> and a simple pairwise interaction branch (described later), form a multitask learning system that can be jointly optimized.</p><p>We evaluate our method, InteractNet, on the challenging V-COCO (Verbs in COCO) dataset <ref type="bibr" target="#b12">[13]</ref> for detecting human-object interactions. Our human-centric model improves accuracy by 26% (relative) from 31.8 to 40.0 AP (evaluated by Average Precision on a triplet, called 'role AP' <ref type="bibr" target="#b12">[13]</ref>), with the gain mainly due to inferring the target object's relative position from the human appearance. In addition, we prove the effectiveness of InteractNet by reporting a 27% relative improvement on the newly released HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>. Finally, our method can run at about 135ms / image for this complex task, showing good potential for practical usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. Bounding-box based object detectors have improved steadily in the past few years. R-CNN, a particularly successful family of methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>, is a two-stage approach in which the first stage proposes candidate RoIs and the second stage performs object classification. Region-wise features can be rapidly extracted <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref> from shared feature maps by an RoI pooling operation. Feature sharing speeds up instance-level detection and enables recognizing higher-order interactions, which would be computationally infeasible otherwise. Our method is based on the Fast/Faster R-CNN frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Human Action &amp; Pose Recognition. The action and pose of humans is indicative of their interactions with objects or other people in the scene. There has been great progress in understanding human actions <ref type="bibr" target="#b10">[11]</ref> and poses <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> from images. These methods focus on the human instances and do not predict interactions with other objects. We rely on action and pose appearance cues in order to predict the interactions with objects in the scene.</p><p>Visual Relationships. Research on visual relationship modeling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> has attracted increasing attention. Recently, Lu et al. <ref type="bibr" target="#b21">[22]</ref> proposed to recognize visual relationships derived from an open-world vocabulary. The set of relationships include verbs (e.g., wear), spatial (e.g., next to), actions (e.g., ride) or a preposition phrase (e.g., drive on). Our focus is related, but different. First, we aim to understand human-centric interactions, which take place in particularly diverse and interesting ways. These relationships involve direct interaction with objects (e.g., person cutting cake), unlike spatial or prepositional phrases (e.g., dog next to dog). Second, we aim to build detectors that recognize interactions in images with high precision, which is a requirement for practical applications. In contrast, in an open-world recognition setting, evaluating precision is not feasible, resulting in recall-based evaluation, as in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Human-Object Interactions. Human-object interactions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref> are related to visual relationships, but present different challenges. Human actions are more fine-grained (e.g., walking, running, surfing, snowboarding) than the actions of general subjects, and an individual person can simultaneously take multiple actions (e.g., drinking tea and reading a newspaper while sitting in a chair). These issues require a deeper understanding of human actions and the objects around them and in much richer ways than just the presence of the objects in the vicinity of a person in an image. Accurate recognition of human-object interaction can benefit numerous tasks in computer vision, such as actionspecific image retrieval <ref type="bibr" target="#b24">[25]</ref>, caption generation <ref type="bibr" target="#b33">[34]</ref>, and question answering <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We now describe our method for detecting human-object interactions. Our goal is to detect and recognize triplets of the form human, verb, object . To detect an interaction triplet, we have to accurately localize the box containing a human and the box for the associated object of interaction (denoted by b h and b o , respectively), as well as identify the action a being performed (selected from among A actions).</p><p>Our proposed solution decomposes this complex and multifaceted problem into a simple and manageable form. We extend the Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> object detection framework with an additional human-centric branch that classifies actions and estimates a probability density over the target object location for each action. The human-centric branch reuses features extracted by Fast R-CNN for object detection so its marginal computation is lightweight.</p><p>Specifically, given a set of candidate boxes, Fast R-CNN outputs a set of object boxes and a class label for each box. Our model extends this by assigning a triplet score S a h,o to pairs of candidate human/object boxes b h , b o and an action a. To do so, we decompose the triplet score into four terms:</p><formula xml:id="formula_0">S a h,o = s h ? s o ? s a h ? g a h,o<label>(1)</label></formula><p>While the model has multiple components, the basic idea is straightforward. s h and s o are the class scores from Fast R-CNN of b h and b o containing a human and object. Our human-centric branch outputs two extra terms. First, s a h is the score assigned to action a for the person at b h . Second, ? a h is the predicted location of the target of interaction for a given human/action pair, computed based on the appearance of the human. This, in turn, is used to compute g a h,o , the likelihood that an object with box b o is the actual target of interaction. We give details shortly and show that this target localization term is key for obtaining good results.</p><p>We discuss each component next, followed by an extension that replaces the action classification output s a h with a dedicated interaction branch that outputs a score s a h,o for an action a based on both the human and object appearances. Finally we give details for training and inference. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates each component in our full framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Components</head><p>Object Detection. The object detection branch of our network, shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), is identical to that of Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>. First, a Region Proposal Network (RPN) is used to generate object proposals <ref type="bibr" target="#b25">[26]</ref>. Then, for each proposal box b, we extract features with RoiAlign <ref type="bibr" target="#b13">[14]</ref>, and perform object classification and bounding-box regression to obtain a new set of boxes, each of which has an associated score s o (or s h if the box is assigned to the person category). These new boxes are only used during inference; during training all branches are trained with RPN proposal boxes.</p><p>Action Classification. The first role of the human-centric branch is to assign an action classification score s a h to each human box b h and action a. Just like in the object classification branch, we extract features from b h with RoiAlign and predict a score for each action a. Since a human can simultaneously perform multiple actions (e.g., sit and drink), our output layer consists of binary sigmoid classifiers for multilabel action classification (i.e. the predicted action classes do not compete). The training objective is to minimize the binary cross entropy losses between the ground-truth action labels and the scores s a h predicted by the model. Target Localization. The second role of the humancentric branch is to predict the target object location based on a person's appearance (again represented as features pooled from b h ). However, predicting the precise target object location based only on features from b h is challenging. Instead, our approach is to predict a density over possible locations, and use this output together with the location of actual detected objects to precisely localize the target.</p><p>We model the density over the target object's location as a Gaussian function whose mean is predicted based on the human appearance and action being performed. Formally, the human-centric branch predicts ? a h , the target object's 4d mean location given the human box b h and action a. We then write our target localization term as:</p><formula xml:id="formula_1">g a h,o = exp( b o|h ? ? a h 2 /2? 2 )<label>(2)</label></formula><p>We can use g to test the compatibility of an object box b o and the predicted target location ? a h . In the above, b o|h is the encoding of b o in coordinates relative to b h , that is:</p><formula xml:id="formula_2">b o|h = { x o ? x h w h , y o ? y h h h , log w o w h , log h o h h }<label>(3)</label></formula><p>This is a similar encoding as used in Fast R-CNN <ref type="bibr">[</ref>  <ref type="figure">Figure 4</ref>. Estimating target object density from the person features. We estimate a 4-d Gaussian density whose mean ? a h represents a 4-d offset for the target object of action a (illustrated as yellow boxes); the variance of the density is illustrated in red for the 2-d translation offsets of (x, y) (the scaling offsets' variance is not visualized). These target locations will be combined with the object detections bo to detect human-object interaction triplets. This figure also shows the predicted actions and their scores from the person RoIs. The rightmost column shows two intriguing examples: even though there are no target objects, our model predicts reasonably densities from the human pose (these predictions will be rejected by the object detection module, which will not find an object in the high density regions).</p><p>are two different objects and moreover b o is not necessarily near or of the same size as b h . The training objective is to minimize the smooth L 1 loss <ref type="bibr" target="#b8">[9]</ref> between ? a h and b o|h , where b o is the location of the ground truth object for the interaction. We treat ? as a hyperparameter that we empirically set to ? = 0.3 using the validation set. <ref type="figure">Figure 4</ref> visualizes the predicted distribution over the target object's location for example human/action pairs. As we can see, a carrying appearance suggests an object in the person's hand, a throwing appearance suggests an object in front of the person, and a sitting appearance implies an object below the person. We note that the yellow dashed boxes depicting ? a h shown in <ref type="figure">Figure 4</ref> are inferred from b h and a and did not have direct access to the objects.</p><p>Intuitively, our formulation is predicated on the hypothesis that the features computed from b h contain a strong signal pointing to the target of an action, even if that target object is outside of b h . We argue that such 'outside-thebox' regression is possible because the person's appearance provides a strong clue for the target location. Moreover, as this prediction is action-specific and instance-specific, our formulation is effective even though we model the target location using a uni-modal distribution. In Section 5 we discuss a variant of our approach which allows us to handle conditionally multi-modal distributions and predict multiple targets for a single action.</p><p>Interaction Recognition. Our human-centric model scores actions based on the human appearance. While effective, this does not take into account the appearance of the target object. To improve the discriminative power of our model, and to demonstrate the flexibility of our framework, we can replace s a h in <ref type="formula" target="#formula_0">(1)</ref> with an interaction branch that scores an action based on the the appearance of both the human and target object. We use s a h,o to denote this alternative term.</p><p>The computation of s a h,o reuses the computation from s a h and additionally in parallel performs a similar computation based on features extracted from b o . The outputs from the two action classification heads, which are A-dimensional vectors of logits, are summed and passed through a sigmoid activation to yield A scores. This process is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(c). As before, the training objective is to minimize the binary cross entropy losses between the ground-truth action labels and the predicted action scores s a h,o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-task Training</head><p>We approach learning human-object interaction as a multi-task learning problem: all three branches shown in <ref type="figure" target="#fig_2">Figure 3</ref> are trained jointly. Our overall loss is the sum of all losses in our model including: (1) the classification and regression loss for the object detection branch, (2) the action classification and target localization loss for the humancentric branch, and (3) the action classification loss of the interaction branch. This is in contrast to our cascaded inference described in ?3.3, where the output of the object detection branch is used as input for the human-centric branch.</p><p>We adopt image-centric training <ref type="bibr" target="#b8">[9]</ref>. All losses are computed over both RPN proposal and ground truth boxes as in Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>. As in <ref type="bibr" target="#b8">[9]</ref>, we sample at most 64 boxes from each image for the object detection branch, with a ratio of 1:3 of positive to negative boxes. The human-centric branch is computed over at most 16 boxes b h that are associated with the human category (i.e., their IoU overlap with a ground-truth person box is ? 0.5). The loss for the interaction branch is only computed on positive example triplets (i.e., b h , a, b o must be associated with a ground truth interaction triplet). All loss terms have a weight of one, except the action classification term in the human-centric branch has a weight of two, which we found performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cascaded Inference</head><p>At inference, our goal is to find high-scoring triplets according to S a h,o in <ref type="bibr" target="#b0">(1)</ref>. While in principle this has O(n 2 ) complexity as it requires scoring every pair of candidate boxes, we present a simple cascaded inference algorithm whose dominant computation has O(n) complexity.</p><p>Object Detection Branch: We first detect all objects (including the person class) in the image. We apply nonmaximum suppression (NMS) with an IoU threshold of 0.3 <ref type="bibr" target="#b8">[9]</ref> on boxes with scores higher than 0.05 (set conservatively to retain most objects). This step yields a new smaller set of n boxes b with scores s h and s o . Unlike in training, these new boxes are used as input to the remaining two branches.</p><p>Human-Centric Branch: Next, we apply the humancentric branch to all detected objects that were classified as human. For each action a and detected human box b h , we compute s a h , the score assigned to a, as well as ? a h , the predicted mean offset of the target object location relative to b h . This step has a complexity of O(n).</p><p>Interaction Branch: If using the optional interaction branch, we must compute s a h,o for each action a and pair of boxes b h and b o . To do so we first compute the logits for the two action classification heads independently for each box b h and b o , which is O(n). Then, to get scores s a h,o , these logits are summed and passed through a sigmoid for each pair. Although this last step is O(n 2 ), in practice its computational time is negligible.</p><p>Once all individual terms have been computed, the computation of (1) is fast. However, rather than scoring every potential triplet, for each human/action pair we find the object box that maximizes S a h,o . That is we compute:</p><formula xml:id="formula_3">b o * = arg max bo s o ? s a h,o ? g a h,o<label>(4)</label></formula><p>Recall that g a h,o is computed according to <ref type="bibr" target="#b1">(2)</ref> and measures the compatibility between b o and the expected target location ? a h . Intuitively, (4) encourages selecting a highconfidence object near the predicted target location of a high-scoring action. With b o selected for each b h and action a, we have a triplet of human, verb, object = b h , a, b o . These triplets, along with the scores S a h,o , are the final outputs of our model. For actions that that do not interact with any object (e.g., smile, run), we rely on s a h and the interaction output s a h,o is not used, even if present. The score of such a predicted human, verb pair is simply s h ? s a h . The above cascaded inference has a dominant complexity of O(n), which involves extracting features for each of the n boxes and forwarding through a small network. The pairwise O(n 2 ) operations require negligible computation. In addition, for the entire system, a portion of computation is spent on computing the full-image shared convolutional feature maps. Altogether, our system takes ?135ms on a typical image running on a single Nvidia M40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets and Metrics</head><p>There exist a number of datasets for human-object interactions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. The most relevant for this work are V-COCO (Verbs in COCO) <ref type="bibr" target="#b12">[13]</ref> and HICO-DET <ref type="bibr" target="#b2">[3]</ref>. V-COCO serves as the primary testbed on which we demonstrate the effectiveness of InteractNet and analyze its various components. The newly released HICO-DET <ref type="bibr" target="#b2">[3]</ref> contains ?48k images and 600 types of interactions and serves to further demonstrate the efficacy of our approach. The older TUHOI <ref type="bibr" target="#b17">[18]</ref> and HICO <ref type="bibr" target="#b3">[4]</ref> datasets only have imagelevel labels and thus do not allow for grounding interactions in a detection setting, while COCO-a <ref type="bibr" target="#b26">[27]</ref> is promising but only a small beta-version is currently available.</p><p>V-COCO is a subset of COCO <ref type="bibr" target="#b20">[21]</ref> and has ?5k images in the trainval set and ?5k images in the test set. <ref type="bibr" target="#b0">1</ref> The trainval set includes ?8k person instances and on average 2.9 actions/person. V-COCO is annotated with 26 common action classes (listed in <ref type="table" target="#tab_2">Table 2</ref>). Of note, there are three actions (cut, hit, eat) that are annotated with two types of targets: instrument and direct object. For example, cut + knife involves the instrument (meaning 'cut with a knife'), and cut + cake involves the direct object (meaning 'cut a cake'). In <ref type="bibr" target="#b12">[13]</ref>, accuracy is evaluated separately for the two types of targets. To address this, for the target estimation, we train and infer two types of targets for these three actions (i.e., they are treated like six actions for target estimation).</p><p>Following <ref type="bibr" target="#b12">[13]</ref>, we evaluate two Average Precision (AP) metrics. We note that this is a detection task, and both AP metrics measure both recall and precision. This is in contrast to metrics of Recall@N that ignore precision.</p><p>The AP of central interest in the human-object interaction task is the AP of the triplet human, verb, object , called 'role AP' (AP role ) in <ref type="bibr" target="#b12">[13]</ref>. Formally, a triplet is considered as a true positive if: (i) the predicted human box b h has IoU of 0.5 or higher with the ground-truth human box, (ii) the predicted object box b o has IoU of 0.5 or higher with the ground-truth target object, and (iii) the predicted and ground-truth actions match. With this definition of a true positive, the computation of AP is analogous to standard object detection (e.g., PASCAL <ref type="bibr" target="#b7">[8]</ref>). Note that this metric does not consider the correctness of the target object category (but only the target object box location). Nevertheless, our method can predict the object categories, as shown in the visualized results <ref type="figure" target="#fig_1">(Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>).</p><p>We also evaluate the AP of the pair human, verb , called 'agent AP' (AP agent ) in <ref type="bibr" target="#b12">[13]</ref>, computed using the above criteria of (i) and (iii). AP agent is applicable when the action has no object. We note that AP agent does note require localizing the target, and is thus of secondary interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Implementation Details. Our implementation is based on Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b19">[20]</ref> backbone built on ResNet-50 <ref type="bibr" target="#b15">[16]</ref>; we also evaluate a non-FPN version in ablation experiments. We train the Region Proposal Network (RPN) <ref type="bibr" target="#b25">[26]</ref> of Faster R-CNN following <ref type="bibr" target="#b19">[20]</ref>. For convenient ablation, RPN is frozen and does not share features with our network (we note that feature sharing is possible <ref type="bibr" target="#b25">[26]</ref>). We extract 7?7 features from regions by RoiAlign <ref type="bibr" target="#b13">[14]</ref>, and each of the three model branches (see <ref type="figure" target="#fig_2">Figure 3</ref>) consist of two 1024-d fullyconnected layers (with ReLU <ref type="bibr" target="#b23">[24]</ref>) followed by specific output layers for each output type (box, class, action, target).</p><p>Given a model pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>, we first train the object detection branch on the COCO train set (excluding the V-COCO val images). This model, which is in essence Faster R-CNN, has 33.8 object detection AP on the COCO val set. Our full model is initialized by this object detection network. We prototype our human-object interaction models on the V-COCO train split and perform hyperparameter selection on the V-COCO val split. After fixing these parameters, we train on V-COCO trainval (5k images) and report results on the 5k V-COCO test set.</p><p>We fine-tune our human-object interaction models for 10k iterations on the V-COCO trainval set with a learning rate of 0.001 and an additional 3k iterations with a rate of 0.0001. We use a weight decay of 0.0001 and a momentum of 0.9. We use synchronized SGD <ref type="bibr" target="#b18">[19]</ref> on 8 GPUs, with each GPU hosting 2 images (so the effective mini-batch size per iteration is 16 images). The fine-tuning time is ?2.5 hours on the V-COCO trainval set on 8 GPUs.</p><p>Baselines. To have a fair comparison with Gupta &amp; Malik <ref type="bibr" target="#b12">[13]</ref>, which used VGG-16 <ref type="bibr" target="#b28">[29]</ref>, we reimplement their best-performing model ('model C' in <ref type="bibr" target="#b12">[13]</ref>) using the same ResNet-50-FPN backbone as ours. In addition, <ref type="bibr" target="#b12">[13]</ref> only reported AP role on a subset of 19 actions, but we are interested in all actions (listed in <ref type="table" target="#tab_2">Table 2</ref>). We therefore report comparisons in both the 19-action and all-action cases.</p><p>The baselines from <ref type="bibr" target="#b12">[13]</ref> are shown in <ref type="table">Table 1</ref>. Our reimplementation of <ref type="bibr" target="#b12">[13]</ref> is solid: it has 37.5 AP role on the 19 action classes tested on the val set, 11 points higher than the 26.4 reported in <ref type="bibr" target="#b12">[13]</ref>. We believe that this is mainly due to ResNet-50 and FPN. This baseline model, when trained on the trainval set, has 31.8 AP role on all action classes tested on the test set. This is a strong baseline (31.8 AP role ) to which we will compare our method.</p><p>Our method, InteractNet, has an AP role of 40.0 evaluated on all action classes on the V-COCO test set. This is an absolute gain of 8.2 points over the strong baseline's 31.8, which is a relative improvement of 26%. This result quantitatively shows the effectiveness of our approach.</p><p>Qualitative Results. We show our human-object interaction detection results in <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>. Each subplot illustrates one detected human, verb, object triplet, showing the location of the detected person, the action taken by this person, and the location (and category) of the detected target object for this person/action. Our method can successfully detect the object outside of the person bounding box and associate it to the person and action. <ref type="figure" target="#fig_5">Figure 7</ref> shows our correctly detected triplets of one person taking multiple actions on multiple objects. We note that in this task, one person can take multiple actions and affect multiple objects. This is part of the ground-truth and evaluation and is unlike traditional object detection tasks <ref type="bibr" target="#b7">[8]</ref> in which one object has only one ground-truth class.</p><p>Moreover, InteractNet can detect multiple interaction instances in an image. <ref type="figure">Figure 6</ref> shows two test images with all detected triplets shown. Our method detects multiple persons taking different actions on different target objects.  <ref type="figure">Figure 6</ref>. All detected triplets on two V-COCO test images. We show all triplets whose scores (1) are higher than 0.01.   <ref type="bibr" target="#b12">[13]</ref>. To have an apples-to-apples comparisons, we reimplement <ref type="bibr" target="#b12">[13]</ref>'s 'model C' using ResNet-50-FPN. In addition, <ref type="bibr" target="#b12">[13]</ref> reported AProle on a subset consisting of 19 actions, and only on the val set. As we evaluate on all actions (more details in <ref type="table" target="#tab_2">Table 2</ref>), for fair comparison, we also report the mean AProle on these 19 actions of val, with models trained on train. Our reimplemented baseline of <ref type="bibr" target="#b12">[13]</ref> is solid, and InteractNet is considerably better than this baseline.</p><p>The multi-instance, multi-action, and multi-target results in <ref type="figure">Figure 6</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> are all detected by one forward pass in our method, running at about 135ms per image on a GPU. Ablation Studies. In <ref type="table" target="#tab_3">Table 3</ref>-5 we evaluate the contributions of different factors in our system to the results.</p><p>With vs. without target localization. Target localization, performed by the human-centric branch, is the key component of our system. To evaluate its impact, we implement a variant without target localization. Specifically, for each type of action, we perform k-means clustering on the offsets between the target RoIs and person RoIs (via crossvalidation we found k = 2 clusters performs best). This plays a role similar to density estimation, but is not aware of the person appearance and thus is not instance-dependent. Aside from this, the variant is the same as our full approach. baseline <ref type="bibr" target="#b12">[13]</ref> InteractNet  <ref type="bibr" target="#b12">[13]</ref>, and because 3 actions (cut, eat, hit) involve two types of target objects (instrument and direct object), there are 26+3 entries (more details in ? 4). We bold the leading entries on AProle. <ref type="table" target="#tab_3">Table 3</ref> (a) vs. (c) shows that our target localization contributes significantly to AP role . Removing it shows a degradation of 5.6 points from 37.5 to 31.9. This result shows the effectiveness of our target localization (see <ref type="figure">Figure 4</ref>). The per-category results are in <ref type="table" target="#tab_2">Table 2</ref>.</p><formula xml:id="formula_4">InteractNet</formula><p>With vs. without the interaction branch. We also evaluate a variant of our method when removing the interaction branch. We can instead use the action prediction from the human-centric branch (see <ref type="figure" target="#fig_2">Figure 3</ref>).  <ref type="table" target="#tab_3">Table 3</ref>. Ablation studies on the V-COCO val set, evaluated by APagent (i.e., AP of the human, verb pairs) and AProle (i.e., AP of the human, verb, object triplets). All methods are based on ResNet-50-FPN, including our reimplementation of <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the detail numbers of three entries: baseline, Interact-Net without target density estimation, and our complete method on the V-COCO test set.   With vs. without FPN. Our model is a generic humanobject detection framework and can support various network backbones. We recommend using the FPN <ref type="bibr" target="#b19">[20]</ref> backbone, because it performs well for small objects that are more common in human-object detection. <ref type="table" target="#tab_5">Table 4</ref> shows a comparison between ResNet-50-FPN and a vanilla ResNet-50 backbone. The vanilla version follows the ResNet-based Faster R-CNN presented in <ref type="bibr" target="#b15">[16]</ref>. Specifically, the full-image convolutional feature maps are from the last residual block of the 4-th stage (res4), on which the RoI features are pooled. On the RoI features, each of the region-wise branches consists of the residual blocks of the 5-th stage (res5). <ref type="table" target="#tab_5">Table 4</ref> shows a degradation of 1.6 points in AP role when not using FPN. We argue that this is mainly caused by the degradation of the small objects' detection AP, as shown in <ref type="bibr" target="#b19">[20]</ref>. Moreover, the vanilla ResNet-50 backbone is much slower, 225ms versus 135ms for FPN, due to use of res5 in the region-wise branches.</p><p>Pairwise Sum vs. MLP. In our interaction branch, the pairwise outputs from two RoIs are added <ref type="figure" target="#fig_2">(Figure 3</ref>). Although simple, we have found that more complex variants do not improve results. We compare with a more complex transform in <ref type="table" target="#tab_6">Table 5</ref>. We concatenate the two 1024-d features from the final fully-connected layers of the interaction branch for the two RoIs and feed it into an 2-layer MLP (512-d with ReLU for its hidden layer), followed by action classification. This variant is slightly worse (  <ref type="table">Table 6</ref>. Results on HICO-DET test set. InteractNet outperforms the approach in <ref type="bibr" target="#b2">[3]</ref> with a 27% relative improvement. We also include our baseline approach, as described in <ref type="table">Table 1.</ref> transform (or there is insufficient data to learn this).</p><p>Per-action accuracy. <ref type="table" target="#tab_2">Table 2</ref> shows the AP for each action category defined in V-COCO, for the baseline, Inter-actNet without target localization, and our full system. We observe leading performance of AP role consistently. The actions with largest improvement are those with high variance in the spatial location of the object such as hold, look, carry, and cut. On the other hand, actions such as ride, kick, and read show small or no improvement.</p><p>Failure Cases. <ref type="figure">Figure 8</ref> shows some false positive detections. Our method can be incorrect because of false interaction inferences (e.g., top left), target objects of another person (e.g., top middle), irrelevant target objects (e.g., top right), or confusing actions (e.g., bottom left, ski vs.surf ). Some of them are caused by a failure of reasoning, which is an interesting open problem for future research.</p><p>Mixture Density Networks. To improve target localization prediction, we tried to substitute the uni-modal regression network with a Mixture Density Network (MDN) <ref type="bibr" target="#b0">[1]</ref>. The MDN predicts the mean and variance of M relative locations for the objects of interaction conditioned on the human appearance. Note that MDN with M = 1 is an extension of our original approach that also learns the variance in <ref type="bibr" target="#b1">(2)</ref>. However, we found that the MDN layer does not improve accuracy. More details and discussion regarding the MDN experiments can be found in Appendix A.</p><p>HICO-DET Dataset. We additionally evaluate InteractNet on HICO-DET <ref type="bibr" target="#b2">[3]</ref> which contains 600 types of interactions, composed of 117 unique verbs and 80 object types (identical to COCO objects). We train InteractNet on the train set, as specified by the authors, and evaluate performance on the test set using released evaluation code. Results are shown in <ref type="table">Table 6</ref> and discussed more in Appendix B.</p><formula xml:id="formula_5">? N b o|h | ? a,m h , ? a,m h<label>(5)</label></formula><p>The mixing coefficients are required to have w m h ? [0, 1] and m w a,m h = 1. We parametrize ? as a 4-D vector for each component and action type. We assume a diagonal covariance matrix and thus parametrize ? as a 4-D vector for each component and action type. Compare the localization term in (5) with the term in <ref type="bibr" target="#b1">(2)</ref>. Inference is unchanged, except we use the new form of g a h,o when evaluating (4). To instantiate MDN, the human-centric branch of our network must predict w, ?, and ?. We train the network to minimize ? log(g a h,o ) given b o (the location of the ground truth object for each interaction). We use a fully connected layer followed by a softmax operation to predict the mixing coefficients w a,m h . For ? and ?, we also use fully connected layers. However, in the case of ?, we use a softplus operation (f (x) = log(e x + 1)) to enforce positive values for the covariance coefficients. This leads to stable training, compared to a variant which parametrizes log ? 2 and becomes unstable due to an exponential term in the gradients. In addition, we found that enforcing a lower bound on the covariance coefficients was necessary to avoid overfitting. We set this value to 0.3 throughout our experiments. <ref type="table">Table 7</ref> shows the performance of InteractNet with MDN and compare it to our original model. Note that the MDN with M = 1 is an extension of our original approach, with the only difference that it learns the variance in <ref type="bibr" target="#b1">(2)</ref>. The performance of MDN with M = 1 is similar to our original model, which suggests that a learned variance does not lead to an improvement. With M = 2, we do not see any gains in performance, possibly because the human appearance is a strong cue for predicting the object's location and possibly due to the limited number of objects per action type in the dataset. Based on these results, along with the relative complexity of MDN, we chose to use our simpler proposed target localization model instead of MDN. Nonetheless, our experiments demonstrate that MDN can be trained within the InteractNet framework, which may prove useful.  <ref type="table">Table 7</ref>. Ablation studies for MDN on the V-COCO val set, evaluated by APagent (i.e., AP of the human, verb pairs) and AProle (i.e., AP of the human, verb, object triplets). All methods are based on ResNet-50-FPN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: HICO-DET Dataset</head><p>As discussed, we also test InteractNet on the HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>. HICO-DET contains approximately 48k images and is annotated with 600 interaction types. The annotations include boxes for the humans and the objects of interactions. There are 80 unique object types, identical to the COCO object categories, and 117 unique verbs.</p><p>Objects are not exhaustively annotated on HICO-DET. To address this, we first detect objects using a ResNet50-FPN object detector trained on COCO as described in <ref type="bibr" target="#b19">[20]</ref>. These detections are kept frozen during training, by setting a zero-valued weight on the object detection loss of Inter-actNet. To train the human-centric and interaction branch, we assign ground truth labels from the HICO-DET annotations to each person and object hypothesis based on box overlap. To diversify training, we jitter the object detections to simulate a set of region proposals. We define 117 action types and use the object detector to identify the type for the object of interaction, e.g. orange vs. apple.</p><p>We train InteractNet on the train set defined in <ref type="bibr" target="#b2">[3]</ref>, for 80k iterations and with a learning rate of 0.001 (a 10x step decay is applied after 60k iterations). In the interaction branch, we use dropout with a ratio of 0.5. We evaluate performance on the test set using released evaluation code.</p><p>Results are shown in <ref type="table">Table 6</ref>. We show a 27% relative performance gain compared to the published results in <ref type="bibr" target="#b2">[3]</ref> and a 9% relative gain compared to our baseline approach. <ref type="figure" target="#fig_6">Figure 9</ref> shows example predictions made by InteractNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Detecting and recognizing human-object interactions. (a) There can be many possible objects (green boxes) interacting with a detected person (blue box). (b) Our method estimates an action-type specific density over target object locations from the person's appearance, which is represented by features extracted from the detected person's box. (c) A human, verb, object triplet detected by our method, showing the person box, action (cut), and target object box and category (knife). (d) Another predicted action (stand), noting that a person can simultaneously take multiple actions and an action may not involve any objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Human-object interactions detected by our method. Each image shows one detected human, verb, object triplet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Model Architecture. Our model consists of (a) an object detection branch, (b) a human-centric branch, and (c) an optional interaction branch. The person features and their layers are shared between the human-centric and interaction branches (blue boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Our results on some V-COCO test images. Each image shows one detected human, verb, object triplet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results of InteractNet on test images. An individual person can take multiple actions and affect multiple objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Our results on the HICO-DET test set. Each image shows one detected human, verb, object triplet. The red box denotes the detected human instance, while the green box the detected object of interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Detailed results on V-COCO test. We show two main baselines and InteractNet for each action. There are 26 actions defined in</figDesc><table><row><cell></cell><cell></cell><cell cols="2">our impl.</cell><cell cols="2">w/o target loc.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">APagent AP role APagent AP role APagent AP role</cell></row><row><cell cols="2">carry</cell><cell>62.2</cell><cell>8.1</cell><cell>63.9</cell><cell>14.4</cell><cell>64.8</cell><cell>33.1</cell></row><row><cell cols="2">catch</cell><cell>47.0</cell><cell>37.0</cell><cell>53.4</cell><cell>38.5</cell><cell>57.1</cell><cell>42.5</cell></row><row><cell cols="2">drink</cell><cell>11.9</cell><cell>18.1</cell><cell>37.5</cell><cell>25.4</cell><cell>46.0</cell><cell>33.8</cell></row><row><cell cols="2">hold</cell><cell>79.4</cell><cell>4.0</cell><cell>77.3</cell><cell>10.6</cell><cell>80.1</cell><cell>26.4</cell></row><row><cell cols="2">jump</cell><cell>75.5</cell><cell>40.6</cell><cell>75.6</cell><cell>39.3</cell><cell>74.7</cell><cell>45.1</cell></row><row><cell>kick</cell><cell></cell><cell>60.9</cell><cell>67.9</cell><cell>68.6</cell><cell>70.6</cell><cell>77.5</cell><cell>69.4</cell></row><row><cell>lay</cell><cell></cell><cell>50.1</cell><cell>17.8</cell><cell>51.1</cell><cell>18.6</cell><cell>47.6</cell><cell>21.0</cell></row><row><cell cols="2">look</cell><cell>68.8</cell><cell>2.8</cell><cell>61.0</cell><cell>2.7</cell><cell>59.4</cell><cell>20.2</cell></row><row><cell>read</cell><cell></cell><cell>34.9</cell><cell>23.3</cell><cell>43.2</cell><cell>22.0</cell><cell>41.6</cell><cell>23.9</cell></row><row><cell>ride</cell><cell></cell><cell>73.2</cell><cell>55.3</cell><cell>76.2</cell><cell>55.0</cell><cell>74.2</cell><cell>55.2</cell></row><row><cell>sit</cell><cell></cell><cell>76.8</cell><cell>15.6</cell><cell>75.6</cell><cell>15.1</cell><cell>76.1</cell><cell>19.9</cell></row><row><cell cols="2">skateboard</cell><cell>89.9</cell><cell>74.0</cell><cell>90.9</cell><cell>71.7</cell><cell>90.0</cell><cell>75.5</cell></row><row><cell>ski</cell><cell></cell><cell>84.0</cell><cell>29.7</cell><cell>83.9</cell><cell>28.2</cell><cell>84.7</cell><cell>36.5</cell></row><row><cell cols="2">snowboard</cell><cell>81.3</cell><cell>52.8</cell><cell>81.1</cell><cell>50.6</cell><cell>81.1</cell><cell>63.9</cell></row><row><cell>surf</cell><cell></cell><cell>94.6</cell><cell>50.2</cell><cell>94.5</cell><cell>50.3</cell><cell>93.5</cell><cell>65.7</cell></row><row><cell cols="2">talk-on-phone</cell><cell>63.3</cell><cell>23.0</cell><cell>74.7</cell><cell>23.8</cell><cell>82.0</cell><cell>31.8</cell></row><row><cell cols="2">throw</cell><cell>54.0</cell><cell>36.0</cell><cell>53.9</cell><cell>35.7</cell><cell>58.1</cell><cell>40.4</cell></row><row><cell cols="2">work-on-computer</cell><cell>70.2</cell><cell>46.1</cell><cell>72.6</cell><cell>46.9</cell><cell>75.7</cell><cell>57.3</cell></row><row><cell>cut</cell><cell>(object) (instrument)</cell><cell>61.2</cell><cell>16.5 15.1</cell><cell>69.1</cell><cell>17.7 19.5</cell><cell>73.6</cell><cell>23.0 36.4</cell></row><row><cell>eat</cell><cell>(object) (instrument)</cell><cell>75.6</cell><cell>26.5 2.7</cell><cell>80.4</cell><cell>26.5 2.9</cell><cell>79.6</cell><cell>32.4 2.0</cell></row><row><cell>hit</cell><cell>(object) (instrument)</cell><cell>82.8</cell><cell>56.7 42.4</cell><cell>83.9</cell><cell>55.3 41.3</cell><cell>88.0</cell><cell>62.3 43.3</cell></row><row><cell cols="2">point</cell><cell>5.0</cell><cell>-</cell><cell>4.0</cell><cell>-</cell><cell>1.8</cell><cell>-</cell></row><row><cell>run</cell><cell></cell><cell>76.9</cell><cell>-</cell><cell>77.8</cell><cell>-</cell><cell>77.2</cell><cell>-</cell></row><row><cell cols="2">smile</cell><cell>60.6</cell><cell>-</cell><cell>60.3</cell><cell>-</cell><cell>62.5</cell><cell>-</cell></row><row><cell cols="2">stand</cell><cell>88.5</cell><cell>-</cell><cell>88.3</cell><cell>-</cell><cell>88.0</cell><cell>-</cell></row><row><cell cols="2">walk</cell><cell>63.9</cell><cell>-</cell><cell>63.5</cell><cell>-</cell><cell>65.4</cell><cell>-</cell></row><row><cell cols="2">mean AP</cell><cell>65.1</cell><cell>31.8</cell><cell>67.8</cell><cell>32.6</cell><cell>69.2</cell><cell>40.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>(b)  vs. (c) shows that removing the interaction branch reduces AP role just slightly by 0.7 point. This again shows the main effectiveness of our system is from the target localization.</figDesc><table><row><cell></cell><cell>APagent</cell><cell>AP role</cell></row><row><cell></cell><cell>human, verb</cell><cell>human, verb, object</cell></row><row><cell>baseline [13] (our implementation)</cell><cell>62.1</cell><cell>31.0</cell></row><row><cell>(a) InteractNet w/o target localization</cell><cell>65.1</cell><cell>31.9</cell></row><row><cell>(b) InteractNet w/o interaction branch</cell><cell>65.5</cell><cell>36.8</cell></row><row><cell>(c) InteractNet</cell><cell>68.0</cell><cell>37.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation on the V-COCO val for vanilla ResNet-50 vs.</figDesc><table><row><cell>ResNet-50-FPN [20] backbones.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>APagent</cell><cell>AP role</cell></row><row><cell></cell><cell>human, verb</cell><cell>human, verb, object</cell></row><row><cell>InteractNet w/ pairwise concat + MLP</cell><cell>67.1</cell><cell>37.5</cell></row><row><cell>InteractNet</cell><cell>68.0</cell><cell>37.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation on the V-COCO val set about the design of the pairwise interaction branch. See main text for explanations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 )</head><label>5</label><figDesc>, indicating that it is not necessary to perform a complex pairwise</figDesc><table><row><cell>talk_on_phone</cell><cell>drink</cell><cell></cell><cell></cell><cell>sit</cell></row><row><cell>cell phone</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">wine glass</cell><cell></cell><cell>chair</cell></row><row><cell></cell><cell>eat</cell><cell></cell><cell></cell><cell>hit</cell></row><row><cell>ski</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">sandwich</cell><cell></cell><cell>tennis racket</cell></row><row><cell>skis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 8. False positive detections of our method.</cell></row><row><cell>method</cell><cell></cell><cell>full</cell><cell>rare</cell><cell>non-rare</cell></row><row><cell>results from [3]</cell><cell></cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell></row><row><cell cols="2">baseline [13] (our impl.)</cell><cell>9.09</cell><cell>7.02</cell><cell>9.71</cell></row><row><cell>InteractNet</cell><cell></cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">V-COCO's trainval set is a subset of COCO's train set, and its test set is a subset of COCO's val set. See<ref type="bibr" target="#b12">[13]</ref> for more details. In this work, COCO's val images are not used during training in any way.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">M ?1 m=0 w a,m h</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Mixture Density Networks</head><p>The target localization module in InteractNet learns to predict the location of the object of interaction conditioned on the appearance of the human hypothesis h. As an alternative, we can allow the relative location of the object to follow a multi-modal conditional distribution. For this, we replace our target localization module with a Mixture Density Network (MDN) <ref type="bibr" target="#b0">[1]</ref>, which parametrizes the mean, variance and mixing coefficients of M components of the conditional normal distribution (M is a hyperparameter). This instantiation of InteractNet is flexible can can capture different modes for the location of the objects of interaction.</p><p>The localization term for scoring b o|h is defined as:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1702.05448</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the location of interactees in novel human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and partbased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual action recognition with R*CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Visual semantic role labeling. arXiv 1505.04474</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TUHOI: trento universal human object interaction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision and Language Workshop at COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic relationships for better action retrieval in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rossenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual Madlibs: Fill in the blank Image Generation and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
