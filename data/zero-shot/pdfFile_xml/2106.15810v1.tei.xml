<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge Proposal Sets for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><forename type="middle">Linda</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Bhalerao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><forename type="middle">He</forename><surname>Facebook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Edge Proposal Sets for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs are a common model for complex relational data such as social networks and protein interactions, and such data can evolve over time (e.g., new friendships) and be noisy (e.g., unmeasured interactions). Link prediction aims to predict future edges or infer missing edges in the graph, and has diverse applications in recommender systems, experimental design, and complex systems. Even though link prediction algorithms strongly depend on the set of edges in the graph, existing approaches typically do not modify the graph topology to improve performance. Here, we demonstrate how simply adding a set of edges, which we call a proposal set, to the graph as a pre-processing step can improve the performance of several link prediction algorithms. The underlying idea is that if the edges in the proposal set generally align with the structure of the graph, link prediction algorithms are further guided towards predicting the right edges; in other words, adding a proposal set of edges is a signal-boosting pre-processing step. We show how to use existing link prediction algorithms to generate effective proposal sets and evaluate this approach on various synthetic and empirical datasets. We find that proposal sets meaningfully improve the accuracy of link prediction algorithms based on both neighborhood heuristics and graph neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Forecasting edges that will form in a time-evolving graph or inferring missing edges in a static graph is a long-studied problem <ref type="bibr" target="#b22">[23]</ref>, due to its many applications in, for example, information retrieval, recommender systems, bioinformatics, and knowledge graph completion <ref type="bibr" target="#b14">[15]</ref>. Traditional methods for this link prediction problem involve heuristics based on neighborhood statistics, such as predicting the existence of edges whose endpoints have many neighbors in common, or distances involving paths between nodes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. More recent methods have been developed using Graph Neural Networks (GNNs). In some cases, GNNs are used to learn an embedding for each node, and pairs of nodes with similar embeddings are candidate edge predictions <ref type="bibr" target="#b18">[19]</ref>. In others, link prediction is set up as a graph classification task that can be approached with GNNs <ref type="bibr" target="#b34">[35]</ref>.</p><p>All of these link prediction methods depend heavily on the topology of the graph -neighborhood and path-based heuristics are defined by the edges, and GNNs use edges to determine which features to aggregate. In most applications of link prediction, the edges in the graph are considered fixed upfront, though inherent randomness in the edge generation process or different data collection practices can dramatically change an algorithm's performance. For example, in online social networks such as Facebook, a user does not necessarily connect to all of their friends, and some online connections  <ref type="figure">Figure 1</ref>: A sample from a two-block stochastic block model forms a graph with two clusters (p = 3/10, q = 1/30; see Section 3.1). We consider predicting within-cluster edges. If we add the edges with the top common neighbors scores to the graph as a proposal set, the cluster structure is more apparent. This simple augmentation improves the Hits@10 score of the common neighbors link prediction algorithm from 56% to 97%.</p><p>might not be true friends. Or, in protein-protein interaction networks, noisy experimental procedures can extraneously omit or include edges <ref type="bibr" target="#b26">[27]</ref>. While link prediction is in some sense designed to manage these issues, changing the edges in the graph before running a link prediction task in order to, e.g., make the graph more closely reflect a latent network, is plausibly beneficial for improving link prediction as a whole. <ref type="figure">Figure 1</ref> shows a simple example of this scenario, where edges are generated by a simple two-block stochastic block model (SBM). In this case, adding edges within each block (and even between blocks) before running a link prediction algorithm dramatically increases accuracy.</p><p>In this paper, we develop several methods for changing the graph topology as a pre-processing step to improve the performance of link prediction algorithms. Specifically, we focus on simply adding a set of edges, which we call a proposal set. In principle, the proposal set could be given by a human expert, a trained model, or a simple heuristic algorithm. For example, in <ref type="figure">Figure 1</ref>, the proposal set was generated by the common neighbors heuristic. We find that proposal sets lead to substantial improvements in link prediction for both synthetic and real-world data. Similar pre-processing ideas have been used to improve algorithms for other graph problems such as community detection <ref type="bibr" target="#b4">[5]</ref>, where including "missing" edges would naturally make the task easier. In contrast, we demonstrate that effective proposal sets can be constructed much more generally. Good proposal sets can include edges that are missing or ones that might appear in the future, as well as edges that add useful structure for the algorithms. At a high level, adding edges that generally align with the structure of the graph tends to increase the accuracy of link prediction algorithms.</p><p>When determining a proposal set, it is useful to have some prior knowledge on the distribution of the edges that is not necessarily reflected in the original graph. Continuing with the synthetic example in <ref type="figure">Figure 1</ref>, we know that common neighbors are more likely to form within a block, and we used this knowledge to construct an effective proposal set. Other types of prior information or human expertise could also be used. For example, a biologist might add some specific types of protein interactions thought to exist (but missing in the data), with the goal of predicting other types of interactions. In these cases, the proposal set has high precision but low recall due to the large number of possibly existing edges in sparse graphs. Even with low recall, we demonstrate through controlled experiments that such proposal sets can still improve the performance of link prediction methods.</p><p>In many situations, though, we might not have such prior knowledge. For this, we develop a framework called Filter &amp; Rank that bootstraps and refines a broad starting set (e.g., all possible edges that would close a triangle) to form a proposal set. Given a large starting set, the Filter stage keeps only the k highest-scoring edges scored by some link prediction model. The Rank step then trains another link prediction model with the original graph combined with the proposal set to give a final prediction. On many real-world datasets, this framework is able to meaningfully improve the performance of both neighborhood heuristics and GNN-based models on standard benchmarks. More generally, the addition of a proposal set of edges can be used as a pre-processing step to any algorithm. Our approach is simple and straightforward, and the often substantial benefits suggest that the study of more complex methods to generate and use proposal sets is a promising direction of research. To improve benchmarking, we also suggest a new design space for the link prediction problem: given a model or heuristic, how good of a proposal set can be constructed?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Additional related work</head><p>Edge Augmentation for Improving Community Detection. Adding edges as a pre-processing step has been used for improving community detection on inaccurate graphs. For example, one approach is to add edges predicted by link prediction algorithms before running a community detection algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. A more involved approach jointly optimizes edges and community assignments <ref type="bibr" target="#b24">[25]</ref>. The effect of these methods, namely enhancing community structure, hints at a potential to benefit link prediction. This is supported by the "cleaning step" proposed by Sarkar et al. for theoretically analyzing common neighbor algorithms in SBMs <ref type="bibr" target="#b30">[31]</ref>. Specifically, clustering precision can provably be improved by thresholding the node degree over an additional set of ground truth edges. We broaden these general approaches on more diverse graphs for link prediction.</p><p>Semi-supervised Learning, Self-training, and Pseudo-labels. Self-training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> is an instance of semi-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, and involves obtaining extra supervision by using a model's predictions on unlabeled data. One simple method is to use an existing model to generate pseudolabels <ref type="bibr" target="#b21">[22]</ref> ("pseudo" insofar as they are not true labels) for unlabeled data as an auxiliary input for training. Augmenting the graph with a proposal set is related to the pseudo-labeling approach in the sense that both methods ideally label all positive edges correctly. However, a major difference is that we do not supervise on the proposal set, in part because pseudo-labeling can often overfit to incorrect pseudo-labels (referred to as a "confirmation bias") <ref type="bibr" target="#b3">[4]</ref>, and our proposal set may contain edges that we do not actually want to predict ("negative edges"). In contrast, our methods are only used to modify the graph topology, and we empirically demonstrate that some models can tolerate having substantial numbers of negative edges added to the graph.</p><p>Neighborhood Aggregation. A core function of a GNN layer is aggregation <ref type="bibr" target="#b11">[12]</ref>, where each node's representation is combined <ref type="bibr" target="#b13">[14]</ref> with its one-hop neighborhood. Some aggregations can be viewed as weighted edge augmentation, as they incorporate features from a node v into the representation of a node u, for which the edge (u, v) does not exist. For example, multiplying the feature matrix by the jth power of the adjacency matrix averages feature information over j-hop neighborhoods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref>. Other approaches add "virtual" nodes and edges to propagate information over longer distances <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. These are all architectural or algorithmic choices, whereas we focus on modifying the graph topology (by adding edges) to optimize the performance of a given algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposal Sets</head><p>We assume that the input to a link prediction task is a graph G = (V, E), where V is a set of nodes, and E is a set of edges connecting pairs of nodes; we also assume that edges are undirected. Additionally, there may be features associated with the nodes and edges, but this isn't critical to our presentation here. For the purposes of this paper, a link prediction algorithm can be thought of as a scoring function s : V ? V ? R, where s(u, v) &gt; s(w, z) implies that edge (u, v) is more likely to appear (or is more likely to be missing) than edge (w, z). This encapsulates nearly all link prediction methods. An algorithm is evaluated based on how well it can predict a set of test edges T (where T ? E = ?). We call a pair of nodes (i, j) ? T a positive edge and a pair of nodes (k, ) / ? E ? T a negative edge.</p><p>Here, we are interested in modifying the topology of the graph in a way that will improve predictions. Specifically, we want to add a proposal set P to the graph before running a link prediction algorithm. Formally, given an algorithm defined by a scoring function s and a set of test edges T , our problem is</p><formula xml:id="formula_0">maximize P ?[(V ?V )\E] f (s, (V, E ? P ), T ),<label>(1)</label></formula><p>where f (s, (V, E ? P ), T ) is the prediction performance for the input graph (V, E ? P ). In this language, typical deployment of link prediction algorithms is just the special case where P = ?.</p><p>Of course, we do not know the set of test edges T in advance -that is what we want to predict. And even if we did know T , the combinatorial optimization problem in Equation (1) may be difficult. In general, due to randomness in the generation or evolution of a graph, the set of test edges T acts more like a random variable, with a distribution governed by some latent process, conditioned on observing E. We evaluate on a set of edges that represents one realization of this process in a given dataset. The best we can hope to do is predict highly probable edges, and not predict improbable edges.</p><p>Proposal set construction by approximating the test set. We argue that for many link prediction algorithms, a reasonable approach to the optimization problem in (1) is to just have the proposal set P be a prediction for the test edges T , which could be generated by, e.g., another link prediction algorithm. For example, consider a social network where both persons a and b are friends with both c and d. In social networks, a common process by which new edges form is triadic closure <ref type="bibr" target="#b9">[10]</ref>, wherein edges form between two nodes with friends in common. In this case, two triangles might close, through the appearance of the edge (a, b) or the edge (c, d), but we have no reason to expect one to be more likely than the other, and we may have that (a, b) ? T but (c, d) / ? T due to chance. If (c, d) is in our proposal set, though, now the addition of edge (a, b) may look more likely, as they share two friends who are friends themselves, and the addition of (a, b) would lead to a social clique of size four. Link prediction algorithms based on neighborhood heuristics might then be more likely to predict (a, b).</p><p>While the preceding example is somewhat contrived, this is the main idea behind the concrete behavior in <ref type="figure">Figure 1</ref>. There, we have a model for two social communities that are blurred by some edges that go between them. The proposal set adds edges between nodes that have more common neighbors, which happens more often inside the communities. This boosts the signal of the social structure, and a link prediction algorithm based on this new social structure becomes more effective.</p><p>More generally, a particularly helpful proposal set has high precision but low recall -we add "relevant" edges, which guide the link prediction algorithm, though there are many potentially possible relevant edges. At the same time, this is about the best we can expect for any relatively small set of test edges T , due to randomness and the large space of possible edges in sparse graphs. Moreover, we show empirically that this simple method has a high tolerance to the inclusion of negative edges in various types of proposal sets. The intuitive reason is that positive edges in the proposal set often enhance the clusters formed by the large number of known positive edges. However, it is unlikely that negative edges included in a proposal set cluster in an adversarial manner, as there is a small fraction of negative edges present in the proposal set relative to the space of all possible negative edges, provided we have a reasonable enough construction of the proposal set. While our reasoning above was based on neighborhood heuristics for link prediction and social network processes, the ideas apply to other types of algorithms and datasets. For instance, suppose a given link prediction algorithm is based on node embedding similarities produced by a GNN. If our proposal set adds an edge between related nodes, then the aggregation step of the GNN can more effectively use this edge in learning good representations. An edge between two nodes facilitates information flow between the pair, allowing representations to become more similar in feature space. Indeed, we will see that proposal sets can dramatically improve common GNN-based link prediction algorithms on real-world datasets.</p><p>Basic proposal set construction algorithms. In the following sections, we will show that adding a proposal set can substantially increase the accuracy of a variety of link prediction algorithms. In practice, there are many possible proposal sets. Our general strategy for constructing one is to begin with a broad starting set S. The set S will have a number of good candidates for the proposal set, but including all of S would be too computationally demanding for the link prediction algorithm. For this paper, we will use a starting set of all pairs of nodes that have at least one neighbor in common. After, we prune S to form a smaller proposal set P , such that the run time of the link prediction algorithm does not greatly increase when using the set P . The size of the proposal can have a big impact on prediction performance. We call this unknown quantity the target size of the proposal set.</p><p>In some cases, we may have a good sense of how to do the pruning, e.g., based on some underlying knowledge of the graph generation process. For example, in synthetic networks like the SBM sample in <ref type="figure">Figure 1</ref>, we knew that the graph structure would be reinforced by adding edges between nodes with many common neighbors. We will explore this idea for a couple of synthetic social network models in Section 3, and this could be a reasonable approach for social networks generally.</p><p>In many cases, though, we may not have sufficient domain knowledge to form a good proposal set. In such cases, we simply choose a link prediction algorithm defined by another score function s . We then construct the proposal set by taking the k edges in the starting set S with the largest scores given by s . Here, the target size k is a hyperparameter that we optimize. We explore a number of combinations of score functions s and s for several datasets in Section 4.</p><p>Finally, we note that the proposal set can contain edges from the test set, and this happens frequently in our experiments. If (u, v) ? P ? T because s (u, v) is large, then the score s(u, v) can still be computed at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposal Sets for Synthetic Data with Graph Generation Knowledge</head><p>To illustrate the source of our method's improvements, we perform link prediction on two synthetic social networks: a simple two-block SBM <ref type="bibr" target="#b15">[16]</ref> and a triangle-closing growth model <ref type="bibr" target="#b17">[18]</ref>. These are concrete yet tractable examples in which we have some knowledge of the edge generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stochastic Block Model</head><p>In our two-block SBM, we begin with 100 nodes equally partitioned into two "blocks" of 50 nodes each, as in <ref type="figure">Figure 1</ref>. For every pair of nodes, if the two nodes are in the same block, an edge is formed with probability p; otherwise, the nodes are in different blocks, and an edge is formed with probability q ? p. The positive and negative validation and test edges are within-block and between-block edges, sampled uniformly from edges that do not appear in the sampled graph. Although highly accurate predictions could be made with algorithms that recover the latent block structure <ref type="bibr" target="#b0">[1]</ref>, our goal here is to demonstrate the usefulness of proposal sets in a controlled and understandable setting.</p><p>As discussed in the previous section, a natural proposal set P for this graph is the one that consists of the k pairs of nodes with the largest number of common neighbors in the graph. We use the common neighbors heuristic for the score functions:</p><formula xml:id="formula_1">s(u, v) = s (u, v) = |?(u) ? ?(v)|, where ?(w)</formula><p>is the set of neighbors of node w in the graph, i.e., the same method is used for link prediction and pruning the starting set. For prediction performance, we use the Hits@k metric to evaluate how well a model ranks positive test edges P test higher than negative test edges N test <ref type="bibr" target="#b16">[17]</ref>. Specifically, every edge in P test ? N test is assigned a score, and we measure the percentage of positive edges that are ranked above the K-th highest scoring negative edge (higher is better). We sample graphs with p = x/3 and q = (1 ? x)/3, for x ? {0.5, 0.6, 0.7, 0.8, 0.9} which correspond to a ratios p/q of 1.0, 1.5, 2.33, 4.0, 9.0. The ratio p/q ratio measures how strong the individual block cluster structure is, with p/q = 1 having no cluster structure for individual blocks. The number of validation and test samples are taken to maintain an 80%/10%/10% train/validation/test split. <ref type="figure" target="#fig_1">Figure 2</ref> plots the performance for varying proposal set sizes and several ratios. For each configuration, we also select a target size k based on the most accurate algorithm on the validation set. When the ratio of p to q is large enough, we see major performance improvements from the proposal set. Furthermore, the optimal proposal set size in the validation set roughly matches the optimal proposal set size for the test set.   <ref type="bibr" target="#b17">[18]</ref>, with and without a proposal set. The proposal set consists of the k edge that close the most triangles, with k selected by performance on the validation set. The proposal set boosts performance and is particularly helpful for a GNN-based algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triangle-closing Growth Model</head><p>Next, we construct a synthetic growing social network based on Jin et al. <ref type="bibr" target="#b17">[18]</ref> with 2000 nodes. With decreasing probability, at each growth step, edges are (1) added to close a triangle (friendship between those with mutual friends), (2) added uniformly (friendship with a stranger), or (3) deleted proportional to node degree (end of a friendship). (In the notation of the original paper, the parameters are: r 1 = 2.0 controls the rate at which triangles are closed, r 0 = 0.0005 controls the rate of uniform edges being added, and ? = 0.005 controls the rate at which uniform edges are deleted, and z * = 5 is the minimum degree required before edge deletion occurs). We run the generation process for 30,000 iterations and record the iteration for the creation of each edge, using 80%/10%/10% train/validation/test splits temporally based on iterations.</p><p>We use an open-source implementation for dataset generation. <ref type="bibr" target="#b1">2</ref> Since we know the graph generation process, and new edges are due to triadic closure, we again use the common neighbors heuristic for generating the proposal set. For the link prediction algorithm s, we also use common neighbors, as well as a GNN-based model, where a SAGE GNN <ref type="bibr" target="#b12">[13]</ref> learns embeddings and then an MLP computes pairwise similarities from the embeddings.</p><p>Again, augmenting the graph with a proposal set can meaningfully improve performance <ref type="table" target="#tab_0">(Table 1)</ref>. For the social network, <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the added edges from the proposal set and corresponding predictions. The edges from the proposal set generally reinforce the existing structure in the graph, forming tighter clusters, although many edges in the proposal set do not actually exist in the test set. For prediction by common neighbor scores, performance improves due to denser subgraphs surrounding positive edges. For the SAGE GNN model, proposal edges additionally boost performance by encouraging propagation of node representations between positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposal Sets for Empirical Data</head><p>In this section, we experiment with constructing proposal sets for real-world datasets without relying on domain knowledge. As described above, we begin with a broad starting set S, consisting of all pairs of nodes that have at least one common neighbor. From this starting set, we use a link prediction method s , taking the top k scoring edges from S to be the proposal set P . The set P is used by the link prediction model s, as in Section 2, to perform the final link prediction. We call s a filtering model and s a ranking model. Together, we call this pipeline Filter &amp; Rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To demonstrate the efficacy of our method, we experiment on a wide range of datasets ( <ref type="table" target="#tab_2">Table 2)</ref>. We treat the edges as undirected in all datasets, even though some have natural directions. The ogbl-ddi dataset is a drug-drug interaction graph, and there are no given input features. For social networks, ogbl-collab is a dataset of coauthorship on academic publications, where node features come from word embeddings of publications by an author; snap-email is formed from emails within a European research institution, and does not have given input features; and twitch-DE is a friendship network of users on the game streaming platform Twitch, where node features come from user behavior. For information networks, snap-reddit represents hyperlinks between subreddits on the social news aggregation site Reddit, which has node embeddings generated from a user-subreddit interaction graph; and fb-page is constructed from page-to-page links of verified Facebook sites and has node features based on descriptions of the sites.</p><p>The ogbl-collab, snap-email, and snap-reddit datasets have timestamps, and we split the edges into train/validation/test in the same way as the previous section. For the twitch-DE and fb-page datasets, we use random splits. The ogbl-ddi datasets has splits based on protein targets from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b16">[17]</ref>. For all datasets, negative validation and test edges are uniformly sampled, and are disjoint with each other and the positive edges. The number of negative validation and test edges are equal to the number of positive validation and test edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms and experimental setup</head><p>Link prediction algorithms. We use link prediction algorithms for both generating the proposal set from the starting set (filtering step) and for the final link predictions (ranking step). For nonparametrized link predictors, we use two widely-known neighborhood heuristics: the aforementioned common neighbor score |?(u) ? ?(v)| and the Adamic-Adar index x??(u)??(v) 1/ log(|?(x)|) <ref type="bibr" target="#b1">[2]</ref>.</p><p>For representative GNN-based link prediction models, we use GCN <ref type="bibr" target="#b19">[20]</ref> and SAGE <ref type="bibr" target="#b12">[13]</ref>. These models learn an embedding for each node and then we use an MLP to compute a pairwise similarity for each node pair using the two embeddings as input. Both models are trained end-to-end, supervised only on the training set. Implementations are from the OGB leaderboard, but we sometimes modify the documented hyperparameters for improved performance.</p><p>Finally, we use a new neighborhood heuristic that we call Cosine Similarity Common Neighbor (cos-common) scores, given by</p><formula xml:id="formula_2">s(u, v) = x??(u)??(v) h u h x h u 2 h x 2 ? h x h v h x 2 h v 2 ,</formula><p>where h z is the feature vector for node z. This weights edges by cosine similarity of node features and then weights the common neighbor by the product of these weights.</p><p>The ogbl-ddi and snap-email datasets do not have given node features. For these two datasets and the GNN-based models, we use learnable initial feature embeddings (i.e., a one-hot encoding input feature). For ogbl-ddi and cos-common, we use a 256-dimensional spectral embedding of the graph (first 256 eigenvectors of the graph Laplacian) as input features. For snap-email on cos-common, we use both the spectral embedding and the learned initial feature embedding. (For other datasets and cos-common, learned initial feature embeddings were too computationally expensive.)</p><p>Training. For models with learnable parameters, training is based on predicting whether a pair of nodes is an edge or not. We loop over the set of "positive" training edges (edges in the graph not used for validation or testing) in batches. Within each batch, we sample an equal number of "negative" non-training edges at random. For GCN and SAGE, the negative training edges are sampled uniformly. For cos-common, a negative training edge is sampled uniformly from those edges that share an end point with the corresponding positive edge. Supervision comes from the cross-entropy loss on these positive and negative edges.</p><p>The validation edges consists of an equal number of positive and negative edges. Model performance on the validation edges in terms of Hits@k is used to select model parameters.</p><p>We use a NVIDIA GeForce RTX 2080 Ti (11 GB) to train our models. Training typically takes less than one hour for each trial on the larger datasets and takes much less time on the smaller datasets.</p><p>Searching for the target size. The target size k (size of proposal set) is selected based on prediction performance on the set of validation edges. To efficiently search for an optimal target size k, we constrain its possible values to be around the total numberk of positive edges in the validation and test set, since we aim for the proposal set P to be similar to the set of edges on which we evaluate our model. Even though the test size might not be known in some cases, this number can be estimated in many practical settings. For large datasets (ogbl-collab, ogbl-ddi, snap-reddit), we search over k ? {k ? 20000,k ? 10000, . . . ,k + 20000}. For the remaining datasets, the search is over k ? {k ? 3000,k ? 2000, . . . ,k + 3000}.</p><p>Inference and evaluation. We include validation edges during test inference only for datasets that are temporally split (ogbl-collab, snap-email, snap-reddit), matching common evaluation techniques <ref type="bibr" target="#b16">[17]</ref>. For all experiments, we perform five random trials and report the mean and standard deviation of the Hits@k performance metric listed in <ref type="table" target="#tab_2">Table 2</ref>. In Appendix A, we have additional results where we force the positive validation edges to be in the proposal set during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We benchmark the performance of various combinations of link prediction algorithms in our Filter &amp; Rank framework against baselines (i.e., where the proposal set is empty) for the ranking methods <ref type="table" target="#tab_3">(Table 3</ref>). Filter &amp; Rank can substantially improve over baseline performance across the datasets. For example, there are &gt; 20% improvements for SAGE on ogbl-ddi, and using common neighbors for filtering with Adamic-Adar for ranking achieves 65.28% Hits@50 on ogbl-collab, which is better than the top score on the OGB leaderboard, as of June 29, 2021.</p><p>From <ref type="table" target="#tab_3">Table 3</ref>, we can see that Filter &amp; Rank can improve upon certain baseline ranking models by adding a proposal set generated by a filtering model. Parameterized ranking methods generally see improvements from at least one generated proposal set, likely due to their learning capacity. For neighborhood heuristic ranking methods, we see improvement from those proposal sets that align with the underlying graph generation rule, e.g., ogbl-collab, which as a citation network, improves from the proposal set generated by Adamic-Adar. On the other hand, neighborhood heuristics do not improve on ogbl-ddi, snap-reddit, and twitch-DE, since these methods perform poorly even as baselines without a proposal set.  Finally, we analyze how good proposal set bring test nodes closer together, in a similar manner to <ref type="figure">Figure 1</ref>. We are motivated by recent work from Alon and Yahav <ref type="bibr" target="#b2">[3]</ref>, who showed that edge rewiring in GNNs can help overcome the challenge of propagating information in networks over a long path of edges. Following this, we can infer why proposal sets improve the performance of GNNs by measuring the decrease in distance between nodes in test edges, while varying proposal set size.</p><p>To measure distance between nodes, we use the commute time between two nodes i and j, i.e., the expected number of steps that a random walk on the graph would take to go from i to j and back to i again. For an undirected graph, this is equal to 2m (M ii + M jj ? 2M ij ), where M is the Moore-Penrose pseudoinverse of the graph Laplacian L and m is the number of edges. This quantity involves both the number of paths between i and j and their lengths, capturing a notion similar to message-passing propagation in GNNs. Corroborating our intuition, <ref type="figure" target="#fig_4">Figure 4</ref> shows that the average commute time between two nodes in a positive edge decreases at a much faster rate than that of two nodes in a negative edge as we increase the size of the proposal set. This implies that the proposal set edges allow for the two nodes in a positive edge to more frequently encounter each other's feature information, relative to two nodes in a negative edge. This could help in a pairwise node similarity computation since the features of two nodes in a positive edge can more easily be combined.</p><p>Finally, we also evaluate our search over the target sizes. <ref type="figure">Figure 5</ref> shows that, for SAGE on ogbl-ddi, the selected proposal set size from the validation is close to the best size in terms of test performance. On ogbl-collab, the performance of Adamic-Adar progressively increases as the proposal set size approachesk, and decreases shortly afterwards, matching our intuition. Interestingly, using GCN and SAGE as filtering models on ogbl-ddi produces bimodal validation and test curves. This is likely a result of both models scoring validation edges higher than test edges, due to the distribution shift between validation and test edges. Our guided hyperparameter search allows us to discover this novel type of protein binding in the test set, as represented by the second peak aroundk. Hits@50 <ref type="figure">Figure 5</ref>: Validation and test performances of SAGE on ogbl-ddi (left) and Adamic-Adar on ogblcollab (right) as ranking models, while varying the size of the proposal sets generated by various filtering models. The yellow highlighted regions are the search range for the target size. The star (*) denotes the final test score, which is determined by using the proposal set corresponding to the highest validation score within our search range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposal Sets of Varying Quality</head><p>Finally, we evaluate the performance of ranking models using proposal sets of varying quality that we control. Here, we use the ratio of negative edges to positive edges as a proxy for quality, with smaller ratios (more positive edges) being of higher quality. (We further measure these ratio in a relative sense for a given dataset, normalizing by the "ground truth" ratio of ( n 2 ? m)/m, where n is the number of nodes and m is the total number of edges.) However, rather than having negative edges that are likely informative, which was the case above when using link prediction algorithms to form proposal sets, negative edges will be included uniformly at random from all node pairs not in E ? T . Intuitively, a smaller ratio reflects a higher quality proposal set, provided the set is reasonably large, and a proposal set containing all positive edges is the highest quality.</p><p>We consider two settings. In the first setting, we initialize the proposal set as the set of all positive test edges and increasingly add negative edges, progressively reducing proposal set quality while increasing the proposal set size. In the second, we initialize the proposal set in the same way but maintain its size as we reduce quality by incrementally replacing positive edges with negative ones. We experiment on ogbl-ddi and snap-email with GCN and SAGE, using the same setup as in Section 4. <ref type="figure">Figures 6 and 7</ref> show the results for the first and second settings. As expected, model performance is improved substantially by using an initially high quality proposal set, and the observed improvement gradually diminishes as the proposal set deteriorates in quality. The trends vary across datasets and models. In the first setting ( <ref type="figure">Figure 6</ref>), we observe that SAGE, relative to GCN, demonstrates significantly greater tolerance to noisy proposal sets across both datasets. This may be due to how SAGE learns distinct weight matrices to separately update individual and neighborhood-aggregated node embeddings. In the second setting <ref type="figure">(Figure 7)</ref>, the performance of both models degrade at a similar rate as the numbers of positive edges in the proposal set decrease.</p><p>Furthermore, for both models we see a more substantial improvement over the baseline performance on ogbl-ddi compared to snap-email. On ogbl-ddi, including a proposal set generally boosts performance as long as the proposal set has a negative-to-positive edge quality ratio lower than the ground truth graph's negative-to-positive edge quality ratio (i.e., the relative value is less than one). However, this is not the case with snap-email, especially for GCN. One possible explanation is that compared to snap-email, ogbl-ddi has a larger distribution shift between train/validation and test splits (recall that snap-email has splits based on time, whereas ogbl-ddi has splits based on protein target). Thus, a high quality proposal set is a greater source of improvement on ogbl-ddi. Overall, our method benefits in particular when the link prediction model has a large learning capacity, is tolerant to noise, and utilizes a high quality proposal set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated that adding a proposal set of edges to a graph as a pre-processing step can improve the performance of both neighborhood heuristics and graph neural network-based link prediction algorithms across various synthetic and real-world datasets. Our experiments demonstrate that this method's performance and limitations can be intuitively explained by the choice of proposal set and ranking model. Future work could focus on generating better proposal sets and improving their utilization, developing models that are tolerant to proposal sets of varying quality, and incorporating proposal sets into other tasks (e.g., node prediction) for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Positive Validation Edges in Proposal Set</head><p>In most cases, the validation edges would be known at inference time. Thus, we repeat the experiment setup in Section 4, but we score the positive validation edges above all other edges in the proposal set to ensure their inclusion. All other hyperparameters and settings remain the same. The results are in <ref type="table">Table 4</ref>, and they differ over the datasets. For example, certain scores on ogbl-collab, snap-email, twitch-de, fb-page improve from the incorporation of validation labels in the proposal set.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>= 9.0 p/q = 4.0 p/q = 2.33 p/q = 1.5 p/q = 1.0 Performance on two-block SBM using common neighbor scores for varying target sizes. Solid and dashed lines are test and validation results. Dots and stars are the highest validation and final test scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Left: A community in the network and edge predictions performed via common neighbor scores (no proposal set). Right: Predictions in the same community with a proposal set, which produces an additional correct prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Percentage change in average commute time as the size of generated proposal sets vary. The solid lines correspond to the decrease in commute time of positive edges, and the dotted lines to that of negative edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Relative Negative to Positive Edges Performance of GCN and SAGE on ogbl-ddi (left) and snap-email (right) using proposal sets with varying relative ratios of negative-to-positive edge quantities, where the number of positive edges in the proposal set is fixed. The bottom row is the same data as the top, just zoomed in to a smaller range of quality ratios. Proposal set quality decreases going right in the plots, and performance generally declines as quality deteriorates. Performance of GCN and SAGE on ogbl-ddi (left) and snap-email (right) using proposal sets with varying relative ratios of negative-to-positive edge quantities, where the proposal set size is fixed to be the total number of positive test edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 :</head><label>4</label><figDesc>Performance over five trials of Filter &amp; Rank, compared to baselines (empty proposal sets), with the positive validation edges guaranteed to all be included in the proposal sets. The intensity of the blue color indicates the relative improvement over the baseline, normalized by the maximum improvement over the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hits@10 scores for link prediction algorithms used on a synthetic growing social network</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary statistics of real-world datasets</figDesc><table><row><cell>Datasets</cell><cell>Nodes</cell><cell cols="3">Edges Node Features Data Split</cell><cell>Metric</cell></row><row><cell>ogbl-ddi [17]</cell><cell cols="2">4,267 1,334,889</cell><cell>No</cell><cell cols="2">Protein target Hits@20</cell></row><row><cell cols="3">ogbl-collab [17] 235,868 1,285,465</cell><cell>Yes</cell><cell>Time</cell><cell>Hits@50</cell></row><row><cell>snap-email [29]</cell><cell>986</cell><cell>24,929</cell><cell>No</cell><cell>Time</cell><cell>Hits@20</cell></row><row><cell>snap-reddit [21]</cell><cell>30,744</cell><cell>277,041</cell><cell>Yes</cell><cell>Time</cell><cell>Hits@50</cell></row><row><cell>twitch-DE [30]</cell><cell>9,498</cell><cell>153,138</cell><cell>Yes</cell><cell>Random</cell><cell>Hits@50</cell></row><row><cell>fb-page [30]</cell><cell>22,470</cell><cell>171,002</cell><cell>Yes</cell><cell>Random</cell><cell>Hits@20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance over five trials of Filter &amp; Rank, compared to baselines (no edges added, i.e., empty proposal set). The intensity of the blue color indicates the relative improvement over the baseline, normalized by the maximum improvement over the dataset. ?3.48 76.32 ?2.54 16.68 ?0.00 16.85 ?0.00 5.40 ?0.00 SAGE 61.79 ?3.82 70.68 ?1.89 17.48 ?0.00 18.75 ?0.00 5.11 ?0.00 Common 23.27 ?1.76 35.70 ?2.22 11.73 ?0.00 12.15 ?0.00 9.16 ?0.00 Adamic-Adar 23.61 ?1.19 34.48 ?2.47 11.62 ?0.00 11.94 ?0.00 9.37 ?0.00 Cos-Common 13.32 ?1.14 17.59 ?0.92 24.07 ?0.00 24.38 ?0.00 0.00 ?0.00 None (Baseline) 41.40 ?8.70 52.68 ?11.8 17.73 ?0.00 ?0.46 60.25 ?0.64 62.93 ?0.00 64.75 ?0.00 63.62 ?0.00 SAGE 60.48 ?0.40 59.89 ?0.78 62.36 ?0.00 64.69 ?0.00 63.75 ?0.00 Common 60.79 ?0.45 60.30 ?0.63 62.78 ?0.00 65.28 ?0.00 62.69 ?0.00 Adamic-Adar 59.87 ?0.34 60.03 ?0.87 62.84 ?0.00 65.23 ?0.00 62.75 ?0.00 Cos-Common 60.97 ?0.78 60.18 ?0.80 62.30 ?0.00 64.86 ?0.00 63.18 ?0.00 None (Baseline) 60.05 ?0.50 60.41 ?0.72 61.37 ?0.00 64.17 ?0.00 63.19 ?0.00 snap-email GCN 50.73 ?3.25 47.00 ?2.51 40.55 ?0.00 49.70 ?0.00 35.58 ?3.33 SAGE 51.43 ?2.32 41.98 ?3.34 42.36 ?0.00 46.21 ?0.00 32.33 ?4.36 Common 47.36 ?2.38 44.22 ?2.84 28.28 ?0.00 32.69 ?0.00 31.10 ?3.80 Adamic-Adar 54.10 ?1.58 43.54 ?1.93 29.60 ?0.00 36.34 ?0.00 24.70 ?7.38 Cos-Common 53.10 ?3.24 43.52 ?4.42 44.52 ?0.00 47.77 ?0.00 34.62 ?4.03 None (Baseline) 54.65 ?3.13 45.33 ?4.53 40.87 ?0.00 46.73 ?0.00 29.56 ?4.89 snap-reddit GCN 50.15 ?1.20 45.48 ?1.57 34.80 ?0.00 40.21 ?0.00 40.50 ?0.00 SAGE 49.70 ?1.08 46.08 ?0.84 35.26 ?0.00 39.99 ?0.00 39.58 ?0.00 Common 50.15 ?0.94 47.48 ?1.44 33.90 ?0.00 38.12 ?0.00 37.12 ?0.00 Adamic-Adar 50.55 ?1.72 45.64 ?1.82 33.76 ?0.00 38.18 ?0.00 38.36 ?0.00 Cos-Common 49.15 ?1.49 45.70 ?1.99 34.32 ?0.00 37.69 ?0.00 35.64 ?0.00 None (Baseline) 51.31 ?2.11 45.17 ?1.85 38.60 ?0.00 43.60 ?0.00 42.63 ?0.00 twitch-DE GCN 32.37 ?0.41 29.14 ?1.52 21.73 ?0.00 24.28 ?0.00 21.44 ?0.00 SAGE 31.35 ?0.90 28.88 ?0.88 17.36 ?0.00 20.85 ?0.00 17.60 ?0.00 Common 31.46 ?0.69 27.54 ?1.07 19.28 ?0.00 20.86 ?0.00 19.62 ?0.00 Adamic-Adar 31.48 ?0.29 28.78 ?1.40 19.75 ?0.00 22.43 ?0.00 20.78 ?0.00 Cos-Common 33.06 ?1.42 28.52 ?0.63 19.86 ?0.00 21.89 ?0.00 20.00 ?0.00 None (Baseline) 32.09 ?1.33 26.38 ?0.87 23.00 ?0.00 26.95 ?0.00 26.69 ?0.00 fb-page GCN 71.14 ?1.84 64.77 ?1.35 61.24 ?0.00 69.74 ?0.00 65.56 ?0.00 SAGE 71.71 ?2.70 64.06 ?4.14 63.11 ?0.00 71.16 ?0.00 65.02 ?0.00 Common 70.76 ?2.36 64.45 ?1.62 54.97 ?0.00 66.87 ?0.00 61.48 ?0.00 Adamic-Adar 69.37 ?3.15 63.59 ?1.94 56.62 ?0.00 67.48 ?0.00 62.75 ?0.00 Cos-Common 70.82 ?3.14 61.96 ?2.93 54.32 ?0.00 67.62 ?0.00 64.53 ?0.00 None (Baseline) 70.44 ?0.56 64.42 ?4.23 60.43 ?0.00 71.73 ?0.00 66.53 ?0.00</figDesc><table><row><cell>Ranking Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jbn/jin_et_al_2001</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was supported in part by ARO Award W911NF19-1-0057, ARO MURI, NSF Award DMS-1830274, and JP Morgan Chase &amp; Co.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Community detection with edge augmentation in criminal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bahulkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Szymanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Baycik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Link-prediction enhanced consensus clustering for complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving network community structure with link prediction ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bahulkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuzmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Szymanski</surname></persName>
		</author>
		<editor>CompleNet</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Networks, Crowds, and Markets: Reasoning about a Highly Connected World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SIGN: Scalable inception graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Survey of Link Prediction in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="243" to="275" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure of growing social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46132</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Community interaction and conflict on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structural inference for uncertain networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12306</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparative assessment of large-scale data sets of protein-protein interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Mering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Snel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="399" to="403" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clustering and preferential attachment in growing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25102</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-Scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The consistency of common neighbors for link prediction in stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3016" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>57.03 ?3.56 71.37 ?4.60 17.04 ?0.00</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
