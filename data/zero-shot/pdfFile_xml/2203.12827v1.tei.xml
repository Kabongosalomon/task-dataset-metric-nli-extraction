<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Instance Activation for Real-Time Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyu</forename><surname>Chen</surname></persName>
							<email>shaoyuchen@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
							<email>wqzhang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
							<email>chang.huang@horizon.aizhaoxiang.zhang@ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Instance Activation for Real-Time Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a conceptually novel, efficient, and fully convolutional framework for real-time instance segmentation. Previously, most instance segmentation methods heavily rely on object detection and perform mask prediction based on bounding boxes or dense centers. In contrast, we propose a sparse set of instance activation maps, as a new object representation, to highlight informative regions for each foreground object. Then instance-level features are obtained by aggregating features according to the highlighted regions for recognition and segmentation. Moreover, based on bipartite matching, the instance activation maps can predict objects in a one-to-one style, thus avoiding non-maximum suppression (NMS) in post-processing. Owing to the simple yet effective designs with instance activation maps, SparseInst has extremely fast inference speed and achieves 40 FPS and 37.9 AP on the COCO benchmark, which significantly outperforms the counterparts in terms of speed and accuracy. Code and models are available at https://github. com/hustvl/SparseInst.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation aims to generate instance-level segmentation for each object in an image. Based on the advances in deep convolutional neural networks and object detection, recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref> have made tremendous progress in instance segmentation and achieved impressive results on large-scale benchmarks, e.g., COCO <ref type="bibr" target="#b24">[25]</ref>. However, developing real-time and efficient instance segmentation algorithms is still challenging and urgent, especially for autonomous driving and robotics.</p><p>Prevalent methods tend to adopt detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref> to localize instances first and then segment through region-based ? Xinggang Wang is the corresponding author. convolutional networks <ref type="bibr" target="#b14">[15]</ref>, dynamic convolutions <ref type="bibr" target="#b35">[36]</ref>, etc. Those methods are conceptually intuitive and achieve great performance. However, when it comes to real-time instance segmentation, those methods suffer from some limitations. Firstly, most methods employ dense anchors (centers) to localize and then segment objects, e.g., more than 5456 instances (given 512 ? 512 input) in CondInst <ref type="bibr" target="#b35">[36]</ref>, which incur lots of redundant predictions and much computation burden. Besides, the receptive field of each pixel is limited and the contextual information is insufficient if we densely localize objects by centers or anchors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. Secondly, most methods require multi-level prediction to handle the scale variation of natural objects, which inevitably increases the latency. Region-based methods <ref type="bibr" target="#b14">[15]</ref> apply RoI-Align to acquire region features, making it difficult to deploy algorithms to edge/embedded devices. Finally, the post-processing also requires attention since the sorting and NMS as well as processing masks are time-consuming, es-  pecially for dense predictions. It's worth noting that even improved NMS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref> still takes ? 2ms, 10% of total time.</p><p>In this paper, we present a new highlight to segment paradigm for real-time instance segmentation. Instead of using boxes or centers to represent objects, we exploit a sparse set of instance activation maps (IAM) to highlight informative object regions, which is motivated by CAM <ref type="bibr" target="#b48">[49]</ref> widely used in weakly-supervised object localization. Instance activation maps are instance-aware weighted maps and instance-level features can be directly aggregated according to the highlighted regions. Then, recognition and segmentation are performed based on the instance features. <ref type="figure" target="#fig_2">Figure 2</ref> compares region-based, center-based, and IAM-based representations. In comparison, IAM has the following advantages: (1) it highlights discriminative instance pixels, suppresses obstructive pixels, and conceptually avoids the incorrect instance feature localization problems in center-/region-based methods; (2) it aggregates instance features from the whole image and offers more contexts; (3) computing instance features with activation maps is rather simple without extra operation like RoI-Align <ref type="bibr" target="#b14">[15]</ref>. However, different from previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> using spatial priors (i.e., anchors and centers) to assign targets, instance activation maps are conditioned on the input and arbitrary for different objects and it is infeasible to assign targets with hand-crafted rules for training. To address that, we formulate the label assignment for instance activation maps as a bipartite matching problem, which is recently proposed in DETR <ref type="bibr" target="#b3">[4]</ref>. Specifically, each target will be assigned to an object prediction as well as its activation map through Hungarian algorithm <ref type="bibr" target="#b30">[31]</ref>. During training, the bipartite matching facilitates the instance activation maps to highlight individual objects and inhibit the redundant predictions, thus avoiding NMS during inference.</p><p>Further, we materialize this paradigm and propose Spar-seInst, an extremely simple but efficient method for instance segmentation. SparseInst adopts single-level prediction and consists of a backbone to extract image features, an encoder to enhance the multi-scale representation for single-level features, and a decoder to compute the instance activation maps, perform recognition and segmentation, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. SparseInst is a pure and fully convolutional framework and independent from detectors. Benefiting from the facts: (1) the sparse predictions through the instance activation maps; (2) single-level prediction; (3) compact structures; (4) simple post-processing without NMS or sorting, SparseInst has extremely fast inference speed and achieves 37.9 mask AP on MS-COCO test-dev with 40.0 FPS on one NVIDIA 2080Ti GPU, outperforming most state-ofthe-art methods for real-time instance segmentation. Given 448? input, SparseInst achieves 58.5 FPS with competitive accuracy, which is faster than previous methods. We hope the proposed SparseInst can serve as a general framework for (real-time) end-to-end instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>According to object representations, existing methods for instance segmentation can be divided into two groups, i.e. region-based methods and center-based methods.</p><p>Region-based Methods. Region-based methods rely on object detectors, e.g., Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, to detect objects and acquire bounding boxes, and then apply RoI-Pooling <ref type="bibr" target="#b29">[30]</ref> or RoI-Align <ref type="bibr" target="#b14">[15]</ref> to extract region features for pixel-wise segmentation. Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, as the representative method, extends Faster R-CNN by adding a mask branch to predict masks for objects and offers a strong baseline for end-to-end instance segmentation. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref> address the low-quality segmentation and coarse boundaries arising in Mask R-CNN and present several approaches to refine the mask predictions for high-quality masks. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> exploit cascade structures to progressively improve the object localization for more accurate mask prediction.</p><p>Center-based Methods. Recently, many approaches employ the single-stage detectors, especially the anchor-free detectors <ref type="bibr" target="#b36">[37]</ref>. These approaches represent objects by center pixels instead of bounding boxes and segment using the center features. Several methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> explores the object contours but show some limitations for objects having hollows or multiple parts. YOLACT <ref type="bibr" target="#b1">[2]</ref> generates instance masks by the assembly of mask coefficients and prototype masks. MEInst <ref type="bibr" target="#b45">[46]</ref> and CondInst <ref type="bibr" target="#b35">[36]</ref> extend FCOS <ref type="bibr" target="#b36">[37]</ref> by predicting the encoded mask vector or mask kernels for dynamic convolution <ref type="bibr" target="#b7">[8]</ref> respectively. SOLO <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, as a detector-free method, yet localize and recognize objects by centers as well as generating the mask kernels. The proposed SparseInst exploits sparse instance activation maps to represent objects with a simple pipeline and high efficiency.</p><p>Bipartite Matching for Object Detection. The bipartite matching has been widely explored for end-to-end object detection <ref type="bibr">[4, 31-34, 39, 51]</ref>, which avoids NMS in postprocessing. Recently, SOLQ <ref type="bibr" target="#b10">[11]</ref> and ISTR <ref type="bibr" target="#b17">[18]</ref> exploit the mask encodings for instance segmentation. QueryInst <ref type="bibr" target="#b13">[14]</ref> extends <ref type="bibr" target="#b33">[34]</ref> by adding dynamic mask heads. Besides, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref> employ transformers with instance and semantic queries to obtain panoptic segmentation results. However, our method aiming at fast speed is motivated by the instance activation maps as object representation for instancelevel recognition and segmentation. And the concise yet effective representation drives the framework rather fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first investigate the instance activation maps for representing objects. Then we present a novel framework which exploits the sparse set of instance activation maps to highlight objects and aggregate instance features for instance-level recognition and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instance Activation Maps</head><p>Formulation. Intuitively, instance activation maps are instance-aware weighted maps which aim to highlight the informative regions for each object. And the features from the highlighted regions are semantically abundant and instance-aware for both recognizing and separating objects. Therefore, we directly aggregate the features according to the activation maps as the instance features. Given the input image features X ? R D?(H?W ) , instance activation maps can be formulated as:</p><formula xml:id="formula_0">A = F iam (X) ? R N ?(H?W ) ,</formula><p>where A is the sparse set of N instance activation maps and F iam (?) is a simple network with a sigmoid non-linearity. Then we can obtain the sparse set of instance features by gathering distinctive information from the input feature maps X with the instance activation maps through: z =? ? X T ? R N ?D , where z = {z i } N are the feature representations for N potential objects in the image and? is normalized to 1 for each instance map. The sparse instanceaware features {z i } N are straightforwardly used for consequent recognition and instance-level segmentation.</p><p>Learning Instance Activations. Instance activation maps don't exploit explicit supervisions, e.g., instance masks, for learning to highlight objects. Essentially, the subsequent modules for recognition and segmentation provide instance activation maps with indirect supervisions, which encourage the F iam to discover informative regions. Additionally, the supervisions are instance-aware due to the bipartite matching, which further enforces the F iam to discriminate objects and activate only one object per map. Consequently, the proposed instance activation maps are capable to highlight discriminative regions for individual objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SparseInst</head><p>As illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>, SparseInst is a simple, compact, and unified framework which consists of a backbone network, an instance context encoder, and an IAM-based decoder. The backbone network, e.g., ResNet <ref type="bibr" target="#b15">[16]</ref>, extracts multi-scale features from the given image. The instance context encoder is attached to the backbone to enhance more contextual information and fuse the multi-scale features. For faster inference, the encoder outputs singlelevel features of <ref type="bibr">1 8</ref> ? resolution w.r.t. the input image, and the features will be fed to subsequent IAM-based decoder to generate instance activation maps to highlight foreground objects for classification and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance Context Encoder</head><p>Objects in natural scenes tend to have wide range of scales, which is prone to degrade the performance of detectors. Most approaches adopt multi-scale feature fusions, e.g., feature pyramids <ref type="bibr" target="#b22">[23]</ref>, and multi-level prediction to facilitate the recognition for objects of different scales. Nevertheless, using multi-level pyramidal features increase the computation burden, especially for detectors using heavy heads <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref>, as well as producing amounts of duplicate predictions. Conversely, our method aiming at faster inference leverages single-level prediction. Considering the limitations of the single-level features for objects of various scales, we reconstruct the feature pyramid networks and present an instance context encoder, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. The instance context encoder adopts a pyramid pooling module <ref type="bibr" target="#b47">[48]</ref> after C 5 to enlarge the receptive fields and fuses features from P 3 to P 5 to further enhance the multiscale representations for the output single-level features. <ref type="figure" target="#fig_4">Figure 3</ref> illustrates the IAM-based segmentation decoder which contains an instance branch and a mask branch. The two branches are composed of a stack of 3 ? 3 convolutions with 256 channels. The instance branch aims to generate instance activation maps and N instance features for recognition and instance-aware kernel. The mask branch is designed to encode instance-aware mask features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">IAM-based Segmentation Decoder</head><p>Location-Sensitive Features. Empirically, objects are localized in different positions and the spatial locations can be used as cues to distinguish instances. Hence, we construct two-channel coordinate features which consists of normalized absolute (x, y) coordinates of spatial locations, which is similar to CoordConv <ref type="bibr" target="#b25">[26]</ref>. Then we concatenate the output features from the encoder with coordinate features to enhance the instance-aware representation.</p><p>Instance Activation Maps F iam . We adopt a simple yet effective 3?3 convolution with sigmoid as the vanilla F iam , which highlights each instance with a single activation map. Accordingly, instance features {z i } are obtained through activation maps, in which each potential object is encoded into a 256-d vector. Then three linear layers are applied for classification, objectness score, and mask kernel {w i } N .   Given the input image, the backbone extracts the multi-scale image features (i.e., {C3,C4,C5}). The encoder employs pyramid pooling module (PPM) <ref type="bibr" target="#b47">[48]</ref> to enlarge the receptive field and fuses the multi-scale features. '4?'or '2?' denote the upsampling by a factor 4 or 2. The IAM-based decoder consists of two branches, i.e. an instance branch and a mask branch. In the instance branch, the 'IAM' module predicts the instance activation maps (shown in the right column) to acquire the instance features {zi} N for recognition and mask kernels. The mask branch aims to provide mask features M and will be multiplied with the predicted kernels to generate segmentation masks.</p><p>Further, to obtain fine-grained instance features, we present the group instance activation maps (Group-IAM) to highlight a groups of regions for each object, i.e., multiple activation maps per object. Specifically, we adopt a 4-group 3?3 convolution as the F iam for Group-IAM and aggregate instance features by concatenating features from a group.</p><p>IoU-aware Objectness. We discover that the one-to-one assignment will enforce most predictions to be background which may lower the classification confidence and cause misalignments between classification scores and segmentation masks. To alleviate the above issues, we introduce the IoU-aware objectness to adjust the classification outputs. We adopt the estimated IoU between predicted masks and ground-truth masks as the targets for foreground objects. The ground-truth objectness for instances is varied and can facilitate the network to separate instances. Different from <ref type="bibr" target="#b18">[19]</ref> using an extra head to predict IoU score based on mask predictions, we only adopt IoUs as the objectness targets. At inference stage, we rescore the classification probability p i with the IoU-aware objectness s i and obtain the ultimate</p><formula xml:id="formula_1">probabilityp i = ? p i ? s i , where i denotes the i-th instance.</formula><p>Mask Head. With the instance-aware mask kernels {w i } N generated by the instance branch, the segmentation mask for each instance can be directly produced by m i = w i ? M, where m i is the i-th predicted mask and its corresponding kernel is w i ? R 1?D . M ? R D?H?W is the mask features. The final segmentation mask will be upsampled (via bilinear interpolation) to 1? w.r.t. original resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Label Assignment and Bipartite Matching Loss</head><p>The proposed SparseInst outputs a fixed-size set of predictions and it's difficult to assign ground-truth objects with hand-crafted rules. To tackle the end-to-end training, we formulate the label assignment as bipartite matching <ref type="bibr" target="#b3">[4]</ref>. Firstly, we propose a pairwise dice-based matching score C(i, k) for i-th prediction and k-th ground-truth object in Eq. (1), which is determined by classification scores and dice coefficients of segmentation masks.</p><formula xml:id="formula_2">C(i, k) = p 1?? i,c k ? DICE(m i , t k ) ? ,<label>(1)</label></formula><p>where ? is a hyper-parameter to balance the impacts of classification and segmentation and empirically set to 0.8. c k is termed as the category label for the k-th ground-truth object and p i,c k indicates the probability for the category c k of i-th prediction. m i and t k are the masks of i-th prediction and k-th ground-truth object respectively. The dice coefficient is defined in Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_3">DICE(m, t) = 2 x,y m xy ? t xy x,y m 2 xy + x,y t 2 xy ,<label>(2)</label></formula><p>where m xy and t xy denote the pixels at (x, y) in the predicted mask m and ground-truth mask t respectively. Then, we adopt Hungarian algorithm <ref type="bibr" target="#b30">[31]</ref> to find the optimal match between K ground-truth objects and N predictions. The training loss is defined in Eq. (3), involving losses for classification, objectness prediction, and segmentation.</p><formula xml:id="formula_4">L = ? c ? L cls + L mask + ? s ? L s ,<label>(3)</label></formula><p>where L cls is focal loss <ref type="bibr" target="#b23">[24]</ref> for object classification, L mask is the mask loss and L s is the binary cross entropy loss for the IoU-aware objectness. Considering the severe imbalance problem between background and foreground in fullresolution instance segmentation, we adopt a hybrid mask loss in Eq. (4) by combining the dice loss <ref type="bibr" target="#b27">[28]</ref> and pixelwise binary cross entropy loss for segmentation mask.</p><formula xml:id="formula_5">L mask = ? dice ? L dice + ? pix ? L pix ,<label>(4)</label></formula><p>where L dice and L pix are dice loss and binary cross entropy loss, ? dice and ? pix are corresponding coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Inference</head><p>The inference stage of SparseInst is much straightforward and concise. Forward the given images through the whole network and we can directly obtain N instances with classification scores {p i } N and corresponding raw segmentation masks {m i } N . Then we can determine the category and confidence score for each instance and obtain the final binary mask by thresholding. Sorting and NMS are not needed, thus making the inference procedure very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the accuracy and inference speed of our proposed SparseInst on the challenging MS-COCO dataset and provide detailed ablation studies about our framework as well as qualitative results.</p><p>Dataset and Evaluation Metrics. Our experiments are conducted on the COCO dataset <ref type="bibr" target="#b24">[25]</ref> which consists of 118k images for training, 5k for validation and 20k for testing. All models are trained on train2017 and evaluated on val2017. As for instance segmentation, we mainly report the AP for segmentation mask. For inference speed, we measure the frames per second (FPS) including the post-processing on one NVIDIA 2080Ti GPU. TensorRT or FP16 is not used for acceleration. Implementation Details. SparseInst is built on Detec-tron2 <ref type="bibr" target="#b41">[42]</ref> and trained over 8 GPUs with a total of 64 images per mini-batch. Following the training schedule in <ref type="bibr" target="#b32">[33]</ref>, we adopt AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer with a small initial learning rate 5 ? 10 ?5 with weight decay 0.0001. All models are trained for 270k iterations and learning rate is divided by 10 at 210k and 250k respectively. The backbone is initialized with the ImageNet-pretrained weights with frozen batchnorm layers and other modules are randomly initialized. We adopt random flip and scale jitter in training. The shorter side of images are randomly sampled from 416 to 640 pixels, while the longer side is less or equal to 864. Unless specified, we evaluated the speed and accuracy with the shorter size 640. Loss coefficients ? c , ? dice , ? pix , and ? s are empirically set to 2.0, 2.0, 2.0, and 1.0 respectively. We adopt N=100 instances for each image. Besides, we provide a MindSpore <ref type="bibr" target="#b28">[29]</ref> implementation of SparseInst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>Since the SparseInst aims for real-time instance segmentation, we mainly compare SparseInst with the state-of-the-art methods towards real-time instance segmentation with respect to accuracy and inference speed. Results are evaluated on COCO test-dev. We provide SparseInst with group instance activation maps and different backbones to achieve the trade-off between speed and accuracy. We adopt ResNet-50 <ref type="bibr" target="#b15">[16]</ref> to reach higher inference speed and its variant ResNet-d <ref type="bibr" target="#b16">[17]</ref> to achieve better accuracy but with higher latency and aim for providing a stronger baseline for realtime instance segmentation. Additionally, we adopt a simple random crop and larger weight decay (0.05) to better compare with OrienMask <ref type="bibr" target="#b11">[12]</ref> and YOLACT <ref type="bibr" target="#b1">[2]</ref>. Table 1 shows that our SparseInst is superior to most realtime methods with better performance and faster inference speed. SparseInst outperforms the popular real-time approach YOLACT by a remarkable margin with faster speed. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the speed-accuracy trade-off curve and the proposed SparseInst with R50-d and DCN <ref type="bibr" target="#b49">[50]</ref> obtains better trade-off compared with the counterparts and achieves 58.5 FPS and 35.5 mask AP with 448? input, which is superior to most real-time methods (? 30FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We conduct a series of ablations to investigate Sparse-Inst, including experimental details about the components.</p><p>Instance Context Encoder. <ref type="table">Table 2</ref> shows the impacts of the modifications to the vanilla feature pyramids <ref type="bibr" target="#b22">[23]</ref>. Adding the pyramid pooling module for larger receptive fields and more object contexts brings significant improvement by 1.5 AP and 2.2 AP for larger objects (AP L ) while incurs negligible latency. Moreover, fusing the multi-scale features from P 3 to P 5 further enhances the multi-scale feature representation and improves the performance by 0.7 AP and 2.0 AP L . The context encoder is rather essential for single-level prediction to cope with the limited receptive fields and provide better multi-scale features, thus bridging the gap between multi-level and single-level methods.</p><p>Structure of the Decoder. In <ref type="table">Table 3</ref>, we compare different structures of the two branches in the IAM-based Decoder. We adopt 4 conv layers with 256 channels as the basic setting for both branches and evaluate the performance of models with different depths or widths. Reducing width or reducing depth will lower the performance but increase the inference speed and it's worth noting that reducing channels to 128 performs worse. Increasing the depth from 4 to 6 brings 0.4 AP improvement. Considering the tradeoff between speed and accuracy, we adopt width=256 and depth=4 in all experiments. Adding coordinate features improves the baseline by 0.5 AP with negligible time consumption, which indicates the effect of the explicit locationaware features as discussed in ?3.4.  <ref type="table">Table 3</ref>. Ablation on the structure of the decoder. 'coord.' denotes coordinates and 'dconv.' denotes deformable convolution. Adding coordinates brings 0.5 AP improvement but with negligible latency. Replacing the last convolution with deformable convolution gives significant improvement on larger objects (APL). Reducing the width or depth improves the inference speed but lower the performance, while increasing the depth can further improve the accuracy but lower the speed.</p><p>tion <ref type="bibr" target="#b49">[50]</ref> is optional and improves larger objects by enlarging the receptive field but consumes much time (+1.7ms).</p><p>Instance Activation Maps. F iam is the key component for highlighting object regions, and we explore different designs for F iam in  <ref type="table" target="#tab_2">Table 4</ref>. Ablation on Fiam. Using softmax or 1?1 conv brings 0.4 AP and 1.2 AP drop respectively, and using two 3 ? 3 conv with ReLU brings no gain. However, Group-IAM with 4 groups obtains 0.7 AP improvement. norm) and softmax can be formulated as</p><formula xml:id="formula_6">s i = f (xi) k f (x k )</formula><p>where f (x) = e x for softmax and f (x) = 1 1+e ?x for sigmoid, which tends to saturate thus activate larger regions then softmax. Adding extra 3 ? 3 conv brings no gain but increases the computation cost. Further, we evaluate the Group-IAM with different groups and <ref type="table" target="#tab_2">Table 4</ref> shows that using 4 groups improves the model by 0.7 AP.</p><p>Hybrid Mask Loss. In <ref type="table" target="#tab_3">Table 5</ref>, we analyze the effects of the hybrid mask loss. Notably, dice loss is the critical component for mask prediction and removing dice loss lead to the collapse (AP rapidly drops 8.1 points). Compared to RoI-based methods <ref type="bibr" target="#b14">[15]</ref>, full-resolution instance segmentation has severe imbalance problem between background and foreground, especially for small objects which may occupy less than 0.5% pixels. Dice loss is more robust to the foreground/background imbalance thus effective to handle the full-resolution segmentation. In <ref type="table" target="#tab_3">Table 5</ref>, adding a pixelwise classification loss can further improve the segmentation accuracy: using binary cross-entropy loss (BCE) or focal loss improves by 1.0 AP and 0.5 AP respectively. Moreover, we note that pixel-wise loss significantly improves AP L (e.g., +1.8 AP from BCE) for large objects. Addition-  <ref type="table">Table 7</ref>. Comparison with cross attention. We evaluate the performance of directly using one 4-head cross attention <ref type="bibr" target="#b3">[4]</ref> with 100 queries to segment objects. Notably, (Group-) IAM with 3 ? 3 conv can offer better results size backbone encoder decoder post 512 10.0 (54.3%) 2.5 (13.5%) 4.1 (22.2%) 1.8 (10.0%) 640 13.3 (55.6%) 2.9 (12.1%) 5.6 (23.4%) 2.1 (8.90%) <ref type="table">Table 8</ref>. Inference time. We report the inference latency of module of the SparseInst. The backbone consumes more than 50% of the total time. ally, increasing the weight for pixel-wise loss (? pix ), e.g., 5.0, will bring some improvements.</p><p>IoU-aware Objectness. We further conduct ablations to investigate the effects of the proposed IoU-aware objectness method. In <ref type="table">Table 6</ref>, employing the IoU-aware objectness can improve the baseline by 1.3 AP. Interestingly, we observe that adding objectness prediction without rescoring still brings 0.7 AP improvements, which has no direct impact to classification or segmentation. The targets for objectness differs among foreground instances and therefore the objectness loss can facilitate the instance branch to learn more instance-aware features for distinguishing objects as discussed in ?3.4. We also compare different types of loss, i.e., L1 loss and cross-entropy, for IoU-aware objectness and <ref type="table">Table 6</ref> shows the superiority of using cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Timing</head><p>Our framework achieves fast inference speed for since it saves much computation costs by using single-level prediction, highlighting a sparse set of instances, fully convolutional design, and adopting extremely simple postprocessing without sorting or NMS. To better understand the efficiency of the proposed method, we measure the inference latency of each module (i.e., backbone, encoder, decoder, and post-processing). We disable the asynchronous execution in GPU for accurately recording the time, which slows down the overall inference speed. <ref type="table">Table 8</ref> shows the inference latency (ms) of each module in SparseInst with different input resolutions. It's worth noting that the backbone (i.e., ResNet-50) consumes most of the inference time and the post-processing inevitably requires nearly 2ms to process the final segmentation and recognition results for evaluation. The 3?3 convolutions in the decoder take much time and can be pruned for more efficient inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Cross Attention</head><p>The proposed IAM has some connections with querybased methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. The cross attention between object queries Q and image features X can be briefly formulated by: A = QX and O = Softmax(A)X T , where A and O are attention maps and output queries. The cross attention has similar formulations with IAM in ?3.1 especially for 1?1 conv, which can be viewed as 1-head cross attention. Differently, we adopt the 3 ? 3 conv as F iam to highlight object regions, which acts as a direct spatial object representation. Compared to queries or 1?1 conv, 3?3 conv perceives larger context and local patterns for instance recognition. Further, we replace IAM with a 4-head cross attention and 100 queries to generate instance features, and <ref type="table">Table 7</ref> shows that the 4-head cross attention drops 0.2 AP or 0.9 AP compared to IAM and Group-IAM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualizations</head><p>Instance Activation Maps. <ref type="figure" target="#fig_5">Figure 4</ref> provides the visualizations for instance activation maps and corresponding segmentation masks. Each instance activation map highlights a prominent region of the object. Segmentation masks are well-localized and aligned with the instance activation maps. Moreover, instance activation maps can highlight objects in despite of the scales, positions, categories and also perform well for crowd scenes.</p><p>For a better understanding of how the instance activation maps can discriminate objects, we further provide the visualizations of the instance activation maps from all images. <ref type="figure">Figure 6</ref> illustrates 12 (of 100) instance activation maps by averaging the activation response over the 5,000 images from COCO val2017. Different instance activation maps highlight regions of different spatial locations, scales, and shapes, which contributes to separating the instances of the same or different categories.</p><p>Qualitative Results. <ref type="figure" target="#fig_6">Figure 5</ref> shows the qualitative results of SparseInst. The proposed SparseInst can generate precise segmentation masks with fine boundaries. For crowd and dense scenes, SparseInst can also distinguish different instances well.  The confidence threshold is set to 0.4. We can observe that SparseInst can generate precise boundaries, highlight and segment well on the crowd scenes, and cope with the scale-variant segmentation. <ref type="figure">Figure 6</ref>. Visualizations for Instance Activation Maps over the COCO dataset. We gather the 100 instance activation maps over the 5,000 images from the COCO val2017 by averaging the activation responses for each map. Instance activation maps from different images are resized to the same size 512 ? 512. We provide 12 instance activation maps for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have explored a novel object representation by instance activation maps, which are instanceaware weighted maps and aim to highlight informative regions of objects. Then we present a new highlight to segment paradigm to exploit a sparse set of instance activation maps to highlight objects and aggregate instance features according to the activation maps for instance-level recognition and segmentation. Following this paradigm, we propose SparseInst, a conceptually novel and efficient end-toend framework, which achieves rather fast inference speed with highly competitive accuracy for real-time instance segmentation. Extensive experiments and qualitative results have demonstrated the effectiveness of the core idea as well as the superiority of the trade-off between speed and accuracy. Finally, we hope that SparseInst can serve as a general framework for end-to-end real-time instance segmentation and be applied to practical scenes for its effectiveness and efficiency. <ref type="figure">Figure 7</ref> shows the error analysis through TIDE <ref type="bibr" target="#b0">[1]</ref> and comparisons among SparseInst without Group-IAM (40.2FPS, 36.9AP), YOLACT++ [2] (38.6FPS, 34.1AP), and SOLOv2 <ref type="bibr" target="#b40">[41]</ref> (38.2FPS, 34.0AP). In detail, the proposed SparseInst has lower miss error, indicating that Spar-seInst can discover more objects. We observe that Spar-seInst has higher portions of classification error and dupe error than YOLACT++ or SOLOv2, and the two types of error can be attributed to classification. SparseInst removes duplicate predictions through classification scores and better classification capability can offer better performance.</p><p>SparseInst YOLACT++ SOLOv2 <ref type="figure">Figure 7</ref>. TIDE Error Analysis. We adopt TIDE <ref type="bibr" target="#b0">[1]</ref> to further analyze the errors of SparseInst, YOLACT++, and SOLOv2. cls: classification error; loc: localization error; miss: missing detections; bkg: background detections; dupe: duplicated detections; both: cls+loc error.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Speed-and-accuracy Trade-off. The proposed Sparse-Inst outperforms most state-of-the-art methods in both speed and accuracy for real-time instance segmentation. Inference speeds are measured on one NVIDIA 2080Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Object Representation. (a) center-based representation may fail to hit the instance; (b) region-based representation may contain features from other instances and background; (c) instance activation map highlights instance-aware pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of SparseInst. SparseInst contains three main components: backbone, encoder and IAM-based decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations for Instance Activation Maps. We present the visualizations of the instance activation maps and segmentation masks. For each input image, the upper row shows the instance activation maps and the bottom row shows the corresponding segmentation masks. The instance activation maps tend to highlight the discriminative regions of the objects regardless of the scales, occlusion, and poses. Best viewed on screen after zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visualizations for Instance Segmentation. The results are obtained by SparseInst on COCO val2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 Table 1 .Table 2 .</head><label>312</label><figDesc>also shows the effects of replacing the last convolution of the two branches with a deformable convolution. Using deformable convolu-COCO Instance Segmentation. Comparisons with state-of-the-art methods for mask AP and speed on COCO test-dev. Inference speeds of all models are tested on our machine with one NVIDIA RTX 2080Ti except those marked with ? , which are inherited from their publications. Ablation on the Instance Context Encoder. The vanilla encoder<ref type="bibr" target="#b22">[23]</ref> is incapable for single-level prediction. Leveraging PPM can enlarge the receptive fields and significantly improve the overall performance and adding multi-scale fusion further improves the accuracy, especially for APL. Notably, the extra latency of the improved encoder compared to the vanilla one is negligible.</figDesc><table><row><cell></cell><cell>method</cell><cell></cell><cell>backbone</cell><cell>size FPS</cell><cell>AP</cell><cell cols="4">AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell>MEInst [46]</cell><cell></cell><cell>R-50-FPN</cell><cell>512 24.0</cell><cell>32.2</cell><cell>53.9</cell><cell>33.0</cell><cell>13.9</cell><cell>34.4</cell><cell>48.7</cell></row><row><cell></cell><cell cols="2">CenterMask [21]</cell><cell>R-50-FPN</cell><cell>600 31.9</cell><cell>32.9</cell><cell>-</cell><cell>-</cell><cell>12.9</cell><cell>34.7</cell><cell>48.7</cell></row><row><cell></cell><cell cols="2">CondInst [36]</cell><cell>R-50-FPN</cell><cell cols="2">800 20.4  ? 35.4</cell><cell>56.4</cell><cell>37.6</cell><cell>18.4</cell><cell>37.9</cell><cell>46.9</cell></row><row><cell></cell><cell>SOLO [40]</cell><cell></cell><cell>R-50-FPN</cell><cell>512 24.4</cell><cell>34.2</cell><cell>55.9</cell><cell>36.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">SOLOv2-Lite [40] R-50-FPN</cell><cell>448 38.2</cell><cell>34.0</cell><cell>54.0</cell><cell>36.1</cell><cell>10.3</cell><cell>36.3</cell><cell>54.4</cell></row><row><cell></cell><cell cols="4">SOLOv2-Lite [40] R-50-DCN-FPN 512 28.2</cell><cell>37.1</cell><cell>57.7</cell><cell>39.7</cell><cell>12.9</cell><cell>40.0</cell><cell>57.4</cell></row><row><cell></cell><cell cols="2">PolarMask [43]</cell><cell>R-50-FPN</cell><cell cols="2">600 21.7  ? 27.6</cell><cell>47.5</cell><cell>28.3</cell><cell>9.8</cell><cell>30.1</cell><cell>43.1</cell></row><row><cell></cell><cell cols="2">PolarMask [43]</cell><cell>R-50-FPN</cell><cell cols="2">800 17.2  ? 29.1</cell><cell>49.5</cell><cell>29.7</cell><cell>12.6</cell><cell>31.8</cell><cell>42.3</cell></row><row><cell></cell><cell cols="2">YOLACT [2]</cell><cell>R-50-FPN</cell><cell>550 50.6</cell><cell>28.2</cell><cell>46.6</cell><cell>29.2</cell><cell>9.2</cell><cell>29.3</cell><cell>44.8</cell></row><row><cell></cell><cell cols="2">YOLACT [2]</cell><cell>R-101-FPN</cell><cell>700 29.0</cell><cell>31.2</cell><cell>50.6</cell><cell>32.8</cell><cell>12.1</cell><cell>33.3</cell><cell>47.1</cell></row><row><cell></cell><cell cols="2">YOLACT++ [2]</cell><cell cols="2">R-50-DCN-FPN 550 38.6</cell><cell>34.1</cell><cell>53.3</cell><cell>36.2</cell><cell>11.7</cell><cell>36.1</cell><cell>53.6</cell></row><row><cell></cell><cell cols="2">OrienMask [12]</cell><cell>D-53-FPN</cell><cell>544 42.7</cell><cell>34.8</cell><cell>56.7</cell><cell>36.4</cell><cell>16.0</cell><cell>38.2</cell><cell>47.8</cell></row><row><cell></cell><cell>SparseInst</cell><cell></cell><cell>R-50</cell><cell>608 44.6</cell><cell>34.7</cell><cell>55.3</cell><cell>36.6</cell><cell>14.3</cell><cell>36.2</cell><cell>50.7</cell></row><row><cell></cell><cell>SparseInst</cell><cell></cell><cell>R-50-DCN</cell><cell>608 41.6</cell><cell>36.8</cell><cell>57.6</cell><cell>38.9</cell><cell>15.0</cell><cell>38.2</cell><cell>55.2</cell></row><row><cell></cell><cell>SparseInst</cell><cell></cell><cell>R-50-d</cell><cell>608 42.8</cell><cell>36.1</cell><cell>57.0</cell><cell>38.2</cell><cell>15.0</cell><cell>37.7</cell><cell>53.1</cell></row><row><cell></cell><cell>SparseInst</cell><cell></cell><cell>R-50-d-DCN</cell><cell>608 40.0</cell><cell>37.9</cell><cell>59.2</cell><cell>40.2</cell><cell>15.7</cell><cell>39.4</cell><cell>56.9</cell></row><row><cell cols="5">w/ fusion w/ PPM t (ms) AP AP50 AP75 APS APM APL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">22.0 29.8 48.7 31.0 12.0 31.8 44.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">22.2 31.3 50.8 32.4 14.0 33.2 46.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">22.8 30.3 49.5 31.6 12.5 32.3 45.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">22.9 32.0 52.0 33.3 13.1 34.5 48.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">depth width coord? dconv? AP APS APM APL t (ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>256</cell><cell cols="3">31.5 13.4 33.5 47.9 22.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>256</cell><cell cols="3">32.0 13.0 34.5 48.2 22.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>256</cell><cell cols="3">32.6 13.1 34.8 49.2 24.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>256</cell><cell cols="3">31.0 12.9 33.2 47.0 20.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>256</cell><cell cols="3">32.4 13.7 35.4 47.9 25.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>128</cell><cell cols="3">30.6 12.4 32.5 46.2 19.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>3?3 conv, ReLU, 3?3 conv sigmoid 31.9 52.2 33.0 23.6 Group 3?3 conv (2 groups) sigmoid 32.2 52.3 33.5 23.1 Group 3?3 conv (4 groups) sigmoid 32.7 53.1 34.0 23.3</figDesc><table><row><cell>Fiam</cell><cell>act.</cell><cell>AP AP50 AP75 t (ms)</cell></row><row><cell>3?3 conv</cell><cell cols="2">sigmoid 32.0 51.9 33.5 22.9</cell></row><row><cell>3?3 conv</cell><cell cols="2">softmax 31.6 51.4 32.9 22.9</cell></row><row><cell>1?1 conv</cell><cell cols="2">sigmoid 30.8 50.7 32.0 22.4</cell></row></table><note>. Using softmax or 1 ? 1 conv brings 0.4 AP and 1.2 AP drop, respectively. Sigmoid (w/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Dice Focal BCE AP AP50 AP75 APL 23.9 40.2 24.3 40.8 31.0 50.8 32.0 46.4 31.5 51.6 32.7 47.5 32.0 52.0 33.3 48.2 Ablation on the hybrid mask loss. Group 3?3 conv 32.7 53.1 34.0 23.3 Cross Attention 31.8 51.7 33.1 23.4</figDesc><table><row><cell></cell><cell cols="2">w/ obj. rescore? loss AP AP50 AP75</cell><cell>Fiam</cell><cell>AP AP50 AP75 t (ms)</cell></row><row><cell></cell><cell>-</cell><cell>-30.7 51.3 31.6</cell><cell>1?1 conv</cell><cell>30.8 50.7 32.0 22.4</cell></row><row><cell></cell><cell></cell><cell>CE 31.4 52.1 32.2</cell><cell>3?3 conv</cell><cell>32.0 51.9 33.5 22.9</cell></row><row><cell></cell><cell></cell><cell>CE 32.0 52.0 33.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>L1 31.5 51.3 32.7</cell><cell></cell></row><row><cell></cell><cell cols="2">Table 6. Ablation on the IoU-aware</cell><cell></cell></row><row><cell>We evaluate the effects of the different hybrid</cell><cell cols="2">objectness. Adding objectness facilitates</cell><cell></cell></row><row><cell>mask loss. Dice loss is an essential compo-</cell><cell cols="2">more instance-aware features and improves</cell><cell></cell></row><row><cell>nent and adding extra BCE loss can further</cell><cell cols="2">the performance even without rescoring.</cell><cell></cell></row><row><cell>improve the performance (+1.0 AP) especially</cell><cell cols="2">Using cross-entropy loss obtains better re-</cell><cell></cell></row><row><cell>for larger objects (+1.8 APL).</cell><cell>sults than L1 loss.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was in part supported by NSFC (No. 61876212 and No. 61733007) and CAAI-Huawei MindSpore Open Fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. TIDE Error Analysis</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tide: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">YOLACT: real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: high quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1483" to="1498" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">You only look one-level feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary-preserving Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SOLQ: segmenting objects by learning queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time instance segmentation with discriminative orientation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiman</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingming</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense reppoints: Representing visual objects with dense point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Bin Feng, and Wenyu Liu. Instances as queries. In ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ISTR: end-to-end instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00637</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03814</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://github.com/mindspore-ai/mindspore.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What makes for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">OneNet: Towards end-to-end onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05780</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse R-CNN: end-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Look closer to segment better: Boundary patch refinement for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SOLO: segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 5</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mask encoding for single shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapedriza</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deformable convnets V2: more deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
