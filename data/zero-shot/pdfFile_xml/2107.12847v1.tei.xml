<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Local Recurrent Models for Human Mesh Recovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Riverside</orgName>
								<address>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bir Bhanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Riverside</orgName>
								<address>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Local Recurrent Models for Human Mesh Recovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We present LMR, a new method for video human mesh recovery. Unlike existing work, LMR captures local human part dynamics and interdependencies by learning multiple local recurrent models, resulting in notable performance improvement over the state of the art. Here, we show a few qualitative results on the 3DPW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We consider the problem of estimating frame-level full human body meshes given a video of a person with natural motion dynamics. While much progress in this field has been in single image-based mesh estimation, there has been a recent uptick in efforts to infer mesh dynamics from video given its role in alleviating issues such as depth ambiguity and occlusions. However, a key limitation of existing work is the assumption that all the observed motion dynamics can be modeled using one dynamical/recurrent model. While this may work well in cases with relatively simplistic dynamics, inference with in-the-wild videos presents many challenges. In particular, it is typically the case that different body parts of a person undergo different dynamics in the video, e.g., legs may move in a way that may be dynamically different from hands (e.g., a person dancing). To address these issues, we present a new method for video mesh recovery that divides the human mesh into several local parts following the standard skeletal model. We then model the dynamics of each local part with separate recurrent models, with each model conditioned appropriately based on the known kinematic structure of the human body. This results in a structure-informed local recurrent learning architecture that can be trained in an end-to-end fashion with available annotations. We conduct a variety of experiments on standard video mesh recovery benchmark datasets such as Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design of modeling local dynamics as well as establishing state-of-the-art results based on standard evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the problem of human mesh recovery in videos, i.e., fitting a parametric 3D human mesh model to each frame of the video. With many practical applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, including in healthcare for COVID-19 <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, there has been much progress in this field in the last few years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, most research effort has been expended in single image-based mesh estimation where one SOTA LMR <ref type="figure">Figure 2</ref>: A qualitative comparison with VIBE <ref type="bibr" target="#b0">[1]</ref>, highlighting local regions (ellipses that show zoomed-in VIBE results) where LMR gives better performance. seeks to fit the human mesh model to a single image. However, such 3D model estimation from only a single 2D projection (image) is a severely under-constrained problem since multiple 3D configurations (in this case poses and shapes of the mesh model) can project to the same image. Such ambiguities can be addressed by utilizing an extra dimension that is typically associated with images-the temporal dimension leading to video data and the problem of video mesh recovery.</p><p>The currently dominant paradigm for video mesh recovery involves the feature-temporal-regressor architecture. A deep convolutional neural network (CNN) is used to extract frame-level image feature vectors, which are then processed by a temporal encoder to learn the motion dynamics in the video. The representation from the temporal encoder is then processed by a parameter regressor module that outputs frame-level mesh parameter vectors. While methods vary in the specific implementation details, they mostly follow this pipeline. For instance, while Kanazawa et al. <ref type="bibr" target="#b8">[9]</ref> implement the temporal encoder using a feed-forward fully convolutional model, Kocabas et al. <ref type="bibr" target="#b0">[1]</ref> uses a recurrent model to encode motion dynamics. However, uniformly across all these methods, the parameter regressor is implemented using a "flat" regression architecture that takes in feature vectors as input and directly regresses all the model parameters, e.g., 85 values (pose, shape, and camera) for the popularly used skinned multi-person linear (SMPL) model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. While this paradigm has produced impressive recent results as evidenced by the mean per-joint position errors on standard datasets (see Arnab et al. <ref type="bibr" target="#b10">[11]</ref> and Kocabas et al. <ref type="bibr" target="#b0">[1]</ref> for a fairly recent benchmark), a number of issues remain unaddressed that provide us with direction and scope for further research and performance improvement.</p><p>First, the above architectures implicitly assume that all motion dynamics can be captured using a single dynamical system (e.g., a recurrent network). While this assumption may be reasonable for fairly simplistic human motions, it is not sufficient for more complex actions. For instance, while dancing, the motion dynamics of a person vary from one part of the body to the other. As a concrete example, the legs may remain static while the hands move vigorously, and these roles may be reversed after a certain period of time (static hands and moving legs several frames later), leading to more "locally" varying dynamics. Intuitively, this tells us that the motion of each local body part should in itself be modeled separately by a dynamical system, and that such a design should help capture this local "part-level" dynamical information more precisely as opposed to a single dynamical system for the entire video snippet.</p><p>Next, as noted above, the regressor in the featuretemporal-regressor architecture involves computing all the parameters of the SMPL model using a direct/flat regression design without due consideration given to the interdependent nature of these parameters (i.e., SMPL joint rotations are not independent but rather conditioned on other joints of other parts such as the root <ref type="bibr" target="#b9">[10]</ref>). It has been noted in prior work <ref type="bibr" target="#b11">[12]</ref> that such direct regression of rotation matrices, which form a predominant part of the SMPL parameter set, is challenging as is and only made further difficult due to these interdependencies in the SMPL model. In addition to direct rotation regression, the temporal module in the above feature-temporal-regressor also does not consider any joint and part interdependencies, i.e., modeling all motion dynamics using a single global dynamical system, thus only further exacerbating this problem.</p><p>To address the aforementioned issues, we present a new architecture for capturing the human motion dynamics for estimating a parametric mesh model in videos. Please note that while we use the SMPL model <ref type="bibr" target="#b9">[10]</ref> in this work, our method can be extensible to other kinds of hierarchical parametric human meshes as well. See <ref type="figure">Figure 1</ref> for some qualitative results with our method on the 3DPW <ref type="bibr" target="#b12">[13]</ref> dataset and <ref type="figure">Figure 2</ref> for a comparison with a current state-of-theart method. Our method, called local recurrent models for mesh recovery (LMR), comprises several design considerations. First, to capture the need for modeling locally varying dynamics as noted above, LMR defines six local recurrent models (root, head, left/right arms, left/right legs), one each to capture the dynamics of each part. As we will describe later, each "part" here refers to a chain of several joints defined on the SMPL model. Note that such a part division is not ad hoc but grounded in the hierarchical and part-based design of the SMPL model itself, which divides the human body into the six parts above following the standard skeletal rigging procedure <ref type="bibr" target="#b9">[10]</ref>. Next, to model the conditional interdependence of local part dynamics, LMR first infers root part dynamics (i.e., parameters of all joints in the root part). LMR then uses these root part parameters to subsequently infer the parameters of all other parts, with the output of each part conditioned on the root output. For instance, the recurrent model responsible for producing the parameters of the left leg takes as input both frame-level feature vectors as well as frame-level root-part parameters from the root-part recurrent model.</p><p>Note the substantial differences between LMR's design and those of prior work-(a) we use multiple local recurrent models instead of one global recurrent model to capture motion dynamics, and (b) such local recurrent modeling enables LMR to explicitly capture local part dependencies. Modeling these local dependencies enables LMR to infer motion dynamics and frame-level video meshes informed by the geometry of the problem, i.e., the SMPL model, which, as noted in prior work <ref type="bibr" target="#b11">[12]</ref>, is an important design consideration as we take a step towards accurate rotation parameter regression architectures. We conduct extensive experiments on a number of standard video mesh recovery benchmark datasets (Human3.6M <ref type="bibr" target="#b13">[14]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b14">[15]</ref>, and 3DPW <ref type="bibr" target="#b12">[13]</ref>), demonstrating the efficacy of such local dynamic modeling as well as establishing stateof-the-art performance with respect to standard evaluation metrics.</p><p>To summarize, the key contributions of our work are:</p><p>? We present LMR, the first local-dynamical-modeling approach to video mesh recovery where unlike prior work, we explicitly model the local dynamics of each body part with separate recurrent networks.</p><p>? Unlike prior work that regresses mesh parameters in a direct or "flat" fashion, our local recurrent design enables LMR to explicitly consider human mesh interdependencies in parameter inference, thereby resulting in a structure-informed local recurrent architecture.</p><p>? We conduct extensive experiments on standard benchmark datasets and report competitive performance, establishing state-of-the-art results in many cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is much recent work in human pose estimation, including estimating 2D keypoints <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, 3D keypoints <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, and a full mesh <ref type="bibr">[1, 7-9, 11, 24, 25]</ref>. Here, we discuss methods that are relevant to our specific problemfitting 3D meshes to image and video data.</p><p>Single-image mesh fitting. Most recent progress in human mesh estimation has been in fitting parametric meshes to single image inputs. In particular, following the availability of differentiable parametric models such as SMPL <ref type="bibr" target="#b9">[10]</ref>, there has been an explosion in interest and activity in this field. Kanazawa et al. <ref type="bibr" target="#b6">[7]</ref> presented an end-to-end trainable regression architecture for this problem that could in principle be trained with 2D-only keypoint data. Subsequently, many improved models have been proposed. Kolotourous et al. <ref type="bibr" target="#b24">[25]</ref> and Georgakis et al. <ref type="bibr" target="#b7">[8]</ref> extended this architecture to include more SMPL-structure-informed design considerations using either graph-based or parameter factorization-based approaches. There have also been attempts at SMPL-agnostic modeling of joint interdependencies, with Fang et al. <ref type="bibr" target="#b25">[26]</ref> employing bidirectional recurrent networks and Isack et al. <ref type="bibr" target="#b26">[27]</ref> learning priors between joints using a pre-defined joint connectivity scheme. While methods such as Georgakis et al. <ref type="bibr" target="#b7">[8]</ref> and Zhou et al. <ref type="bibr" target="#b27">[28]</ref> also take a local part-based kinematic approach, their focus is on capturing inter-joint spatial dependencies. On the other hand, LMR's focus is on capturing inter-part temporal dependencies which LMR models using separate recurrent networks.</p><p>Video mesh fitting. Following the success of imagebased mesh fitting methods, there has been a recent uptick in interest and published work in fitting human meshes to videos. Arnab et al. <ref type="bibr" target="#b10">[11]</ref> presented a two-step approach that involved generating 2D keypoints and initial mesh fits using existing methods, and then using these initial estimates to further refine the results using temporal consistency constraints, e.g., temporal smoothness and 3D priors. However, such a two-step approach is susceptible to errors in either steps and our proposed LMR overcomes this issue with an end-to-end trainable method that provides deeper integration of the temporal data dimension both in training and inference. On the other hand, Kanazawa et al. <ref type="bibr" target="#b8">[9]</ref> and Kocabas et al. <ref type="bibr" target="#b0">[1]</ref> also presented end-to-end variants of the feature-temporal-regressor where frame-level feature vectors are first encoded using a temporal encoder (e.g., a single recurrent network) and finally processed by a parameter regressor to generate meshes. However, such a global approach to modeling motion dynamics (with only one RNN) does not capture the disparities in locally varying dynamics (e.g., hands vs. legs) which is typically the case in natural human motion. LMR addresses this issue by design with multiple local RNNs in its architecture, one for each predefined part of the human body. Such a design also makes mesh parameter regression more amenable by grounding this task in the geometry of the problem, i.e., the SMPL model itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parametric Mesh Representation</head><p>We use the Skinned Multi-Person Linear (SMPL) model <ref type="bibr" target="#b9">[10]</ref> to parameterize the human body. SMPL uses two sets of parameter vectors to capture variations in the human body: shape and pose. The shape of the human body is represented using a 10-dimensional vector ? ? R 10 whereas the pose of the body is represented using a 72- dimensional vector ? ? R 72 . While ? corresponds to the first ten dimensions of the PCA projection of a shape space, ? captures, in axis-angle format <ref type="bibr" target="#b28">[29]</ref>, the global rotation of the root joint (3 values) and relative (to the root) rotations of 23 other body joints (69 values). Given ?, ?, and a learned model parameter set ?, SMPL defines the mapping M (?, ?, ?) : R 82 ? R 3?N from the 82-dimensional parametric space to a vertex space of N = 6890 3D mesh vertices. One can then infer the 24 3D joints of interest (e.g., hips, legs, etc.) X ? R 3?K , K = 24 using a prelearned joint regression matrix W as X = W J . Using a known camera model, e.g., a weak-perspective model as in prior work <ref type="bibr" target="#b6">[7]</ref>, one can then obtain the corresponding 24 2D image points x ? R 2?K as:</p><formula xml:id="formula_0">x = s?(X(?, ?)) + t,<label>(1)</label></formula><p>where the scale s ? R and translation t ? R 2 represent the camera model, and ? is an orthographic projection. Therefore, fitting 3D SMPL mesh to a single image involves estimating the parameter set ? = {?, ?, s, t}. In video mesh recovery, we take this a step forward by estimating ? for every frame in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Local Recurrent Models</head><p>As noted in Section 1, existing video mesh fitting methods formulate the problem in the feature-temporal-regressor design where all motion dynamics in the video are captured using a single RNN. We argue that this is insufficient for mesh estimation due to the inherently complex nature of human actions/motion, more so in challenging in-the-wild scenarios. Our key insight is that natural human motion dynamics has a more locally varying characteristic that can more precisely be captured using locally learned recurrent networks. We then translate this idea into a conditional local recurrent architecture, called LMR and visually summarized in <ref type="figure" target="#fig_0">Figure 3</ref>, where we define multiple recurrent models, one each to capture the dynamics of the corresponding local region in the human body. During training and inference, LMR takes as input a segment of an input video V = {I 1 , I 2 , . . . , I t , t = 1, 2, . . . , T }, where T is a design parameter corresponding to the length of the input sequence. LMR first processes each frame with its feature extraction module to produce frame-level feature vectors ? = {? 1 , ? 2 , . . . , ? t } for each of the T frames. LMR then processes ? with its local part-level recurrent models and associated parameter regressors, and aggregates all part-level outputs to obtain the mesh and camera parameters ? t , t = 1, 2, . . . , T for each frame, finally producing the output video mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">LMR Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>(a), our architecture comprises a feature extractor followed by our proposed LMR module. The LMR module is responsible for processing the frame-level representation ? to output the per-frame parameter vectors ? t . Following the design of the SMPL model and prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, we divide the human body into six local parts-root (4 joints in the root region), head (2 joints in the head region), left arm (5 joints on left arm), right arm (5 joints on right arm), left leg (4 joints on left leg), and right leg (4 joints on right leg). Given this division, the pose of local part p i , i = 1, . . . , 6 can be expressed as ? i = [r 1 , . . . , r ni ], i = 1, . . . , 6, where r q (q = 1, . . . , n i ) is a rotation parameterization (e.g., r q ? R 3 in case of axis angle) of joint q and n i is the number of joints defined in part i. The overall pose parameter vector ? can then be aggregated as ? = [? 1 , . . . , ? 6 ].</p><p>To capture locally varying dynamics across the video sequence, LMR defines one recurrent model for each of the six parts defined above (see <ref type="figure" target="#fig_0">Figure 3(b)</ref>). The recurrent model for part i is responsible for predicting its corresponding ? i . To capture the conditional dependence between parts, the information propagation during training and inference is defined as follows. Given the frame-level feature representation ?, the mean pose vector ? mean , and the mean shape vector ? mean (note that it is common <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> to initialize mesh fitting with these mean values), the recurrent model responsible for the root part (number 1) first predicts its corresponding pose vector ? 1 t , t = 1, . . . , T for each of the t frames using the concatenated vector [? t , ? 1 mean , ? mean ] as input for the current frame t. Note that ? t is the feature vector for frame t and ? 1 mean represents the mean pose parameters of part p 1 . All other recurrent models (parts 2 through 6) then take in as input the concatenated vector [? t , ? k mean , ? mean , ? 1 t ] in predicting their corresponding pose vectors ? k t , k = 2, . . . , 6 and t = 1, . . . , T , where ? k mean represents the mean pose parameters of part p k . Note this explicit dependence of part k on the root (part 1) prediction ? 1 . Given the aggregated (over all 6 parts) pose vector ? t , LMR has a fully-connected module that takes as input the concatenated vector [? t , ? t , ? mean ] for each frame t to predict the per-frame shape vectors ? t , t = 1, . . . , T . Finally, given an initialization for the camera model c init = [s init , t init ], LMR uses the concatenated vector [? t , ? t , ? t , c init ] as part of its camera recurrent model to predict the camera model c t , t = 1, . . . , T for each frame. Note that while we have simplified the discussion and notation here for clarity of exposition, LMR actually processes each batch of input in an iterative fashion, which we next describe in more mathematical detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training an LMR model</head><p>As noted above and in <ref type="figure" target="#fig_0">Figure 3</ref>, the proposed LMR module takes as input the video feature set ? and the mean pose and shape parameters ? mean and ? mean and produces the set of parameter vectors ? t = [? t , ? t , c t ] for each frame t. The LMR block processes each input set in an iterative fashion, with the output after each iteration being used as a new initialization point to further refine the result. The final output ? t is then obtained at the end of L such iterations. Here, we provide further details of this training strategy.</p><p>Let each iteration step above be denoted by the letter v. At step v = 0, the initial pose and shape values for frame t will then be ? t,v = ? mean and ? t,v = ? mean . The t, v notation refers to the v th iterative step of LMR for frame number t. So, given ?, ? t,v , and the root pose ? 1 t,v (recall root is part number 1 from above), the input to the root RNN will be the set of t vectors [? t , ? 1 t,v , ? t,v ] for each of the t frames. The root RNN then estimates an intermediate residual pose ?? 1 t,v , which is added to the input ? 1 t,v to give the root RNN output ? 1 t,v = ? 1 t,v + ?? 1 t,v . Given the root prediction ? 1 t,v at iteration v, each of the other dependent part RNNs then use this information to produce their corresponding pose outputs. Specifically, for part RNN k, the input vector set (across the t frames) will be</p><formula xml:id="formula_1">[? t , ? k t,v , ? t,v , ? 1 t,v ] for k = 2, .</formula><p>. . , 6. Each part RNN first gives its corresponding intermediate residual pose ?? k t,v . This is then added to its corresponding input part pose, giving the outputs ? k t,v = ? k t,v + ?? k t,v for k = 2, . . . , 6. After producing all the updated pose values at iteration v = 0, LMR then updates the shape values. Recall that the shape initialization used at v = 0 is ? t,v = ? mean . Given ?, the updated and aggregated pose vector set ? t,v = [? 1 t,v , . . . , ? 6 t,v ], and the shape vector set ? mean , LMR then uses the input vector set [? t , ? t,v , ? mean ] as part of the shape update module to produce the new shape vector set ? t,v for each frame t during the iteration v.</p><p>Given these updated ? t,v and ? t,v , LMR then updates the camera model parameters (used for image projection) with a camera model RNN. We use an RNN to model the camera dynamics to cover scenarios where the camera might be moving, although a non-dynamical fully-connected neural network can also be used in cases where the camera is known to be static. Given an initialization for the camera model c t,v = c init at iteration v = 0, the camera RNN processes the input vector set [? t , ? t,v , ? t,v , c init ] to produce the new camera model set c t,v for each frame t.</p><p>After going through one round of pose update, shape update, and camera update as noted above, LMR then reinitializes this prediction process with the updated pose and shape vectors from the previous iteration. Specifically, given the updated ? t,v and ? t,v at the end of iteration v = 0, the root RNN at iteration v = 1 then takes as input the set</p><formula xml:id="formula_2">[? t , ? 1 t,v , ? t,v ],</formula><p>where the pose and shape values are not the mean vectors (as in iteration v = 0) but the updated vectors from iteration v = 0. LMR repeats this process for a total of V iterations, finally producing the parameter set ? t = [? t , ? t , c t ] for each frame t. Note that this iterative strategy is similar in spirit to the iterative error feedback strategies commonly used in pose estimators <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</p><p>All the predictions above are supervised using several cost functions. First, if ground-truth SMPL model parameters ? gt t are available, we enforce a Euclidean loss between the predicted and the ground-truth set:</p><formula xml:id="formula_3">L smpl = 1 T T t=1 ? gt t ? ? t 2<label>(2)</label></formula><p>where the summation is over the t = T input frames in the current batch of data. Next, if ground-truth 3D joints X gt t ? R 3?K (recall K=24 from Section 3.1) are available, we enforce a mean per-joint L1 loss between the prediction 3D joints X t ? R 3?K and X gt t . To compute X t , we use the predicted parameter set ? t and the SMPL vertex mapping function M (?, ?, ?) : R 82 ? R 3?N and the joint regression matrix W (see Section 3.1). The loss then is:</p><formula xml:id="formula_4">L 3D = 1 T 1 K T t=1 K k=1 X gt k,t ? X k,t 1<label>(3)</label></formula><p>where each column of X gt k,t ? R 3 and X k,t ? R 3 is one of K joints in three dimensions and the outer summation is over t = T frames as above.</p><p>Finally, to provide supervision for camera prediction, we also enforce a mean per-joint L1 loss between the prediction 2D joints x t ? R 2?K and the ground-truth 2D joints x gt t . To compute x t , we use the 3D joints prediction X t and the camera prediction c t to perform an orthographic projection following Equation 1. The loss then is:</p><formula xml:id="formula_5">L 2D = 1 T 1 K T t=1 K k=1 x gt k,t ? x k,t 1<label>(4)</label></formula><p>where each column x gt k,t ? R 2 and x k,t ? R 2 of x gt t and x t respectively is one of K joints on the image and the outer summation is over t = T frames as above.</p><p>The overall LMR training objective then is:</p><formula xml:id="formula_6">L LMR = w smpl L smpl + w 3D L 3D + w 2D L 2D<label>(5)</label></formula><p>where w smpl , w 3D , and w 2D are the corresponding loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation</head><p>Following Kocabas et al. <ref type="bibr" target="#b0">[1]</ref>, we use a mixture of both datasets with both 2D (e.g., keypoints) as well as 3D (e.g., mesh parameters) annotations. For 2D datasets, we use Pen-nAction <ref type="bibr" target="#b35">[36]</ref>, PoseTrack <ref type="bibr" target="#b36">[37]</ref>, and InstaVariety <ref type="bibr" target="#b8">[9]</ref>, whereas for 3D datasets, we use Human3.6M <ref type="bibr" target="#b13">[14]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b14">[15]</ref>, and 3DPW <ref type="bibr" target="#b12">[13]</ref>. In all our experiments, we use exactly the same settings as Kocabas et al. <ref type="bibr" target="#b0">[1]</ref> for a fair benchmarking of the results. To report quantitative performance, we use evaluation metrics that are now standard in the human mesh research community. On all the test datasets, we report both mean-per-joint position error (MPJPE) as well as Procrustes-aligned mean-per-joint position error (PA-MPJPE). Additionally, following Kanazawa et al. <ref type="bibr" target="#b8">[9]</ref> and Kocabas et al. <ref type="bibr" target="#b0">[1]</ref>, on the 3DPW test set, we also report the acceleration error ("Accel."), which is the average (across all keypoints) difference between the ground truth and predicted acceleration of keypoints, and the per-vertex error (PVE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Results</head><p>We first present results of an ablation experiment conducted to study the efficacy of the proposed design of LMR, i.e., the use of multiple local recurrent models as opposed to a single recurrent model as is done in prior work <ref type="bibr" target="#b0">[1]</ref>. Here, we follow the same pipeline as <ref type="figure" target="#fig_0">Figure 3</ref> in spirit, with the only difference being the use of only one RNN to infer all the pose parameters ? instead of the six RNNs depicted in <ref type="figure" target="#fig_0">Figure 3</ref>(b). All other design choices, e.g., for the shape model or the camera model, remain the same as LMR. We show qualitative results of this experiment in <ref type="figure" target="#fig_1">Figure 4</ref> and quantitative results in <ref type="table" target="#tab_1">Table 1</ref>. In <ref type="figure" target="#fig_1">Figure 4</ref>, we show two frames from two different video sequences in (a) and (b). The first row shows results with this single RNN baseline and the second row shows corresponding results with our full model, i.e., LMR. One can note that LMR results in better mesh fits, with more accurate ?-inference in regions such as hands and legs. We further substantiate this performance gap quantitatively in <ref type="table" target="#tab_1">Table 1</ref>, where one can note the proposed LMR gives consistently better performance than its baseline single RNN counterpart across all datasets as well as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state-of-the-art results</head><p>We compare the performance of LMR with a wide variety of state-of-the-art image-based and video-based methods. We first begin with a discussion on relative qualitative performance. In <ref type="figure" target="#fig_2">Figure 5</ref>, we show three frames from two different video sequences in (a) and (b) comparing the performance of the image-based HMR method <ref type="bibr" target="#b6">[7]</ref> (first row) and our proposed LMR. Since LMR is a videobased method, one would expect substantially better performance, including in cases where there are self-occlusions. From <ref type="figure" target="#fig_2">Figure 5</ref>, one can note this is indeed the case. In the first column of <ref type="figure" target="#fig_2">Figure 5</ref>, HMR is unable to infer the correct head pose (it infers front facing when the person is actually back back facing), whereas LMR is able to use the video information from prior to this frame to infer the head pose correctly. Note also HMR's incorrect inference in other local regions, e.g., legs, in the subsequent frames in <ref type="figure" target="#fig_2">Figure 5</ref>(a). This aspect of self-occlusions (i.e., invisible face keypoints) is further demonstrated in <ref type="figure" target="#fig_2">Figure 5(b)</ref>, where HMR is unstable (front facing on a few and back facing on a few frames), whereas LMR consistently infers the correct pose.</p><p>Next, we compare the performance of LMR with the state-of-the-art video-based VIBE method <ref type="bibr" target="#b0">[1]</ref>. In <ref type="figure">Figure 6</ref>, we show three frames from two different video sequences in (a) and (b). One can note substantial performance improvement in several local regions from these results. In    particular, LMR infers more accurate hand pose and camera model parameters in <ref type="figure">Figure 6</ref>(a) when compared to VIBE. The results in <ref type="figure">Figure 6</ref>(b), a more challenging scenario, best illustrates the benefits offered by proposed local design of LMR. Given the variety of body movements in this set of frames, one can note the improved performance of LMR in several regions-hands and legs in the first column, head in the second column, and hands and legs again in the third column. These results are further substantiated in the quantitative comparison we discuss next.  <ref type="table">Table 2</ref>: Comparing LMR to the state of the art ("-": unavailable result in the corresponding paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIBE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LMR (a) (b)</head><p>We provide a quantitative comparison of the performance of LMR to various state-of-the-art image-and videobased methods in <ref type="table">Table 2</ref>. We make several observations. First, as expected, LMR gives substantially better performance when compared to the image-based method of Kanazawa et al. <ref type="bibr" target="#b6">[7]</ref> (MPJPE of 61.9 mm for LMR vs. 88.0 mm for HMR on Human3.6M, 94.6 mm for LMR vs. 124.2 mm for HMR on MPI-INF-3DHP, and 81.7 mm for LMR vs. 130.0 mm for HMR on 3DPW). This holds with other image-based methods as well (first half of <ref type="table">Table 2</ref>). Next, LMR gives competitive performance when compared to state-of-the-art video-based methods as well. In particular, further substantiating the discussion above, LMR generally outperforms Kocabas et al. <ref type="bibr" target="#b0">[1]</ref> with margins that are higher on the "in-the-wild" datasets (MPJPE of 94. <ref type="bibr" target="#b5">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We considered the problem of video human mesh recovery and noted that the currently dominant design paradigm of using a single dynamical system to model all motion dynamics, in conjunction with a "flat" parameter regressor is insufficient to tackle challenging in-the-wild scenarios. We presented an alternative design based on local recurrent modeling, resulting in a structure-informed learning architecture where the output of each local recurrent model (representing the corresponding body part) is appropriately conditioned based on the known human kinematic structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The proposed local recurrent modeling approach to human mesh recovery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Two sets of qualitative results comparing LMR with a single-RNN baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Two sets of qualitative results comparing the performance of LMR with the image-based HMR<ref type="bibr" target="#b6">[7]</ref> method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of an ablation study comparing LMR with a single RNN baseline.</figDesc><table><row><cell>(a)</cell></row><row><cell>HMR</cell></row><row><cell>LMR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 6: Two sets of qualitative results comparing the performance of LMR with the video-based VIBE [1] method. Rec. Error ? MPJPE ? Rec. Error ? MPJPE ? Rec. Error ? PVE ? Accel ?</figDesc><table><row><cell cols="4">Human3.6M 88.0 56.8 -59.9 -75.9 -50.1 MPJPE ? Image-based Methods Kanazawa et al. [7] Omran et al. [33] Pavlakos et al. [24] Kolotouros et al. [25] Georgakis et al. [8] 67.7 50.1</cell><cell cols="2">MPI-INF-3DHP 124.2 89.8 --------</cell><cell>130 ----</cell><cell>3DPW 76.7 --70.2 -</cell><cell>-----</cell><cell>37.4 ----</cell></row><row><cell cols="2">Extra-fitting Kolotouros et al. [34]</cell><cell>62.2</cell><cell>41.1</cell><cell>105.2</cell><cell>67.5</cell><cell>96.9</cell><cell>59.2</cell><cell>116.4</cell><cell>29.8</cell></row><row><cell>Video-based</cell><cell>Kanazawa et al. [9] Arnab et al. [11] Doersch et al. [35] Kocabas et al. [1] LMR</cell><cell>-77.8 -65.6 61.9</cell><cell>56.9 54.3 -41.4 42.5</cell><cell>---96.6 94.6</cell><cell>---64.6 62.4</cell><cell>116.5 --82.9 81.7</cell><cell>72.6 72.2 74.7 51.9 51.2</cell><cell>139.3 --99.1 93.6</cell><cell>15.2 --23.4 15.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>mm for LMR vs. 96.6 mm for Kocabas et al. [1] on MPI-INF-3DHP, Accel. of 15.6 mm/s 2 for LMR vs. 23.4 mm/s 2 for Kocabas et al. [1] on 3DPW), further highlighting the efficacy of LMR's local dynamic modeling. Finally, in Table 2, we also compare our results with those of Kolotouros et al. [34] that uses an additional step of in-the-loop model fitting. Note that despite our proposed LMR not doing this extra model fitting, it outperforms Kolotouros et al. [34] in most cases, with particularly substantial performance improvements on MPI-INF-3DHP (MPJPE of 94.6 mm for LMR vs. 105.2 mm for Kolotouros et al. [34]) and 3DPW (MPJPE of 81.7 mm for LMR vs. 96.9 mm for Kolotouros et al. [34]).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We presented results of an extensive set of experiments on various challenging benchmark datasets to demonstrate the efficacy of the proposed local recurrent modeling approach to video human mesh recovery.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DARWIN: Deformable patient avatar representation with deep image network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgi</forename><surname>Tamersoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time convolutional networks for depth-based human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Mart?nez-Gonz?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villamizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Can?vet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic patient centering for MDCT: effect on radiation dose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Unni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Udayasankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seamans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannudeep K</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of roentgenology</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="547" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Patientbased radiographic exposure factor selection: a systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mcentee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical radiation sciences</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards contactless patient positioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Srikrishna Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Geometric pose affordance: 3d human pose with scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauray</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<editor>Arxiv</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossam</forename><surname>Isack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03933</idno>
		<title level="m">Repose: Learning deep kinematic priors for fast human pose estimation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Six-dof impedance control based on angle/axis representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciro</forename><surname>Fabrizio Caccavale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Siciliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
