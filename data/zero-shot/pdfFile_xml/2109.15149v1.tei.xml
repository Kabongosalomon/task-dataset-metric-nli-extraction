<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Embedded K-Means Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Guo</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyan</forename><surname>Lin</surname></persName>
							<email>ky.lin@163.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tongji University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Embedded K-Means Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep clustering methods have gained momentum because of the high representational power of deep neural networks (DNNs) such as autoencoder. The key idea is that representation learning and clustering can reinforce each other: Good representations lead to good clustering while good clustering provides good supervisory signals to representation learning. Critical questions include: 1) How to optimize representation learning and clustering? 2) Should the reconstruction loss of autoencoder be considered always? In this paper, we propose DEKM (for Deep Embedded K-Means) to answer these two questions. Since the embedding space generated by autoencoder may have no obvious cluster structures, we propose to further transform the embedding space to a new space that reveals the cluster-structure information. This is achieved by an orthonormal transformation matrix, which contains the eigenvectors of the within-class scatter matrix of K-means. The eigenvalues indicate the importance of the eigenvectors' contributions to the clusterstructure information in the new space. Our goal is to increase the cluster-structure information. To this end, we discard the decoder and propose a greedy method to optimize the representation. Representation learning and clustering are alternately optimized by DEKM. Experimental results on the real-world datasets demonstrate that DEKM achieves state-of-the-art performance. arXiv:2109.15149v1 [cs.</p><p>LG] 30 Sep 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Clustering, an essential data exploratory analysis tool, has been widely studied. Numerous clustering methods have been proposed, e.g., K-means <ref type="bibr" target="#b23">[24]</ref>, Gaussian Mixture Models <ref type="bibr" target="#b2">[3]</ref>, and spectral clustering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. These traditional clustering methods have achieved promising results. However, when the dimension of the data is very high, these traditional methods become inefficient and ineffective because of the notorious problem of the curse of dimensionality.</p><p>To deal with this problem, people usually perform dimensionality reduction before clustering. The commonly used dimensionality reduction methods are Principle Component Analysis (PCA) <ref type="bibr" target="#b31">[32]</ref>, Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b0">[1]</ref>, and Nonnegative Matrix Factorization (NMF) <ref type="bibr" target="#b20">[21]</ref>. But all these methods are linear and shallow models, whose representational abilities are limited. They cannot capture the complex and non-linear relations hidden in the data. Benefiting from the high representational power of deep neural networks (DNNs), autoencoder has been widely used as a dimensionality reduction method for clustering in recent years. Autoencoder can learn meaningful representations of the input data through an unsupervised manner. It consists of an encoder and a decoder. The encoder transforms the input data into a lower- * Corresponding author. The color of data points marks the ground truth and the color of the background block marks the clustering results. Data points falling into the same block belong to the same cluster. Cluster centroids are marked by black crosses. The black line shows the direction of the last eigenvector of the within-class scatter matrix of K-means. In this direction, the cluster-structure information is the lowest. We would like to optimize the representation to increase the cluster-structure information in this direction. We use a mini-batch updating strategy. Numbers in parentheses denote ACC and NMI, respectively. dimensional space (the embedding space), and the decoder is responsible for reconstructing the input data from that embedding space. A plethora of clustering methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> based on autoencoder have been proposed. One of the most critical problems with deep clustering is how to design a proper clustering loss function. Methods such as DEC <ref type="bibr" target="#b40">[41]</ref> and IDEC <ref type="bibr" target="#b9">[10]</ref> minimize the Kullback-Leibler (KL) divergence between the cluster distribution and an auxiliary target distribution. The basic idea is to refine the clusters by learning from their high confidence assignments. However, the auxiliary target distribution is hard to choose for better clustering results. Other methods such as DCN <ref type="bibr" target="#b42">[43]</ref> and DKM <ref type="bibr" target="#b5">[6]</ref> combine the objective of K-means with that of autoencoder and jointly optimize them. However, the discrimination of clusters in the embedding space is not directly related to the reconstruction loss of auoencoder. Thus, in this paper, after using an autoencoder to generate an embedding space, we discard the decoder and do not optimize the reconstruction loss anymore. Because no matter what we do to the embddding space, we can separately train the decoder to reconstruct the input data from their embeddings.</p><p>Considering the simplicity of K-means, we extend it to a deep version, which is called DEKM that uses an autoencoder to generate an embedding space. Since this embedding space may not be discriminative for clustering, we propose to further transform this embedding space to a new space by an orthonormal transformation matrix, which consists of the eigenvectors of the within-class scatter matrix of K-means. These eigenvectors are sorted in ascending order with respect to their eigenvalues. In this new space, the cluster-structure information is revealed. And each eigenvalue indicates how much its corresponding eigenvector contributes to the quantity of the cluster-structure information in the new space. The last eigenvector contributes the least. To increase the clusterstructure information in the direction of the last eigenvector, we discard the decoder and propose a greedy method to optimize the representation. Optimizing representation is equivalent to minimizing the entropy. And this optimization is also in consistent with the loss of K-means. Inspired by the idea that "good representations are beneficial to clustering and clustering provides supervisory signals to representation learning" <ref type="bibr" target="#b43">[44]</ref>, we alternately optimize representation learning and clustering until some criterion is met. <ref type="figure" target="#fig_0">Figure 1</ref> shows these two alternating processes on a subset of MNIST dataset, containing randomly selected four clusters. We set the units of the embedding layer to two for better inspection. <ref type="figure" target="#fig_0">Figure 1</ref>(a)-(d) show the data representations and clustering results in the embedding space during these two alternating processes. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows the initial embedding space generated by an autoencoder. The direction where the black line lies denotes the second eigenvector of the scatter matrix of K-means. The cluster-structure information in this direction is the lowest. To increase the cluster-structure information in this direction, we move data points closer to their centroids. Note that in this process, we use a mini-batch of data to optimize the representation, which makes only some data points closer to their centroids. Compared with <ref type="figure" target="#fig_0">Figure 1</ref>(a), we can see from <ref type="figure" target="#fig_0">Figure 1</ref>(b) that the data points in the red cluster do not move closer to their centroids, while the data points in the other three clusters do. We find in the experiments that this mini-batch updating strategy outperforms the full-batch updating strategy, which moves all the data points closer to their centroids.</p><p>Our contributions can be summarized as follows:</p><p>? We propose to transform the embedding space generated by an autoencoder to a new space using an orthonormal transformation matrix, which contains the eigenvectors of the within-class scatter matrix of K-means. The eigenvalues indicate the importance of their corresponding eigenvectors' contributions to the cluster-structure information in the new space.</p><p>? We develop a greedy method to optimize the representation so that the cluster-structure information in the new space is increased. ? We alternately optimize representation learning and clustering. <ref type="bibr">?</ref> We show the effectiveness of DEKM by carrying out extensive comparative studies with state-of-the-art methods on various real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. K-means and Its Variants</head><p>K-means is one of the most fundamental clustering methods. It is usually used as one of the building blocks of many advanced clustering methods such as spectral clustering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. K-means has inspired many extentions. For example, the basic idea of <ref type="bibr" target="#b13">[14]</ref> is to replace the mean with the median. K-means++ <ref type="bibr" target="#b1">[2]</ref> improves the selection of the initial centroids, which are based on their proportional distance to the previous selected centroids. SubKmeans <ref type="bibr" target="#b25">[26]</ref> assumes that the input space can be split into two independent subspaces, i.e., the clustering subspace and the noise subspace. The former subspace contains only the cluster-structure information and the latter subspace contains only the noise information. SubKmeans performs clustering in the clustering subspace. Nr-Kmeans <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> finds non-redundant K-means clusterings in multiple mutually orthogonal subspaces via an orthogonal transformation matrix. Fuzzy-c-means <ref type="bibr" target="#b4">[5]</ref> assigns each data point proportionally to multiple clusters. It relaxes the hard cluster assignments of K-means to soft cluster assignments. Mini-batch K-means <ref type="bibr" target="#b33">[34]</ref> extends K-means to the scenario of user-facing web applications. Mini-batch K-means can be used in the deep learning frameworks because it supports the online Stochastic Gradient Descent (SGD).</p><p>For the real-world datasets, the number of clusters is unknown. To solve this problem, researchers propose to automatically find the proper number of clusters. X-means <ref type="bibr" target="#b32">[33]</ref> uses the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC) as measures to evaluate the clustering results under different cluster number k. G-means <ref type="bibr" target="#b11">[12]</ref> assumes that each cluster follows a Gaussian distribution. It runs K-means with increasing k hierarchically until the statistical test accepts that the clusters follow Gaussian distributions. PG-means <ref type="bibr" target="#b6">[7]</ref> first constructs an one-dimensional projection of the dataset and the learned model. Then the model fitness is evaluated in the projected space. It is able to discover a proper number of Gaussian clusters. Dip-means <ref type="bibr" target="#b14">[15]</ref> assumes that each cluster follows a unimodal distribution. It first computes the pairwise distance between a data point and the other data points. Then, it applies a univariate statistical hypothesis test <ref type="bibr" target="#b12">[13]</ref> (called Hartigans' dip-test) for unimodality on the distributions of distances to find unimodal clusters and the proper cluster number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Clustering</head><p>Since shallow clustering models suffer from non-linearity of real-world data, they cannot perform well. Deep clustering models use deep neural networks (DNNs) with stronger nonlinear representational capabilities to extract features, which achieve better clustering performances. Earlier deep clustering methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref> perform representation learning and clustering sequentially. Recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> have shown that jointly performing representation learning and clustering yields better performance.</p><p>JULE <ref type="bibr" target="#b43">[44]</ref> proposes a recurrent framework for jointly unsupervised learning of deep representations and clustering. During the optimization procedure, clustering is conducted in the forward pass and representation learning is performed in the backward pass. DCC <ref type="bibr" target="#b34">[35]</ref> jointly performs non-linear dimensionality reduction and clustering. The clustering process includes the optimization of the autoencoder. Since the objective function is continuous without the discrete cluster assignment, it can be solved by standard gradient-based methods. DEC <ref type="bibr" target="#b40">[41]</ref> pre-trains an autoencoder with reconstruction loss and performs clustering to obtain the soft clustering assignment of each data point. Then, an auxiliary target distribution is derived from the current soft cluster assignments. Finally, it iteratively refines clustering by minimizing the Kullback-Leibler (KL) divergence between the soft assignments and the auxiliary target distribution. DCN <ref type="bibr" target="#b42">[43]</ref> combines the objective of K-means with that of autoencoder to find a "K-meansfriendly" space. The cluster assignments of DCN are not soft (probability) like that in DEC, but strict (discrete), which limit the direct use of gradient-based SGD solvers. DCN refines clustering by alternately optimizing the objectives of autoencoder and K-means. DEPICT <ref type="bibr" target="#b7">[8]</ref> consists of two parts, i.e., a convolutional autoencoder for learning the embedding space and a multinomial logistic regression layer functioning as a discriminative clustering model. For jointly learning the embedding space and clustering, DEPICT employs an alternating approach to optimize a unified objective function. IDEC <ref type="bibr" target="#b9">[10]</ref> combines an under-complete autoencoder with DEC. The under-complete autoencoder not only learns the embedding space, but also preserves the local structure of data. Like IDEC, DCEC <ref type="bibr" target="#b10">[11]</ref> combines a convolutional autoencoder with DEC. DKM <ref type="bibr" target="#b5">[6]</ref> proposes a new approach for jointly clustering with K-means and learning representations. The Kmeans objective is considered as a limit of a differentiable function, so that the representation learning and clustering can be optimized by the simple stochastic gradient descent. RED-KC (for Robust Embedded Deep K-means Clustering) <ref type="bibr" target="#b45">[46]</ref> uses the ?-norm metric to constrain the feature mapping of the autoencoder so that data embeddings are more conductive to the robust K-means clustering.</p><p>Our proposed DEKM is also a method that jointly performs representation learning and clustering. Like DCEC, DEKM first uses an autoencoder to find the embedding space. Then, it discards the decoder and optimizes the representation for better clustering. The representation optimization of DEKM is different from that of DCEC, which does not optimize the Kullback-Leibler (KL) divergence between the cluster distribution and an auxiliary target distribution. Instead, DEKM optimizes the representation by reducing its entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP EMBEDDED K-MEANS</head><p>We assume that the cluster structures exist in a lowerdimensional subspace. Instead of clustering directly in the original space, DEKM uses autoencoder to transform the original space into an embedding space to reduce the dimensionality before clustering. DEKM alternately optimizes representation learning and clustering. DEKM has three steps: (1) generating an embedding space by an autoencoder, <ref type="bibr" target="#b1">(2)</ref> detecting clusters in the embedding space by K-means, and <ref type="formula" target="#formula_3">(3)</ref> optimizing the representation to increase the cluster-structure information. The last two steps are alternately optimized to generate better embedding space and clustering results. <ref type="table" target="#tab_0">Table I</ref> shows the used symbols and their corresponding interpretations in this paper. </p><formula xml:id="formula_0">Symbol Interpretation d ? N The dimension of input space e ? N The dimension of embedding space n ? N The number of data points k ? N The number of clusters x ? R d?1 A data point ? ? R e?1 A cluster centroid h ? R e?1 A data point embedding X = [x 1 ; . . . ; xn] Data matrix I ? R e?e Identity matrix V ? R e?e Orthonormal transformation matrix H = [h 1 ; . . . ; hn] Data embeddings U = {? i } k i=1 Set of all cluster centroids C = {C 1 , . . . , C k } Set of clusters f (?)</formula><p>Encoder g(?) Decoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generating an Embedding Space</head><p>Autoencoder is a type of deep neural networks (DNNs) that can learn low-dimensional representations of the input data in an unsupervised way. It consists of an encoder and a decoder. The encoder f (?) transforms the input data into a lower-dimensional space (the embedding space), and the decoder g(?) reconstructs the input data from the embedding space. Autoencoder is trained to minimize the reconstruction loss, such as the least-squares error:</p><formula xml:id="formula_1">min L 1 = n i=1 x i ? x i 2 = n i=1 x i ? g (f (x i )) 2 (1) where x i is the i-th data point, f (x i ) is the output of the encoder f (?)</formula><p>, and x i is the reconstructed output of the decoder g(?). The dimension of the embedding space usually is set to a number that is much smaller than that of the original space. This not only mitigates the curse of dimensionality, but also helps to avoid the trivial solution of autoencoder, of which both f (?) and g(?) equal to the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detecting Clusters</head><p>In the above section, we train the autoencoder using the least-squares error loss to generate an embedding space H = f (X), without considering the characteristics of the embedding space. This embedding space may not contain any cluster structures. DCN <ref type="bibr" target="#b42">[43]</ref> combines the objective function of the autoencoder with that of K-means and optimizes them alternately. DCN wants to find a "K-means-friendly" subspace. However, the relative importance parameter between the two objective functions is hard to set. In addition, this paradigm is difficult to generate a "K-means-friendly" subspace because of the reconstruction loss of the autoencoder. In the optimization procedure, the reconstruction loss of the autoencoder should not be used anymore. The reason is that whatever we modify on the encoder, we can still train the decoder to make Equation (1) minimized.</p><p>We use K-means <ref type="bibr" target="#b23">[24]</ref> to find a partition {C i } k i=1 of the data points in the embedding space H. Its objective function is as follows:</p><formula xml:id="formula_2">min L 2 = k i=1 h?Ci h ? ? i 2 (2)</formula><p>where h is a data point in the embedding space, k is the number of clusters, C i denotes the set of data points assigned to the i-th cluster, ? i = 1 |Ci| h?Ci h denotes the centroid of the i-th cluster.</p><p>To reveal the cluster structures in the embedding space, we propose to transform the embedding space H to a new space by an orthonormal transformation matrix V. In the new space Y = VH, Equation (2) becomes the following:</p><formula xml:id="formula_3">min L 3 = k i=1 h?Ci Vh ? V? i 2 = k i=1 h?Ci (Vh ? V? i ) (Vh ? V? i ) = k i=1 h?Ci (h ? ? i ) V V (h ? ? i ) = k i=1 h?Ci Trace ((h ? ? i ) V V (h ? ? i ))<label>(3)</label></formula><p>where we use the trace-trick in the last step, because a scalar can also be considered as a matrix of size 1 ? 1.</p><p>Since V V = I, minimizing Equation <ref type="formula" target="#formula_3">(3)</ref> is equivalent to minimizing Equation (2).</p><p>The above equation can be further written as:</p><formula xml:id="formula_4">min L 3 = Trace V k i=1 h?Ci (h ? ? i ) (h ? ? i ) V = Trace (VS w V )<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">S w = k i=1 h?Ci (h ? ? i ) (h ? ? i )</formula><p>is the withinclass scatter matrix of K-means.</p><p>Since V is an orthonormal matrix, minimizing Equation <ref type="formula" target="#formula_4">(4)</ref> is a standard trace minimization problem. A version of the Rayleigh-Ritz theorem <ref type="bibr" target="#b24">[25]</ref> indicates that the solution V contains the eigenvectors of S w with ascending eigenvalues. The eigenvalues indicate the importance of the eigenvectors' contributions to the cluster structures in the transformed space Y = VH. The smaller the eigenvalue, the more important its corresponding eigenvector contributes to the cluster structures in the transformed space Y. We should note that S w is symmetric and therefore it is orthogonally diagonalizable. So it is feasible to find the orthonormal matrix V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimizing Representation</head><p>As discussed above, minimizing Equation (3) equals to minimizing Equation <ref type="bibr" target="#b1">(2)</ref>. We can first perform K-means in the embedding space H to get S w , and then eigendecompose S w to get V. Finally, we transform the embedding space to a new space Y that reveals the cluster-structure information. We also know the importance of each dimension of Y in terms of the cluster-structure information, i.e., the last dimension has the least cluster-structure information.</p><p>We can rewrite Equation <ref type="formula" target="#formula_3">(3)</ref> as follows:</p><formula xml:id="formula_6">min L 3 = k i=1 y?Ci y ? m i 2<label>(5)</label></formula><p>where y = Vh and m i = V? i . Now the question is how to optimize the representation to improve the cluster-structure information in Y. In this paper, we measure the cluster-structure information by entropy. The lower the entropy of data, the higher the cluster-structure information it contains. For example, <ref type="figure" target="#fig_5">Figure 2</ref> as shown in <ref type="bibr" target="#b28">[29]</ref>. Thus, the entropy of data in <ref type="figure" target="#fig_5">Figure 2</ref>(b) is 1.419 bits, and that of data in <ref type="figure" target="#fig_5">Figure 2</ref>(c) is 0.033 bits. From <ref type="figure" target="#fig_5">Figure 2</ref>, we have two observations: 1) Compared with data that has no cluster-structure information, data that contains the cluster-structure information has lower entropy. 2) Reducing the variance of a Gaussian cluster will reduce its entropy. Also note that the clusters found by K-means follow isotropic Gaussian distributions. Based on these, we propose a new objective to optimize the representation to increase the clusterstructure information. Specifically, we would like to move the data points in clusters found by K-means closer to their centroids. This strategy is also in consistent with Equation <ref type="formula">(2)</ref>, which is further minimized. Instead of moving data points closer to their respective centroids in each dimension of Y, we propose a greedy method that only moves data points closer to their respective    centroids in the last dimension of Y. In this way, the representation will be easily optimized to increase the cluster-structure information. We found in our experiments that this greedy method performs the best. The details of the greedy method is as follows: We first replicate y as y and then replace the last dimension of y with the last dimension of m i . Finally, The objective function is defined as follows:</p><formula xml:id="formula_7">min L 4 = k i=1 y?Ci y ? y 2<label>(6)</label></formula><p>We do not use a full-batch updating strategy that moves all the data points closer to their centroids in the last dimension. Instead, we use a mini-batch updating strategy that moves only some mini-batches of data points closer to their centroids. We find that the mini-batch updating strategy is superior to the full-batch updating strategy. After optimizing the representation, we have a new embedding space H. Then, we perform K-means again to find clusters and their respective centroids. We alternately repeat the second and third steps until some criterion is met, such as the predefined number of iterations or there are less than 0.1% of samples that change their clustering assignments between two consecutive iterations. The pseudo-code of DEKM is shown in Algorithm 1. Line 1 uses Equation (1) to train the autoencoder. Line 3 uses the encoder to generate an embedding space H = f (X). Then in the embedding space, line 4 performs K-means to find clusters. Lines 5-6 compute the within-class scatter matrix S w and eigendecompose it to get the orthonormal transformation matrix V. Line 7 uses Equation (6) to optimize the representation. We repeat the process for a number Iter of iterations and return the final cluster set C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION A. Datasets</head><p>To evaluate the performance and generality of DEKM, we conduct experiments on benchmark datasets and compare with state-of-the-art methods. In order to show that DEKM Compute the within-class scatter matrix</p><formula xml:id="formula_8">S w ? k i=1 h?Ci (h ? ? i ) (h ? ? i ) ; 6 V ? Eig (S w );</formula><p>/ * V contains the eigenvectors sorted in ascending order w.r.t. their eigenvalues * / <ref type="bibr" target="#b6">7</ref> Using Equation (6) to optimize the representation; 8 end 9 return C works well on various datasets, we choose four image datasets that cover domains such as handwritten digits, objects, and human faces and three text datasets. <ref type="table" target="#tab_0">Table II</ref> provides a brief description for each dataset. And some examples in the image datasets are shown in <ref type="figure">Figure 3</ref>.</p><p>MNIST <ref type="bibr" target="#b19">[20]</ref> is a dataset that consists of 70,000 hand-written grayscale digit images. Each image is of size 28?28 pixels. USPS is a dataset of handwritten grayscale digit images from the USPS postal service, containing 9,298 images of size 16?16 pixels. COIL-20 <ref type="bibr" target="#b29">[30]</ref> is a dataset containing 1,440 color images of 20 objects (72 images per object). The objects have a wide variety of complex geometric and reflectance characteristics. The size of each image is resized to 28?28 pixels. FRGC is a human face dataset. Following <ref type="bibr" target="#b43">[44]</ref>, we randomly select 20 subjects from the original dataset and collect their 2,462 face images. Similarly, we crop the face regions and resize them into 28?28 pixels. For all the image datasets, each image is normalized by scaling between 0 and 1. REUTERS-10K <ref type="bibr" target="#b40">[41]</ref> contains a random subset of 10,000 samples from REUTERS dataset which has about 810,000 English news stories. REUTERS-10K contains four categories: corporate/industrial, government/social, markets, and economics. The 20 Newsgroups dataset (20NEWS) <ref type="bibr" target="#b18">[19]</ref> contains 18,846 documents labeled into 20 different classes, each corresponding to a different topic. Reuters Corpus Volume I (RCV1) <ref type="bibr" target="#b21">[22]</ref> contains 804,414 manually categorized newswire stories. Following <ref type="bibr" target="#b5">[6]</ref>, we sample from the full RCV1 collection a random subset of 10,000 documents from the largest four categories, denoted by RCV1-10K. For the three text datasets, we represent each document as a tf-idf feature vector on the 2,000 most frequently occurring word stems. And each sample x i is normalized so that 1 d x i 2 2 is approximately 1, where d is the dimension of the input space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Methods</head><p>We compare the proposed DEKM with the following methods:</p><p>? K-means: We perform K-means on the original data. ? PCA+K-means: We perform K-means in the space spanned by the first p principal components of data using Principal Component Analysis (PCA), where p is selected to keep 90% of data variance. ? AE+K-means: We perform K-means in the embedding space of our pretrained convolutional/MLP autoencoder. ? DEC <ref type="bibr" target="#b40">[41]</ref>: It uses an MLP autoencoder to find the embedding space and then performs clustering in the embedding space by minimizing the Kullback-Leibler (KL) divergence between the cluster distribution and a target Student's t-distribution.</p><p>? DCEC <ref type="bibr" target="#b10">[11]</ref>: It replaces the MLP autoencoder in DEC with a convolutional autoencoder. ? IDEC <ref type="bibr" target="#b9">[10]</ref>: It replaces the MLP autoencoder in DEC with a under-complete autoencoder that preserves the local structure of data. ? DCN <ref type="bibr" target="#b42">[43]</ref>: It combines the objective function of the MLP autoencoder with that of K-means and alternately optimizes them. ? DKM <ref type="bibr" target="#b5">[6]</ref>: It considers the K-means objective as a limit of a differentiable function and adopts stochastic gradient descent to jointly optimize representation learning and clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Settings</head><p>Since convolutional neural network (CNN) is good at capturing the semantic visual features of input images, we exploit convolutional autoencoder to find the embedding space for the image datasets. Specifically, we use three convolutional layers followed by a dense layer (embedding layer) in the encoder-to-decoder pathway. The channel numbers of the three convolutional layers are 32, 64, and 128 respectively. The kernel sizes are set to 5?5, 5?5, and 3?3 respectively. The stride of all the convolutional layers is set to two. The number of neurons in the embedding layer is set to the number of clusters of datasets. The decoder is a mirror of the encoder and the output of each layer of the decoder is appropriately zero-padded to match the input size of the corresponding encoder layer. All the intermediate layers of the convolutional autoencoder are activated by ReLU <ref type="bibr" target="#b16">[17]</ref>. For the text datasets, we use a fully connected multilayer perceptron (MLP) for the backbone of autoencoder. Following the settings in DEC <ref type="bibr" target="#b40">[41]</ref>, the encoder has dimensions of d-500-500-2000-10, where d is the dimension of the input data. The decoder is a mirror of the encoder. All the intermediate layers are activated by ReLU. The weights of all the layers are initialized by Xavier approach <ref type="bibr" target="#b8">[9]</ref>. The Adam <ref type="bibr" target="#b15">[16]</ref> optimizer is adopted with the initial learning rate l = 0.001, ? 1 = 0.9, ? 2 = 0.999. We stop the clustering process when there are less than 0.1% of samples that change their clustering assignments between two consecutive iterations. Our code is available at: https://github.com/spdj2271/DEKM. All the experiments are conducted on a machine with one Intel(R) Xeon(R) CPU (2.00GHz) and one Nvidia Tesla P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metric</head><p>To evaluate the clustering methods, we adopt two standard evaluation metrics: normalized mutual information (NMI) <ref type="bibr" target="#b38">[39]</ref> and unsupervised clustering accuracy (ACC) <ref type="bibr" target="#b41">[42]</ref>. Both the NMI and ACC values are in the range [0, 1]. The higher the values, the better the clustering results. NMI is an informationtheoretic measure, which calculates the normalized measure of similarity between the ground-truth labels and the obtained cluster assignments. NMI is defined as follows:</p><formula xml:id="formula_9">NMI = 2 ? I(G; C) H(G) + H(C)<label>(7)</label></formula><p>where G is the ground-truth, C is the cluster assignments, I denotes mutual information, and H denotes entropy. ACC measures the proportion of samples whose cluster assignments can be correctly mapped to the ground-truth labels. ACC is defined as follows:</p><formula xml:id="formula_10">ACC = max m n i=1 1 {g i = m (c i )} n (8)</formula><p>where g i is the ground-truth label of the i-th data point, c i is the cluster assignment of the i-th data point, m ranges over all possible one-to-one mappings between ground-truth labels and cluster assignments. The mapping is based on the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Clustering Results</head><p>Due to randomizations of the weights of autoencoder and the centroids of K-means, we run DEKM on each dataset for three times and report the average results. <ref type="table" target="#tab_0">Table III</ref> reports the clustering performances of different methods on the benchmark datasets in terms of normalized mutual information (NMI) and unsupervised clustering accuracy (ACC). For the comparison methods, we present the reported results from their original papers if they are available (marked by * in <ref type="table" target="#tab_0">Table III</ref>). For unreported results on specific datasets, we run the released code mentioned in the original papers. When the released code is not available or running it is not practical, we put dash marks (?) in <ref type="table" target="#tab_0">Table III</ref>.</p><p>We can see from <ref type="table" target="#tab_0">Table III</ref> that DEKM outperforms the comparison methods on most of the datasets. DEKM achieves competitive results to DCEC on COIL-20 in terms of NMI. DEKM outperforms all the comparison methods with a large margin on MNIST, 20NEWS and RCV1-10K. DEKM F uses the full-batch updating strategy to optimize the representation. Compared with DEKM, DEKM F performs slightly worse on four datasets MNIST, USPS, FRGC, and RCV1-10K. As can be seen, all the deep clustering methods perform much better than the traditional shallow clustering methods (i.e., K-means and K-means+PCA). This suggests that the embedding space generated by autoencoder is more advantageous for clustering. The performance gap between DEKM and AE+K-means is large, which means our representation optimization strategy is promising. Both DEKM and DCEC use convolutional autoencoders to find the embedding space for the image datasets. The performance gap between DEKM and DCEC reflects the effect of different representation optimization strategy. The representation optimization strategy of DEKM is superior to that of DCEC. Note that DCEC replaces the MLP autoencoder in DEC with a convolutional autoencoder. For the text datasets, DCEC using the MLP autoencoder is equivalent to DEC. Compared with DCEC on the text datasets, we can see that DEKM also performs better. Thus, the representation optimization strategy of DEKM works in different scenarios, making DEKM a universal clustering framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Representation Optimization Strategy</head><p>We examine the effects of several representation optimization strategies on the MNIST dataset. Specifically, we compare the following strategies: 1. reducing the entropy of the last dimension of Y, 2. reducing the entropy of a random dimension of Y, and 3. reducing the entropy of all the dimensions of Y. We also compare with another two strategies: 4. reducing the entropy of a random dimension of H, and 5. reducing the entropy of all the dimensions of H. Note that all these strategies use the mini-batch updating strategy. <ref type="figure" target="#fig_7">Figure 4</ref> shows the comparison results. We can see that the first strategy of entropy reduction in the last dimension of Y is the best. It outperforms the other four strategies with a large margin. Strategy 2 performs better than strategy 4. Strategy 3 performs similarly to strategy 5.     <ref type="table" target="#tab_0">Table III</ref>. Compared with the clusters in the initial embedding space (as shown in <ref type="figure" target="#fig_8">Figure 5(b)</ref>) of the convolutional autoencoder, the clusters in the embedding space (as shown in <ref type="figure" target="#fig_8">Figure 5(d)</ref>) of DEKM are more focused and isotropic, which are good for K-means. The two clusters in the embedding space of DEC are mixed, which leads to a lower performance compared with DEKM. The clusters in the embedding space of PCA are not isotropic Gaussian clusters, which is the reason why K-means does not perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Embedding Space Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed a new deep clustering method called DEKM that alternately learns the deep embedding space and finds clusters inside. First, we train an autoencoder to generate an embedding space. Then in this embedding  <ref type="table" target="#tab_0">Method  MNIST  USPS  COIL-20  FRGC  REUTERS-10K  20NEWS  RCV1-10K  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  K-means  53.</ref>  space we find clusters by K-means. We eigendecompose the within-class scatter matrix of K-means to get an orthonormal transformation matrix, which is further used to transform the embedding space to a new space that reveals the clusterstructure information. Each row vector of the orthonormal transformation matrix is the eigenvector of the within-class scatter matrix. The eigenvalue indicates the importance of the corresponding eigenvector's contribution to the clusterstructure information in the new space. Finally, we propose a greedy method to optimize the representation to increase the cluster-structure information in the new space. We optimize DEKM in an alternating way. Experimental results show that DEKM achieves superior performances compared to the baselines. In the future, we would like to extend DEKM to find non-redundant clusters in multiple independent subspaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Iter 0, (92.0, 79.9) (b) Iter 1, (94.6, 84.3) (c) Iter 2, (94.8, 84.9) (d) Iter 20, (97.0, 89.8) Visualization of the representation learning and clustering by DEKM on a subset of MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 400 ln 1 400= 5 .</head><label>115</label><figDesc>(a) shows 400 data points that follow a uniform distribution in the range [0, 10]. The entropy is ? 400 i=1 992 bits. Compared with Figure 2(a), there are two Gaussian clusters in Figure 2(b), each of which has 200 data points. The two Gaussian clusters have the same variance of one. The two Gaussian clusters in Figure 2(c) have the same variance of 0.5, each of which also has 200 data points. Note that the entropy of a ddimensional Gaussian distribution is d 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A uniform distribution. The entropy is 5.992 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Two Gaussian distributions with large variance. The entropy is 1.419 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Two Gaussian distributions with low variance. The entropy is 0.033 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Demonstration of the relationships between entropy and cluster-structure information. The lower the entropy of data, the higher the cluster-structure information it contains. Entropy decreases from (a) to (c), while cluster-structure information increases from (a) to (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 4 (</head><label>14</label><figDesc>DEKM Input: Data matrix X, the number of clusters k, and the maximum number of iterations Iter Output: Cluster set C = {C 1 , . . . , C k } 1 Using Equation (1) to train the autoencoder; 2 for i ? 1 to Iter do 3 Generate an embedding space by the encoder H = f (X); C, U) ? Kmeans (H, k);5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>the last dimension of Y Strategy 2: a random dimension of Y Strategy 3: all the dimensions of Y Strategy 4: a random dimension of H Strategy 5: all the dimensions of H The clustering performances of the different representation optimization strategies on MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5</head><label>5</label><figDesc>illustrates the t-SNE<ref type="bibr" target="#b37">[38]</ref> visualization of the embedding spaces of different algorithms on MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 (</head><label>5</label><figDesc>a) demonstrates the embedding space of PCA. Figure 5(b) illustrates the embedding space of a convolutional autoencoder, which is the initial embedding space for DEKM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 (</head><label>5</label><figDesc>c) shows the embedding space of DEC. And Figure 5(d) shows the embedding space of DEKM. Note that all these embedding spaces are used to get the clustering results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 .</head><label>5</label><figDesc>The t-SNE visualization of the embedding spaces of different algorithms on MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SYMBOLS</head><label>I</label><figDesc></figDesc><table /><note>AND INTERPRETATIONS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">DESCRIPTION OF DATASETS</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Sample # Cluster # Dimensions</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>10</cell><cell>28?28?1</cell></row><row><cell>USPS</cell><cell>9,298</cell><cell>10</cell><cell>16?16?1</cell></row><row><cell>COIL-20</cell><cell>1,440</cell><cell>20</cell><cell>28?28?1</cell></row><row><cell>FRGC</cell><cell>2,462</cell><cell>20</cell><cell>32?32?3</cell></row><row><cell>REUTERS-10K</cell><cell>10,000</cell><cell>4</cell><cell>2,000</cell></row><row><cell>20NEWS</cell><cell>18,846</cell><cell>20</cell><cell>2,000</cell></row><row><cell>RCV1-10K</cell><cell>10,000</cell><cell>4</cell><cell>2,000</cell></row><row><cell cols="4">Fig. 3. Some image samples, from top row to bottom row, come from MNIST,</cell></row><row><cell cols="3">USPS, COIL-20 and FRGC datasets, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLUSTERING</head><label>III</label><figDesc>RESULTS OF DIFFERENT ALGORITHMS IN TERMS OF THE UNSUPERVISED CLUSTERING ACCURACY (ACC%) AND NORMALIZED MUTUAL INFORMATION (NMI%). THE RESULTS MARKED BY * COME FROM THE ORIGINAL PAPERS. ? DENOTES THAT THE RESULT IS NOT AVAILABLE. DEKM F MEANS THAT DEKM USES THE FULL-BATCH UPDATING STRATEGY.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank anonymous reviewers for their constructive and helpful comments. This work was </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A fuzzy relative of the isodata process and its use in detecting compact well-separated clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep kmeans: Jointly clustering with k-means and learning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pg-means: learning the number of clusters in data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5736" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>AIS-TATS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning the k in k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="281" to="288" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The dip test of unimodality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="84" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Algorithms for clustering data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dip-means: an incremental clustering method for estimating the number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalogeratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2393" to="2401" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminatively boosted image clustering with fully convolutional auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Handbook of matrices. Computational statistics and Data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">243</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards an optimal subspace for k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>B?hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discovering non-redundant k-means clusterings in optimal subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>B?hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1973" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Non-redundant subspace clusterings with nr-kmeans and nr-dipmeans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>B?hm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TKDD</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Theory of information and coding. a mathematical framework for communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mceliece</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Columbia image object library (coil-20)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>CUCS- 006-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">X-means: Extending kmeans with efficient estimation of the number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01449</idno>
		<title level="m">Deep continuous clustering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A deep semi-nmf model for learning hidden representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fuse: Full spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goebl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>B?hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1985" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust embedded deep k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1181" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
