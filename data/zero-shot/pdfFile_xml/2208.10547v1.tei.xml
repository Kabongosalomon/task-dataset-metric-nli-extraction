<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InstanceFormer: An Online Video Instance Segmentation Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Koner</surname></persName>
							<email>koner@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Hannan</surname></persName>
							<email>hannan@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suprosanna</forename><surname>Shit</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schubert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Seidl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InstanceFormer: An Online Video Instance Segmentation Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent transformer-based offline video instance segmentation (VIS) approaches achieve encouraging results and significantly outperform online approaches. However, their reliance on the whole video and the immense computational complexity caused by full Spatio-temporal attention limit them in real-life applications such as processing lengthy videos. In this paper, we propose a single-stage transformer-based efficient online VIS framework named InstanceFormer, which is especially suitable for long and challenging videos. We propose three novel components to model short-term and long-term dependency and temporal coherence. First, we propagate the representation, location, and semantic information of prior instances to model short-term changes. Second, we propose a novel memory cross-attention in the decoder, which allows the network to look into earlier instances within a certain temporal window. Finally, we employ a temporal contrastive loss to impose coherence in the representation of an instance across all frames. Memory attention and temporal coherence are particularly beneficial to long-range dependency modeling, including challenging scenarios like occlusion. The proposed InstanceFormer outperforms previous online benchmark methods by a large margin across multiple datasets. Most importantly, InstanceFormer surpasses offline approaches for challenging and long datasets such as YouTube-VIS-2021 and OVIS. Code is available at https://github.com/rajatkoner08/InstanceFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video Instance Segmentation (VIS) <ref type="bibr" target="#b30">[31]</ref> aims to simultaneously classify, segment, and track objects throughout video sequences. Therefore, VIS provides a holistic video understanding for various downstream tasks like autonomous driving and AR/VR applications. A vast amount * Both authors contributed equally to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTVIS-21 AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask2Former</head><p>Online Offline # Params (Millions) <ref type="figure">Figure 1</ref>. Comparison of performance (AP) vs. model size (params) in YTVIS-21 with existing offline and online models. InstanceFormer outperforms all existing methods with comparable or fewer parameters. of literature has been proposed to address VIS and can be grouped into two categories offline and online. Offline methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref> process an entire video at once while online methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> process it sequentially as a stream.</p><p>Recent transformer <ref type="bibr" target="#b24">[25]</ref> based offline methods achieve remarkable success on VIS. These approaches predominantly employ Spatio-temporal attention <ref type="bibr" target="#b25">[26]</ref> on the complete video or accumulate all frame-specific instance representations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref>. Thanks to the simultaneous attention over all frames from past and future, offline approaches can classify and segment instances with high precision even under challenging scenarios. However, relying on full videos and having an intractable Spatio-temporal complexity limits the application of offline methods to real-world scenarios where the videos are very long or streamed online. In contrast, online approaches can be employed in real-time and are applicable to long videos because of their sequential frame-by-frame processing. However, current online approaches suffer from a large performance gap compared to offline methods. The performance gap can mostly be attributed to a lack of long-term temporal dependency <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> or semantic data association <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>. We set three objectives to address these major obstacles for improving online VIS.  <ref type="figure">Figure 2</ref>. Comparison of attention scheme for a particular instance query (red) for recent transformer-based offline VIS and ours. Active objects (green) send information to the instance query (red) via self-/cross-attention or any combinations, while passive objects (grey) remain idle. Note that InstanceFormer drastically sparsifies Spatio-temporal attention by stressing the valuable past and the current information.</p><p>In a video, object instances usually change their appearance or location gradually. To explicitly capture the gradual changes in a scene, our first objective is to emphasize the immediate past with a robust instance propagation module. Next, we argue that instead of full Spatio-temporal attention, which requires intractable computational memory for long videos, one could encode the necessary semantic and temporal information in a considerably more compact way. Humans rarely need to memorize every detail in a frame; instead, we extract a high-level compact representation of the recent past and differentially recognize the upcoming video feed. A key component in this process is our working memory. To this end, our second objective is to adopt a realization of working memory into VIS. Finally, as a third objective, this memory must be temporally coherent to model real-life challenges such as occlusion.</p><p>Keeping the above objectives in mind, we propose In-stanceFormer, an efficient framework for online VIS based on Deformable-DETR <ref type="bibr" target="#b33">[34]</ref>. In Deformable-DETR, instance queries are responsible for efficient instance representation and learned through sparsified cross-attention from image features by restricting the attention to a set of reference points. Hence, we argue that the reference points are excellent markers of compact instance location. Complementary to the reference points, the class-score provides semantic information about the instances. Utilizing reference points, class scores, and instance queries, we efficiently establish inter-frame communication via a prior propagation module without cumbersome Spatio-temporal attention or additional data-association <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. We derive the current frame's instance queries and reference points from those of the previous frame. Thus, a specific instance query can be used to represent an instance throughout the sequence. In addition, the current frame's categorization incorporates historical confidence scores to ensure the consistent and reliable classification of instances. Note that TrackFormer <ref type="bibr" target="#b20">[21]</ref> addresses multi-object-tracking with query propagation as well. However, we significantly differ from them in terms of query initialization, reference point propagation, and scope of application.</p><p>In itself, prior propagation is insufficient for modeling scenarios like occlusions. Therefore, we introduce a memory mechanism that maintains a consistent representation and anticipates possible instance trajectories. We propose a memory queue of fixed temporal width that stores a compact representation of a specified number of past instances. We exploit this stored memory in an additional cross-attention layer with the instance queries, which enables the current queries to be aware of past changes and anticipate deviations. Further, we aim to inject discriminative features into the instance representation to increase the similarity of the same instances and promote diversity among dissimilar instances across different time points. Therefore, we impose supervised contrastive <ref type="bibr" target="#b12">[13]</ref> training in the temporal direction to facilitate easy differentiability of the instances.</p><p>In summary, our contributions are fourfold:</p><p>? We propose InstanceFormer, a simple single-stage transformer-based online VIS framework eliminating the need for an external tracking head or data association. ? We introduce a novel prior propagation module using reference points, class scores, and instance queries to enable efficient communication between consecutive frames. ? We propose a novel memory module to which the current instance queries attend to recollect the recent past. Additionally, temporally contrastive training makes the memory discriminative and easy to identify. ? InstanceFormer sets a new state-of-the-art online VIS by a large margin on YTVIS-19/21 and OVIS datasets. Crucially, InstanceFormer is the first online method that even outperforms the offline methods in the challenging YTVIS-21 (c.f. <ref type="figure">Fig. 1</ref>) and OVIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video instance segmentation <ref type="bibr" target="#b30">[31]</ref> extends the task of image segmentation by tracking each instance throughout the  <ref type="figure">Figure 3</ref>. The overall architecture of InstanceFormer. Our key contributions are proposed propagation (representation, reference point, and class distribution) module and the memory decoder with a compact memory queue derived from valid instance queries. For a given frame, a deformable encoder extracts the image feature. In memory decoder, the initial instance queries attend to the image features and memory queue. Thereafter, the learned queries and reference points for deformable attention and class distribution are passed to the next frame. The memory decoder is elaborated on the right inset.</p><p>videos. It can be grouped into two primary categories; offline, which processes an entire video simultaneously, and online, which processes a video sequentially.</p><p>Offline VIS: Initial attempts for offline VIS, relied on mask propagation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Inspired by DETR <ref type="bibr" target="#b3">[4]</ref>, there has been a recent surge of offline transformer-based end-toend VIS frameworks. These approaches exploited instance queries for video-level instance understanding. VisTR <ref type="bibr" target="#b25">[26]</ref> trivially extended DETR and proposed frame-specific instance queries and their association for the whole video. IFC <ref type="bibr" target="#b11">[12]</ref> established an inter-frame communication mechanism using a custom token. Later, Mask2Former <ref type="bibr" target="#b4">[5]</ref>, and SeqFormer <ref type="bibr" target="#b26">[27]</ref> significantly improved the performance across multiple datasets. SeqFormer shares the initial instance query of each frame to learn a video-level instance embedding. Despite their impressive performance, offline methods are not suitable for real-life applications or long video sequences due to their reliance on full videos and substantial memory requirements. <ref type="figure">Figure 2</ref> compares the attention mechanism of existing transformer-based VIS methods with our proposed InstanceFormer.</p><p>Online VIS: In contrast, online methods are more challenging because of their inaccessibility to future frames. In recent time, a large number of online-VIS methods have been proposed that employ sequential frame-by-frame processing. Mask-Track-RCNN <ref type="bibr" target="#b30">[31]</ref> uses Mask-RCNN with a tracking head to assign instances to candidate boxes. <ref type="bibr" target="#b31">[32]</ref> proposes a cross-over learning scheme over the past temporal domain. QueryInst <ref type="bibr" target="#b5">[6]</ref> uses query-based frame-wise segmentation and a tracking head. The current state-of-the-art (SOTA) online VIS method is VISOLO <ref type="bibr" target="#b7">[8]</ref> which uses grid memory to store information from earlier frames. However, the current online methods, including VISOLO, suffer from a large performance gap compared to the offline ones due to inadequate exploration of trajectories and representations of past time frames. In this work, we aim to strengthen the performance of online methods by propagating prior instance characteristics over short-and long-term intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall architecture of InstanceFormer is shown in <ref type="figure">Fig. 3</ref>. Our method inherits the deformable encoder from Deformable-DETR <ref type="bibr" target="#b33">[34]</ref> and employs a novel prior propagation module followed by the proposed contrastively trained memory decoder. Finally, it has three heads: classification, box regression, and segmentation. It sequentially processes each frame in an online fashion. In the following, we will discuss each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deformable Encoder</head><p>Given an input video frame x t ? R H?W ?3 at time t, of height H and width W , a feature extractor, e.g., a ResNet-50 <ref type="bibr" target="#b10">[11]</ref> provides a multi-scale spatial feature map. We apply a projection layer (1?1 convolution with channel dimension of C = 256) on the extracted features. The projection layer transforms the feature map from different scales to a fixed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prior propagation Module</head><p>The deformable decoder learns an instance representation by restricting attention to a few reference points close to the object instead of the whole image. Reference points are learnable and conditional to their instance query. This makes the attention sparse, computationally fast, and improves the overall object detection. Instances in a video often change their appearance and trajectory gradually over time. Hence, once an instance is localized, it is easier to calculate the offset from the earlier location than from scratch. Therefore to compute the current frame, we take advantage of instance representations, reference points, and class scores from the previous frame.</p><p>Location Propagation: Representation propagation is helpful for high-level instance continuation, but it alone may not be sufficient for consistent low-level location and trajectory modeling. Hence, we explicitly propagate the prior location in terms of reference points of deformable attention. The reference points are dedicated markers of important spatial regions, including object boundaries or key points to which the instance queries attend to. Thus, the reference points are arguably the most representative feature for a specific instance as shown in <ref type="figure" target="#fig_1">figure 4</ref>. Note that InstanceFormer is the first method propagating reference points instead of bounding box <ref type="bibr" target="#b20">[21]</ref> or segmentation masks <ref type="bibr" target="#b1">[2]</ref>.</p><p>As reference points are learnable, this propagation is flexible and easy to adapt under incremental changes. Thus, the incremental modification of reference points is analogous to the implicit trajectory, or motion prediction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref>. At time t for the i th instance, reference points (ref i t ) are computed as:</p><formula xml:id="formula_0">ref i t = ?(W ref (q i t )) if t = 0 ?(?(W ref (q i t )) ? ref i t?1 ) if t &gt; 0,<label>(1)</label></formula><p>where W ref , a linear layer, predicts reference points for each query, ? is the sigmoid function that projects a reference point to a normalized 2D image coordinate. Using the previous location as a starting point, the network learns to determine the offset for the current frame. As a result, the deformable cross attention benefits from the past location and helps to maintain a consistent instance trajectory.</p><p>Class Prior: Despite the representation and location prior, variation in appearance across frames makes the final instance classification inconsistent. To smooth out the probability of the class across frames, we propagate the class probability of past frames as a weighted sum. At time t the class probability c i t of the i th instance is calculated as,</p><formula xml:id="formula_1">c i t = ?(W cls (q i t )), c i t =? i t ? Softmax(?(T cls ([c i f ] t?1 f =t?d ))),<label>(2)</label></formula><p>where the classification head consists of W cls , temporal linear layer (T cls ), and a sigmoid activation. We concatenate the class distribution of the previous d frames, process them through a linear layer T cls , and apply a Sof tmax over the temporal dimension. Eq. 2 essentially gathers class distribution over a short temporal domain and amplifies the most consistent class score in the current frame. Altogether, our prior propagation provides a frame-toframe inductive bias to replace the cumbersome Spatiotemporal attention and eliminates external tracking. This, in turn, reduces network overhead by computing the offset from the earlier frame. At the same time, it strengthens the temporal agreement between frames in the presence of gradual changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Memory Attention</head><p>Although prior propagation is sufficient for simple videos, it suffers in complex scenarios and accumulates errors from past frames. For example, in the case of occlusion, a heavy reliance on the immediate prior may lose its effectiveness. Hence, an instance may lose its trace, resulting in performance drop. To alleviate this problem in an online fashion, we introduce a memory module to recollect instances from past frames. This recollection is achieved via our proposed instance query to memory cross-attention. Consequently, it allows an instance query to look into its past and correct any possible mistake. Memory in this case consists of memory tokens ([m i f ] i,f ) where i and f denote the instance index and relative temporal positions. The memory token m i f is defined as:</p><formula xml:id="formula_2">m i f = W m proj (sg(q i f , b i f , c i f )),<label>(3)</label></formula><p>where b i t?1 is box location, sg is the stop-gradient and W m proj ? R C is a linear projection. To get a compact temporal perspective, we take k(k &lt;&lt; N ) number of instances for each of the past d(d &lt;&lt; T ) frames. Note that the number of tokens and memory size is much smaller than the number of instance queries (N ) and the video length (T ), respectively. Thus, having a small number of instances with discriminative features from past frames helps the network to learn continual temporal dependencies while keeping the additional computation in check. During training, we keep the matched query instances in the memory, and during inference, we keep the top-k query instances based on the class confidence. Additionally, we introduce a custom temporal-index embedding for memory tokens. This temporal-index embedding contains two parts: first, an index position embedding <ref type="bibr" target="#b3">[4]</ref> of the instance queries at a given frame and second, a temporal 1D sinusoidal <ref type="bibr" target="#b24">[25]</ref> embedding of memory tokens relative to the current frame. Our memory module maintains a fixed-size temporal queue of d frames with k number of instances per frame. As time proceeds, the oldest memory is replaced with the newest one through an enqueue/dequeue operation. One can express query to memory cross attention as</p><formula xml:id="formula_3">q i t = Attn(q i t , M t ),<label>(4)</label></formula><p>where M t is flattened memory tokens across all frames that serve as key and value to attention. The ordering of memory attention in the decoder is a design choice. Hierarchically, memory attention enables a higher order information gateway to assist the output of the deformable cross-attention through past instance cues. Therefore, we select the order of different attention layers in the decoder as 1) self, 2) deformable-cross, and 3) memory attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Temporal Contrastive Loss</head><p>Like DETR, we use the Hungarian algorithm to uniquely assign one instance query to a ground truth instance upon the appearance of any new instance in a frame. Thus, a particular instance query remains responsible for a specific instance throughout the video.</p><p>In addition to classification loss (L cls ), box loss (L box ) and cross-entropy mask loss (L mask ) as in Deformable-DETR, we introduce a temporal contrastive loss (TCL) as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. As mentioned in Section 1, our third objective is to get temporally coherent embeddings of recurring instances while being discriminative to other instances across frames. Such a coherent embedding allows memory attention to effectively identify instances in scenarios like reappearance from occlusion. We take inspiration from <ref type="bibr" target="#b12">[13]</ref> and adopt the supervised contrastive loss to the temporal domain. In loss computation, we consider the embedding of an instance at different time points as a positively augmented pair, while the remaining combinations constitute negative pairs. Therefore, for the i th instance at time t, L i tcl can be expressed as</p><formula xml:id="formula_4">L i tcl = ? t?1 f =t?d log exp(q i t ? q i f /? ) k j=0 exp(q i t ? q j f /? ) (5)</formula><p>where ? is a temperature parameter. Finally, our joint training loss with their respective weights ([? i ] 4 i=1 ) is:</p><formula xml:id="formula_5">L = ? 1 L cls + ? 2 L box + ? 3 L mask + ? 4 L tcl .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets and Metrics: We evaluate InstanceFormer on four challenging datasets OVIS <ref type="bibr" target="#b21">[22]</ref>, YTVIS-19 <ref type="bibr" target="#b30">[31]</ref>, YTVIS-21 <ref type="bibr" target="#b27">[28]</ref> and YTVIS-22 <ref type="bibr" target="#b28">[29]</ref>. The recently proposed OVIS data set is more complex than the YTVIS data sets as it primarily contains long video sequences with high percentages of occlusion and a large number of objects. It contains 296k instance masks (2x of YTVIS-19) of 25 semantic categories, 5.8 instances per video (3.4x of YTVIS-19), and the longest sequence has more than 500 frames (13.5x longer than the same of YTVIS-19). Our second dataset, YTVIS-19, is the most popular VIS dataset, with 3, 859 videos, 40 different object categories, and an average length of 27 frames. Our third dataset, YTVIS-21, contains 33% more videos and almost double the amount of annotation compared to the 2019 version, making it more challenging than its predecessor. Finally, we evaluate our approach on the recently proposed YTVIS-22 challenge. Note that it shares the same training set with YTVIS-21, and only introduces additional 71 long videos in the validation set containing 259 unique instances with 9304 high-quality annotations. We use standard metrics for YTVIS-19/21 and OVIS, namely Average Precision (AP) and Average Recall (AR). For YTVIS-22, we report the same set of metrics solely for long videos (e.g., AP L and AR L ) as proposed in the official challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Here, we present and discuss the quantitative and qualitative findings for the datasets. Please refer to the supplementary for more results.</p><p>OVIS: Quantitatively, <ref type="table" target="#tab_1">Table 1</ref> shows the excellent performance of InstanceFormer against the previous online methods. Additionally, due to the longer sequences, most current offline counterparts <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref> cannot be applied due to their quadratic memory requirement. The competitive edge of In-stanceFormer is also reflected in outperforming recent near online ones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>. In OVIS, occlusions are either longterm or short-term. Long-term occlusions involve the disappearance and reappearance of instances that one can solve efficiently with discriminative visual features. However, the more abundant short-term occlusion is far more challenging because similar-looking objects cross each other, resulting in id switches. Recent online approaches heavily rely on visual <ref type="bibr" target="#b31">[32]</ref> cues and lack trajectory information which considerably weakens the performance in this scenario. In contrast, our implicit trajectory modeling through reference point propagation coupled with memory attention is more effective in mitigating this problem. <ref type="table">Table 2</ref> compares InstanceFormer with stateof-the-art online and offline methods. InstanceFormer significantly outperforms all previous online methods by at least 8% AP. This trend is consistent in other metric like AP 50 , AP 75 , AR 1 and AR 10 . Since YTVIS-19 contains mostly short videos with gradual changes, our proposed prior propagation convincingly supplies a continuous stream of information from past to present. Specifically, the reference point propagation glues consecutive frames in a sustained temporal trajectory. The benefit of such past-to-current communication, for the first time, bridges  the gap between online and offline methods. Notably, In-stanceFormer outperforms most offline methods while being marginally behind the recent benchmark like SeqFormer <ref type="bibr" target="#b26">[27]</ref>. For a fair comparison, we mention the backbone of different competing methods. <ref type="figure" target="#fig_3">Figure 6</ref> shows typical qualitative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTVIS-19:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTVIS-21:</head><p>Similar to YTVIS-19, InstanceFormer not only maintains its advantage over all online methods but also conquers the gap with the offline ones and outperforms them. <ref type="table">Table 2</ref> shows, InstanceFormer offers 4% and 0.2% higher AP than the closest online <ref type="bibr" target="#b7">[8]</ref> and offline <ref type="bibr" target="#b4">[5]</ref> methods respectively. In YTVIS-21 increased number of instances per frame demands solving local interaction among instances apart from delineating their correct mask. A global offline model easily handles this interaction and delineation for a small number of instances. However, In-stanceFormer's explicit emphasis on local interaction efficiently handles the increased complexity for a large number  of instances.</p><formula xml:id="formula_6">Memory+TCL AP L AP50 L AP75 L AR1 L AR10 L 23</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTVIS-22:</head><p>YTVIS-22 inherits the setup and data distribution of YTVIS-21 and extends the VIS challenge to the long-range arena. Although videos are shorter than OVIS, they introduce substantial complex scenarios compared to its YTVIS predecessor. As no peer-reviewed approaches for YTVIS-2022 are available, we report only our result in <ref type="table" target="#tab_4">Table 3</ref>. The result is shown in two parts, with and without the memory module of InstanceFormer, demonstrating the efficacy of our memory attention and temporal contrastive loss for long videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation</head><p>We perform ablations focusing on three perspectives: first, the effectiveness of the proposed prior propagation and memory module. Second is the influence of the number of memory tokens and past frames. Third, through t-SNE we examine the temporal coherency of instances across the video. All experiments are conducted on the OVIS dataset with a ResNet-50 backbone.</p><p>Prior Propagation and Memory: First, we inspect the contribution coming from different components of prior propagation in <ref type="table">Table 4</ref>. Reference point propagation improves 2% AP over the baseline model with only query propagation. Further, class scores improve 0.6% AP. This shows our joint representation, location, and semantic propagation facilitate seamless information integration from the past frame with no need for an extra tracking module.</p><p>Subsequently, we probe the efficacy of our memory attention and TCL in table 4 given the full prior module. Memory attention improves 1.7% AP while TCL further boost 1.2% AP. Hence, memory plays a critical role in solving occlusion, and TCL assists it through instance-wise temporally coherent and discriminative cross-instance embedding. <ref type="table">Table 5</ref> illustrates the effect of memory token count per frame. We find optimal performance with 10 tokens per frame. Decreasing the number will not cover the average count of instances per frame. However, increasing it will introduce unnecessary background instances. Further, we notice that a memory queue size of four is adequate to cover essential representation from near history. It suggests that a near history is more important for short-term occlusion and preventing error propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Tokens and Frames in Memory:</head><p>t-SNE Embedding of Instance Queries: <ref type="figure" target="#fig_4">Fig. 7</ref> shows the t-SNE embedding of instance queries across video frames. We took one of the challenging videos from OVIS (please refer to the 3rd video of OVIS from <ref type="figure">Fig. 8</ref>   <ref type="table">Table 5</ref>. Ablation on the number of memory tokens and length of temporal memory frame in the OVIS dataset. crossing each other. The queries of the same instance across frames are considered positives and represented with the same color. We can observe that temporal contrastive loss considerably reduces the overlap of various instances while maintaining a temporally consistent and coherent embedding for an instance even in highly dense and occluded video. Thus InstanceFormer can act as a base network for extracting discriminative temporal embedding of instances for various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed InstanceFormer, a single-stage online VIS framework. It specifically tackles real-life challenges such as long sequences and short-term occlusion. Further it simplifies the VIS pipeline by eliminating external data association or tracking. Thanks to our novel prior propagation and memory attention module, it can precisely track and segment instances by accumulating representation, trajectory, and semantic information from the past. Our proposed approach not only outperforms all existing online methods but also establishes a new state-ofthe-art performance on challenging YTVIS-21 and OVIS datasets by outperforming existing offline and online methods. In summary, our carefully engineered adaptation of transformers pioneers high-performance online VIS while keeping the computation tractability in check for long video sequences. We hope to accelerate and direct future research towards sparse and efficient video-level interaction to improve VIS and beyond.</p><p>The inherent nature of attention in transformer <ref type="bibr" target="#b24">[25]</ref> is global, where a single token attends to every other token. DETR <ref type="bibr" target="#b3">[4]</ref> uses the transformer and its global attention for object detection. It proposes a single-stage architecture that exploits transformers for direct set-based object predictions. Inspired by its simplicity and success, there has been a surge of DETR like architecture across various vision tasks such as object detection <ref type="bibr" target="#b33">[34]</ref>, relation detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and segmentation <ref type="bibr" target="#b4">[5]</ref>. However, attention, as mentioned above, has quadratic complexity. Due to this, DETR suffers from a long convergence time and intractable computational memory for a large image. To alleviate this problem, Deformable-DETR <ref type="bibr" target="#b33">[34]</ref>, introduces deformable attention that restricts the attention to a few spatial points in the local neighborhood. Deformable attention attends to a small set of sampling locations as a pre-filter for prominent key elements among the feature maps or image tokens. The key and sampling points are learnable and calculated from their object query. Therefore the reference points not only sparsify the attention but also reduces its memory footprint and significantly fasten the convergence. Given the multi-scale input feature map {x l } L l=1 where the x l ? R C?H l ?W l . Letp q be the normalized coordinate of the reference point for each query element q, and its representation z q ? R C then the multi-scale deformable attention as per Deformble-DETR <ref type="bibr" target="#b33">[34]</ref> is:</p><formula xml:id="formula_7">MSDeformAttn(z q ,p q , {x l } L l=1 ) = (7) M m=1 W m [ L l=1 K k=1 A mlqk ? W m x l ((? l (p q ) + ?p mlqk )]</formula><p>where m indexes the attention head, l indexes the input feature level, and k indexes the sampling point. W m ? R Cv?C and W m ? R C?Cv are learnable weights. ?p mlqk and A mlqk denote the sampling offset and attention weight of the k th sampling point in the l th feature level and the m th attention head respectively. ? l re-scales the normalized coordinates p q to the input feature map of the l th level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation Details</head><p>Network Architecture: We use ResNet-50 <ref type="bibr" target="#b9">[10]</ref> as our backbone. Following Deformable-DETR <ref type="bibr" target="#b33">[34]</ref>, we calculate the multi-scale feature maps {x l } and applied 1 ? 1 convolution. For the deformable attention modules, we follow the standard setting similar to Deformable-DETR and use 4 reference points, 6 layers of an encoder, and a decoder. The hidden dimension of the decoder and encoder layers is fixed to 256. Finally, we use a 300 instance query to learn the video instance representation.</p><p>Training: We use the AdamW optimizer with ? 1 = 0.9, ? 2 = 0.999, lr = 1e ?4 , and weight decay= 1e ?4 . The learning rate of the backbone is set to 1e ?5 . A Mul-tiStepLR scheduler reduced the learning rate by a factor of 10 at epochs 4 and 10. The input frame sizes are reduced to a maximum of 768 pixels while maintaining the aspect ratio. During training, we do not select frames sequentially but rather randomly while keeping the order of the temporal sequence. The random ordering acts as data augmentation and helps the network to lean diverse poses or appearances while maintaining their temporally coherent representation. Our training loss coefficient as per eq. 6 are ? 1 = 2, ? 2 = 5, ? 3 = 2, ? 4 = 2</p><p>Inference During inference, we set the number of memory frames d=4 and the number of memory tokens k=10. As the VIS data-set takes a single class category per instance for the whole video, we merge the classification score of the entire video into one single prediction. We take the top-k prediction scores for the first three frames and then calculate their class probability with a mean of all frames' class probability distribution. We sequentially use one frame as input, scaled down to 360p in line with MaskTrack R-CNN <ref type="bibr" target="#b30">[31]</ref>. InstanceFormer can handle video of any arbitrary length while relying only on the current and recent past information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Qualitative Results:</head><p>Instanceformer consistently performs well on multiple datasets, as seen in <ref type="figure">Fig. 8</ref> where we present the qualitative examples of long and challenging datasets YTVIS-21 and OVIS. It also shows the generalizability and robustness of our method. In <ref type="figure">Fig. 9</ref>, we qualitatively compare In-stanceFormer with state-of-the-art online and offline methods. Despite being an online method, InstanceFormer generates equal quality or even better segmentation masks in some cases than the offline ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YTVIS-21</head><p>OVIS Time <ref type="figure">Figure 8</ref>. Typical examples on long and complex videos are shown above. Note that InstanceFormer handles short-term occlusions in presence of multiple similar looking instances successfully. At the same time, one can see that InstanceFormer maintains a consistent segmentation quality for small, medium and large objects at various depth. <ref type="figure">Figure 9</ref>. Qualitative comparison of InstanceFormer with state-of-the-art online and offline methods. The first four rows are taken from SeqFormer <ref type="bibr" target="#b26">[27]</ref>. Note that InstanceFormer predicts fine details in segmentation, such as capturing the missing leg of the standing zebra and the head details of the lying zebra in the first frame and the gap between two legs and head details in the second frame, respectively. In addition, Instanceformer and CrossVIS are the only online methods, with the others being offline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The reference points of the last layer of the decoder are overlayed on the image. The sequence shows the compact propagation of object locations through reference points. Note that this propagation handles considerable occlusion and motion. size. Next, the projected features are flattened, and a fixed positional encoding is added. Then a series of deformable attention layers processes the sequence to obtain the final feature map f t 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Temporal contrastive loss considers different appearances of an instance (anchor) throughout time as positive and all other instances as negative against the anchor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative examples of InstanceFormer on the YTVIS-19 validation set. It includes occlusion and different poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>t-SNE visualization of instance queries across time in OVIS. Different color represents unique instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>and Methods AP AP 50 AP 75 AR 1 AR 10 Comparisons on OVIS dataset without feature calibration. ? denotes the near-online methods. number of memory token per frame to 10. We train our network using the AdamW [20] optimizer on 4 NVIDIA RTX A6000 GPUs with a batch size of 4 and learning rate of 1e ?4 for 16 epochs. Ablation on these hyper-parameters can be found in Sec. 4.2. More details about other model parameters and optimization can be found in the supplementary.</figDesc><table><row><cell>Implementation Details: InstanceFormer is based on</cell></row><row><cell>Deformable-DETR [34] and employs a similar architecture</cell></row><row><cell>with 6 layers of encoder and decoder. We first pre-train our</cell></row><row><cell>network on COCO [18] for 12 epochs. Following [27, 1],</cell></row></table><note>the pre-trained network is used for training both YTVIS and OVIS along with COCO to prevent over-fitting. We evalu- ate our model on the official validation split. In our exper- iments, we have set the size of memory queue d = 4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AP 75 AR 1 AR 10 AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell></cell><cell>Method</cell><cell>Backbone</cell><cell></cell><cell cols="2">YTVIS-19 [31]</cell><cell></cell><cell cols="3">YTVIS-21 [28]</cell><cell></cell><cell>Params</cell></row><row><cell cols="5">MaskProp [2] VisTR [26] IFC [12] SeqFormer [27] AP AP 50 Offline R50 40.0 ? R50 36.2 59.2 R50 41.2 65.1 R50 47.4 69.8 Mask2Former [5] R50 46.4 68.0</cell><cell>42.9 36.9 37.2 42.4 ? ? 44.6 42.3 49.6 51.8 45.5 54.8 50.0 ? ?</cell><cell cols="5">? 31.8 51.7 ? 36.6 57.9 40.5 62.5 43.6 36.2 48.0 ? ? ? 34.5 29.7 36.9 39.3 ? ? 40.6 60.9 41.8 ? ?</cell><cell>? 57.2M 39.3M 49.3M 45.0M</cell></row><row><cell></cell><cell>TeViT [33]</cell><cell>MsgShifT</cell><cell cols="2">46.6 71.3</cell><cell>51.6 44.9 54.3</cell><cell>37.9</cell><cell>61.2</cell><cell>42.1</cell><cell>35.1</cell><cell cols="2">44.6 172.3M</cell></row><row><cell></cell><cell>M-RCNN [31]</cell><cell>R50</cell><cell cols="2">30.3 51.1</cell><cell>32.6 31.0 35.5</cell><cell cols="2">28.6 48.9</cell><cell cols="2">29.6 26.5</cell><cell>33.8</cell><cell>58.1M</cell></row><row><cell></cell><cell>SipMask [3]</cell><cell>R50</cell><cell cols="2">33.7 54.1</cell><cell>35.8 35.4 40.1</cell><cell cols="2">31.7 52.5</cell><cell cols="2">34.0 30.8</cell><cell>37.8</cell><cell>33.5M</cell></row><row><cell></cell><cell>SG-Net [19]</cell><cell>R50</cell><cell cols="2">36.3 57.1</cell><cell>39.6 35.9 43.0</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>CompFeat [7]</cell><cell>R50</cell><cell>?</cell><cell>35.3</cell><cell>56.0 38.6 33.1</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Online</cell><cell>CrossVIS [32] CrossVIS [32] STMask [16]</cell><cell>R50 R101 R50-DCN</cell><cell cols="2">36.3 56.8 36.6 57.3 33.5 52.1</cell><cell>38.9 35.6 40.7 39.7 36.0 42.0 36.9 31.1 39.2</cell><cell cols="2">34.2 54.4 ? ? 30.6 49.4</cell><cell cols="2">37.9 30.4 ? ? 32.0 26.4</cell><cell>38.2 ? 36.0</cell><cell>37.5M ? 47.8M</cell></row><row><cell></cell><cell>STMask [16]</cell><cell cols="3">R101-DCN 36.8 56.8</cell><cell>38.0 34.8 41.8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>VISOLO [8]</cell><cell>R50</cell><cell cols="2">38.6 56.3</cell><cell>43.7 35.7 42.5</cell><cell cols="2">36.9 54.7</cell><cell cols="2">40.2 30.6</cell><cell>40.9</cell><cell>35.0M</cell></row><row><cell></cell><cell cols="2">InstanceFormer (Ours) R50</cell><cell cols="2">45.6 68.6</cell><cell cols="6">49.6 42.1 53.5 40.8 62.4 43.7 36.1 48.1</cell><cell>44.3M</cell></row><row><cell></cell><cell></cell><cell cols="7">Table 2. Quantitative evaluation on YTVIS-19/21 validation set.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>InstanceFormer performs better with memory and TCL when applied on YVIS-22 Long Videos. The memory with TCL enables the model to learn discriminative long range representations.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Representation Propagation: Let q i t be the final latent representation of the i th instance query at time t. We propagate this information to the next frame at t+1 by initializing its query weights with q i t . During the training, we only call the Hungarian matcher<ref type="bibr" target="#b3">[4]</ref> in the presence of new ground truth instances to uniquely assign them to corresponding queries. Since we keep the index of matched queries the same between the previous frame and the current one, we do not need to solve the matching for every frame. Consequently, the query indices act as tracking ids for the instances, eliminating the need for any explicit tracking<ref type="bibr" target="#b30">[31]</ref> or data association<ref type="bibr" target="#b31">[32]</ref>.<ref type="bibr" target="#b0">1</ref> One can find details of deformable attention and reference points in the supplementary.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix A.1. Deformable Attention and Reference Points:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="158" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mask2former for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10764</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1361" to="1369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visolo: Grid-based space-time aggregation for efficient online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2896" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Box supervised video segmentation proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kobold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07025,2022.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learningfor image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ComputerScience</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video instance segmentation using inter-frame communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13352" to="13363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06193,2020.11</idno>
		<title level="m">Relation transformer network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scenes and surroundings: Scene graph generation using relation transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinhamahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05448</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video instance segmentation with a propose-reduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1739" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9816" to="9825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Relationformer: A unified framework for image-to-graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ezhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10202</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video instance segmentation via multi-scale spatio-temporal split attention transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Thawakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13253</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08275</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Youtube-vis dataset 2021 version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis,2021" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The 4th large-scale video object segmentation challenge -youtube-vos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/challenge/2022/,2022" />
		<imprint/>
	</monogr>
	<note>Accessed on 08/18/2022</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised video object segmentation by motion grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7177" to="7188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8043" to="8052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporally efficient vision transformer for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
