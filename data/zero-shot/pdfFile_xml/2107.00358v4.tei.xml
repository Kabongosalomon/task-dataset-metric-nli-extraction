<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-domain Few-shot Learning with Task-specific Adapters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VICO Group</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-domain Few-shot Learning with Task-specific Adapters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/VICO-UoE/URL</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we look at the problem of cross-domain few-shot classification that aims to learn a classifier from previously unseen classes and domains with few labeled samples. Recent approaches broadly solve this problem by parameterizing their few-shot classifiers with task-agnostic and task-specific weights where the former is typically learned on a large training set and the latter is dynamically predicted through an auxiliary network conditioned on a small support set. In this work, we focus on the estimation of the latter, and propose to learn task-specific weights from scratch directly on a small support set, in contrast to dynamically estimating them. In particular, through systematic analysis, we show that task-specific weights through parametric adapters in matrix form with residual connections to multiple intermediate layers of a backbone network significantly improves the performance of the state-of-the-art models in the Meta-Dataset benchmark with minor additional cost. Recently, inspired from [36], task-specific adapters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>, small capacity transformations that are applied to multiple layers of a deep network, have been successfully used to steer the few-shot classifiers to new tasks and domains. Their weights are often estimated dynamically through an auxiliary network conditioned on the support set <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47</ref>] (see <ref type="figure">Fig.</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods have seen remarkable progress in various fields where large quantities of data and compute power are available. However, the ability of deep networks to learn new concepts from small data remains limited. Fewshot classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref> is inspired from this limitation and aims at learning a model that can be efficiently adapted to recognize unseen classes from few samples. In particular, the standard setting for learning few-shot classifiers involves two stages: (i) learning a model, typically from a large training set, (ii) adapting this model to learn new classes from a given small support set. These two stages are called meta-training and meta-testing respectively. The adapted model is finally evaluated on a query set where the task is to assign each query sample to one of the classes in the support set.</p><p>Early methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref> pose the few-shot classification problem in a learning-to-learn formulation by training a deep network over a distribution of related tasks, * Xialei Liu is the corresponding author.  <ref type="figure">Figure 1</ref>. Cross-domain Few-shot Learning considers to learn a model from one or multiple domains to generalize to unseen domains with few samples. Prior works often learn a task-agnostic model with an auxiliary network during meta-training (a) and a set of adapters are generated by the auxiliary network to adapt to the given support set <ref type="bibr">(b)</ref>. While in this work, we propose to attach adapters directly to a pretrained task-agnostic model (c), which can be estimated from scratch during meta-testing <ref type="bibr">(d)</ref>. We also propose different architecture topologies of adapters and their efficient approximations.</p><p>which are sampled from the training set, and transfer this experience to improve its performance for learning new classes. Concretely, Vinyals et al. <ref type="bibr" target="#b48">[49]</ref> learn a feature encoder that is conditioned on the support set in meta-training and does not require any further training in meta-test thanks to its nonparametric classifier. Ravi and Larochelle <ref type="bibr" target="#b36">[37]</ref> take the idea of learning a feature encoder in meta-train further by also learning an update rule through an LSTM that produces the updates for a classifier in meta-test. Finn et al. <ref type="bibr" target="#b15">[16]</ref> pose the task as a meta-learning problem and learn the parameters of a deep network in meta-training such that a network initialized with the learned parameters can be efficiently finetuned on a new task. We refer to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref> for comprehensive review of early works.</p><p>Despite the significant progress, the scope of the early methods has been limited to a restrictive setting where training and test samples come from a single domain (or data distribution) such as Omniglot <ref type="bibr" target="#b23">[24]</ref>, miniImageNet <ref type="bibr" target="#b48">[49]</ref> and tieredImageNet <ref type="bibr" target="#b39">[40]</ref>. They perform poorly in the more challenging cross-domain few-shot tasks, where test data is sampled from an unknown or previously unseen domain <ref type="bibr" target="#b47">[48]</ref>. This setting poses an additional learning challenge, not only requires leveraging the limited information from the small support set for learning the target task but also selectively transferring relevant knowledge from previously seen domains to the target task.</p><p>Broadly, recent approaches address this challenge by parameterizing deep networks with a large set of task-agnostic and a small set of task-specific weights that encode generic representations valid for multiple tasks and private representations are specific to the target task respectively. While the task-agnostic weights are learned over multiple tasks, typically, from a large dataset in meta-training, the taskspecific weights are estimated from a given small support set (e.g. 5 images per category) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>. In the literature, the task-agnostic weights are used to parameterize a single network that is trained on large data from one domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41]</ref> or on multiple domains <ref type="bibr" target="#b26">[27]</ref>, or to be distributed over multiple networks, each trained on a different domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>  <ref type="bibr" target="#b0">1</ref> . The task-specific weights are utilized to parameterize a linear classifier <ref type="bibr" target="#b25">[26]</ref>, a pre-classifier feature mapping <ref type="bibr" target="#b26">[27]</ref> and an ensemble of classifiers at each layer of a deep neural network <ref type="bibr" target="#b0">[1]</ref>. the prior work, we learn the weights of these adapters from scratch by directly optimizing them on a small support set (see <ref type="figure">Fig. 1.(c,d)</ref>). Moreover, we systematically study various combinations of several design choices for task-specific adaptation, which have not been explored before, including adapter connection types (serial or residual), parameterizations (matrix and its decomposed variations, channelwise operations) and estimation of task-specific parameters. Extensive experiments demonstrate that attaching parameteric adapters in matrix form to convolutional layers with residual connections significantly boosts the state-of-the-art performance in most domains, especially resulting in superior performance in unseen domains on Meta-Dataset with negligible increase in computations.</p><p>More related work. Here we provide more detailed discussion of the most related work. Both CNAPS <ref type="bibr" target="#b40">[41]</ref> and Simple CNAPS <ref type="bibr" target="#b2">[3]</ref> employ task-specific adapters via FiLM layers (which uses a channelwise affine transformation and connected to the backbone in a serial way) <ref type="bibr" target="#b35">[36]</ref> to adapt their feature extractors to the target task and estimate them via an auxiliary network. Compared to them, we propose learning residual adapters in matrix form directly on the support set. SUR <ref type="bibr" target="#b14">[15]</ref> and URT <ref type="bibr" target="#b28">[29]</ref> learn an attention mechanism to select/fuse features from multiple domain-specific models in meta-train respectively. As we build on a single multidomain feature extractor, our method does not require such attention but we attach task-specific adapters to the feature extractor to adapt the features to unseen tasks. URL <ref type="bibr" target="#b26">[27]</ref> learns a pre-classifier feature mapping to adapt the feature from a single task-agnostic model learned from multiple domains for unseen tasks. While we build on their feature extractor and pre-classifier alignment, the pre-classifier alignment provides very limited capacity for task adaptation, which we address by adapting the feature extractor with adapters at multiple layers. FLUTE <ref type="bibr" target="#b46">[47]</ref> follows a hybrid three step approach that first learns the parameters of domain-specific FiLM layers so called templates, employs an auxiliary network to initialize the parameters of a new FiLM layer for unseen task by combining the templates and finetunes them on the small support set. Different from FLUTE, our method learns such adaptation in a single step by learning residual adapters in meta-test.</p><p>There are also methods (e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44]</ref>) that do not fit into task-agnostic and task-specific parameterization grouping. BOHB <ref type="bibr" target="#b43">[44]</ref> proposes to use multi-domain data as validation objective for hyper-parameter optimization such that the feature learned on ImageNet with the optimized hyperparameter generalizes well to multi-domain. CTX <ref type="bibr" target="#b13">[14]</ref> proposes to learn spatial correspondences from ImageNet and evaluates on the remaining (unseen) domains. We also compare our method to them in the setting where we use a standard single domain learning network learned from ImageNet and adapt its representations through residual adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Few-shot classification aims at learning to classify samples of new categories efficiently from few samples only. Each few-shot learning task consists of a support set S =</p><formula xml:id="formula_0">{(x i , y i )} |S| i=1</formula><p>with |S| sample and label pairs respectively and a query set Q = {(x j )} |Q| j=1 with |Q| samples to be classified. The goal is to learn a classifier on S that accurately predicts the labels of Q. Note that this paper focuses on few-shot image classification problem, i.e. x and y denote an image and its label.</p><p>As in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>, we solve this problem in two steps involving i) representation learning where we learn a taskagnostic feature extractor f from a large dataset D b , ii) task adaptation where we adapt the task-agnostic representations through various task-specific weights to the target tasks (S, Q) that are sampled from another large dataset D t by taking the subsets of the dataset to build S and Q. Note that D b and D t contain mutually exclusive classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Task-agnostic representation learning</head><p>Learning task-agnostic or universal representations <ref type="bibr" target="#b4">[5]</ref> has been key to the success of cross-domain generalization. Representations learned from a large diverse dataset such as ImageNet <ref type="bibr" target="#b11">[12]</ref> can be considered as universal and successfully transferred to tasks in different domains with minor adaptations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. We denote this setting as single domain learning (SDL).</p><p>More powerful and diverse representations can be obtained by training a single network over multiple domains. Let D b = {D k } K k=1 consists of K subdatasets, each sampled from a different domain. The vanilla multi-domain learning (MDL) strategy jointly optimizes network parameters over the images from all K subdatasets:</p><formula xml:id="formula_1">min ?,? k K k=1 1 |D k | x,y?D k ?(g ? k ? f ? (x), y),<label>(1)</label></formula><p>where ? is cross-entropy loss, f is feature extractor that takes an image as input and outputs a d dimensional feature. f is parameterized by ? which is shared across K domains. g ? k is the classifier for domain k and parameterized by ? k which is discarded in meta-test. We denote this setting as MDL. The challenge in MDL is to allow efficiently sharing the knowledge across the domains while preventing negative transfer between them and also carefully balancing the individual loss functions ( <ref type="bibr" target="#b8">[9]</ref>). URL <ref type="bibr" target="#b26">[27]</ref>, a variant of MDL, mitigates these challenges by first training individual domain-specific networks offline and then distilling their knowledge into a single multi-domain network. We refer to <ref type="bibr" target="#b26">[27]</ref> for more details. Another way of obtaining multi-domain representations is to employ multiple domain-specific feature extractors, one for each domain, and adaptively "fuse" their features for each task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>. While these methods are effective, they require computing features for each image through multiple feature extractors and are thus computationally expensive. Due to its simplicity and effectiveness, we conduct experiments with the feature extractor of URL <ref type="bibr" target="#b26">[27]</ref> along with the SDL one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Task-specific weight learning</head><p>A good task-agnostic feature extractor f ? is expected to produce representations that generalize to many previously unseen tasks and domains. However this gets more challenging when there is a large domain gap between the training set D b and test set D t which requires further adaptation to the target task. In this work, we propose to incorporate additional capacity to the task-agnostic feature extractor by adding task-specific weights to adapt the representations to the target task by using the support set. Specifically, we directly attach task-specific weights to a learned task-agnostic model, and estimate them from scratch given the support set. We denote the task-specific weights with ? and task-adapted classifier with p (?,?) that outputs a softmax probability vector whose dimensionality equals to the number of categories in the support set S.</p><p>To obtain the task-specific weights, we freeze the taskagnostic weights ? and minimize the cross-entropy loss ? over the support samples in meta-test w.r.t. the task-specific weights ? <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46]</ref>:</p><formula xml:id="formula_2">min ? 1 |S| (x,y)?S ?(p (?,?) (x), y),<label>(2)</label></formula><p>where S is sampled from the test set D t . Most previous works freeze the task-agnostic weights but estimate the taskspecific weights through an auxiliary network (or a task encoder) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>, where inaccurate prediction of parameters can lead to noisy adaptation and wrong prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Task-specific adapter parameterization (?)</head><p>Task adaptation techniques can be broadly grouped into two categories that aims to adapt the feature extractor or classifier to a given target task. We use ? and ? to denote task-specific weights for adapting the feature extractor and classifier respectively where ? = {?, ?}. Feature extractor adaptation. A simple method to adapt f ? is finetuning its parameters on the support set <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. However, this strategy tends to suffer from the unproportionate optimization, i.e. updating very high-dimensional weights from a small number of support samples. In this paper, we propose to attach task-specific adapters directly to the existing task-agnostic model, e.g. we attach the adapters to each module of a ResNet backbone in <ref type="figure">Fig. 2 (a)</ref>, and the adapters can be efficiently learned/estimated from few samples. Concretely, let f ? l denote the l-th layer of the feature Serial adapter. Residual adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different options for</head><p>Matrix Channel-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapter topologies Adapter parameterization</head><p>Figure 2. Illustration of our task adaptation for cross-domain few-shot learning. In meta-test stage (a), our method first attaches a parametric transformation r? to each layer, where ? can be constructed by (b) a serial or (c) a residual topology. They can be parameterized with matrix multiplication (d) or channel-wise scaling (e). We found that (c) is the best configuration with matrix parameterization which is further improved by attaching a linear transformation A ? to the end of the network. We adapt the network for a given task by optimizing ? and A ? on a few labeled images from the support set, then map query images to the task-specific space and assign them to the nearest class center.</p><p>extractor f ? (i.e. a convolutional layer) with the weights ? l . Given a support set S, the task-specific adapters r ? parameterized by ?, can be incorporated to the output of the layer</p><formula xml:id="formula_3">f ? l as f {? l ,?} (h) = r ? (f ? l (h), h)<label>(3)</label></formula><p>where h ? R W ?H?C is the input tensor, f ? l is a convolutional layer in f ? . Importantly, the number of the taskspecific adaptation parameters ? are significantly smaller than the task-agnostic ones. The adapters can be designed in different ways. Next we propose two connection types for incorporating r ? to f ? l : i) serial connection by subsequently applying it to the output of layer f ? l (h) as</p><formula xml:id="formula_4">f {? l ,?} (h) = r ? ? f ? l (h)</formula><p>which is illustrated in <ref type="figure">Fig. 2(b)</ref>, and ii) parallel connection by a residual addition as in <ref type="bibr" target="#b38">[39]</ref> </p><formula xml:id="formula_5">f {? l ,?} (h) = r ? (h) + f ? l (h)</formula><p>which is illustrated in <ref type="figure">Fig. 2(c)</ref>. In our experiments, we found the parallel setting performing the best when ? is learned on a support set during meta-test (illustrated in <ref type="figure">Fig. 2</ref>(c)) which we discuss in Sec. 3.</p><p>For the parameterization of r ? , we consider two options. Matrix multiplication (illustrated in <ref type="figure">Fig. 2</ref></p><formula xml:id="formula_6">(d)) with ? ? R C?C : r ? (h) = h * ?,</formula><p>where * denotes a convolution, ? ? R C?C and the transformation is implemented as a convolutional operation with 1 ? 1 kernels in our code. And channelwise scaling (illustrated in <ref type="figure">Fig. 2</ref>(e)):</p><formula xml:id="formula_7">r ? (h) = h ? ?,</formula><p>where ? is a Hadamard product and ? ? R C . Note that one can also use an additive bias weight in both settings, however, this has not resulted in any significant gains in our experiments. While the matrix multiplication is more powerful than the scaling operation, it also requires more parameters to be estimated or learned. Note that, in a deep neural network, the number of input C in and output channels C out for a layer can be different. In that case, one can still use a non-square matrix: ? ? R Cout?Cin , however, it is not possible to use a scaling operator in the parallel setting. In our experiments, we use ResNet architecture <ref type="bibr" target="#b16">[17]</ref> where most input and output channels are the same. r ? connected in parallel with matrix multiplication form, when its parameters ? are learned on the support set, is known as residual adapter <ref type="bibr" target="#b38">[39]</ref> and r ? connected serial in channelwise is known as FiLM <ref type="bibr" target="#b35">[36]</ref>. An alternative to reduce the dimensionality of ? in case of matrix multiplication is matrix decomposition: ? = V ? ? , where V ? R C?B and ? ? R C?B , B ? C. Using a bottleneck, i.e. setting B &lt; C/2, reduces the number of parameters in the multiplication. In this work, we set B = [C/N ] and evaluate the performance for various N in Sec. 3. Classifier learning. Finally, the adapted feature extractor f (?,?) can be combined with a task-specific classifier c ? , parameterized by ? to obtain the final model, i.e. c ? f (?,?) . Based on the recent works, we investigate use of various linear classifiers in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>, also nonparameteric ones including nearest centroid classifier (NCC) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref> and their variants based on Mahalanobis distance (MD) <ref type="bibr" target="#b2">[3]</ref>. Recently, it was shown in <ref type="bibr" target="#b26">[27]</ref> that nonparametric classifiers can be successfully combined with a pre-classifier transformation. Concretely, the transformation in <ref type="bibr" target="#b26">[27]</ref> that takes in the features computed from the network f {?,?} ? R d and apply an affine transformation A ? :</p><formula xml:id="formula_8">R d ? R d parameterized by ? ? R d?d to obtain the network embedding that is fed into the classifier, i.e. p ?,? = c ? A ? ? f {?,?} .</formula><p>Note that in the case of non-parametric classifier, c is not parameterized by ? and we use ? to denote the transformation parameters.</p><p>In our experiments, the best performing setting uses parallel adapters, whose parameters are in the matrix form, to adapt the feature extractor and followed by the pre-classifier transformation and NCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Here we start with experimental setup, and then we compare our method to the state-of-the-art methods and rigorously evaluate various design decisions. We finally provide further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setup</head><p>Dataset. We use the Meta-Dataset <ref type="bibr" target="#b47">[48]</ref> which is the standard benchmark for few-shot classification. It contains images from 13 diverse datasets and we follow the standard protocol in <ref type="bibr" target="#b47">[48]</ref> (more details in the supplementary). Implementation details. As in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>, we build our method on ResNet-18 <ref type="bibr" target="#b16">[17]</ref> backbone, which is trained over eight training subdatasets by following <ref type="bibr" target="#b26">[27]</ref> with the same hyperparameters in our experiments, unless stated otherwise. Once learned, we freeze its parameters and use them as the task-agnostic weights. For learning task-specific weights (?), including the pre-classifier transformation ? and the adapter parameters, we directly attach them to the task-agnostic weights and learn them on the support samples in meta-test by using Adadelta optimizer <ref type="bibr">[52]</ref>.</p><p>In the study of various task adaptation strategies in Section 3.3, we consider to only estimate the adapter parameters and learn the auxiliary network parameters by using Adam optimizer as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref> in meta-train. Note that estimation of pre-classifier and classifier weights via the auxiliary network leads to noisy and poor results and we do not report them. Similarly, we found that the auxiliary network fails to estimate very high-dimensional weights. Hence we only use it to estimate adapter weights that are parameterized with a vector for channelwise multiplication but not with a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison to state-of-the-art methods</head><p>We evaluate our method in two settings, with multidomain or single-domain feature extractor and compare our method to existing state-of-the-art methods. We also evaluate our method incorporated with different feature extractors, i.e. SDL, MDL, and URL in the supplementary. Multi-domain feature extractor. Here we incorporate the proposed residual adapters in matrix form to the multidomain feature extractor of <ref type="bibr" target="#b26">[27]</ref> and compare its performance with the the state-of-the-art methods (CNAPS <ref type="bibr" target="#b40">[41]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, Transductive CNAPS <ref type="bibr" target="#b1">[2]</ref>, FLUTE <ref type="bibr" target="#b46">[47]</ref>, tri-M <ref type="bibr" target="#b29">[30]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref>) in Tab. 1. To better analyze the results, we divide the table into two blocks that show the few-shot classification accuracy in previously seen domains and unseen domains along with their average accuracy. We also report average accuracy over all domains and the average rank as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>. <ref type="bibr" target="#b1">2</ref> Simple CNAPS improves over CNAPS by adopting a simple Mahalanobis distance in stead of learning adapted linear classifier.   From the results, our method outperforms other methods on most domains (10 out of 13), especially obtaining significant improvement on 5 unseen datasets than the second best method, i.e. Average Unseen (+7.5). More specifically, our method obtains significant better results than the second best approach on Traffic Sign (+19.5), CIFAR-10 (+8.7), and CIFAR-100 (+6.8). Achieving improvement on unseen domains is more challenging due to the large gap between seen and unseen domain and the scarcity of labeled samples for the unseen task. We address this problem by attaching light-weight adapters to the feature extractor residually and learn the attached adapters on support set from scratch. This allows the model to learn more accurate and effective task-specific parameters (adapters) from the support set to efficiently steer the task-agnostic features for the unseen task, compared with predicting task-specific parameters by an auxiliary network learned in meta-train, e.g. Simple CNAPS, tri-M, or fusing representations from multiple feature extractors e.g. SUR, URT. Though FLUTE uses a hybrid approach which uses auxiliary networks learned from meta-train to initialize the FiLM parameters for further fine-tuning, their results are not better than URL, which achieves very competitive results as it learns a good universal representation that generalizes well to seen domains and can be further improved with the adaptation strategy proposed in this work, especially significant improvements on unseen domains. Single-domain feature extractor. We also evaluate our method with a single-domain feature extractor trained on ImageNet only on ResNet-18 as in <ref type="bibr" target="#b47">[48]</ref> or ResNet-34 as in <ref type="bibr" target="#b13">[14]</ref>. This setting is more challenging than the multidomain one, as the model is trained only on one domain and tested on both test split of ImageNet but also of other domains. We report the results of our method and state-ofthe-art methods (BOHB <ref type="bibr" target="#b43">[44]</ref>, FLUTE <ref type="bibr" target="#b46">[47]</ref>, Finetune <ref type="bibr" target="#b47">[48]</ref>, ProtoNet <ref type="bibr" target="#b47">[48]</ref>, fo-Proto-MAML <ref type="bibr" target="#b47">[48]</ref>, and ALFA+fo-Proto-MAML <ref type="bibr" target="#b47">[48]</ref>, CTX <ref type="bibr" target="#b13">[14]</ref>) in Tab. 2. ALFA+fo-Proto-MAML achieves the prior best performance by combining the complementary strengths of Prototypical Networks and MAML (fo-Proto-MAML), with extra meta-learning of per-step hyperparameters: learning rate and weight decay coefficients. FLUTE fails to surpass it with one training source domain, probably due to the lack of FiLM parameters from multiple domains. Our method, when using ResNet18 backbone, outperforms other methods on all domains, especially obtaining significant improvement, i.e. Average Unseen (+9.5), on 12 unseen datasets than the second best method. We compare our method to CTX and ProtoNet, which use ResNet-34 backbone. <ref type="bibr" target="#b2">3</ref> CTX is very competitive by learning coarse spatial correspondence between the query and the support images with an attention mechanism. Ours is orthogonal to CTX and both CTX and our method can potentially be complementary, but we leave this as future work due to high computational cost of CTX. Specifically, we see that our method obtains the best average rank and outperforms CTX on most domains (6 out of 10) while our method being more  </p><formula xml:id="formula_9">.0 ? 1.1 51.5 ? 1.0 59.2 ? 1.2 50.3 ? 1.0 57.5 ? 1.0 67.6 ? 0.9 53.3 ? 1.1 72.7 ? 0.8 71.0 ? 0.8 Fungi 38.2 ? 1.0 39.7 ? 1.1 40.0 ? 1.1 41.5 ? 1.2 41.4 ? 1.1 31.8 ? 1.0 44.7 ? 1.0 40.7 ? 1.1 51.6 ? 1.1 51.4 ? 1.2 VGG Flower 85.5 ? 0.7 85.3 ? 0.8 87.2 ? 0.7 86.0 ? 0.8 87.3 ? 0.6 80.1 ? 0.9 90.9 ? 0.6 87.0 ? 0.7 95.3 ? 0.4 94.0 ? 0.5 Traffic Sign 66.8 ? 1.3 47.1 ? 1.1 48.8 ? 1.1 60.8 ? 1.3 51.8 ? 1.0 46.5 ? 1.1 82.5 ? 0.8 58.1 ? 1.1 82.7 ? 0.8 81.7 ? 0.9 MSCOCO 34.9 ? 1.0 41.0 ? 1.1 43.7 ? 1.1 48.1 ? 1.1 48.0 ? 1.0 41.4 ? 1.0 59.0 ? 1.0 41.7 ? 1.1 59.9 ? 1.0 61.7 ? 0.9 MNIST - - - - - 80.8 ? 0.8 93.9 ? 0.6 - - 94.6 ? 0.5 CIFAR-10 - - - - - 65.4 ? 0.8 82.1 ? 0.7 - - 86.0 ? 0.6 CIFAR-100 - - - - - 52.7 ? 1.1 70.7 ? 0.9 - - 78.3 ? 0.8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of task-specific parameterizations</head><p>Classifier learning. First we study the adaptation strategies for learning only a task-specific classifier on the pretrained feature extractor of <ref type="bibr" target="#b26">[27]</ref>. We evaluate non-parametric classifiers including nearest cetroid classifier (NCC) and NCC Mahalanobis Distance (MD) and parametric classifiers including logistic regression (LR), support vector machine SVM whose parameters are learned on support samples. We also include another baseline with NCC that finetunes all the feature extractor parameters, and report the results in Tab. 3. We observe that NCC obtains the best results for the seen domains and its performance is further improved by MD, while SVM achieves the best for the unseen domains among other classifiers. Finetuning baseline provides competitive results especially for the unseen domains. However, it performs poor in most seen domains. Feature extractor adaptation. Next we analyze various design decisions for the feature extractor adaptation including connection types (serial, residual), i.e. <ref type="figure">Fig. 2(b)</ref>, (c), its parameterization including channelwise modulation (CW) when they are estimated by an auxiliary network (Aux-Net), which has around 77% capacity of the feature extractor. We use with each combination with two nonparameteric classifier, either NCC or MD. While the adaptation strategies using residual connections performs better than the serial one in almost all cases, the gains are more substantial when generalizing to unseen domains. Learning adapter weights from few samples only can be very noisy. With residual addi-tion, it is not necessary to change all connections for passing the information forward, which can improve the robustness of useful features and reduce learning burdens for new task, hence increase the generalization ability. While the serial connections may damage the previous learned structures. We also observe that NCC and MD obtain comparable performances. Note that Aux-S-CW with MD corresponds to our implementation of Simple CNAPS <ref type="bibr" target="#b2">[3]</ref> with the more powerful feature extractor. We show that replacing its serial connection with a residual one leads to a strong performance boost.</p><p>Next we look at the adaptation strategy that learns the task-specific weights directly on the support set as in Eq. (2). We evaluate serial and residual connection types with channelwise and matrix parameterizations by using NCC. We denote this setting as Ad in Tab. 3. Note that we omit MD here, as it produces similar results to NCC. First we observe that learning the weights on the support set outperforms the strategy of estimating them through an auxiliary network almost in all cases. In addition, the learnable weights requires less number of parameters per task, while the capacity of auxiliary network is fixed. We again observe that the residual connections are more effective, especially when used with the matrix parameterization (Ad-R-M). However,the channelwise ones provide a good performance/computation tradeoff. Finally using the pre-classifier alignment (Ad-R-CW-PA and Ad-R-M-PA) further boosts the performance of the best models and we use our best model Ad-R-M-PA to compare against the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Further results</head><p>Varying-way Five-shot. After evaluating our method over a broad range of varying shots (e.g. up to 100 shots), we fol-     low <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref> to further analyze our method in 5-shot setting of varying number of categories. In this setting, we sample a varying number of ways with a fixed number of shots to form balanced support and query sets. As shown in Table 4, overall performance for all methods decreases in most datasets compared to results in <ref type="table" target="#tab_1">Table 1</ref> indicating that this is a more challenging setting. It is due to that five-shot setting samples much less support images per class than the standard setting. The top-2 methods remain the same and ours still outperforms the state-of-the-art URL when the number of support images per class is fewer, especially on unseen domains (Average Unseen +6.2). Five-way One-shot. The similar conclusion can be drawn from this challenging case. Note that there are extremely few samples available for training in this case. As we can see, Ours achieves similar results with URL on seen domains but much better performance on unseen domains due to the learning of attached residual adapters is less over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying-Way Five-Shot</head><p>Five-Way One-Shot  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Further ablation study</head><p>Here, we conduct ablation study for the sensitivity analysis for number of iterations, layer analysis for adapters, and decomposed residual adapters. We summarize results in figures and refer to supplementary for more detailed results. Sensitivity analysis for number of iterations. In our method, we optimize the attached parameters (?, ?) with 40 iterations. <ref type="figure" target="#fig_2">Figure 3</ref> reports the results with 10, 20, 40, 60 iterations and indicates that our method (solid green) converges to a stable solution after 20 iterations and achieves better average performance on all domains than the baseline URL (dash green). Layer analysis for adapters. Here we investigate whether it is sufficient to attach the adapters only to the later layers. We evaluate this on ResNet18 which is composed of four blocks and attach the adapters to only later blocks (block4, block3,4, block2,3,4 and block-all, see <ref type="figure">Fig. 2</ref>). <ref type="figure" target="#fig_3">Figure 4</ref> shows that applying our adapters to only the last block (block4) obtains around 78% average accuracy on all domains which outperforms the URL. With attaching residual adapters to more layers, the performance on unseen domains is improved significantly while the one on seen domains remains stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decomposing residual adapters.</head><p>Here we investigate whether one can reduce the number of parameters in the adapters while retaining its performance by using matrix decomposition (see Sec. 2). As in deep neural network, the adapters in earlier layers are relatively small, we then decompose the adapters in the last two blocks only where the adapter dimensionality goes up to 512?512. <ref type="figure" target="#fig_5">Figure 5</ref> shows that our method can achieve good performance with less parameters by decomposing large residual adapters, (e.g. when N = 32 where the number of additional parameters equal to around 4% vs 13%, the performance is still comparable to the original form of residual adapters, i.e. N=0). We refer to supplementary for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Limitations</head><p>In this work, we investigate various strategies for adapting deep networks to few-shot classification tasks and show that light-weight adapters connected to a deep network with residual connections achieves strong adaptation to new tasks and domains only from few samples and obtains state-ofthe-art performance while being efficient in the challenging Meta-Dataset benchmark. We demonstrate that the proposed solution can be incorporated to various feature extractors with a negligible increase in number of parameters.</p><p>Our method has limitations too. We build our method on existing backbones such ResNet-18 and ResNet-34, employ fixed adapter parameterizations and connection types which may not be optimal for every layer and task in multi-domain few-shot learning. Thus it would be desirable to have more flexible adapter structures that can be altered and tuned based on the target task.</p><p>[52] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. 5, 12</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Meta-Dataset <ref type="bibr" target="#b47">[48]</ref> is a few-shot classification benchmark that initially consists of ten datasets: ILSVRC 2012 <ref type="bibr" target="#b41">[42]</ref> (ImageNet), Omniglot <ref type="bibr" target="#b23">[24]</ref>, FGVC-Aircraft <ref type="bibr" target="#b30">[31]</ref> (Aircraft), CUB-200-2011 <ref type="bibr" target="#b49">[50]</ref> (Birds), Describable Textures <ref type="bibr" target="#b9">[10]</ref> (DTD), QuickDraw <ref type="bibr" target="#b20">[21]</ref>, FGVCx Fungi <ref type="bibr" target="#b5">[6]</ref> (Fungi), VGG Flower <ref type="bibr" target="#b33">[34]</ref> (Flower), Traffic Signs <ref type="bibr" target="#b18">[19]</ref> and MSCOCO <ref type="bibr" target="#b27">[28]</ref> then further expands with MNIST <ref type="bibr" target="#b24">[25]</ref>, CIFAR-10 <ref type="bibr" target="#b21">[22]</ref> and CIFAR-100 <ref type="bibr" target="#b21">[22]</ref>. We follow the standard procedure in <ref type="bibr" target="#b47">[48]</ref> and consider both the 'Training on all datasets' (multidomain learning) and 'Training on ImageNet only' (singledomain learning) settings. In 'Training on all datasets' setting, we follow the standard procedure and use the first eight datasets for meta-training, in which each dataset is further divided into train, validation and test set with disjoint classes. While the evaluation within these datasets is used to measure the generalization ability in the seen domains, the remaining five datasets are reserved as unseen domains in meta-test for measuring the cross-domain generalization ability. In 'Training on ImageNet only' setting, we follow the standard procedure and only use train split of ImageNet for metatraining. The evaluation of models is in the test split of ImageNet and the rest 12 datasets which are reserved as unseen domains in meta-test. As in <ref type="bibr" target="#b47">[48]</ref>, we evaluate our method on 600 randomly sampled tasks for each dataset with varying number of ways and shots, and report average accuracy and 95% confidence score in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In this section, we explain the details of task-agnostic (feature extractor) learning and then task-specific (adapter) learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Task-agnostic learning</head><p>Here we consider learning the parameters of the feature extractor from either multiple or single domains. Multi-domain learning. When we learn the feature extractor from multiple domains, we consider two cases. In the first case, which we call vanilla multiple domain learning (or MDL), we design a deep network where we share all the layers across all domains and have domain-specific classifiers. This setting corresponds to Eq (1) in the main text. Second we consider a variant of MDL, URL <ref type="bibr" target="#b26">[27]</ref> which also involves learning a single network with shared and domainspecific layers as such, however, it is learned by distilling information from multiple domain-specific networks as described <ref type="bibr" target="#b26">[27]</ref>. In these two settings, as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>, we build MDL and URL on the ResNet-18 <ref type="bibr" target="#b16">[17]</ref> backbone and use 84 ? 84 image size.</p><p>For optimization of both MDL and URL, we follow the same protocol in <ref type="bibr" target="#b26">[27]</ref>, use SGD optimizer and cosine annealing with a weight decay of 7 ? 10 ?4 for learning 240,000 iterations. The learning rate is 0.03 and the annealing frequency is 48,000. As in <ref type="bibr" target="#b26">[27]</ref>, the batch size for ImageNet is 64 ? 7 and is 64 for the other 7 datasets. We refer readers to <ref type="bibr" target="#b26">[27]</ref> for more details. Single domain learning (SDL). We also evaluate our method on a feature extractor that is learned on single domain which we call SDL. Here we evaluate our method on two backbones, ResNet-18 (SDL-ResNet-18) and ResNet34 (SDL-ResNet-34).  SDL-ResNet-18. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>, we train a ResNet-18 on the train split of ImageNet and use 84 ? 84 image size, which is denoted as SDL-ResNet-18. For optimization, we follow the training protocol in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. Specifically, we use SGD optimizer and cosine annealing for all experiments with a momentum of 0.9 and a weight decay of 7 ? 10 ?4 . Some other hyperparameters are shown in Tab. 5 as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. To regularize training, we also use the exact same data augmentations as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, e.g. random crops and random color augmentations. SDL-ResNet-34. We also apply our method to the single domain learning model with ResNet-34 backbone learned on ImageNet only as in <ref type="bibr" target="#b13">[14]</ref>. We follow <ref type="bibr" target="#b13">[14]</ref> and use higherresolution (224 ? 224) images for meta-training and metatesting. For optimization, we follow the training protocol as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. Specifically, we use SGD optimizer and cosine annealing with a momentum of 0.9, a weight decay of 1 ? 10 ?4 with a batch size of 128. Other hyperparameters are the same as in SDL-ResNet-18 and are shown in Tab. 5.</p><p>To regularize training, we also use the exact same data augmentations as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, e.g. random crops and random color augmentations with an additional stage that randomly downsamples and upsamples images as in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Task-specific learning</head><p>Attaching and learning adapters. For the optimization of the adaptation parameters ? which is attached directly and learned on support set and the pre-classifier adaptation ?, we follow the optimization strategy in <ref type="bibr" target="#b26">[27]</ref>, initialize ? as an identity matrix and optimize both ? and ? for 40 iterations using Adadelta [52] as optimizer. The learning rate of ? is 0.1 for first eight datasets and 1 for the last five datasets as in <ref type="bibr" target="#b26">[27]</ref> and we set the learning rate of ? as half of the learning rate of ?, i.e. 0.05 for the first eight datasets and 0.5 for the last five datasets. Note that, we learn ? and ? on a per-task basis using the task's support set during meta-test. That is, ? and ? are not re-used across the test tasks drawn from D t .  Predicting r ? . In case of modulating ? with the auxiliary network, we follow the auxiliary training protocols in <ref type="bibr" target="#b2">[3]</ref>. We train for 10K episodes to optimize the task encoder using Adam with a learning rate of 1 ? 10 ?5 on eight training domains in meta-train. We validate every 5K iterations to save the best model for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Our method with different feature extractors</head><p>Tab. 6 shows the results of our method (the proposed residual adapters in matrix form) when incorporated to different feature extractors, single domain model with ResNet-18 backbone (SDL-ResNet-18) pre-trained on ImageNet, single domain model with ResNet-34 (SDL-ResNet-34) pretrained on ImageNet, vanilla multi-domain learning (MDL) and URL <ref type="bibr" target="#b26">[27]</ref>. We see that attaching and learning residual adapters can significantly improve the performance on all domains over SDL-ResNet-18, SDL-ResNet-34 and MDL and obtain better performance on most domains over URL (11 out of 13 domains). This strongly indicates that our method can efficiently adapt the model for unseen categories and domains with few support samples while being agnostic to the feature extractor with different backbone and resolution of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Task-specific parameterizations</head><p>In Tab. 7, we report additional 95% confidence interval of each dataset to the main paper for the comparison of different r ? choices based on the URL model. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. We can see that the confidence intervals for different methods have marginal differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Varying-way 5-shot and 5-way-1-shot</head><p>In the main paper, we only report the average accuracy of Varying-Way Five-Shot and Five-Way One-Shot scenarios due to limited space, and detailed results are depicted in Tab. <ref type="bibr" target="#b7">8</ref>   <ref type="table">Table 9</ref>. Comparison state-of-the-art methods on Meta-Dataset (using a multi-domain feature extractor of <ref type="bibr" target="#b26">[27]</ref>). Mean accuracy, 95% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>domains.</head><p>C.4. Results evaluated with updated evaluation protocol.</p><p>As the code from Meta-dataset has been updated, we evaluate all methods with the updated evaluation protocol from the Meta-dataset 4 and report the results 5 in Tab. 9. As shown in Tab. 9, the update does not affect much on the results and our method rank 1.5 in average and the state-of- <ref type="bibr" target="#b3">4</ref> As mentioned in https://github.com/google-research/ meta-dataset/issues/54, we also set the shuffle buffer size as 1000 to evaluate all methods and report the results in Tab. 9. This change does not affect much on the results as the datasets we used were shuffled using the latest data convert code from Meta-Dataset. <ref type="bibr" target="#b4">5</ref> The results of Simple CNAPS <ref type="bibr" target="#b2">[3]</ref> and Transductive CNAPS <ref type="bibr" target="#b1">[2]</ref> are reproduced by the authors and reported at https://github.com/ peymanbateni/simple-cnaps. Results of FLUTE <ref type="bibr" target="#b46">[47]</ref> and tri-M <ref type="bibr" target="#b29">[30]</ref> are from their papers. We reproduce the results of SUR <ref type="bibr" target="#b14">[15]</ref> and URT <ref type="bibr" target="#b28">[29]</ref> with the updated evaluation protocol for fair comparison. the-art method URL rank 2.7. Our method outperforms other methods on most domains (9 out of 13), especially obtaining significant improvement on 5 unseen datasets than the second best method, i.e. Average Unseen (+7.9). More specifically, our method obtains significant better results than the second best approach (URL) on Traffic Sign (+20.2), CIFAR-10 (+8.7), and CIFAR-100 (+7.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Ablation study</head><p>Here, we conduct ablation study of our method with the URL model, unless stated otherwise. Sensitivity analysis for number of iterations. In our method, we optimize the attached parameters (?, ?) with 40 iterations. <ref type="figure" target="#fig_6">Figure 6</ref> and <ref type="figure" target="#fig_8">Figure 7</ref> report the results with 10, 20, 40, 60 iterations and indicates that our method (solid green) converges to a stable solution after 20 iterations and achieves better average performance on all domains than the baseline URL (dash green). The mean accuracy with 95%     Initialization analysis for adapters. Here, we investigate using different initialization strategies for adapters: i) Identity initialization: in this work we initialize each residual adapter as an identity matrix scaled by a scalar ? and we set ? = 1e ? 4; ii) randomly initialization: alternatively, we can randomly initialize each residual adapter. The results of different initialization are summarized in <ref type="figure" target="#fig_10">Fig. 8</ref>. We can see that our methods with different initialization strategies obtain similar results, which indicates that our method works also with randomly initialization and again verifies the stability of our method. Detailed results of each datasets are shown in Tab. <ref type="bibr" target="#b12">13</ref>.    Layer analysis for adapters. Here we investigate whether it is sufficient to attach the adapters only to the later layers. We evaluate this on ResNet18 which is composed of four blocks and attach the adapters to only later blocks (block4,     <ref type="figure" target="#fig_12">Figure 9</ref> shows that applying our adapters to only the last block (block4) obtains around 78% average accuracy on all domains which outperforms the URL. With attaching residual adapters to more layers, the performance on unseen domains is improved significantly while the one on seen domains remains stable. The mean accuracy with 95% confidence interval for layer analysis are shown in Tab. 14.</p><p>Decomposing residual adapters. Here we investigate whether one can reduce the number of parameters in the adapters while retaining its performance by using matrix decomposition. As in deep neural network, the adapters in earlier layers are relatively small, we then decompose the adapters in the last two blocks only where the adapter dimensionality goes up to 512 ? 512. <ref type="figure" target="#fig_14">Figure 10</ref> shows that our method can achieve good performance with less parameters by decomposing large residual adapters, (e.g. when N = 32 where the number of additional parameters equal to around 4% vs 13%, the performance is still comparable to the original form of residual adapters, i.e. N=0). Results of each datasets in Tab. <ref type="bibr" target="#b14">15</ref>, also show that, by decomposing large residual adapters, the performance of our method is still comparable to the original form of residual adapters (i.e. Ours) with less parameters. The similar conclusion can be drawn from results (shown in <ref type="figure" target="#fig_16">Fig. 11</ref>) of our method using decomposed residual adapters in all layers. When N increases, i.e., smaller residual adapters, the average accuracy on all domains is still comparable to the original form of residual adapters (i.e. N=0) with less parameters though the average accuracy on unseen domains drops slightly. From the results depicted in Tab. 16, we can see that when N increases, the performance        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Qualitative results</head><p>We qualitatively analyze our method and compare it to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Figs. 12 to 24 by illustrating the nearest neighbors in all test datasets given a query image as in <ref type="bibr" target="#b26">[27]</ref>. It is clear that our method produces more correct neighbors than other methods. While other methods retrieve images with more similar colors, shapes and backgrounds, e.g. in Figs. 20, 21, 23 and 24, our method is able to retrieve semantically similar images. More specifically, as shown in <ref type="figure" target="#fig_5">Fig. 15</ref>, our method correctly produces neighbors of the bird in the query image while other methods pick images with similar appearances or similar background, e.g. images with twigs. In <ref type="figure">Fig. 20</ref>, other methods mainly retrieve the triangle sign while our method is able to retrieve the correct sign with illumination distortion. In <ref type="figure" target="#fig_3">Fig. 24</ref>, other methods including SUR, URT are distracted by the blue background but our method select the correct shark images. It again suggests that our method is able to quickly adapt the features for unseen few-shot tasks. <ref type="figure">Figure 12</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in ImageNet. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_2">Figure 13</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Omniglot. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_3">Figure 14</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Aircraft. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_5">Figure 15</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Birds. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_6">Figure 16</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Textures. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_8">Figure 17</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Quick Draw. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_10">Figure 18</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Fungi. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_12">Figure 19</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in VGG Flower. Green and red colors indicate correct and false predictions respectively. <ref type="figure">Figure 20</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in Traffic Sign. Green and red colors indicate correct and false predictions respectively. <ref type="figure">Figure 21</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in MSCOCO. Green and red colors indicate correct and false predictions respectively. <ref type="figure">Figure 22</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in MNIST. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_2">Figure 23</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in CIFAR-10. Green and red colors indicate correct and false predictions respectively. <ref type="figure" target="#fig_3">Figure 24</ref>. Qualitative comparison to Simple CNAPS <ref type="bibr" target="#b2">[3]</ref>, SUR <ref type="bibr" target="#b14">[15]</ref>, URT <ref type="bibr" target="#b28">[29]</ref>, and URL <ref type="bibr" target="#b26">[27]</ref> in CIFAR-100. Green and red colors indicate correct and false predictions respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Sensitivity of performance to number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Block (layer) analysis for adapters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Decomposed residual adapters on block-3,4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Sensitivity of performance to number of iterations based on MDL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Sensitivity of performance to number of iterations based on URL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Ours(MDL)-I Ours(MDL)-R Ours(URL)-I Ours(URL)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Initialization analysis for adapters. '-I' indicates identity initialization and '-R' is randomly initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Block (layer) analysis for adapters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Decomposed residual adapters on block-3,4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 .</head><label>11</label><figDesc>Decomposed residual adapters on all layers.of most domains are still comparable to the original form of residual adapters (i.e. Ours) while the performance on Traffic Sign drops slightly as the adapters in earlier layers are small and when N is larger the decomposed residual adapters might be too small to tranform the features. In overall, our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>prediction Support Set Query Set cos Support set Features Class Centroids Query FeatureTask adaptation in Meta-test.</head><label></label><figDesc></figDesc><table><row><cell>block1</cell><cell></cell><cell>block2</cell><cell></cell><cell></cell><cell>block4</cell></row><row><cell>module1 module2</cell><cell cols="3">module1 module2</cell><cell>...</cell><cell>module1 module2</cell></row><row><cell>3x3</cell><cell>BN</cell><cell>ReLU</cell><cell>3x3</cell><cell></cell><cell>BN</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell cols="2">module1/2</cell><cell></cell><cell></cell></row><row><cell>3x3</cell><cell>3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Test Dataset</cell><cell cols="4">CNAPS [41] Simple CNAPS [3] TransductiveCNAPS [2] SUR [15]</cell><cell cols="3">URT [29] FLUTE [47] tri-M [30]</cell><cell>URL [27]</cell><cell>Ours</cell></row><row><cell>ImageNet</cell><cell>50.8 ? 1.1</cell><cell>58.4 ? 1.1</cell><cell>57.9 ? 1.1</cell><cell cols="2">56.2 ? 1.0 56.8 ? 1.1</cell><cell>58.6 ? 1.0</cell><cell cols="3">51.8 ? 1.1 58.8 ? 1.1 59.5 ? 1.0</cell></row><row><cell>Omniglot</cell><cell>91.7 ? 0.5</cell><cell>91.6 ? 0.6</cell><cell>94.3 ? 0.4</cell><cell cols="2">94.1 ? 0.4 94.2 ? 0.4</cell><cell>92.0 ? 0.6</cell><cell cols="3">93.2 ? 0.5 94.5 ? 0.4 94.9 ? 0.4</cell></row><row><cell>Aircraft</cell><cell>83.7 ? 0.6</cell><cell>82.0 ? 0.7</cell><cell>84.7 ? 0.5</cell><cell cols="2">85.5 ? 0.5 85.8 ? 0.5</cell><cell>82.8 ? 0.7</cell><cell cols="3">87.2 ? 0.5 89.4 ? 0.4 89.9 ? 0.4</cell></row><row><cell>Birds</cell><cell>73.6 ? 0.9</cell><cell>74.8 ? 0.9</cell><cell>78.8 ? 0.7</cell><cell cols="2">71.0 ? 1.0 76.2 ? 0.8</cell><cell>75.3 ? 0.8</cell><cell cols="3">79.2 ? 0.8 80.7 ? 0.8 81.1 ? 0.8</cell></row><row><cell>Textures</cell><cell>59.5 ? 0.7</cell><cell>68.8 ? 0.9</cell><cell>66.2 ? 0.8</cell><cell cols="2">71.0 ? 0.8 71.6 ? 0.7</cell><cell>71.2 ? 0.8</cell><cell cols="3">68.8 ? 0.8 77.2 ? 0.7 77.5 ? 0.7</cell></row><row><cell>Quick Draw</cell><cell>74.7 ? 0.8</cell><cell>76.5 ? 0.8</cell><cell>77.9 ? 0.6</cell><cell cols="2">81.8 ? 0.6 82.4 ? 0.6</cell><cell>77.3 ? 0.7</cell><cell cols="3">79.5 ? 0.7 82.5 ? 0.6 81.7 ? 0.6</cell></row><row><cell>Fungi</cell><cell>50.2 ? 1.1</cell><cell>46.6 ? 1.0</cell><cell>48.9 ? 1.2</cell><cell cols="2">64.3 ? 0.9 64.0 ? 1.0</cell><cell>48.5 ? 1.0</cell><cell cols="3">58.1 ? 1.1 68.1 ? 0.9 66.3 ? 0.8</cell></row><row><cell>VGG Flower</cell><cell>88.9 ? 0.5</cell><cell>90.5 ? 0.5</cell><cell>92.3 ? 0.4</cell><cell cols="2">82.9 ? 0.8 87.9 ? 0.6</cell><cell>90.5 ? 0.5</cell><cell cols="2">91.6 ? 0.6 92.0 ? 0.5</cell><cell>92.2 ? 0.5</cell></row><row><cell>Traffic Sign</cell><cell>56.5 ? 1.1</cell><cell>57.2 ? 1.0</cell><cell>59.7 ? 1.1</cell><cell cols="2">51.0 ? 1.1 48.2 ? 1.1</cell><cell>63.0 ? 1.0</cell><cell cols="3">58.4 ? 1.1 63.3 ? 1.1 82.8 ? 1.0</cell></row><row><cell>MSCOCO</cell><cell>39.4 ? 1.0</cell><cell>48.9 ? 1.1</cell><cell>42.5 ? 1.1</cell><cell cols="2">52.0 ? 1.1 51.5 ? 1.1</cell><cell>52.8 ? 1.1</cell><cell cols="3">50.0 ? 1.0 57.3 ? 1.0 57.6 ? 1.0</cell></row><row><cell>MNIST</cell><cell>-</cell><cell>94.6 ? 0.4</cell><cell>94.7 ? 0.3</cell><cell cols="2">94.3 ? 0.4 90.6 ? 0.5</cell><cell>96.2 ? 0.3</cell><cell cols="3">95.6 ? 0.5 94.7 ? 0.4 96.7 ? 0.4</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell>74.9 ? 0.7</cell><cell>73.6 ? 0.7</cell><cell cols="2">66.5 ? 0.9 67.0 ? 0.8</cell><cell>75.4 ? 0.8</cell><cell cols="3">78.6 ? 0.7 74.2 ? 0.8 82.9 ? 0.7</cell></row><row><cell>CIFAR-100</cell><cell>-</cell><cell>61.3 ? 1.1</cell><cell>61.8 ? 1.0</cell><cell cols="2">56.9 ? 1.1 57.3 ? 1.0</cell><cell>62.0 ? 1.0</cell><cell cols="3">67.1 ? 1.0 63.5 ? 1.0 70.4 ? 0.9</cell></row><row><cell>Average Seen</cell><cell>71.6</cell><cell>73.7</cell><cell>75.1</cell><cell>75.9</cell><cell>77.4</cell><cell>74.5</cell><cell>76.2</cell><cell>80.4</cell><cell>80.4</cell></row><row><cell>Average Unseen</cell><cell>-</cell><cell>67.4</cell><cell>66.5</cell><cell>64.1</cell><cell>62.9</cell><cell>69.9</cell><cell>69.9</cell><cell>70.6</cell><cell>78.1</cell></row><row><cell>Average All</cell><cell>-</cell><cell>71.2</cell><cell>71.8</cell><cell>71.4</cell><cell>71.8</cell><cell>72.7</cell><cell>73.8</cell><cell>76.6</cell><cell>79.5</cell></row><row><cell>Average Rank</cell><cell>-</cell><cell>6.1</cell><cell>5.5</cell><cell>5.6</cell><cell>5.5</cell><cell>4.8</cell><cell>4.4</cell><cell>2.5</cell><cell>1.6</cell></row></table><note>Comparison state-of-the-art methods on Meta-Dataset (using a multi-domain feature extractor of [27]). Mean accuracy, 95% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 1.1 50.5 ? 1.1 49.5 ? 1.1 52.8 ? 1.1 51.9 ? 1.1 46.9 ? 1.1 59.5 ? 1.1 53.7 ? 1.1 62.8 ? 1.0 63.7 ? 1.0 Omniglot 60.9 ? 1.6 60.0 ? 1.4 63.4 ? 1.3 61.9 ? 1.5 67.6 ? 1.2 61.6 ? 1.4 78.2 ? 1.2 68.5 ? 1.3 82.2 ? 1.0 82.6 ? 1.1 Aircraft 68.7 ? 1.3 53.1 ? 1.0 56.0 ? 1.0 63.4 ? 1.1 54.1 ? 0.9 48.5 ? 1.0 72.2 ? 1.0 58.0 ? 1.0 79.5 ? 0.9 80.1 ? 1.0 Birds 57.3 ? 1.3 68.8 ? 1.0 68.7 ? 1.0 69.8 ? 1.1 70.7 ? 0.9 47.9 ? 1.0 74.9 ? 0.9 74.1 ? 0.9 80.6 ? 0.9 83.4 ? 0.8 Textures 69.0 ? 0.9 66.6 ? 0.8 66.5 ? 0.8 70.8 ? 0.9 68.3 ? 0.8 63.8 ? 0.8 77.3 ? 0.7 68.8 ? 0.8 75.6 ? 0.6 79.6 ? 0.7 Quick Draw 42.6 ? 1.2 49</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet-34</cell><cell></cell></row><row><cell>Test Dataset</cell><cell>Finetune [48]</cell><cell>ProtoNet [48]</cell><cell>fo-Proto-MAML [48]</cell><cell>ALFA+fo-Proto -MAML [48]</cell><cell>BOHB [44]</cell><cell>FLUTE [47]</cell><cell>Ours</cell><cell>ProtoNet [14]</cell><cell>CTX [14]</cell><cell>Ours</cell></row><row><cell>ImageNet</cell><cell>45.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison to state-of-the-art methods on Meta-Dataset (using a single-domain feature extractor which is trained only on ImageNet). Mean accuracy, 95% confidence interval are reported. Only ImageNet is seen during training and the rest datasets are unseen for test only.Transductive CNAPS further improves by using unlabelled test images. SUR and URT fuse multi-domain features to get better performance. FLUTE improves URT by fusing FiLM parameters as initialization which is further finetuned on the support set in meta-test. tri-M adopts the same strategy of learning modulation parameters as CNAPS, where the parameters are further divided into the domain-specific set and the domain-cooperative set to explore the intra-domain information and inter-domain correlations, respectively. URL surpasses previous methods by learning a universal representation with distillation from multiple domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Comparisons to methods that learn classifiers and model adaptation methods during meta-test stage based on URL model. NCC, MD, LR, SVM denote nearest centroid classifier, Mahalanobis distance, logistic regression, support vector machines respectively. 'Aux-Net or Ad' indicates using Auxiliary Network to predict ? or attaching adapter ? directly. 'M or CW' means using matrix multiplication or channel-wise scaling adapters.</figDesc><table /><note>'S' and 'R' denote serial adapter and residual adapter, respectively. '?' indicates using the pre-classifier adaptation. The standard deviation results can be found in the supplementary. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.efficient (We train our model on one single Nvidia GPU for around 33 hours while CTX requires 8 Nvidia V100 GPUs and 7 days for training. Please refer to the supplementary for more details).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Results of Varying-Way Five-Shot and Five-Way One-Shot scenarios. Mean accuracies are reported and more detailed results can be found in the supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Training hyper-parameters of single domain learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Ours (MDL) 55.6 ? 1.0 94.3 ? 0.4 86.7 ? 0.5 79.4 ? 0.8 73.2 ? 0.8 81.7 ? 0.6 64.0 ? 0.9 90.9 ? 0.5 81.1 ? 0.9 51.4 ? 1.1 96.9 ? 0.3 78.5 ? 0.8 64.3 ? 1.1 Ours (URL) 59.5 ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7 81.7 ? 0.6 66.3 ? 0.9 92.2 ? 0.5 82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0 Ours (SDL-ResNet-18) 59.5 ? 1.1 78.2 ? 1.2 72.2 ? 1.0 74.9 ? 0.9 77.3 ? 0.7 67.6 ? 0.9 44.7 ? 1.0 90.9 ? 0.6 82.5 ? 0.8 59.0 ? 1.0 93.9 ? 0.6 82.1 ? 0.7 70.7 ? 0.9 Ours (SDL-ResNet-34) 63.7 ? 1.0 82.6 ? 1.1 80.1 ? 1.0 83.4 ? 0.8 79.6 ? 0.7 71.0 ? 0.8 51.4 ? 1.2 94.0 ? 0.5 81.7 ? 0.9 61.7 ? 0.9 94.6 ? 0.5 86.0 ? 0.6 78.3 ? 0.8 Results of attaching residual adapters to different baselines. 'SDL-ResNet-18' is the single domain model with ResNet-18 backbone pretrained on ImageNet. 'SDL-ResNet-34' is the single domain model with ResNet-34 backbone pretrained on ImageNet. 'MDL' is a vanilla Multi-Domain Learning (MDL) model trained on eight seen datasets jointly.</figDesc><table><row><cell>Test Dataset</cell><cell></cell><cell></cell><cell>ImageNet</cell><cell>Omniglot</cell><cell cols="2">Aircraft</cell><cell></cell><cell>Birds</cell><cell>Textures</cell><cell cols="2">Quick Draw</cell><cell cols="2">Fungi</cell><cell cols="5">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>MDL</cell><cell></cell><cell cols="2">53.4 ? 1.1</cell><cell>93.8 ? 0.4</cell><cell cols="2">86.6 ? 0.5</cell><cell cols="2">78.6 ? 0.8</cell><cell>71.4 ? 0.7</cell><cell cols="2">81.5 ? 0.6</cell><cell cols="2">61.9 ? 1.0</cell><cell cols="2">88.7 ? 0.6</cell><cell>51.0 ? 1.0</cell><cell cols="2">49.7 ? 1.1</cell><cell>94.4 ? 0.3</cell><cell>66.7 ? 0.8</cell><cell>53.6 ? 1.0</cell></row><row><cell>URL [27]</cell><cell></cell><cell cols="2">58.8 ? 1.1</cell><cell>94.5 ? 0.4</cell><cell cols="2">89.4 ? 0.4</cell><cell cols="2">80.7 ? 0.8</cell><cell>77.2 ? 0.7</cell><cell cols="4">82.5 ? 0.6 68.1 ? 0.9</cell><cell cols="2">92.0 ? 0.5</cell><cell>63.3 ? 1.2</cell><cell cols="2">57.3 ? 1.0</cell><cell>94.7 ? 0.4</cell><cell>74.2 ? 0.8</cell><cell>63.6 ? 1.0</cell></row><row><cell cols="2">SDL-ResNet-18</cell><cell cols="2">55.8 ? 1.0</cell><cell>67.4 ? 1.2</cell><cell cols="2">49.5 ? 0.9</cell><cell cols="2">71.2 ? 0.9</cell><cell>73.0 ? 0.6</cell><cell cols="2">53.9 ? 1.0</cell><cell cols="2">41.6 ? 1.0</cell><cell cols="2">87.0 ? 0.6</cell><cell>47.4 ? 1.1</cell><cell cols="2">53.5 ? 1.0</cell><cell>78.1 ? 0.7</cell><cell>67.3 ? 0.8</cell><cell>56.6 ? 0.9</cell></row><row><cell cols="2">SDL-ResNet-34</cell><cell cols="2">62.2 ? 1.1</cell><cell>72.8 ? 1.1</cell><cell cols="2">62.9 ? 0.9</cell><cell cols="2">79.6 ? 0.8</cell><cell>75.6 ? 0.6</cell><cell cols="2">64.5 ? 0.8</cell><cell cols="2">47.4 ? 1.1</cell><cell cols="2">90.4 ? 0.6</cell><cell>54.8 ? 1.0</cell><cell cols="2">56.1 ? 1.0</cell><cell>79.3 ? 0.6</cell><cell>83.0 ? 0.6</cell><cell>74.8 ? 0.8</cell></row><row><cell>Test Dataset</cell><cell>classifier</cell><cell cols="4">Aux-Net serial or M or ? #params or Ad parallel CW</cell><cell>ImageNet</cell><cell></cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell cols="2">Textures</cell><cell cols="2">Quick Draw</cell><cell>Fungi</cell><cell cols="4">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>NCC</cell><cell>NCC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">57.0 ? 1.1</cell><cell>94.4 ? 0.4</cell><cell>88.0 ? 0.5</cell><cell>80.3 ? 0.7</cell><cell cols="2">74.6 ? 0.7</cell><cell cols="2">81.8 ? 0.6</cell><cell>66.2 ? 0.9</cell><cell>91.5 ? 0.5</cell><cell>49.8 ? 1.1</cell><cell cols="2">54.1 ? 1.0</cell><cell>91.1 ? 0.4</cell><cell>70.6 ? 0.7</cell><cell>59.1 ? 1.0</cell></row><row><cell>MD</cell><cell>MD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">53.9 ? 1.0</cell><cell>93.8 ? 0.5</cell><cell>87.6 ? 0.5</cell><cell>78.3 ? 0.7</cell><cell cols="2">73.7 ? 0.7</cell><cell cols="2">80.9 ? 0.7</cell><cell>57.7 ? 0.9</cell><cell>89.7 ? 0.6</cell><cell>62.2 ? 1.1</cell><cell cols="2">48.5 ? 1.0</cell><cell>95.1 ? 0.4</cell><cell>68.9 ? 0.8</cell><cell>60.0 ? 0.9</cell></row><row><cell>LR</cell><cell>LR</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">56.0 ? 1.1</cell><cell>93.7 ? 0.5</cell><cell>88.3 ? 0.6</cell><cell>79.7 ? 0.8</cell><cell cols="2">74.7 ? 0.7</cell><cell cols="2">80.0 ? 0.7</cell><cell>62.1 ? 0.8</cell><cell>91.1 ? 0.5</cell><cell>59.7 ? 1.1</cell><cell cols="2">51.2 ? 1.1</cell><cell>93.5 ? 0.5</cell><cell>73.1 ? 0.8</cell><cell>60.1 ? 1.1</cell></row><row><cell>SVM</cell><cell>SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">54.5 ? 1.1</cell><cell>94.3 ? 0.5</cell><cell>87.7 ? 0.5</cell><cell>78.1 ? 0.8</cell><cell cols="2">73.8 ? 0.8</cell><cell cols="2">80.0 ? 0.6</cell><cell>58.5 ? 0.9</cell><cell>91.4 ? 0.6</cell><cell>65.7 ? 1.2</cell><cell cols="2">50.5 ? 1.0</cell><cell>95.4 ? 0.4</cell><cell>72.0 ? 0.8</cell><cell>60.5 ? 1.1</cell></row><row><cell>Softmax</cell><cell>Softmax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">42.2 ? 1.0</cell><cell>85.3 ? 0.7</cell><cell>71.9 ? 0.8</cell><cell>59.6 ? 1.0</cell><cell cols="2">62.0 ? 0.8</cell><cell cols="2">61.2 ? 1.0</cell><cell>37.3 ? 0.9</cell><cell>66.7 ? 1.0</cell><cell>51.4 ? 1.1</cell><cell cols="2">48.2 ? 1.1</cell><cell>93.5 ? 0.5</cell><cell>70.4 ? 0.8</cell><cell>59.3 ? 1.0</cell></row><row><cell>KNN</cell><cell>KNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">48.1 ? 1.1</cell><cell>94.1 ? 0.4</cell><cell>84.5 ? 0.6</cell><cell>70.7 ? 0.8</cell><cell cols="2">65.9 ? 0.8</cell><cell cols="2">74.8 ? 0.7</cell><cell>53.5 ? 0.9</cell><cell>86.0 ? 0.6</cell><cell>56.9 ? 1.2</cell><cell cols="2">44.7 ? 1.1</cell><cell>91.4 ? 0.5</cell><cell>60.3 ? 0.8</cell><cell>49.4 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table /><note>Comparisons to methods that learn classifiers and model adaptation methods during meta-test stage based on URL model. NCC, MD, LR, SVM, Softmax, KNN denote nearest centroid classifier, Mahalanobis distance, logistic regression, support vector machines, softmax classifier and k-nearest neighbors classifier respectively. PA indicates pre-classifier alignment. 'Aux-Net or Ad' indicates using Auxiliary Network to predict ? or attaching adapter ? directly. 'M or CW' means using matrix multiplication or channel-wise scaling adapters. 'S' and 'R' denote serial adapter and residual adapter, respectively. '?' indicates using the pre-classifier adaptation. Mean accuracy, 95% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>. In the table, we report the Mean accuracy, 95% confidence interval of each dataset. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. URT and URL are two strong baselines surpassing both Simple CNAPS and SUR, while Ours outperforms them on most datasets, especially on unseen ? 1.0 46.7 ? 1.0 48.6 ? 1.0 49.4 ? 1.0 48.3 ? 1.0 42.6 ? 0.9 40.7 ? 1.0 47.4 ? 1.0 49.6 ? 1.1 48.0 ? 1.0 Omniglot 95.1 ? 0.3 95.8 ? 0.3 96.0 ? 0.3 96.0 ? 0.3 96.8 ? 0.3 93.1 ? 0.5 93.0 ? 0.7 95.6 ? 0.5 95.8 ? 0.5 96.3 ? 0.4 Aircraft 74.6 ? 0.6 82.1 ? 0.6 81.2 ? 0.6 84.8 ? 0.5 85.5 ? 0.5 65.8 ? 0.9 67.1 ? 1.4 77.9 ? 0.9 79.6 ? 0.9 79.6 ? 0.9 Birds 69.6 ? 0.7 62.8 ? 0.9 71.2 ? 0.776.0 ? 0.6 76.6 ? 0.6 67.9 ? 0.9 59.2 ? 1.0 70.9 ? 0.9 74.9 ? 0.9 74.5 ? 0.9</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Varying-Way Five-Shot</cell><cell></cell><cell></cell><cell cols="2">Five-Way One-Shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Dataset</cell><cell>Simple CNAPS [3]</cell><cell>SUR [15]</cell><cell>URT [29]</cell><cell>URL [27]</cell><cell>Ours</cell><cell>Simple CNAPS [3]</cell><cell>SUR [15]</cell><cell>URT [29]</cell><cell>URL [27]</cell><cell>Ours</cell></row><row><cell cols="6">ImageNet 47.2 Textures 57.5 ? 0.7 60.2 ? 0.7 65.2 ? 0.7 69.1 ? 0.6 68.3 ? 0.7</cell><cell>42.2 ? 0.8</cell><cell cols="4">42.5 ? 0.8 49.4 ? 0.9 53.6 ? 0.9 54.5 ? 0.9</cell></row><row><cell>Quick Draw</cell><cell cols="4">70.9 ? 0.6 79.0 ? 0.5 79.2 ? 0.5 78.2 ? 0.5</cell><cell>77.9 ? 0.6</cell><cell cols="4">70.5 ? 0.9 79.8 ? 0.9 79.6 ? 0.9 79.0 ? 0.8</cell><cell>79.3 ? 0.9</cell></row><row><cell>Fungi</cell><cell cols="3">50.3 ? 1.0 66.5 ? 0.8 66.9 ? 0.9</cell><cell cols="3">70.0 ? 0.8 70.4 ? 0.8 58.3 ? 1.1</cell><cell cols="4">64.8 ? 1.1 71.0 ? 1.0 75.2 ? 1.0 75.3 ? 1.0</cell></row><row><cell>VGG Flower</cell><cell cols="3">86.5 ? 0.4 76.9 ? 0.6 82.4 ? 0.5</cell><cell cols="3">89.3 ? 0.4 89.5 ? 0.4 79.9 ? 0.7</cell><cell cols="4">65.0 ? 1.0 72.7 ? 0.0 79.9 ? 0.8 80.3 ? 0.8</cell></row><row><cell>Traffic Sign</cell><cell cols="3">55.2 ? 0.8 44.9 ? 0.9 45.1 ? 0.9</cell><cell cols="3">57.5 ? 0.8 72.3 ? 0.6 55.3 ? 0.9</cell><cell cols="4">44.6 ? 0.9 52.7 ? 0.9 57.9 ? 0.9 57.2 ? 1.0</cell></row><row><cell>MSCOCO</cell><cell cols="5">49.2 ? 0.8 48.1 ? 0.9 52.3 ? 0.9 56.1 ? 0.8 56.0 ? 0.8</cell><cell>48.8 ? 0.9</cell><cell cols="4">47.8 ? 1.1 56.9 ? 1.1 59.2 ? 1.0 59.9 ? 1.0</cell></row><row><cell>MNIST</cell><cell cols="3">88.9 ? 0.4 90.1 ? 0.4 86.5 ? 0.5</cell><cell cols="7">89.7 ? 0.4 92.5 ? 0.4 80.1 ? 0.9 77.1 ? 0.9 75.6 ? 0.9 78.7 ? 0.9 80.1 ? 0.9</cell></row><row><cell>CIFAR-10</cell><cell cols="3">66.1 ? 0.7 50.3 ? 1.0 61.4 ? 0.7</cell><cell cols="3">66.0 ? 0.7 72.0 ? 0.7 50.3 ? 0.9</cell><cell cols="4">35.8 ? 0.8 47.3 ? 0.9 54.7 ? 0.9 55.8 ? 0.9</cell></row><row><cell>CIFAR-100</cell><cell cols="3">53.8 ? 0.9 46.4 ? 0.9 52.5 ? 0.9</cell><cell cols="3">57.0 ? 0.9 64.1 ? 0.8 53.8 ? 0.9</cell><cell cols="4">42.9 ? 1.0 54.9 ? 1.1 61.8 ? 1.0 63.7 ? 1.0</cell></row><row><cell>Average Seen</cell><cell>69.0</cell><cell>71.2</cell><cell>73.8</cell><cell>76.6</cell><cell>76.7</cell><cell>65.0</cell><cell>64.0</cell><cell>70.6</cell><cell>73.4</cell><cell>73.5</cell></row><row><cell>Average Unseen</cell><cell>62.6</cell><cell>56.0</cell><cell>59.6</cell><cell>65.2</cell><cell>71.4</cell><cell>57.7</cell><cell>49.6</cell><cell>57.5</cell><cell>62.4</cell><cell>63.4</cell></row><row><cell>Average All</cell><cell>66.5</cell><cell>65.4</cell><cell>68.3</cell><cell>72.2</cell><cell>74.6</cell><cell>62.2</cell><cell>58.5</cell><cell>65.5</cell><cell>69.2</cell><cell>69.6</cell></row><row><cell>Average Rank</cell><cell>4.1</cell><cell>3.9</cell><cell>3.4</cell><cell>2.1</cell><cell>1.5</cell><cell>3.8</cell><cell>4.5</cell><cell>3.3</cell><cell>1.7</cell><cell>1.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell>Test Dataset</cell><cell cols="4">CNAPS [41] Simple CNAPS [3] TransductiveCNAPS [2] SUR [15]</cell><cell>URT [29]</cell><cell>FLUTE [47]</cell><cell>tri-M [30]</cell><cell>URL [27]</cell><cell>Ours</cell></row><row><cell>ImageNet</cell><cell>50.8 ? 1.1</cell><cell>56.5 ? 1.1</cell><cell>57.9 ? 1.1</cell><cell cols="2">54.5 ? 1.1 55.0 ? 1.1</cell><cell>51.8 ? 1.1</cell><cell cols="2">58.6 ? 1.0 57.5 ? 1.1</cell><cell>57.4 ? 1.1</cell></row><row><cell>Omniglot</cell><cell>91.7 ? 0.5</cell><cell>91.9 ? 0.6</cell><cell>94.3 ? 0.4</cell><cell cols="2">93.0 ? 0.5 93.3 ? 0.5</cell><cell>93.2 ? 0.5</cell><cell>92.0 ? 0.6</cell><cell cols="2">94.5 ? 0.4 95.0 ? 0.4</cell></row><row><cell>Aircraft</cell><cell>83.7 ? 0.6</cell><cell>83.8 ? 0.6</cell><cell>84.7 ? 0.5</cell><cell cols="2">84.3 ? 0.5 84.5 ? 0.6</cell><cell>87.2 ? 0.5</cell><cell>82.8 ? 0.7</cell><cell cols="2">88.6 ? 0.5 89.3 ? 0.4</cell></row><row><cell>Birds</cell><cell>73.6 ? 0.9</cell><cell>76.1 ? 0.9</cell><cell>78.8 ? 0.7</cell><cell cols="2">70.4 ? 1.1 75.8 ? 0.8</cell><cell>79.2 ? 0.8</cell><cell>75.3 ? 0.8</cell><cell cols="2">80.5 ? 0.7 81.4 ? 0.7</cell></row><row><cell>Textures</cell><cell>59.5 ? 0.7</cell><cell>70.0 ? 0.8</cell><cell>66.2 ? 0.8</cell><cell cols="2">70.5 ? 0.7 70.6 ? 0.7</cell><cell>68.8 ? 0.8</cell><cell>71.2 ? 0.8</cell><cell cols="2">76.2 ? 0.7 76.7 ? 0.7</cell></row><row><cell>Quick Draw</cell><cell>74.7 ? 0.8</cell><cell>78.3 ? 0.7</cell><cell>77.9 ? 0.6</cell><cell cols="2">81.6 ? 0.6 82.1 ? 0.6</cell><cell>79.5 ? 0.7</cell><cell>77.3 ? 0.7</cell><cell>81.9 ? 0.6</cell><cell>82.0 ? 0.6</cell></row><row><cell>Fungi</cell><cell>50.2 ? 1.1</cell><cell>49.1 ? 1.2</cell><cell>48.9 ? 1.2</cell><cell cols="2">65.0 ? 1.0 63.7 ? 1.0</cell><cell>58.1 ? 1.1</cell><cell cols="3">48.5 ? 1.0 68.8 ? 0.9 67.4 ? 1.0</cell></row><row><cell>VGG Flower</cell><cell>88.9 ? 0.5</cell><cell>91.3 ? 0.6</cell><cell>92.3 ? 0.4</cell><cell cols="2">82.2 ? 0.8 88.3 ? 0.6</cell><cell>91.6 ? 0.6</cell><cell>90.5 ? 0.5</cell><cell>92.1 ? 0.5</cell><cell>92.2 ? 0.5</cell></row><row><cell>Traffic Sign</cell><cell>56.5 ? 1.1</cell><cell>59.2 ? 1.0</cell><cell>59.7 ? 1.1</cell><cell cols="2">49.8 ? 1.1 50.1 ? 1.1</cell><cell>58.4 ? 1.1</cell><cell>63.0 ? 1.0</cell><cell cols="2">63.3 ? 1.2 83.5 ? 0.9</cell></row><row><cell>MSCOCO</cell><cell>39.4 ? 1.0</cell><cell>42.4 ? 1.1</cell><cell>42.5 ? 1.1</cell><cell cols="2">49.4 ? 1.1 48.9 ? 1.1</cell><cell>50.0 ? 1.0</cell><cell>52.8 ? 1.1</cell><cell cols="2">54.0 ? 1.0 55.8 ? 1.1</cell></row><row><cell>MNIST</cell><cell>-</cell><cell>94.3 ? 0.4</cell><cell>94.7 ? 0.3</cell><cell cols="2">94.9 ? 0.4 90.5 ? 0.4</cell><cell>95.6 ? 0.5</cell><cell>96.2 ? 0.3</cell><cell cols="2">94.5 ? 0.5 96.7 ? 0.4</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell>72.0 ? 0.8</cell><cell>73.6 ? 0.7</cell><cell cols="2">64.2 ? 0.9 65.1 ? 0.8</cell><cell>78.6 ? 0.7</cell><cell>75.4 ? 0.8</cell><cell cols="2">71.9 ? 0.7 80.6 ? 0.8</cell></row><row><cell>CIFAR-100</cell><cell>-</cell><cell>60.9 ? 1.1</cell><cell>61.8 ? 1.0</cell><cell cols="2">57.1 ? 1.1 57.2 ? 1.0</cell><cell>67.1 ? 1.0</cell><cell>62.0 ? 1.0</cell><cell cols="2">62.6 ? 1.0 69.6 ? 1.0</cell></row><row><cell>Average Seen</cell><cell>71.6</cell><cell>74.6</cell><cell>75.1</cell><cell>75.2</cell><cell>76.7</cell><cell>76.2</cell><cell>74.5</cell><cell>80.0</cell><cell>80.2</cell></row><row><cell>Average Unseen</cell><cell>-</cell><cell>65.8</cell><cell>66.5</cell><cell>63.1</cell><cell>62.4</cell><cell>69.9</cell><cell>69.9</cell><cell>69.3</cell><cell>77.2</cell></row><row><cell>Average All</cell><cell>-</cell><cell>71.2</cell><cell>71.8</cell><cell>70.5</cell><cell>71.2</cell><cell>73.8</cell><cell>72.7</cell><cell>75.9</cell><cell>79.0</cell></row><row><cell>Average Rank</cell><cell>-</cell><cell>6.3</cell><cell>4.9</cell><cell>5.8</cell><cell>5.7</cell><cell>4.3</cell><cell>4.8</cell><cell>2.7</cell><cell>1.5</cell></row></table><note>Results of Varying-Way Five-Shot and Five-Way One-Shot scenarios. Mean accuracy, 95% confidence interval are reported.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>? 1.1 93.9 ? 0.5 86.4 ? 0.5 78.6 ? 0.7 73.3 ? 0.7 81.9 ? 0.6 63.1 ? 0.9 90.3 ? 0.5 77.6 ? 1.0 50.6 ? 1.1 96.9 ? 0.3 77.0 ? 0.8 62.6 ? 1.1 20 iterations 56.2 ? 1.1 94.7 ? 0.4 86.3 ? 0.5 78.3 ? 0.8 73.9 ? 0.7 81.6 ? 0.6 63.4 ? 0.9 90.1 ? 0.6 79.4 ? 1.0 52.8 ? 1.1 97.2 ? 0.3 78.6 ? 0.8 65.9 ? 1.1 40 iterations 55.6 ? 1.0 94.3 ? 0.4 86.7 ? 0.5 79.4 ? 0.8 73.2 ? 0.8 81.7 ? 0.6 64.0 ? 0.9 90.9 ? 0.5 81.1 ? 0.9 51.4 ? 1.1 96.9 ? 0.3 78.5 ? 0.8 64.3 ? 1.1 60 iterations 55.9 ? 1.1 95.1 ? 0.4 85.9 ? 0.6 77.5 ? 0.8 74.7 ? 0.7 80.9 ? 0.6 62.1 ? 0.9 90.7 ? 0.6 82.2 ? 0.9 52.2 ? 1.1 97.0 ? 0.4 78.4 ? 0.8 64.4 ? 1.1</figDesc><table><row><cell>Test Dataset ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell>VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>10 iterations 55.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 .</head><label>10</label><figDesc>Sensitivity of performance to number of iterations based on MDL model.Table 11. Sensitivity of performance to number of iterations based on URL model. confidence interval are reported in Tabs. 10 and 11 Influence of ? and ?. We evaluate different components of our method and report the results in Tab. 12. The results show that both residual adapters ? and the linear transformation ? help adapt features to unseen classes while residual adapters significantly improve the performance on unseen domains. The best results are achieved by using both ? and ?.</figDesc><table><row><cell>Test Dataset ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell cols="2">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell cols="5">10 iterations 58.4 ? 1.1 94.8 ? 0.4 89.9 ? 0.4 81.3 ? 0.7 76.6 ? 0.7</cell><cell>81.8 ? 0.6</cell><cell>68.4 ? 0.9</cell><cell>92.5 ? 0.5</cell><cell cols="3">76.5 ? 1.1 55.6 ? 1.1 96.4 ? 0.4 79.0 ? 0.7 66.9 ? 1.0</cell></row><row><cell cols="5">20 iterations 58.2 ? 1.1 94.8 ? 0.4 89.9 ? 0.4 81.1 ? 0.7 77.5 ? 0.8</cell><cell>81.9 ? 0.6</cell><cell>68.0 ? 0.9</cell><cell>92.4 ? 0.5</cell><cell cols="3">81.8 ? 1.0 57.8 ? 1.1 96.7 ? 0.4 81.7 ? 0.8 69.1 ? 0.9</cell></row><row><cell cols="5">40 iterations 59.5 ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7</cell><cell>81.7 ? 0.6</cell><cell>66.3 ? 0.9</cell><cell>92.2 ? 0.5</cell><cell cols="3">82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0</cell></row><row><cell cols="5">60 iterations 58.7 ? 1.1 94.9 ? 0.4 89.5 ? 0.5 80.8 ? 0.7 77.4 ? 0.8</cell><cell>81.8 ? 0.6</cell><cell>66.2 ? 0.9</cell><cell>92.5 ? 0.5</cell><cell cols="3">83.7 ? 0.9 56.9 ? 1.0 96.6 ? 0.3 82.0 ? 0.8 72.0 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Ours 59.5 ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7 81.7 ? 0.6 66.3 ? 0.9 92.2 ? 0.5 82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0</figDesc><table><row><cell>Test Dataset</cell><cell>ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell cols="3">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell cols="2">Ours w/o ? &amp; ? 57.0 ? 1.1</cell><cell>94.4 ? 0.4</cell><cell>88.0 ? 0.5</cell><cell>80.3 ? 0.7</cell><cell>74.6 ? 0.7</cell><cell>81.8 ? 0.6</cell><cell>66.2 ? 0.9</cell><cell>91.5 ? 0.5</cell><cell>49.8 ? 1.1</cell><cell>54.1 ? 1.0</cell><cell>91.1 ? 0.4</cell><cell>70.6 ? 0.7</cell><cell>59.1 ? 1.0</cell></row><row><cell>Ours w/o ?</cell><cell cols="3">57.3 ? 1.1 94.9 ? 0.4 88.9 ? 0.5</cell><cell>81.0 ? 0.7</cell><cell>76.7 ? 0.7</cell><cell>80.6 ? 0.6</cell><cell>65.4 ? 0.9</cell><cell>91.4 ? 0.5</cell><cell>82.6 ? 1.0</cell><cell>55.0 ? 1.1</cell><cell>96.6 ? 0.4</cell><cell>82.1 ? 0.7</cell><cell>66.4 ? 1.1</cell></row><row><cell>Ours w/o ?</cell><cell>58.8 ? 1.1</cell><cell>94.5 ? 0.4</cell><cell>89.4 ? 0.4</cell><cell>80.7 ? 0.8</cell><cell>77.2 ? 0.7</cell><cell cols="2">82.5 ? 0.6 68.1 ? 0.9</cell><cell>92.0 ? 0.5</cell><cell>63.3 ? 1.2</cell><cell>57.3 ? 1.0</cell><cell>94.7 ? 0.4</cell><cell>74.2 ? 0.8</cell><cell>63.6 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 .</head><label>12</label><figDesc>Effect of each component. We build our method on the URL model and 'Ours w/o ? &amp; ?' means we remove both residual adapters ? and the pre-classifier adaptation layer ? in our method.</figDesc><table><row><cell>Test Dataset</cell><cell>ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell cols="2">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>Ours(SDL-ResNet-18)-I</cell><cell cols="5">59.5 ? 1.1 78.2 ? 1.2 72.2 ? 1.0 74.9 ? 0.9 77.3 ? 0.7</cell><cell>67.6 ? 0.9</cell><cell>44.7 ? 1.0</cell><cell>90.9 ? 0.6</cell><cell cols="3">82.5 ? 0.8 59.0 ? 1.0 93.9 ? 0.6 82.1 ? 0.7 70.7 ? 0.9</cell></row><row><cell cols="6">Ours(SDL-ResNet-18)-R 58.2 ? 1.0 78.4 ? 1.2 71.1 ? 1.1 74.4 ? 1.0 77.1 ? 0.7</cell><cell>67.2 ? 1.0</cell><cell>45.9 ? 1.0</cell><cell>90.7 ? 0.6</cell><cell cols="3">81.9 ? 1.0 57.7 ? 1.1 94.1 ? 0.5 81.9 ? 0.7 70.5 ? 0.9</cell></row><row><cell>Ours(MDL)-I</cell><cell cols="5">55.6 ? 1.0 94.3 ? 0.4 86.7 ? 0.5 79.4 ? 0.8 73.2 ? 0.8</cell><cell>81.7 ? 0.6</cell><cell>64.0 ? 0.9</cell><cell>90.9 ? 0.5</cell><cell cols="3">81.1 ? 0.9 51.4 ? 1.1 96.9 ? 0.3 78.5 ? 0.8 64.3 ? 1.1</cell></row><row><cell>Ours(MDL)-R</cell><cell cols="5">56.0 ? 1.1 94.1 ? 0.4 87.1 ? 0.5 79.7 ? 0.8 74.0 ? 0.7</cell><cell>82.0 ? 0.6</cell><cell>62.6 ? 0.9</cell><cell>90.6 ? 0.6</cell><cell cols="3">80.9 ? 0.9 51.7 ? 1.1 96.9 ? 0.4 77.7 ? 0.9 65.8 ? 1.1</cell></row><row><cell>Ours(URL)-I</cell><cell cols="5">59.5 ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7</cell><cell>81.7 ? 0.6</cell><cell>66.3 ? 0.9</cell><cell>92.2 ? 0.5</cell><cell cols="3">82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0</cell></row><row><cell>Ours(URL)-R</cell><cell cols="5">58.8 ? 1.1 94.9 ? 0.4 90.5 ? 0.4 81.8 ? 0.6 77.7 ? 0.7</cell><cell>82.3 ? 0.6</cell><cell>66.8 ? 0.9</cell><cell>92.6 ? 0.5</cell><cell cols="3">83.7 ? 0.8 57.7 ? 1.1 96.9 ? 0.4 82.5 ? 0.7 72.0 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 .</head><label>13</label><figDesc>Initialization analysis of adapters. 'Ours(URL)-I' indicates our method using URL as the pretrained model and initializing residual adapters as identity matrix (scaled by ? = 0.0001) while 'Ours(URL)-R' means our method initialize residual adapters randomly.</figDesc><table><row><cell>Test Dataset</cell><cell>ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell cols="2">VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>Ours (block4)</cell><cell cols="5">59.0 ? 1.1 95.0 ? 0.4 90.0 ? 0.4 80.6 ? 0.8 77.8 ? 0.7</cell><cell>82.3 ? 0.6</cell><cell>68.2 ? 0.9</cell><cell>91.8 ? 0.6</cell><cell cols="3">70.6 ? 1.1 57.1 ? 1.1 95.9 ? 0.4 77.2 ? 0.8 65.9 ? 1.0</cell></row><row><cell>Ours (block3,4)</cell><cell cols="5">60.4 ? 1.1 94.7 ? 0.4 90.0 ? 0.5 80.4 ? 0.7 77.8 ? 0.7</cell><cell>82.2 ? 0.6</cell><cell>67.2 ? 0.8</cell><cell>92.5 ? 0.5</cell><cell cols="3">77.2 ? 1.0 57.9 ? 1.0 96.7 ? 0.3 78.8 ? 0.9 68.6 ? 0.9</cell></row><row><cell cols="6">Ours (block2,3,4) 59.6 ? 1.1 94.9 ? 0.4 89.9 ? 0.5 81.0 ? 0.8 78.2 ? 0.7</cell><cell>82.4 ? 0.6</cell><cell>67.6 ? 0.9</cell><cell>92.3 ? 0.5</cell><cell cols="3">81.5 ? 1.0 57.9 ? 1.0 96.6 ? 0.4 81.5 ? 0.8 70.6 ? 1.0</cell></row><row><cell>Ours (block-all)</cell><cell cols="5">59.5 ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7</cell><cell>81.7 ? 0.6</cell><cell>66.3 ? 0.9</cell><cell>92.2 ? 0.5</cell><cell cols="3">82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 .</head><label>14</label><figDesc>Block (layer) analysis for adapters based on URL model.</figDesc><table /><note>block3,4, block2,3,4 and block-all.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7 81.7 ? 0.6 66.3 ? 0.9 92.2 ? 0.5 82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0 Ours(N=2) 58.9 ? 1.1 95.2 ? 0.4 89.7 ? 0.5 80.9 ? 0.7 76.7 ? 0.7 81.4 ? 0.6 67.7 ? 0.9 92.2 ? 0.5 82.4 ? 1.0 57.1 ? 1.0 96.5 ? 0.4 82.4 ? 0.7 70.3 ? 1.0 Ours(N=4) 58.7 ? 1.1 94.9 ? 0.4 89.7 ? 0.5 80.3 ? 0.7 77.0 ? 0.7 82.5 ? 0.6 67.2 ? 0.9 92.5 ? 0.5 82.6 ? 1.0 57.5 ? 1.1 96.5 ? 0.4 82.5 ? 0.7 70.8 ? 0.9 Ours(N=8) 59.1 ? 1.1 95.0 ? 0.4 89.8 ? 0.5 80.2 ? 0.8 77.2 ? 0.7 82.1 ? 0.6 67.0 ? 0.9 92.2 ? 0.5 82.5 ? 1.0 57.2 ? 1.1 96.8 ? 0.4 82.6 ? 0.7 71.8 ? 0.9 Ours(N=16) 58.2 ? 1.1 94.7 ? 0.4 90.1 ? 0.4 80.3 ? 0.8 76.9 ? 0.7 81.7 ? 0.6 67.6 ? 0.9 92.0 ? 0.5 81.8 ? 1.0 58.1 ? 1.1 96.4 ? 0.4 81.8 ? 0.7 71.1 ? 0.9 Ours(N=32) 59.2 ? 1.1 94.8 ? 0.4 89.6 ? 0.5 80.0 ? 0.8 77.3 ? 0.6 82.4 ? 0.6 67.2 ? 0.9 92.1 ? 0.5 82.1 ? 1.0 57.1 ? 1.0 96.7 ? 0.3 81.6 ? 0.8 71.1 ? 0.9</figDesc><table><row><cell cols="2">Test Dataset ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell>VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>Ours</cell><cell>59.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 .</head><label>15</label><figDesc>Results of using decomposed RA on layer3,4. ? 1.0 94.9 ? 0.4 89.9 ? 0.4 81.1 ? 0.8 77.5 ? 0.7 81.7 ? 0.6 66.3 ? 0.9 92.2 ? 0.5 82.8 ? 1.0 57.6 ? 1.0 96.7 ? 0.4 82.9 ? 0.7 70.4 ? 1.0 Ours(N=2) 58.1 ? 1.1 94.8 ? 0.4 89.7 ? 0.5 80.2 ? 0.8 76.9 ? 0.7 82.1 ? 0.6 67.8 ? 0.9 92.0 ? 0.6 82.5 ? 0.9 56.9 ? 1.1 96.7 ? 0.3 82.0 ? 0.8 70.3 ? 1.0 Ours(N=4) 59.6 ? 1.1 94.8 ? 0.4 89.9 ? 0.5 80.3 ? 0.8 77.4 ? 0.7 82.6 ? 0.6 66.6 ? 0.9 92.9 ? 0.5 79.7 ? 1.1 57.6 ? 1.1 96.5 ? 0.4 80.9 ? 0.8 70.6 ? 1.0 Ours(N=8) 58.2 ? 1.1 94.6 ? 0.4 89.6 ? 0.5 81.2 ? 0.8 76.6 ? 0.7 82.7 ? 0.6 66.5 ? 0.9 92.3 ? 0.5 78.1 ? 1.1 57.3 ? 1.0 96.3 ? 0.3 81.0 ? 0.8 70.9 ? 0.9 Ours(N=16) 58.9 ? 1.1 94.6 ? 0.4 89.7 ? 0.5 80.1 ? 0.7 77.0 ? 0.7 82.1 ? 0.6 68.4 ? 0.9 91.9 ? 0.5 78.3 ? 1.0 57.8 ? 1.1 96.0 ? 0.4 82.0 ? 0.7 70.3 ? 1.0</figDesc><table><row><cell cols="2">Test Dataset ImageNet</cell><cell>Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>Textures</cell><cell>Quick Draw</cell><cell>Fungi</cell><cell>VGG Flower Traffic Sign MSCOCO</cell><cell>MNIST</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>Ours</cell><cell>59.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 .</head><label>16</label><figDesc>Results of using decomposed RA on all layers. method can achieve good performance with less parameters by decomposing large residual adapters. Training time. The training time (meta-train) of our method is equal to the one of URL (hence no additional cost), i.e. 48 hours in multi-domain setting, 6 hours for Resnet-18 and 33 hours for Resnet-34 in single-domain learning in one Nvidia V100 GPU. Whereas CTX meta-training requires 8 Nvidia V100 GPUs for 7 days and approximately 40 times more expensive than ours. During the meta-test stage, the model parameters are further trained using support set of each episode. Meta-test training cost is depicted in Tab. 12 for Meta-Dataset tasks. URL baseline only finetunes parameters of PA ?. Finetune+NCC updates the entire backbone parameters. Ours learn RA and PA parameters. While URL is the fastest baseline, as it does not require backpropagating the error to early layers, ours is more efficient than finetuning all the backbone parameters.</figDesc><table><row><cell>Test Dataset</cell><cell cols="4">Image Omni Air-Birds -Net -glot craft</cell><cell cols="3">Tex-Quick Fungi tures Draw</cell><cell>VGG Flower</cell><cell>Traffic Sign</cell><cell>MS-COCO</cell><cell>MNIST</cell><cell cols="2">CIFAR CIFAR -10 -100</cell></row><row><cell>URL</cell><cell>0.7</cell><cell>0.7</cell><cell>0.4</cell><cell>0.7</cell><cell>0.4</cell><cell>1.0</cell><cell>1.0</cell><cell>0.5</cell><cell>0.9</cell><cell>0.9</cell><cell>0.4</cell><cell>0.4</cell><cell>1.0</cell></row><row><cell>Finetune+NCC</cell><cell>7.7</cell><cell>2.5</cell><cell>7.4</cell><cell>7.0</cell><cell>5.8</cell><cell>9.3</cell><cell>8.7</cell><cell>6.6</cell><cell>9.1</cell><cell>9.0</cell><cell>6.5</cell><cell>6.7</cell><cell>9.3</cell></row><row><cell>Ours (URL+RA+PA)</cell><cell>7.2</cell><cell>2.4</cell><cell>6.1</cell><cell>6.8</cell><cell>4.8</cell><cell>8.9</cell><cell>7.4</cell><cell>5.2</cell><cell>8.8</cell><cell>8.3</cell><cell>6.0</cell><cell>6.2</cell><cell>8.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 12 .</head><label>12</label><figDesc>Computation cost (# second per task) during meta-test.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the task-agnostic weights can also be finetuned on the target task (e.g.<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As mentioned in https://github.com/google-research/ meta-dataset/issues/54, we further update the evaluation protocol and report the updated results of all methods in the supplementary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that CTX also uses augmentation strategies such as AutoAugment<ref type="bibr" target="#b10">[11]</ref> and other ones from SimClr<ref type="bibr" target="#b6">[7]</ref>. We expect applying the same augmentation strategies to our method would yield further improvements, but we leave this for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. HB is supported by the EPSRC programme grant Visual AI EP/T028572/1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cross-domain few-shot learning by representation fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06498</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enhancing few-shot image classification with unlabelled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarred</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12245</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning feedforward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
	</analytic>
	<monogr>
		<title level="m">The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge. online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schroeder</forename><surname>Brigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 6</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selecting relevant features from a multi-domain representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The quick, draw! a.i. experiment. online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowley</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawashima</forename><surname>Takashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Jongmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fox-Gieg</forename><surname>Nick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal representation learning from multiple domains for fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A universal representation transformer layer for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-mode modulator for multi-domain few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul A</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Boris N Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Optimized generic feature learning for few-shot classification across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07926</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a universal template for few-shot dataset generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
