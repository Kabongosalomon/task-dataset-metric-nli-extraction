<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Afham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isuru</forename><surname>Dissanayake</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinithi</forename><surname>Dissanayake</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaya</forename><surname>Dharmasiri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Thilakarathna</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><surname>Rodrigo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electronic and Telecommunication Engineering</orgName>
								<orgName type="institution">Univeristy of Moratuwa</orgName>
								<address>
									<country key="LK">Sri Lanka</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manual annotation of large-scale point cloud dataset for varying tasks such as 3D object classification, segmentation and detection is often laborious owing to the irregular structure of point clouds. Self-supervised learning, which operates without any human labeling, is a promising approach to address this issue. We observe in the real world that humans are capable of mapping the visual concepts learnt from 2D images to understand the 3D world. Encouraged by this insight, we propose CrossPoint, a simple cross-modal contrastive learning approach to learn transferable 3D point cloud representations. It enables a 3D-2D correspondence of objects by maximizing agreement between point clouds and the corresponding rendered 2D image in the invariant space, while encouraging invariance to transformations in the point cloud modality. Our joint training objective combines the feature correspondences within and across modalities, thus ensembles a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised fashion. Experimental results show that our approach outperforms the previous unsupervised learning methods on a diverse range of downstream tasks including 3D object classification and segmentation. Further, the ablation studies validate the potency of our approach for a better point cloud understanding. Code and pretrained models are available at https://github. com/MohamedAfham/CrossPoint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D vision, which is critical in applications such as autonomous driving, mixed reality and robotics has drawn extensive attention due its ability to understand the human world. In light of that, there have been plethora of work in 3D vision research problems such as object classification <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b59">59]</ref>, detection <ref type="bibr" target="#b35">[35]</ref> and segmentation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b59">59]</ref>   <ref type="figure">Figure 1</ref>. An illustration of the proposed approach. Given a 3D point cloud of an object and its rendered 2D image from a random camera view-point, CrossPoint enforces 3D-2D correspondence while preserving the model being invariant to affine and spatial transformations via self-supervised contrastive learning. This facilitates generalizable point cloud representations which can then be utilized for 3D object classification and segmentation. Note that the 2D images shown in the right are directly rendered from the available 3D point clouds <ref type="bibr" target="#b67">[67]</ref>.</p><p>in the recent years with point clouds as the most popularly 3D data representation method. However, the success of deep learning crucially relies on large-scale annotated data. Even though the advancements in 3D sensing technology (e.g., LIDAR) facilitates extensive collection of 3D point cloud samples, owing to the irregular structure of point clouds, manually annotating such large-scale 3D point cloud datasets is laborious. Self-supervised learning is one of the predominant approaches to address this issue and is proven to be effective in 2D domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">37]</ref>.</p><p>Several works have explored self-supervised representation learning on point clouds and are mainly based on generative models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b61">61]</ref>, reconstruction <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b57">57]</ref> and other pre-text tasks <ref type="bibr" target="#b39">[39]</ref>. In addition to that, with the success in exploitation of contrastive learning for image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">36]</ref> and video <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b58">58]</ref> understanding, recent works have investigated self-supervised contrastive learning for point cloud understanding as well <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b72">72]</ref>. However, the existing contrastive learning based approaches for point cloud understanding only rely on imposing invariance to augmentations of 3D point clouds. Learning from different modalities i.e., cross-modal learning has produced substantial results in self-supervised learning. Vision + language <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b49">49]</ref> and video + audio <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref> are some notable combinations for multimodal learning. Multi-modal setting has been adopted in various 2D vision tasks such as object detection <ref type="bibr" target="#b24">[25]</ref>, few-shot image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b64">64]</ref> and visual question and answering <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b60">60]</ref>. Inspired by the advancements in multimodal learning, we introduce CrossPoint, a simple yet effective cross-modal contrastive learning approach for 3D point cloud understanding.</p><p>The goal of our work is to capture the correspondence between 3D objects and 2D images to constructively learn transferable point cloud representations. As shown in <ref type="figure">Fig.  1</ref>, we embed the augmented versions of point cloud and the corresponding rendered 2D image close to each other in the feature space. In real world, humans are proficient at mapping the visual concepts learnt from 2D images to understand the 3D world. For example, a person would be able to recognize an object easily, if he/she has observed that object via an image. Cognitive scientists argue that 3D-2D correspondence is a part of visual learning process of children <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">47]</ref>. Similarly, in real world applications such as robotics and autonomous driving, the model being aware of such 3D-2D correspondence will immensely facilitate an effective understanding of the 3D world. Our approach in particular, follows a joint objective of embedding the augmented versions of same point cloud close together in the feature space, while preserving the 3D-2D correspondence between them and the rendered 2D image of the original 3D point cloud.</p><p>The joint intra-modal and cross-modal learning objective enforces the model to attain the following desirable attributes: (a) relate the compositional patterns occurring in both point cloud and image modalities e.g., fine-grained part-level attribute of an object; (b) acquire knowledge on spatial and semantic properties of point clouds via imposing invariance to augmentations; and (c) encode the rendered 2D image feature as a centroid to augmented point cloud features thus promoting 3D-2D correspondence agnostic to transformations. Moreover, CrossPoint does not require a memory bank for negative sampling similar to SimCLR <ref type="bibr" target="#b6">[7]</ref>. Formulation of rich augmentations and hard positive samples have been proved to boost the contrastive learning despite having memory banks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b76">76]</ref>. We hypothesize that the employed transformations in intra-modal set-ting and cross-modal correspondence provide adequate feature augmentations. In particular, the rendered 2D image feature acts as a hard positive to formulate a better representation learning.</p><p>We validate the generalizability of our approach with multiple downstream tasks. Specifically, we perform shape classification in both synthetic <ref type="bibr" target="#b62">[62]</ref> and real world <ref type="bibr" target="#b56">[56]</ref> object datasets. Despite being pretrained on a synthetic object dataset <ref type="bibr" target="#b5">[6]</ref>, the performance of CrossPoint in out-ofdistribution data certifies the importance of the joint learning objective. In addition, the ablation studies demonstrate the component-wise contribution of both intra-modal and cross-modal objectives. We also adopt multiple widely used point cloud networks as our feature extractors, thus proving the generic nature of our approach.</p><p>The main contributions of our approach can be summarized as follows:</p><p>? We show that a simple 3D-2D correspondence of objects in the feature space using self-supervised contrastive learning facilitates an effective 3D point cloud understanding.</p><p>? We propose a novel end-to-end self-supervised learning objective encapsulating intra-modal and crossmodal loss functions. It encourages the 2D image feature to be embedded close to the corresponding 3D point cloud prototype, thus avoiding bias towards a particular augmentation.</p><p>? We extensively evaluate our proposed method across three downstream tasks namely: object classification, few-shot learning and part segmentation on a diverse range of synthetic and real-world datasets, where CrossPoint outperforms previous unsupervised learning methods.</p><p>? Additionally, we perform few-shot image classification on CIFAR-FS dataset to demonstrate that finetuning the pretrained image backbone from CrossPoint outperforms the standard baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Representation Learning on Point Clouds. Learning point cloud representation is a challenging task when compared to other modalities (e.g., images). This is because of the irregular structure and also the need for permutation invariance when processing each points. Recent line of works pioneered by PointNet <ref type="bibr" target="#b41">[41]</ref> proposed methods and architectures which directly consume 3D point cloud without any preprocessing. Since then, numerous advancements have been made in point cloud based tasks such as 3D object classification <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b74">74]</ref>, 3D object detection <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b73">73]</ref> and 3D point cloud synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b61">61]</ref>.</p><p>Further, several data augmentation strategies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">29]</ref> have also been proposed to enhance the representation capability of the model regardless of the backbone used. However, the performance of such representation learning methods depend on the annotated point cloud data which is often hard to acquire. Sharma et al. <ref type="bibr" target="#b51">[51]</ref> introduced cTree, where point cloud representations can be learnt in a label efficient scenario (i.e., few-shot learning). In contrast, our approach focuses on learning transferable point cloud representations without leveraging any annotations, which can then be utilized for various downstream tasks such as classification and segmentation.</p><p>Self -Supervised Learning on Point Clouds. Several approaches have been explored to perform self-supervised representation learning on point clouds. Initial line of works exploit generative modelling using generative adversarial networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref> and auto-encoders <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b75">75]</ref>, which aims to reconstruct a given input point cloud with varying architectural designs. Recent line of works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b68">68]</ref> introduce various pretext self-supervision tasks with the goal of learning rich semantic point attributes which eventually leads to high-level discriminative knowledge. For instance, Wang et al. <ref type="bibr" target="#b57">[57]</ref> trains an encoderdecoder model to complete the occluded point clouds, and Poursaeed et al. <ref type="bibr" target="#b39">[39]</ref> defines estimation of rotation angle of the point cloud as the pretext task. However, in this work we leverage contrastive learning <ref type="bibr" target="#b14">[15]</ref> to learn an invariant mapping in the feature space. Inspired by the success of self-supervised contrastive learning for image understanding, numerous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b72">72]</ref> have analyzed such a setting for point cloud understanding. PointContrast <ref type="bibr" target="#b63">[63]</ref> performs point-level invariant mapping on two transformed view of the given point cloud. Similarly Liu et al. <ref type="bibr" target="#b31">[31]</ref> also analyzes a point-level invariant mapping, by introducing point discrimination loss, which enforces the features to be consistent with points belonging to the shape surface and inconsistent with randomly sampled noisy point. STRL <ref type="bibr" target="#b21">[22]</ref>, which is a direct extension of BYOL <ref type="bibr" target="#b13">[14]</ref> to 3D point clouds, unsupervisedly learns the representations through the interactions of online and target networks. Contrary to the existing works which leverage contrastive learning, we introduce an auxiliary cross-modal contrastive objective which captures 3D-2D correspondence yielding a better representation capability.</p><p>Cross-Modal Learning. Learning from different modalities tend to provide rich learning signals from which semantic information of the given context can be addressed easily. Recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54]</ref> have demonstrated that pretraining in a cross-modal setting produces transferable representations which can then be deployed for various downstream tasks. CLIP <ref type="bibr" target="#b44">[44]</ref> aims to learn a multimodal embedding space by maximizing cosine similarity between image and text modalities. Similarly, Morgado et al. <ref type="bibr" target="#b37">[37]</ref> combines audio and video modalities to perform a cross-modal agreement which then achieves significant gains in action recognition and sound recognition tasks. A joint learning approach with point clouds and voxels was introduced by Zhang et al. <ref type="bibr" target="#b72">[72]</ref>. Further, <ref type="bibr" target="#b65">[65]</ref> transfers the pretrained 2D image model to a point-cloud model by filter inflation. Our work is closely related to the concurrent work <ref type="bibr" target="#b33">[33]</ref>, which uses a fixed image feature extractor to perform pixel-to-point knowledge transfer. In contrast to the existing approaches, aligning with the objective of 3D understanding, CrossPoint is designed in such a way that the 2D image feature is encouraged to be embedded close to the corresponding 3D point cloud prototype while invariance to transformations is imposed in the point cloud modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this work, we revamp the unsupervised 3D point cloud representation learning by introducing a fusion of intra-modal and cross-modal contrastive learning objectives. This section begins by introducing the network architecture details of the proposed method (Sec. 3.1). Then we describe the contrastive learning loss functions formulated in both intra-modal (Sec. 3.2) and cross-modal (Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Suppose we are given a dataset,</p><formula xml:id="formula_0">D = {(P i , I i )} |D| i=1 with P i ? R N ?3 and I i ? R H?W ?3 where I i is a rendered 2D</formula><p>image of the 3D point cloud P i . Note that I i is obtained by capturing P i from a random camera view-point <ref type="bibr" target="#b5">[6]</ref>. We aim to train a point cloud feature extractor f ?P (.) in a selfsupervised manner to be effectively transferable to downstream tasks. To this end, we use an image feature extractor f ?I (.), multi-layer perceptron (MLP) projection heads g ?P (.) and g ?I (.) for point cloud and image respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Intra-Modal Instance Discrimination</head><p>Inspired by the success of contrastive pretraining in image modality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">36]</ref>, we formulate our Intra-Modal Instance Discrimination (IMID) to enforce invariance to a set of point cloud geometric transformations T by performing self-supervised contrastive learning. Given an input 3D point cloud P i , we construct augmented versions P t1 i and P t2 i of it. We compose t 1 and t 2 by randomly combining transformations from T in a sequential manner. We use transformations such as rotation, scaling and translation. In addition to that we also utilize spatial transformations such as jittering, normalization and elastic distortion. Regardless of the augmentation, the corresponding transformation matrix parameters are initialized randomly.  The point cloud feature extractor f ?P maps both P t1 i and P t2 i to a feature embedding space and the resulted feature vectors are projected to an invariant space R d where the contrastive loss is applied, using the projection head g ?P . We denote the projected vectors of P t1 i and P t2 i as z t1 i and z t2 i respectively where, z t i = g ?P f ?P P t i . The goal is to maximize the similarity of z t1 i with z t2 i while minimizing the similarity with all the other projected vectors in the mini-batch of point clouds. We leverage NT-Xent loss proposed in SimCLR <ref type="bibr" target="#b6">[7]</ref> for instance discrimination at this stage. Note that our approach doesn't use any memory bank following the recent advancements in self-supervised contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. We compute the loss function l(i, t 1 , t 2 ) for the positive pair of examples z t1 i and z t2 i as:</p><formula xml:id="formula_1">! " # ! ! " ! ! " " Point Cloud Branch ? ! ! ! ! " # # ? ? Intra-Modal Correspondence " " Mean Cross-Modal</formula><formula xml:id="formula_2">l(i, t 1 , t 2 ) = ? log exp(s(z t 1 i , z t 2 i )/? ) N k=1 k? =i exp(s(z t 1 i , z t 1 k )/? ) + N k=1 exp(s(z t 1 i , z t 2 k )/? )<label>(1)</label></formula><p>where N is the mini-batch size, ? is the temperature co-efficient and s (.) denotes the cosine similarity function. Our intra-modal instance discrimination loss function L imid for a mini-batch can be described as:</p><formula xml:id="formula_3">L imid = 1 2N N i=1 [l(i, t 1 , t 2 ) + l(i, t 2 , t 1 )]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Modal Instance Discrimination</head><p>In addition to the feature alignment within point cloud modality, we introduce an auxiliary contrastive objective across point cloud and image modalities to learn discriminative features, thus yielding better representation learning capability of 3D point clouds. As discussed in Sec. 2, several works aim to learn transferable point cloud representations in a cross-modal setting. However, to the best of our knowledge, the joint learning objective of enforcing 3D-2D correspondence, while performing instance discrimination within point cloud modality has not been well explored. We empirically validate with the experimental results in Sec. 4.2 that our joint objective outperforms existing unsupervised representation methods, thus facilitating an effective representation learning of 3D point clouds.</p><p>To this end, we first embed the rendered 2D image I i of P i to a feature space using the visual backbone f ?I . We opt for the commonly used ResNet <ref type="bibr" target="#b19">[20]</ref> architecture as f ?I . We then project the feature vectors to the invariant space R d using the image projection head g ?I . The projected image feature is defined as h i where h i = g ?I (f ?I (I i )). In contrast to previous cross-modal approaches <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b72">72]</ref>, we do not explicitly perform IMID on both the modalities (point cloud and image). Instead, we implement IMID on point cloud and leverage image modality for a better point cloud understanding. We propose a learning objective which specifically induces a bias towards 3D point cloud understanding when compared to image understanding. To this end, we compute the mean of the projected vectors z t1 i and z t2 i to obtain the projected prototype vector z i of P i .</p><formula xml:id="formula_4">z i = 1 2 z t1 i + z t2 i<label>(3)</label></formula><p>In the invariance space, we aim to maximize the similarity of z i with h i since they both correspond to same objects. Our cross-modal alignment enforces the model to learn from harder positive and negative samples, thus enhances the representation capability than learning only from intra-modal alignment. We compute the loss function l(i, z, h) for the positive pair of examples z i and h i as:</p><formula xml:id="formula_5">c(i, z, h) = ? log exp(s(z i , h i )/? ) N k=1 k? =i exp(s(z i , z k )/? ) + N k=1 exp(s(z i , h k )/? )<label>(4)</label></formula><p>where s, N, ? refers to the same parameters as in Eq. 1. The cross-modal loss function L cmid for a mini-batch is then formulated as:</p><formula xml:id="formula_6">L cmid = 1 2N N i=1 [c(i, z, h) + c(i, h, z)]<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Objective</head><p>Finally, we obtain the resultant loss function during training as the combination of L imid and L cmid where L imid imposes invariance to point cloud transformation while L cmid injects the 3D-2D correspondence.</p><formula xml:id="formula_7">L = L imid + L cmid<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training</head><p>Dataset. We use ShapeNet <ref type="bibr" target="#b5">[6]</ref> as the dataset for pretraining CrossPoint. It originally consists of more than 50,000 CAD models from 55 categories. We obtain the rendered RGB images from <ref type="bibr" target="#b67">[67]</ref>, which has 43,783 images from 13 object categories. For a given point cloud, we randomly select a 2D image out of all the rendered images, which is captured from an arbitrary viewpoint. We use 2048 points for each point cloud while we resize the corresponding rendered RGB image to 224 ? 224. In addition to the augmentations applied for point cloud as described in Sec. 3.2, we perform random crop, color jittering and random horizontal flip for rendered images as data augmentation.</p><p>Implementation Details. To draw fair comparison with the existing methods, we deploy PointNet <ref type="bibr" target="#b41">[41]</ref> and DGCNN <ref type="bibr" target="#b59">[59]</ref> as the point cloud feature extractors. We use ResNet-50 <ref type="bibr" target="#b19">[20]</ref> as the image feature extractor. We employ a 2-layer MLP as the projection heads which yield a 256-dimensional feature vector projected in the invariant space R d . We use Adam <ref type="bibr" target="#b26">[27]</ref> optimizer with weight decay 1?10 ?4 and initial learning rate 1 ? 10 ?3 . Cosine annealing <ref type="bibr" target="#b34">[34]</ref> is employed as the learning rate scheduler and the model is trained endto-end for 100 epochs. After pre-training we discard the image feature extractor f ?I (.) and projection heads g ?P (.) and g ?I (.). All downstream tasks are performed on the pretrained point cloud feature extractor f ?P (.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Tasks</head><p>We evaluate the transferability of CrossPoint on three widely used downstream tasks in point cloud representation learning namely: (i) 3D object classification (synthetic and real-world), (ii) Few-shot object classification (synthetic and real-world) and (iii) 3D object part segmentation.</p><p>(i) 3D Object classification. We perform our classification experiments on both ModelNet40 <ref type="bibr" target="#b62">[62]</ref> and ScanOb-jectNN <ref type="bibr" target="#b56">[56]</ref> to demonstrate the generalizability of our approach in both synthetic and real-world 3D shape representation learning. ModelNet40 is a synthetic dataset, where the point clouds are obtained by sampling 3D CAD models. It contains 12,331 objects (9,843 for training and 2,468 for testing) from 40 categories. ScanObjectNN <ref type="bibr" target="#b56">[56]</ref> is more realistic and challenging 3D point cloud classification dataset which comprises of occluded objects extractred from realworld indoor scans. It contains 2,880 objects (2304 for training and 576 for testing) from 15 categories.</p><p>We follow the standard protocol <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">57]</ref> to test the accuracy of our model in object classification. We freeze the pretrained point cloud feature extractor and fit a simple linear SVM classifier on the train split of the classification datasets. We randomly sample 1024 points from each object for both training and testing the classification results. Our CrossPoint also delivers a consistent performance in different backbones. We perform experiments in both PointNet <ref type="bibr" target="#b41">[41]</ref> and DGCNN <ref type="bibr" target="#b59">[59]</ref> where PointNet is an MLP based feature extractor while DGCNN is built on graph convolutional networks. <ref type="table">Table.</ref> 1 reports the linear classification results on ModelNet40. It is clear that CrossPoint outperforms previous state-of-the-art unsupervised methods in both of the feature extractors, thus establishing a new benchmark in self-supervised learning for point clouds. In particular, our model outperforms Depth- <ref type="table">Table 1</ref>. Comparison of ModelNet40 linear classification results with previous self-supervised methods. A linear classifier is fit onto the training split of ModelNet40 using the pretrained model and overall accuracy for classification in test split is reported. Our method CrossPoint, surpasses existing works in both PointNet and DGCNN backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ModelNet40</head><p>3D-GAN <ref type="bibr" target="#b61">[61]</ref> 83.3 Latent-GAN <ref type="bibr" target="#b0">[1]</ref> 85.7 SO-Net <ref type="bibr" target="#b27">[28]</ref> 87.3 FoldingNet <ref type="bibr" target="#b69">[69]</ref> 88.4 MRTNet <ref type="bibr" target="#b12">[13]</ref> 86.4 3D-PointCapsNet <ref type="bibr" target="#b75">[75]</ref> 88.9 DepthContrast <ref type="bibr" target="#b72">[72]</ref> 85.4 ClusterNet <ref type="bibr" target="#b71">[71]</ref> 86.8 VIP-GAN <ref type="bibr" target="#b15">[16]</ref> 90.2 PointNet + Jigsaw <ref type="bibr" target="#b50">[50]</ref> 87.3 PointNet + STRL <ref type="bibr" target="#b21">[22]</ref> 88.3 PointNet + Rotation <ref type="bibr" target="#b39">[39]</ref> 88.6 PointNet + OcCo <ref type="bibr" target="#b57">[57]</ref> 88.7 PointNet + CrossPoint (Ours) 89.1 DGCNN + Multi-Task <ref type="bibr" target="#b17">[18]</ref> 89.1 DGCNN + Self-Contrast <ref type="bibr" target="#b10">[11]</ref> 89.6 DGCNN + Jigsaw <ref type="bibr" target="#b50">[50]</ref> 90.6 DGCNN + STRL <ref type="bibr" target="#b21">[22]</ref> 90.9 DGCNN + Rotation <ref type="bibr" target="#b39">[39]</ref> 90.8 DGCNN + OcCo <ref type="bibr" target="#b57">[57]</ref> 89.2 DGCNN + CrossPoint (Ours) 91.2</p><p>Contrast <ref type="bibr" target="#b72">[72]</ref>, which also employ a cross-modal setting for point cloud representation learning, by a significant margin of 5.8%. While our approach surpasses prior works which utilizes self-supervised contrastive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> by considerable margins, some other methods <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b63">63]</ref> cannot be compared in a fair manner for varying reasons such as different pretraining mechanisms and discrepancy in feature extractors.  <ref type="table">Table 3</ref>. Few-shot object classification results on ModelNet40. We report mean and standard error over 10 runs. Top results of each backbone is colored in red and blue. Proposed CrossPoint improves the few-shot accuracy in all the reported settings. previous state-of-the-art unsupervised methods show that our proposed joint learning approach generalizes to out-ofdistribution data as well.</p><p>(ii) Few-shot object classification. Few-shot learning (FSL) aims to train a model that generalizes with limited data. We conduct experiments on conventional few-shot task (N-way K-shot learning), where the model is evaluated on N classes, and each class contains N samples. Several works in 2D domain <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b55">55]</ref> have shown the effectiveness of training a simple linear classifier on top of representation trained in a self-supervised manner. Similar to standard 3D object classification, we use ModelNet40 and ScanOb-jectNN datasets to carry out FSL experiments. While there is no standard split for FSL in both of the datasets, for a fair comparison with previous methods <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b57">57]</ref>, we randomly sample 10 few-shot tasks and report the mean and standard deviation. <ref type="table">Table.</ref> 3 shows the FSL results on Mod-elNet40, where CrossPoint outperforms prior works in all the FSL settings in both PointNet and DGCNN backbones. It is noticeable that our method with DGCNN backbone performs poorly in some of the FSL settings compared to that with PointNet backbone. Similar pattern is also observed in previous methods <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b57">57]</ref> as well. We attribute to the fact that complex backbones might degrade the few-shot learning performance which has been observed consistently in the FSL literature in images <ref type="bibr" target="#b55">[55]</ref>.</p><p>We report the FSL results on ScanObjectNN dataset in <ref type="table">Table.</ref> 4. CrossPoint produces significant accuracy gains in most of the settings in both PointNet and DGCNN feature extractors, proving the capability of generalizing with a limited data even in an out-of-distribution setting. <ref type="table">Table 4</ref>. Few-shot object classification results on ScanOb-jectNN. We report mean and standard error over 10 runs. Top results of each backbone is colored in red and blue. Proposed Cross-Point improves the few-shot accuracy in all the reported settings. (iii) 3D Object part segmentation. We perform object part segmentation in the widely used ShapeNetPart dataset <ref type="bibr" target="#b70">[70]</ref>. It contains 16881 3D objects from 16 categories, annotated with 50 parts in total. We initially pretrain the backbone proposed in DGCNN <ref type="bibr" target="#b59">[59]</ref> for part segmentation using our approach in ShapeNet dataset and fine tune in an endto-end manner in the train split of ShapeNetPart dataset. We report the mean IoU (Intersection-over-Union) metric, calculated by averaging IoUs for each part in an object before averaging the obtained values for each object class in Table. 5. Part segmentation using the backbone pretrained via CrossPoint outperforms the randomly initialised DGCNN backbone by 0.4%. This shows that CrossPoint provides a better weight initialization to the feature extractors. Accuracy gains over the previous self-supervised learning frameworks indicates that CrossPoint, by imposing intra-modal and cross-modal correspondence in a joint manner, tends to capture fine-grained part-level attributes which is crucial in part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations and Analysis</head><p>Impact of joint learning objective. As described in Sec. 3, our approach aims to train the model with a joint learning objective. We hypothesize that, addressing both intra-modal and cross-modal correspondence in a joint manner contribute to a better representation learning than the individual learning objectives. Intra-modal correspondence encourages the model to capture the fine-grained part semantics via imposing invariance to transformations and cross-modal correspondence establishes hard positive feature samples for contrastive learning to make the learning even more challenging, thus yielding better results. We empirically test this hypothesis by training the model in all possible settings and evaluating a linear SVM classifier in both Mod-elNet40 and ScanObjectNN datasets. <ref type="figure">Figure.</ref> 3 graphically <ref type="table">Table 5</ref>. Part segmentation results on ShapeNetPart dataset. We report the mean IoU across all the object classes. Supervised indicates the models trained with randomly initialising feature backbones, while Self-Supervised models are the ones initialised with pretrained feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Method Mean IoU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised</head><p>PointNet <ref type="bibr" target="#b41">[41]</ref> 83.7 PointNet++ <ref type="bibr" target="#b42">[42]</ref> 85.1 DGCNN <ref type="bibr" target="#b59">[59]</ref> 85.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised</head><p>Self-Contrast <ref type="bibr" target="#b10">[11]</ref> 82.3 Jigsaw <ref type="bibr" target="#b50">[50]</ref> 85.3 OcCo <ref type="bibr" target="#b57">[57]</ref> 85.0 PointContrast <ref type="bibr" target="#b63">[63]</ref> 85.1 Liu et al. <ref type="bibr" target="#b31">[31]</ref> 85.3 CrossPoint (Ours) 85.5</p><p>illustrates that in all the learning settings, the proposed joint learning paradigm performs better than the individual objectives. In particular, the combination of intra-modal and cross-modal learning objectives obtains accuracy gains of 1.2% and 0.7% over the second best approach in Model-Net40 and ScanObjectNN respectively with the DGCNN feature extractor. We specifically observe that linear evaluation with crossmodal learning objective marginally outperforms that with intra-modal learning objective in the classification accuracy metrics. We believe that the cross-modal learning objective facilitates a part semantic understanding, by embedding the image feature in the close proximity to the features of both the augmented point clouds by leveraging the point cloud prototype feature. <ref type="figure" target="#fig_4">Figure. 4</ref> visualizes the t-SNE plot of the features obtained from the test split of ModelNet10 dataset. It is visible that both CMID and IMID settings pro- vide a good discrimination to classes even without explicitly trained with labeled data. However, the class boundaries of some classes (e.g., desk, table) are not precise and compact. Joint learning objective is able to create a better discrimination boundaries in such classes. Number of corresponding 2D images. We investigate the contribution of the image branch by varying the number of rendered 2D images (n). We select the rendered 2D image captured from different random directions. In case of more than one rendered 2D image, we compute the mean of all the projected features of rendered images to perform the cross-modal instance discrimination (CMID). <ref type="table">Table.</ref> 6 reports the linear SVM classification results on ModelNet40 dataset. Our approach, even with a single rendered 2D image captures the cross-modal correspondence to yield better linear classification results. It is clear that, when using more than 2 rendered images, the information gathered from 2D image modality might have become redundant, hence the drop in accuracy. <ref type="table">Table 6</ref>. Linear classification results on ModelNet40 with varying number of rendered 2D images (n). CrossPoint with single corresponding image performs better than or equal to multiple rendered images. We choose n=1 for all the experiments.</p><p>No. of rendered 2D images (n) 1 2 3 4 5</p><p>Linear Accuracy 91.2 91.2 90.9 91.0 90.5</p><p>Few-shot image classification on CIFAR-FS. Even though we discard the image feature extractor during point cloud downstream tasks, we perform a simple few-shot image classification to investigate the image understanding capability of it. We use CIFAR-FS <ref type="bibr" target="#b3">[4]</ref>, which is a widely used dataset for few-shot image classification, that contains 100 categories with 64, 16, and 20 train, validation, and test splits. <ref type="table">Table.</ref> 7 reports the results in comparison with the standard baseline, RFS <ref type="bibr" target="#b55">[55]</ref> in 5-way 1-shot and 5-way 5shot settings. It is to be noticed that CrossPoint, without any supervised fine-tuning, fails to generalize well in the few-shot image classification setting. We believe that this is because, there is a considerable discrepancy between the rendered 2D images from point clouds and the images in CIFAR-FS which are real world images. Hence, CrossPoint fails to generalize to such an out-of-distribution data which is a limitation of our work. However, initializing the backbone with the unsupervisedly trained image feature extractor in CrossPoint and finetuning using the method proposed in RFS outperforms the baseline results by significant margins in both the few-shot settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose CrossPoint, a simple selfsupervised learning framework for 3D point cloud representation learning. Even though our approach is trained on synthetic 3D object dataset, experimental results in downstream tasks such as 3D object classification and 3D object part segmentation in both synthetic and real-world datasets demonstrate the efficacy of our approach in learning transferable representations. Our ablations empirically validate our claim that joint learning of imposing intra-modal and cross-modal correspondences leads to more generic and transferable point cloud features. Additional few-shot image classification experiment provides a powerful insight for cross-modal understanding which can be explored in future researches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>s sm o d a l C o r r e s p o n d e n c e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3.3) settings. Finally, we present our overall training objective (Sec. 3.4). The overview of the proposed method is shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of the proposed method (CrossPoint). It comprises of two branches namely: point cloud branch which establishes an intra-modal correspondence by imposing invariance to point cloud augmentations and image branch which simply formulates a cross-modal correspondence by introducing a contrastive loss between the rendered 2D image feature and the point cloud prototype feature. CrossPoint jointly train the model combining the learning objectives of both the branches. We discard the image branch and use only the point cloud feature extractor as the backbone for the downstream tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Impact of the joint learning objective when compared to individual intra-modal and cross-modal objectives. Classification results with Linear SVM on the pretrained embedding on Model-Net40 (left) and ScanObjectNN (right) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization of features on the test split of ModelNet10 dataset after training the DGCNN backbone in a self-supervised manner. The proposed joint learning approach provide better discrimination of classes (e.g., desk, table) when compared to models learnt with individual objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of ScanObjectNN linear classification results with previous self-supervised methods. CrossPoint shows consistent improvements over prior works in both PointNet and DGCNN backbones. This illustrates the effectiveness of our approach in real-world setting.Table. 2 demonstrates the linear evaluation results on ScanObjectNN. Significant accuracy gains of 1.3% in PointNet backbone and 3.4% in DGCNN backbone, over</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone PointNet DGCNN</cell></row><row><cell>Jigsaw [50]</cell><cell>55.2</cell><cell>59.5</cell></row><row><cell>OcCo [57]</cell><cell>69.5</cell><cell>78.3</cell></row><row><cell>STRL [22]</cell><cell>74.2</cell><cell>77.9</cell></row><row><cell>CrossPoint (Ours)</cell><cell>75.6</cell><cell>81.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table is</head><label>is</label><figDesc>Jigsaw [50] 34.3?1.3 42.2?3.5 26.0?2.4 29.9?2.6 DGCNN + cTree [51] 60.0?2.8 65.7?2.6 48.5?1.8 53.0?1.3 DGCNN + OcCo [57] 90.6?2.8 92.5?1.9 82.9?1.3 86.5?2.2 DGCNN + CrossPoint 92.5?3.0 94.9?2.1 83.6?5.3 87.9?4.2</figDesc><table><row><cell cols="2">an extended version from [57]</cell><cell></cell></row><row><cell>Method</cell><cell>5-way 10-shot 20-shot</cell><cell>10-way 10-shot 20-shot</cell></row><row><cell>3D-GAN [61]</cell><cell cols="2">55.8?3.4 65.8?3.1 40.3?2.1 48.4?1.8</cell></row><row><cell>FoldingNet [69]</cell><cell cols="2">33.4?4.1 35.8?5.8 18.6?1.8 15.4?2.2</cell></row><row><cell>Latent-GAN [1]</cell><cell cols="2">41.6?5.3 46.2?6.2 32.9?2.9 25.5?3.2</cell></row><row><cell cols="3">3D-PointCapsNet [75] 42.3?5.5 53.0?5.9 38.0?4.5 27.2?4.7</cell></row><row><cell>PointNet++ [42]</cell><cell cols="2">38.5?4.4 42.4?4.5 23.1?2.2 18.8?1.7</cell></row><row><cell>PointCNN [30]</cell><cell cols="2">65.4?2.8 68.6?2.2 46.6?1.5 50.0?2.3</cell></row><row><cell>RSCNN [32]</cell><cell cols="2">65.4?8.9 68.6?7.0 46.6?4.8 50.0?7.2</cell></row><row><cell>PointNet + Rand</cell><cell cols="2">52.0?3.8 57.8?4.9 46.6?4.3 35.2?4.8</cell></row><row><cell cols="3">PointNet + Jigsaw [50] 66.5?2.5 69.2?2.4 56.9?2.5 66.5?1.4</cell></row><row><cell cols="3">PointNet + cTree [51] 63.2?3.4 68.9?3.0 49.2?1.9 50.1?1.6</cell></row><row><cell cols="3">PointNet + OcCo [57] 89.7?1.9 92.4?1.6 83.9?1.8 89.7?1.5</cell></row><row><cell cols="3">PointNet + CrossPoint 90.9?4.8 93.5?4.4 84.6?4.7 90.2?2.2</cell></row><row><cell>DGCNN + Rand</cell><cell cols="2">31.6?2.8 40.8?4.6 19.9?2.1 16.9?1.5</cell></row><row><cell>DGCNN +</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table is</head><label>is</label><figDesc></figDesc><table><row><cell cols="2">an extended version from [57]</cell><cell></cell></row><row><cell>Method</cell><cell>5-way 10-shot 20-shot</cell><cell>10-way 10-shot 20-shot</cell></row><row><cell>PointNet + Rand</cell><cell cols="2">57.6?2.5 61.4?2.4 41.3?1.3 43.8?1.9</cell></row><row><cell cols="3">PointNet + Jigsaw [50] 58.6?1.9 67.6?2.1 53.6?1.7 48.1?1.9</cell></row><row><cell cols="3">PointNet + cTree [51] 59.6?2.3 61.4?1.4 53.0?1.9 50.9?2.1</cell></row><row><cell cols="3">PointNet + OcCo [57] 70.4?3.3 72.2?3.0 54.8?1.3 61.8?1.2</cell></row><row><cell cols="3">PointNet + CrossPoint 68.2?1.8 73.3?2.9 58.7?1.8 64.6?1.2</cell></row><row><cell>DGCNN + Rand</cell><cell cols="2">62.0?5.6 67.8?5.1 37.8?4.3 41.8?2.4</cell></row><row><cell cols="3">DGCNN + Jigsaw [50] 65.2?3.8 72.2?2.7 45.6?3.1 48.2?2.8</cell></row><row><cell cols="3">DGCNN + cTree [51] 68.4?3.4 71.6?2.9 42.4?2.7 43.0?3.0</cell></row><row><cell cols="3">DGCNN + OcCo [57] 72.4?1.4 77.2?1.4 57.0?1.3 61.6?1.2</cell></row><row><cell cols="3">DGCNN + CrossPoint 74.8?1.5 79.0?1.2 62.9?1.7 73.9?2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Few-shot image classification results on CIFAR-FS. Fine-tuning CrossPoint with RFS improves the performance. 24.12?0.48 28.18?0.54 RFS [55] ResNet-50 60.20?0.87 76.79?0.71 CrossPoint + RFS ResNet-50 64.45?0.86 80.14?0.65</figDesc><table><row><cell>Method</cell><cell>Backbone 5-way 1-shot 5-way 5-shot</cell></row><row><cell>CrossPoint</cell><cell>ResNet-50</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Salman Khan (MBZUAI, UAE) and Sadeep Jayasumana (Google Research, NY) for their valuable comments and suggestions on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich semantics improve few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Afham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PointMixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Picture perception in infancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><forename type="middle">S</forename><surname>Deloache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Maynard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Infant Behavior and Development</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="77" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-contrastive learning with hard negative sampling for self-supervised point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bi&amp;apos;an Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised learning on 3D point clouds by learning discrete generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8248" to="8257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3D point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">View inter-prediction gan: Unsupervised representation learning for 3D shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-angle point cloud-vae: Unsupervised feature learning for 3D point clouds from multiple angles by joint self-reconstruction and half-to-half prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning by context and motion decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13886" to="13895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-temporal self-supervised representation learning for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6535" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arad</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21798" to="21809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mdetr -modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
	<note>Ishan Misra, and Nicolas Carion</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Point cloud augmentation with weighted local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyeok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointaugment: An auto-augmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Point discriminative learning for unsupervised representation learning on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02104</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yueh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Tang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04687</idno>
		<title level="m">Learning from 2D: Contrastive pixel-topoint knowledge transfer for 3D pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An end-toend transformer model for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2906" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Audiovisual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="12475" to="12486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised learning of point clouds via orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quintessa</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3D object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring complementary strengths of invariant and equivariant representations for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Mamshad Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10836" to="10846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Infants&apos; transfer of response between twodimensional and three-dimensional stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1086" to="1091" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Info3D: Representation learning on 3D objects using mutual information maximization and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sanghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-supervised few-shot learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Point cloud pre-training by mixing and disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised point cloud pre-training via occlusion completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-supervised temporal discriminative learning for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02129</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Interpretable visual reasoning via induced symbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11603</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">PointContrast: Unsupervised pretraining for 3D point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Leonidas Guibas, and Or Litany</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro O O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04180</idno>
		<title level="m">Image2Point: 3D point-cloud understanding with pretrained 2d convnets</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">DISN: Deep implicit surface network for high-quality single-view 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Progressive seed generation auto-encoder for unsupervised point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyoung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyunghwan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6413" to="6422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for point cloud understanding by contrasting and clustering using graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining of 3D features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">H3DNet: 3D object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">3D point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Improving contrastive learning by visualizing feature transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
