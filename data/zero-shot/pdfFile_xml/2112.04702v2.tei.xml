<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Point Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH GSAI &amp; CSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonwoo</forename><surname>Jeong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH GSAI &amp; CSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH GSAI &amp; CSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH GSAI &amp; CSE</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Point Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) 3D object detection (b) 3D semantic segmentation Figure 1. Fast Point Transformer can process large-scale scenes using a local self-attention mechanism. Unlike Point Transformer [50], our approach can infer the scene at one shot without searching for point-wise neighbors. The average inference time of our network is 0.14 seconds per scene, resulting in 129 times faster than Point Transformer in 3D semantic segmentation on S3DIS dataset [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The recent success of neural networks enables a better interpretation of 3D point clouds, but processing a large-scale 3D scene remains a challenging problem. Most current approaches divide a large-scale scene into small regions and combine the local predictions together. However, this scheme inevitably involves additional stages for pre-and post-processing and may also degrade the final output due to predictions in a local perspective. This paper introduces Fast Point Transformer that consists of a new lightweight self-attention layer. Our approach encodes continuous 3D coordinates, and the voxel hashing-based architecture boosts computational efficiency. The proposed method is demonstrated with 3D semantic segmentation and 3D detection. The accuracy of our approach is competitive to the best voxel-based method, and our network achieves 129 times faster inference time than the state-of-the-art, Point Transformer, with a reasonable accuracy trade-off in 3D semantic segmentation on S3DIS dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D scene understanding is a fundamental task due to its importance to various fields, such as robotics, intelligent agents, and AR/VR. Recent approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> utilize the deep learning frameworks, but processing a large-scale 3D scene as a whole remains a challenging problem because it involves extensive computation and memory budgets. As an alternative, some methods crop 3D scenes and stitch predictions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">42]</ref>, or others approximate point coordinates for efficiency <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">51]</ref>. Such techniques, however, typically lead to a substantial increase of inference time and/or degrade the final output due to the local or approximate predictions. Achieving both fast inference time and high accuracy is thus one of the primary challenges in the 3D scene understanding tasks.</p><p>The pioneering 3D understanding approaches, Point-Net <ref type="bibr" target="#b26">[27]</ref> and PointNet++ <ref type="bibr" target="#b27">[28]</ref> process point clouds with multi-layer perceptrons (MLPs), which preserve permutation-invariance of the point clouds. Such pointbased methods introduce impressive results <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref> recently, and Point Transformer <ref type="bibr" target="#b50">[50]</ref> shows superior accuracy based on the local self-attention mechanism. However, it involves manual grouping of point clouds using k nearest neighbor search. Furthermore, scene-level inference with the pointbased methods typically requires dividing a large-scale scene into smaller regions and stitching the predictions on them. While Voxel-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">51]</ref> are alternatives for a large-scale 3D scene understanding due to their effectiveness of the network design, they may lose fine geometric patterns due to quantization artifacts. Hybrid methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> reduce the quantization artifacts by utilizing both point-level and voxel-level features. However, approaches in this category require additional memory space to cache both features.</p><p>We propose Fast Point Transformer, which effectively encodes continuous positional information of large-scale point clouds. Our approach leverages local self-attention <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> of point clouds with voxel hashing architecture. To achieve higher accuracy, we present centroid-aware voxelization and devoxelization techniques that preserve the embedding of continuous coordinates. The proposed approach reduces quantization artifacts and allows the coherency of dense predictions regardless of rigid transformations. We also introduce a reformulation of the standard local self-attention equation to reduce space complexity further. The proposed local self-attention module can replace the convolutional layers for 3D scene understanding. Based on this, we introduce a local self-attention based U-shaped network, which naturally builds a feature hierarchy without manual grouping of point clouds. As the result, Fast Point Transformer collects rich geometric representations and exhibits a fast inference time even for large-scale scenes.</p><p>We conduct experiments using two datasets of large-scale scenes: S3DIS <ref type="bibr" target="#b1">[2]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref>. Our method shows competitive accuracy in the semantic segmentation task on various voxel hashing configurations. We also apply the Fast Point Transformer network as a backbone of VoteNet <ref type="bibr" target="#b25">[26]</ref> to show the applicability in the 3D object detection task. We use ScanNet <ref type="bibr" target="#b6">[7]</ref> dataset for the 3D detection, and our model shows better accuracy (mAP) than other baselines that use point-or voxel-based network backbones. Besides, we introduce a novel consistency score metric, named CScore, and demonstrate that our model outputs more coherent predictions under rigid transformations.</p><p>In summary, our contributions are as follows:</p><p>1. We propose a novel local self-attention-based network, called Fast Point Transformer that can handle largescale 3D scenes quickly.</p><p>2. We introduce a lightweight local self-attention module that effectively learns continuous positional information of 3D point clouds while reducing space complexity.</p><p>3. We show that our model produces significantly more coherent predictions than the previous voxel-based approaches using the proposed evaluation metric. <ref type="bibr" target="#b3">4</ref>. We demonstrate fast inference of our voxel-hashingbased architecture; our network performs a 129 times faster inference than Point Transformer does, obtaining a reasonable accuracy trade-off in 3D semantic segmentation on S3DIS dataset <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review point-based, voxel-based, and hybrid methods for 3D scene understanding and then revisit the attention-based models. Point-based methods. PointNet <ref type="bibr" target="#b26">[27]</ref> introduces a multilayer perceptrons (MLP) based approach for understanding 3D scenes. PointNet++ <ref type="bibr" target="#b27">[28]</ref> advances the PointNet <ref type="bibr" target="#b26">[27]</ref> by adding hierarchical sampling strategies. Recent studies attempt to apply convolution on point clouds since the heuristic local sampling and grouping mechanisms used in Point-Net++ <ref type="bibr" target="#b27">[28]</ref> can be represented by the convolution. However, applying convolution on point clouds is challenging since 3D points are sparse and unordered. KPConv <ref type="bibr" target="#b37">[38]</ref> mimics convolution using kernel points defined in the continuous space. They construct a k-d tree to perform point-wise convolution on the query points within a certain radius at the inference stage in exchange for inefficiency at the data preprocessing stage. Mao et al. <ref type="bibr" target="#b22">[23]</ref> adopt discretized convolution kernels instead of continuous kernels for efficiency and perform convolution on every point in a point cloud, which poses a bottleneck when processing large-scale 3D scene point clouds. More recently, Guo et al. <ref type="bibr" target="#b10">[11]</ref> and Zhao et al. <ref type="bibr" target="#b50">[50]</ref> utilize local self-attention operations to learn richer feature representations than the fixed kernel-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. In fact, most point-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">50]</ref> adopt expensive operations, such as k nearest neighbor search or k-d tree construction, resulting in heavy computational overhead when processing large-scale 3D scenes. Voxel-based methods. Sparse convolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> constructs fully convolutional neural networks using discrete sparse tensors for fast processing of voxel data. The sparse convolution performs convolution on all valid neighbor voxels that are efficiently found using a hash table with constant time complexity, i.e., O(1). Mao et al. <ref type="bibr" target="#b23">[24]</ref> propose a voxelbased transformer architecture that adopts both local and dilated attention to enlarge receptive fields of the model. Despite the effectiveness of voxel-based work on large-scale point clouds, they often fail to capture fine patterns of point clouds due to the quantization artifacts produced during voxelization. In other words, the features extracted by voxelbased methods are inconsistent with respect to the voxel size <ref type="bibr" target="#b47">[47]</ref>. Hybrid methods. Another approach to handle point clouds is to extract both point-and voxel-level features. Recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref> attaches point-based layers, e.g., mini-PointNet, on top of the voxel-based methods to relieve the quantization artifacts produced during voxelization. They take advantage of fast neighbor search of voxel-based methods and high capability of capturing fine-geometries of pointbased methods. However, the hybrid methods suffer from larger computation and memory budgets since these approaches store both point-and voxel-level features. Attention-based networks. Discussions regarding the at- <ref type="figure">Figure 2</ref>. Overall architecture. We illustrate the overall architecture of the proposed Fast Point Transformer. The red points are input points and their features, and the purple points are output points and their features. The colored squares are non-empty voxels produced by voxelization. The blue and green points are centroids of non-empty voxels with their features. tention operation have dominated research in recent years in natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Moreover, recent vision work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">44]</ref> has attempted to exploit the advantages of attention-based models. Prior research generally confirms that global self-attention is infeasible to be adopted in 3D vision tasks due to its costly operations. Thus, recent work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">50]</ref> widely utilizes local selfattention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> to process 3D point clouds. Guo et al. <ref type="bibr" target="#b10">[11]</ref> and Zhao et al. <ref type="bibr" target="#b50">[50]</ref> handle irregularity of point clouds with k nearest neighbor search, resulting in a remarkable performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centroid-aware voxelization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centroid-aware devoxelization Lightweight self-attention block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fast Point Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Fast Point Transformer processes the point cloud through three steps: (Step 1) Centroid-aware voxelization, (Step 2) Lightweight self-attention, and (Step 3) Centroid-aware devoxelization. <ref type="figure">Figure 2</ref> shows the overall architecture.</p><p>(Step 1) Let P in = {(p n , i n )} N n=1 be an input point cloud, where p n is the n-th point coordinate and i n is any raw input feature of p n , e.g., color of point. For the computational efficiency, our approach voxelizes</p><formula xml:id="formula_0">P in into V = {(v i , f i , c i )} I i=1</formula><p>, a set of tuples. Each tuple contains i-th voxel coordinate v i , voxel feature f i , and voxel centroid coordinate c i . We introduce a centroid-aware voxelization process that utilizes learnable positional embedding e n between n-th point and its voxel centroid to minimize the loss from the quantization procedure.</p><p>(Step 2) The lightweight self-attention (LSA) block takes</p><formula xml:id="formula_1">V = {(v i , f i , c i )} I i=1</formula><p>and updates the feature f i to the output feature f ? i using local self-attention. In this procedure, querying neighbor voxels can be done with voxel hashing having O(1) complexity for a single query.</p><p>(</p><formula xml:id="formula_2">Step 3) The output voxels V ? = {(v i , f ? i , c i )} I i=1</formula><p>from the attention block are devoxelized into the output point cloud P out = {(p n , o n )} N n=1 , where o n is the output point feature. We propose to use learnable positional embedding e n to properly assign voxel-wise features to the continuous 3D points for accurate point-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Centroid-aware Voxel &amp; Devoxelization</head><p>Centroid-aware voxelization. Let us consider an input point cloud P in = {(p n , i n )}. We voxelize input points for fast and scalable querying. The output voxels are denoted by</p><formula xml:id="formula_3">V = {(v i , f i , c i )}.</formula><p>We introduce a novel centroid-to-point positional encoding e n ? R Denc to mitigate the geometric information loss during voxelization. With an encoding layer ? enc : R 3 ? R Denc , the centroid-to-point positional encoding e n is defined as follows:</p><formula xml:id="formula_4">e n = ? enc (p n ? c i=?(n) ),<label>(1)</label></formula><p>where centroid c i is c i = 1</p><formula xml:id="formula_5">|M(i)| n?M(i) p n , M(i)</formula><p>is a set of point indices within the i-th voxel, and ? : N ? N is an index mapping from a point index n to its corresponding voxel index i. We define the voxel feature f i ? R Din+Denc with the input point feature i n ? R Din and the encoding e n :</p><formula xml:id="formula_6">f i = ? n?M(i) (i n ? e n ),<label>(2)</label></formula><p>where ? denotes vector concatenation and ? is a permutation-invariant operator, e.g., average(?). We state that some voxel-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">46]</ref> introduce barycentric interpolation to embed f i into regular grids v i for voxelization. The proposed centroid-aware voxelization is different from those methods in that it encodes the centroid-to-point position into f i at continuous centroid coordinate c i . The proposed centroid-aware voxeliztion is also different from other class of voxel-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> that apply average-or max-pool voxel features without using intra-voxel coordinates of points. Centroid-aware devoxelization. Since the centroid-topoint positional encoding e n has useful information about the relative position between p n and c i , we can propose a centroid-aware devoxelization process. Given an output voxels</p><formula xml:id="formula_7">V ? = {(v i , f ? i , c i )} with the output voxel feature f ? i ? R Dout</formula><p>, the proposed centroid-aware devoxelization process is formulated as follows:</p><formula xml:id="formula_8">o n = MLP(f ? i=?(n) ? e n ),<label>(3)</label></formula><p>where o n ? R Dout is the n-th output point feature of the output point cloud P out = {(p n , o n )} and MLP(?) : R Dout+Denc ? R Dout denotes a multilayer perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Lightweight Self-Attention</head><p>Local self-attention on centroids. Once an input point cloud</p><formula xml:id="formula_9">P in = {(p n , i n )} N n=1 is transformed into a set of vox- els V = {(v i , f i , c i )} I i=1 ,</formula><p>we can apply local self-attention mechanism <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52]</ref> with V. In this procedure, we can query neighboring voxels quickly via voxel-hashing, which requires O(N ) complexity. Note that point-based methods <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b50">50]</ref> need to build neighbors using k nearest neighbor search having the complexity of O(N log N ), which become burdensome for processing large-scale point clouds. Given local neighbor indices of c i denoted by N (i), local self-attention on c i can be formulated as follows:</p><formula xml:id="formula_10">f ? i = j?N (i) a(f i , ?(c i , c j ))?(f j ), (4) where f ? i is output feature, a(f i , ?(c i , c j )</formula><p>) is a function of attention weights using positional encoding ?(c i , c j ) ? R D and ? is the value projection layer.</p><p>Although the voxel hashing enables a fast neighbor search with time complexity of O(1) for a single query, designing a memory-efficient form of continuous positional encoding ?(c i , c j ) still remains a challenging problem. Specifically, inspired by MLP(p i ? p j ) in Point Transformer <ref type="bibr" target="#b50">[50]</ref>, implementing ?(c i , c j ) as MLP(c i ? c j ) requires O(IKD) space complexity, where K is the cardinality of neighboring voxels. This is because there can be O(IK) different relative positions of (c i ? c j ) for possible (i, j) pairs due to the continuity of c as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Reducing space complexity. We introduce a coordinate decomposition approach to reduce space complexity. Given a query voxel (v i , f i , c i ) and a key voxel (v j , f j , c j ), the relative position of centroids c i ? c j can be decomposed as</p><formula xml:id="formula_11">c i ? c j = (c i ? v i ) ? (c j ? v j ) + (v i ? v j ). (5)</formula><p>With Eq. (5), we can decompose the memory-consuming ?(c i , c j ) into two kinds of positional encodings: (1) a continuous positional encoding ? abs (c i ? v i ) whose space complexity is O(ID) due to continuity of c, and (2) a discretized positional encoding</p><formula xml:id="formula_12">? rel (v i ? v j ) whose space complexity is O(KD). ? rel (v i ? v j ) is memory-efficient because there can be only K different discretized relative positions of (v i ? v j ) ? R 3 for all possible (i, j)</formula><p>pairs. In addition, it is due to the fact that the K is significantly smaller than number of voxels I. ? abs (c j ? v j ) in Eq.  Given, Eq. <ref type="formula">(4)</ref> and <ref type="formula">(5)</ref>, we see that local self-attention uses continuous positional encoding ? abs (c i ? v i ) and input voxel feature f i . Therefore, the local self-attention pipeline has a centroid-aware property that can reduce quantization artifacts. Based on these insights, we propose to use an aggregated feature</p><formula xml:id="formula_13">g i = f i + ? abs (c i ? v i ) and name it as centroid-aware voxel feature. We compute attention weights with ? rel (v i ? v j ) as f ? i = j?N (i) a(g i , ? rel (v i ? v j ))?(g j ).<label>(6)</label></formula><p>We illustrate the reduction of the space complexity in <ref type="figure" target="#fig_1">Figure 3</ref>, and evaluate the effectiveness of the decomposition in <ref type="table" target="#tab_3">Table A4</ref> and <ref type="table" target="#tab_4">Table A5</ref> of the supplementary material. Lightweight self-attention layer. Now, we propose the new local self-attention layer, named LSA layer, by defining attention function a(?) in Eq. <ref type="formula" target="#formula_13">(6)</ref> as</p><formula xml:id="formula_14">f ? i = j?N (i) ?(g i ) ? ? rel (v i ? v j ) ??(g i )??? rel (v i ? v j )? ?(g j ).<label>(7)</label></formula><p>It is worth noting that the LSA layer uses the cosine similarity between ?(g i ) and</p><formula xml:id="formula_15">? rel (v i ? v j ). Instead of us- ing softmax(?(g i ) ? ? rel (v i ? v j ))</formula><p>, cosine similarity can effectively handle the sparsity issue of input voxels V properly. For example, an issue arises if we use softmax(?) and |N (i)| is 1. In this case, softmax(?) normalizes the attention weights into 1.0, and it can make the LSA layer to be a simple linear layer ?. In addition, as the LSA layer queries local neighbor indices, |N (i)| varies from 1 to the number of neighboring voxels. Therefore, cosine similarity is more natural choice for handling varying number of voxels than softmax(?) as shown in <ref type="table">Table 6</ref>.</p><p>The dynamics of the LSA layer (Eq. <ref type="formula" target="#formula_14">(7)</ref>) generates weights using the centroid-aware features ?(g i ) and relative voxel features ? rel (v i ? v j ). This design enables LSA layer to learn more coherent representation under the rigid transformations than sparse convolution based approach <ref type="bibr" target="#b5">[6]</ref>, as shown in <ref type="table">Table 1</ref> and to outperform sparse convolution on various tasks (e.g., 3D semantic segmentaion, 3D object detection) as shown in <ref type="table">Table 2</ref>, <ref type="table">Table 3, and Table 8</ref>. We also experimentally show that the reformulation from Eq. (4) to Eq. (6) works reasonably (as shown in <ref type="table" target="#tab_4">Table 5</ref> and <ref type="table">Table 6</ref>) and introduces extra efficiency (as shown in <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>We develop Fast Point Transformer for dense prediction on point cloud based on the modules introduced above. Using coordinate hashing (Sec. 3.2) and decomposed positional encodings (Sec. 3.3), Fast Point Transformer is less prone to quantization errors than previous voxel-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>, while also being significantly faster than point-based methods <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b50">50]</ref> in terms of both space and time. Furthermore, the proposed local self-attention layer can be easily be integrated to voxel-based downsampling and upsampling layer without introducing heuristic sampling and grouping mechanisms that are often used in the point-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b50">50]</ref>. Note that we can build local selfattention networks by substituting convolution layers with LSA layers. Therefore, any sparse CNN architecture can be modified to faciliate local self-attention, e.g., ResNet <ref type="bibr" target="#b13">[14]</ref> and U-Net <ref type="bibr" target="#b30">[31]</ref>. We implement our model for semantic segmentation using the U-Net <ref type="bibr" target="#b30">[31]</ref> architecture. Further details are described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our model on two popular large-scale 3D scene datasets: S3DIS <ref type="bibr" target="#b1">[2]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref>. We have selected the two datasets due to their rich diversity and densely annotated labels. We first validate the robustness of our approach to voxel hashing configurations described in Sec. 4.3. Then, we compare the proposed method with the state of the art and discuss the results in Sec. 4.4 and Sec. 4.5. Specifically, we provide stochastic numbers averaged from three different experiments with the same training configuration except random seed numbers for the comparison tables: <ref type="table">Table 1</ref>, <ref type="table">Table 2</ref>, <ref type="table">Table 3</ref>, <ref type="table" target="#tab_3">Table 4, and Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>S3DIS is a large-scale indoor dataset which consists of six large-scale areas with 271 room scenes. We test on Area 5 and utilize the other splits during training. Following <ref type="bibr" target="#b5">[6]</ref>, we do not use any preprocessing methods, e.g., cropping into small blocks, that are widely used in point-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">42]</ref>. ScanNet. We use the second official release of ScanNet <ref type="bibr" target="#b6">[7]</ref>, which consists of 1.5k room scenes with some rooms captured repeatedly with different sensors. Following the experimental settings of prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, our model uses point-wise RGB colors as input point features {i n } both for 3D semantic segmentation task and 3D objection detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We have selected PointNet <ref type="bibr" target="#b26">[27]</ref>, PointWeb <ref type="bibr" target="#b49">[49]</ref>, SP-Graph <ref type="bibr" target="#b17">[18]</ref>, PointConv <ref type="bibr" target="#b41">[41]</ref>, PointASNL <ref type="bibr" target="#b43">[43]</ref>, KP-Conv <ref type="bibr" target="#b37">[38]</ref>, PAConv <ref type="bibr" target="#b42">[42]</ref>, Point Transformer <ref type="bibr" target="#b50">[50]</ref>, SparseCon-vNet <ref type="bibr" target="#b9">[10]</ref>, and MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> as the baseline approaches. MinkowskiNet32 and MinkowskiNet42 <ref type="bibr" target="#b5">[6]</ref> are compared as representative voxel-based methods that comprise 32 and 42 U-Net layers, respectively. We reproduce Minkowsk-iNet42 <ref type="bibr" target="#b5">[6]</ref> with the official source code and denote it as MinkowskiNet42 ? , with different voxel sizes. PointNet <ref type="bibr" target="#b26">[27]</ref>, SPGraph <ref type="bibr" target="#b17">[18]</ref>, PointWeb <ref type="bibr" target="#b49">[49]</ref>, KPConv <ref type="bibr" target="#b37">[38]</ref>, PAConv <ref type="bibr" target="#b42">[42]</ref> and Point Transformer <ref type="bibr" target="#b50">[50]</ref> are selected since they are representative point-based methods. The main difference between KPConv <ref type="bibr" target="#b37">[38]</ref> and the others is that KPConv <ref type="bibr" target="#b37">[38]</ref> uses a k-d tree to boost its inference time while the others do not. We follow the official guideline of the methods and reproduce the results. A more recent method, Point Transformer <ref type="bibr" target="#b50">[50]</ref> has also been selected due to its superiority on several datasets. Unlike our method and selected baselines, other approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> use additional inputs, e.g., 2D images or meshes. Accordingly, we have excluded these methods from the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Consistency Test</head><p>We introduce a new evaluation metric to measure the coherency of predictions under various rigid transformations, such as translation and rotation. Let us consider a set of point clouds S = {P in } and a 3D semantic segmentation model f : P in ? C which predicts a semantic class of each point in P in = {(p n , i n )}. Given S and a set of rigid transformations T = {T m }, we introduce the consistency score (CScore(f ; S, T )) as follows:</p><formula xml:id="formula_16">1 |S| P in ?S 1 |P in ||T | |P in | n |T | m I f (p n , i n ), f (T m p n , i n ) ,<label>(8)</label></formula><p>where I(?) is the indicator function, and it checks whether class predictions of the original point and the transformed point are the same. CScore is an averaged accuracy over S, P, and T . Similarly, we use the point-wise CScore of f on P to show which points in P are vulnerable to T . We apply 41 different rigid transformations that consist of 26 translations and 15 rotations around the gravity axis. For the voxel size L, 26 translations are set to [0, L/3, 2L/3] 3 except zero translation [0, 0, 0]. Fifteen rotation angles along gravity axis is set to [0.125?, 0.25?, ? ? ? , 1.875?]. We evaluate CScore of MinkowskiNet42 and Fast Point Transformer on the Scan-Net validation split. The evaluation results <ref type="table">(Table 1)</ref> and <ref type="table">Table 1</ref>. Comparison of consistency score (CScore) and mIoU. We compare the consistency scores of Fast Point Transformer and MinkowskiNet42 ? , which is the reproduced model, on different transformation sets. The transformation sets are 1) rotation only (R), 2) translation only (t), and 3) both (R and t). The size of voxel is set to 10cm, 5cm, and 2cm for 3D semantic segmentation on the ScanNet validation dataset <ref type="bibr" target="#b6">[7]</ref>. Fast Point Transformer reduces the prediction inconsistency that occurred by voxelization artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CScore <ref type="formula">(</ref>  We visualize consistency scores of MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> and the proposed Fast Point Transformer with the hot heatmap. Points with high CScore (consistently predicted with the same class) are colored black, and points with low CScore (the predicted class is not consistent with arbitrary rigid transformations) are colored white. <ref type="table">Table 1</ref> shows the quantitative evaluation.</p><p>the qualitative results ( <ref type="figure" target="#fig_2">Figure 4)</ref> show that Fast Point Transformer outputs more coherent feature representations than MinkowskiNet42 <ref type="bibr" target="#b5">[6]</ref>. Moreover, the coherent predictions indicate that the Fast Point Transformer successfully relieves quantization artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">3D Semantic Segmentation</head><p>We compare our approach with the state of the art in 3D semantic segmentation on S3DIS <ref type="bibr" target="#b1">[2]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref>. We use the mean of class-wise IoU scores as the primary evaluation metric for both datasets. S3DIS. We compare the computational complexity, the mean accuracy, and the mean IoU of Fast Point Transformer with the state of the arts on the S3DIS Area 5 test split. Since Choy et al. <ref type="bibr" target="#b5">[6]</ref> reported results with a lightweight network (MinkowskiNet32), we utilize the official code of MinkowskiNet42 and reproduce the results denoted by MinkowskiNet42 ? with voxel size 4cm. We also provide the performance of MinkowskiNet42 ? and Fast Point Trans-former with voxel size 5cm in the supplementary material. <ref type="table">Table 2</ref> theoretically analyzes the time complexity and reports the average wall-time latency of each method when processing S3DIS Area 5 scenes. We measure the inference time of MinkowskiNet42 ? , PointNet <ref type="bibr" target="#b26">[27]</ref>, SPGraph <ref type="bibr" target="#b17">[18]</ref>, PointWeb <ref type="bibr" target="#b49">[49]</ref>, KPConv <ref type="bibr" target="#b37">[38]</ref>, PAConv <ref type="bibr" target="#b42">[42]</ref>, and Point Transformer <ref type="bibr" target="#b50">[50]</ref> using the official codes. We use the same machine with Intel(R) Core(TM) i7-5930K CPU and a single NVIDIA Geforce RTX 3090 GPU to measure the latency of methods. Detailed information about the time complexity analysis is included in the supplementary material.</p><p>Due to the preprocessing stage and stitching the multiple local predictions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49]</ref> or multiple inferences <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">50]</ref>, the point-based methods take much more time to inference a single scene than our approach. Note that KPConv <ref type="bibr" target="#b37">[38]</ref> constructs k-d tree, but we do not include this process into inference time. Our Fast Point Transformer processes a large-scale scene at least 83 times faster than point-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50]</ref> as shown in <ref type="table">Table 2</ref>. Specifically, PointNet <ref type="bibr" target="#b26">[27]</ref> takes 18.16 seconds for processing a scene on average because it crops the scene into 1m?1m?1m blocks, predicts on the blocks, and stitches the predictions for the scene-level prediction (denoted by 'Cropand-stitch' in <ref type="table">Table 2</ref>). Moreover, Fast Point Transformer outperforms MinkowskiNet42 ? by 1.4 absolute percentage score in mean IoU (%) with a comparable speed. Given the reported results by Zhao et al. <ref type="bibr" target="#b50">[50]</ref>, Point Transformer shows the best accuracy. However, Point Transformer <ref type="bibr" target="#b50">[50]</ref> shows 129 times slower inference speed than our approach. This is because it grid-subsamples points and inferences the sampled points multiple times with the expensive k nearest neighbor search to cover the whole scene (denoted by 'Multi-shot' in <ref type="table">Table 2</ref>), while our approach can handle the whole scene with a single feed-forward operation (denoted by 'Single-shot' in <ref type="table">Table 2</ref>). ScanNet. We evaluate the models on the ScanNet validation split due to strict submission policies of ScanNet online test benchmark, where one method can be tested at most once. Our proposed method outperforms MinkowskiNet42 ? at voxel sizes of 2cm, 5cm, and 10cm by 0.2, 3.3, and 5.4 absolute percentage point gain in mean IoU (%) respectively. The experimental results in <ref type="table">Table 1</ref> and <ref type="table">Table 3</ref> indicate that the proposed method can represent a large-scale point cloud as features that are more robust to quantization error. mIoU vs. model size. We compare the accuracy of both Fast Point Transformer and MinkowskiNet with the different number of parameters. We build small network models by reducing the number of building blocks as MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> does and maintaining the number of channels. Detailed illustration about network architecture is shown in the supplementary material. <ref type="table" target="#tab_3">Table 4</ref> shows the evaluation results.</p><p>Interestingly, we observe that Fast Point Transformer is more resilient to the network parameter reduction, and <ref type="table">Table 2</ref>. 3D semantic segmentation on S3DIS [2] Area 5 test. We mark the reproduced models using the official source codes with ? . We analyze the theoretical time complexity of neighbor search algorithms and evaluate the per-scene wall-time latency of each network. We denote N as the number of dataset points, M as the number of query points (or voxel centroids), and K as the number of neighbors to search. Both M and N are much larger than K in a large-scale point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Neighbor  <ref type="table">Table 3</ref>. 3D semantic segmentation on ScanNet <ref type="bibr" target="#b6">[7]</ref> validation. We make the reproduced models using the official codes with ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU (%)</head><p>PointNet <ref type="bibr" target="#b26">[27]</ref> 53.5 PointConv <ref type="bibr" target="#b41">[41]</ref> 61.0 PointASNL <ref type="bibr" target="#b43">[43]</ref> 63.5 KPConv deform <ref type="bibr" target="#b37">[38]</ref> 69.2 Fast Point Transformer models outperform their counterpart models of MinkowskiNet. We can observe that the most lightweight Fast Point Transformer with voxel size 10cm outperforms the most lightweight MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> with voxel size 5cm. MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> requires lots of parameters to overcome voxelization artifacts, whereas Fast Point Transformer shows a consistent accuracy even with 71.5% fewer network parameters. These results imply that the proposed lightweight selfattention (LSA) layer can learn a 3D geometry more effectively than an over-parameterized sparse convolutional layer thanks to its dynamic kernel weights. Ablation study. We conduct ablation studies on (1) the proposed positional encodings, (2) attention types, and (3) the local window size. We have followed the same setup with the main experiments with a voxel size of 10cm using a fixed random seed on ScanNet <ref type="bibr" target="#b6">[7]</ref> validation dataset. <ref type="table" target="#tab_4">Table 5</ref> shows ablation results on the proposed positional encodings, i.e., ? enc and ? abs . Models with full positional encodings achieved the best mIoU score. When removing ? abs   <ref type="table">Table 6</ref>. Ablation study on attention types. Note that ?(gi) and ?(gj) denote a query and its neighboring key, respectively. We use ScanNet validation dataset <ref type="bibr" target="#b6">[7]</ref> with voxel size 10cm.</p><formula xml:id="formula_17">a(?) in Eq. (6) mIoU (%) softmax(?(g i ), ? rel (v i ? v j )) 61.0 cosine(?(g i ), ?(g j ) + ? rel (v i ? v j )) 62.1 cosine(?(g i ), ? rel (v i ? v j ))</formula><p>65.3 <ref type="table">Table 7</ref>. Ablation study on the local window size. Note that k is the local window size used to find the neighbors, N (i), in Eq. <ref type="bibr" target="#b6">(7)</ref>. We use ScanNet validation dataset <ref type="bibr" target="#b6">[7]</ref> with voxel size 10cm. means that the self-attention mechanism is a more proper way to use ? abs than sparse convolution. <ref type="table">Table 6</ref> shows the effects of attention types used in the proposed LSA layer. cosine(?) handles the varying number of neighbors more effectively than softmax(?) as shown in <ref type="table">Table 6</ref>. However, as reported in local self-attention literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>, additional usage of the similarity between query ?(g i ) and key ?(g j ) does not enhance the LSA layer.</p><p>In <ref type="table">Table 7</ref>, we show the effect of the local window size in the proposed LSA layer. Since we currently use learnable tokens for ? rel (v i ? v j ), increasing the local window size degrades the performance due to the sparsity of 3D data. Introducing an inductive bias, such as concatenating the positional encodings <ref type="bibr" target="#b29">[30]</ref> or a shared mapping layer <ref type="bibr" target="#b50">[50]</ref> can be one of the possible solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">3D Object Detection</head><p>We have conducted experiments on the ScanNet 3D object detection dataset, where a fine-grained point cloud representation is essential to detect and localize 3D objects. Setups. For a fair comparison of Fast Point Transformer <ref type="table">Table 8</ref>. 3D object detection on ScanNet <ref type="bibr" target="#b6">[7]</ref> validation. We report two mAP scores of VoteNet <ref type="bibr" target="#b25">[26]</ref> with different backbones on ScanNet <ref type="bibr" target="#b6">[7]</ref> dataset. Numbers except that of MinkowskiNet ? and Fast Point Transformer are taken from Chaton et al. <ref type="bibr" target="#b3">[4]</ref>. with previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>, we use Torch-Points3D, an open-source library implemented by Chaton et al. <ref type="bibr" target="#b3">[4]</ref> for reproducible deep learning on 3D point clouds. Torch-Points3D sub-samples a fixed number of points from an input point cloud, which is widely used for PointNet++ <ref type="bibr" target="#b27">[28]</ref> to process a scene-level point cloud-like ScanNet. We notice that the library also sub-samples points for the voxel-based methods, such as MinkowskiNet <ref type="bibr" target="#b5">[6]</ref>, which is not a suitable experimental configuration. Therefore, we reproduce VoteNet with the MinkowskiNet backbone, which is denoted by MinkowskiNet ? in <ref type="table">Table 8</ref>, without input point sub-sampling, and we use the original experimental configurations. Additionally, we train a new VoteNet <ref type="bibr" target="#b25">[26]</ref> with the Fast Point Transformer backbone without any change of detection network (e.g., voting module).</p><p>Results. As shown in <ref type="table">Table 8</ref>, the VoteNet <ref type="bibr" target="#b25">[26]</ref> model with Fast Point Transformer as a backbone outperforms other baselines with a large margin. The results show that the proposed continuous positional encodings that Fast Point Transformer uses can effectively encode point cloud representation and help the 3D detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced the Fast Point Transformer and demonstrated its speed and accuracy on 3D semantic segmentation and 3D detection tasks. The experimental results on large-scale 3D datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> show that our approach is competitive to the best voxel-based method <ref type="bibr" target="#b5">[6]</ref>, and our network achieves 129 times faster inference time than the state-of-theart, Point Transformer, with a reasonable accuracy trade-off in 3D semantic segmentation <ref type="bibr" target="#b1">[2]</ref>. However, there is room for improvement of the Fast Point Transformer at a small voxel size. In the future, we will explore architectures for Fast Point Transformer rather than U-shaped architectures <ref type="bibr" target="#b30">[31]</ref> that are initially designed for convolutional layers. Our code and data are going to be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we provide additional details and results of the proposed method, Fast Point Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experimental Details</head><p>In this section, we clarify the experimental settings for training models, latency evaluation, and model architectures in detail. Each experiment has been conducted with a fixed random seed for the reproducibility. Training details. For 3D semantic segmentation, we use the same training configuration except the batch size and training iterations for both ScanNet <ref type="bibr" target="#b6">[7]</ref> and S3DIS <ref type="bibr" target="#b1">[2]</ref>. We use the SGD optimizer with momentum and weight decay as 0.9 and 0.0001, respectively. The learning rate is scheduled by the linear warm-up and cosine annealing policy from the initial learning rate 0.1 to the final learning rate 0. We train models with batch size 8 both for ScanNet and S3DIS. We train models with 100k and 40k iterations for ScanNet and S3DIS, respectively. Latency evaluation. We describe the detailed setups that have been used during the inference time evaluation on Table 2 of the main paper. We measure the latency of each model with batch size 1 under the following environments:  <ref type="figure" target="#fig_3">Figure A1</ref> illustrates detailed model designs of MinkowskiNet42 <ref type="bibr" target="#b5">[6]</ref> and our Fast Point Transformer. To set the total parameter numbers to be similar, we adjust the feature dimensions as Hu et al. <ref type="bibr" target="#b14">[15]</ref> does, resulting in similar parameter numbers; 37.9M for both models. For small models used in both <ref type="table" target="#tab_3">Table 4</ref> of the main paper and <ref type="table" target="#tab_12">Table A6</ref> of this supplementary material, we modify the number of residual blocks as the official code of MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> does. <ref type="table">Table A1</ref> provides the exact number of residual blocks. <ref type="table">Table A1</ref>. The number of residual blocks. We apply the same configuration for both MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> and Fast Point Transformer. S1,? ? ? , S16 denote the tensor stride in the feature map hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Encoder Decoder S2 S4 S8 S16 S8 S4 S2 S1 <ref type="table" target="#tab_3">baseline 2  3  4  6  2  2  2  2  small  2  2  2  2  2  2  2  2  smaller  1  1  1  1  1  1</ref> 1 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Analysis on Centroid-aware Voxelization</head><p>Color reconstruction. We conduct an experiment to evaluate the effeciveness of our centroid-aware voxelization. We compare ours and the conventional voxelization <ref type="bibr" target="#b5">[6]</ref> with the same setting from <ref type="table" target="#tab_4">Table 5</ref> of the main paper on ScanNet <ref type="bibr" target="#b6">[7]</ref> validation set. We reconstruct colors (RGB) of input point clouds with MinkowskiNet <ref type="bibr" target="#b5">[6]</ref>, optimized by l2-difference between input colors and reconstructed colors. As shown in <ref type="table" target="#tab_7">Table A2</ref>, ours achieves a higher PSNR by 1.27 than the conventional one, showing the effectiveness of the centroidaware property to mitigate quantization artifact. Robustness to voxel size. We evaluate the robustness of both MinkowskiNet42 ? and Fast Point Transformer to the voxel size for inference by using a larger voxel size than one used for training. For both models trained with voxel size 4cm, we measure the performance drop of each method when it use voxel size 5cm for inference. The results show that the Fast Point Transformer is more robust to inference voxel size than MinkowskiNet42 ? as shown in <ref type="table" target="#tab_8">Table A3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Experimental Results</head><p>In this section, we show further experimental results about the effect of model size on its performance, the proposed decomposition of positional encodings, and the class-wise IoU scores of both MinkowskiNet42 ? and our Fast Point Transformer on S3DIS <ref type="bibr" target="#b1">[2]</ref> Area 5 test dataset. Decomposition of positional encodings. We quantitatively measure how much memory the proposed decomposition of positional encodings can reduce. We measure the peak memory usage of both models with and without the decomposition as varying the local window size for neighbor points on ScanNet <ref type="bibr" target="#b6">[7]</ref>. We keep the voxel size as 2cm for the all measurements. As shown in <ref type="table" target="#tab_3">Table A4</ref>, the models with the    proposed decomposition which has the space complexity of O(ID + KD) show an almost constant memory usage since the number of voxel centroids I is much bigger than the number of neighbor points K. However, the models without the decomposition which has the space complexity of O(IKD) show a growing usage of memory. Moreover, the model with local window size 7 raises the out-of-memory error in single NVIDIA Geforce RTX 3090 GPU whose VRAM capacity is 24GB. This results show the memory-efficient property of the proposed lightweight self-attention (LSA). Furthermore, <ref type="table" target="#tab_4">Table A5</ref> shows that the LSA layer saves memory consumption and preserves fast inference time. The mIoU is almost identical to the exact approaches. We hypothesize that this is because the reduction of residual blocks reduces the receptive field, and the reduced receptive field is not sufficient for the model to recognize a 3D scene. Comparison with hybrid methods. For a fair comparison, we re-implement SPVCNN <ref type="bibr" target="#b33">[34]</ref> with MinkowskiEngine-0.5.4 since MinkowskiEngine-0.5.4 is faster than TorchSparse. As shown in <ref type="table" target="#tab_13">Table A7</ref>, Fast Point Transformer outperforms SPVCNN by 2.5 mIoU on Scan-Net <ref type="bibr" target="#b6">[7]</ref> validation set with voxel size 10cm. Detailed experimental results on S3DIS <ref type="bibr" target="#b1">[2]</ref>. We report the class-wise IoU scores of both MinkowskiNet42 ? and the proposed Fast Point Transformer on S3DIS [2] Area 5 in <ref type="table" target="#tab_14">Table A9</ref>. We report the performance of the best model among three different experiments with the same training configuration except random seed numbers both for MinkowskiNet42 ? and Fast Point Transformer. There is a large gap in the latency between point-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50]</ref> and voxel hashing-based methods <ref type="bibr" target="#b5">[6]</ref> including our Fast Point Transformer as shown in <ref type="table" target="#tab_14">Table A9</ref>. Fast Point Transformer (4cm) outperforms MinkowskiNet42 ? (4cm) with rotation average by 0.4 mIoU with a 4.7 times faster speed. <ref type="table">Table A8</ref>. Time complexity analysis. We denote N as the number of dataset points, M as the number of query points (or voxel centroids), and K as the number of neighbors to search. Both M and N are much larger than K in a large-scale point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Time Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Neighbor Search Preparation Inference</p><formula xml:id="formula_18">PointNet [27] ? ? SPGraph [18] ? ? PointWeb [49] O(1) O(M N K) KPConv deform [38] O(N log N ) O(KM log N ) PAConv [42] O(1) O(M N log K) PointTransformer [50] O(1) O(M N log K) MinkowskiNet [6] O(N ) O(M ) FastPointTransformer (ours) O(N ) O(M )</formula><p>In this section, we analyze the time complexity of neighbor search used in both voxel hashing-based methods <ref type="bibr" target="#b5">[6]</ref> including ours and point-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50]</ref>. We recap the reported time complexity as shown in <ref type="table">Table A8</ref>.</p><p>MinkowskiNet <ref type="bibr" target="#b5">[6]</ref> and Fast Point Transformer require the same process for neighbor search since both methods benefit from voxel hashing. We analyze preparation and inference time complexity on Alg. 1 and Alg. 2, respectively. We denote ours as the representative method.</p><p>PointWeb <ref type="bibr" target="#b49">[49]</ref> uses a brute-force algorithm to search the k nearest neighbors. We analyze the time complexity of the brute-force algorithm in Alg. 5. for point = 1, 2, ? ? ? , N do</p><formula xml:id="formula_19">for k = 1, 2, ? ? ? , K do if d(query, point) &lt; b[k] then for i = K ? 1, ? ? ? , k + 1 do b[i] = b[i ? 1] end for b[k] = d(query, point) end if end for end for end for</formula><p>PAConv <ref type="bibr" target="#b42">[42]</ref> and Point Transformer <ref type="bibr" target="#b50">[50]</ref> do not require preparation steps for neighbor search. Thus, we set the preparation time to constant time. For analyzing inference time, we have followed the official implementation. As both methods use the same algorithm for neighbor search, we denote PAConv as the representative method in Alg. 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Qualitative Results</head><p>In this section, we show further qualitative results of consistency scores, 3D semantic segmentation results, and 3D object detection on ScanNet <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Figure A2</ref> shows the pointwise consistency scores of MinkowskiNet42 ? and our Fast Point Transformer. In addition to this consistency, Fast Point Transformer predicts more accurate 3D semantic labels <ref type="figure" target="#fig_1">(Figure A3</ref>) and 3D bounding boxes ( <ref type="figure" target="#fig_2">Figure A4</ref>) qualitatively.  <ref type="figure">Figure A2</ref>. Qualitative results of consistency scores (CScore) on ScanNet <ref type="bibr" target="#b5">[6]</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 5 )</head><label>5</label><figDesc>does not add any additional space complexity because we already have ? abs (c i ? v i ) for every voxel. As a result, space complexity of ?(c i , c j ) goes down from O(IKD) to O(ID + KD) as illustrated in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Decomposition of relative position. Note that we use the continuous positional encoding ? abs (ci ? vi) to transform the input voxel feature fi to the centroid-aware voxel feature gi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Heatmap visualization of consistency score (CScore).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>CUDA version: 11.0 2. cuDNN version: 8.2.1 3. PyTorch version: 1.7.1 4. MinkowskiEngine version: 0.5.4 5. GPU: single NVIDIA Geforce RTX 3090 6. CPU: Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz Network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A1 .</head><label>A1</label><figDesc>Network architectures. (Top) MinkowskiNet42 [6] and (Bottom) our Fast Point Transformer. LSA denotes the proposed lightweight self-attention. Note that both models have the same number of learnable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 5 (</head><label>5</label><figDesc>PointWeb) Inference: O(M N K) Number of training points: N Number of query points: M Number of neighbors to search: K for query = 1, 2, ? ? ? , M do Best score buffer: b[K]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 6 (</head><label>6</label><figDesc>PAConv) Inference: O(M N log K) Number of training points: N Number of query points: M Number of neighbors to search: K for query = 1, 2, ? ? ? , M do H = InitHeap() // O(K) MinD = 10 10 MinIdx = 0 for point = 1, 2, ? ? ? , N do if d(point, query) &lt; MinD then Reheap(H, MinD, MinIdx, K) // O(log K) MinD = d(point, query) MinIdx = point end if end for Heapsort(H, MinIdx, MinD, K) // O(K log K) end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(Left) Input point cloud, (Middle) CScore of MinkowskiNet42 ? , and (Right) CScore of the proposed Fast Point Transformer. Both models are trained with voxel size as 10cm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A3 .</head><label>A3</label><figDesc>Qualitative results of 3D semantic segmentation on ScanNet [6]. (First column) Input point cloud, (Second column) Predicted semantic labels by MinkowskiNet42 ? , (Third column) Predicted semantic labels by the proposed Fast Point Transformer, and (Fourth column) Ground truth. Both models are trained with voxel size as 10cm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A4 .</head><label>A4</label><figDesc>Qualitative results of 3D object detection on ScanNet<ref type="bibr" target="#b5">[6]</ref>. (Left) Predicted bounding boxes by VoteNet<ref type="bibr" target="#b25">[26]</ref> with Minkowsk-iNet backbone, (Middle) Predicted bounding boxes by VoteNet<ref type="bibr" target="#b25">[26]</ref> with the Fast Point Transformer backbone, and (Right) Ground truth.A.6. NotationsP in = {(p n , i n )} Input point cloud p n ? R 3The n-th point coordinatei n ? R DinThe n-th input point featureP out = {(p n , o n )} Output point cloud o n ? R Dout The n-th point feature V = {(v i , f i , c i )} Input voxels with centroids v i ? R 3The i-th voxel center coordinatef i ? R Din The i-th input voxel feature c i ? R 3The i-th voxel centroid coordinateM(i)A set of point indices within the i-th voxel ? A permutation-invariant operator (e.g., average)V ? = {(v i , f ? i , c i )} Output voxels with centroids f ? i ? R Dout The i-th output voxel feature N (i) A setof neighbor voxel indices the i-th voxel e n The centroid-to-point positional encoding ? enc An encoding layer used in centroid-to-point positional encoding o n The n-th output point feature of the output point cloud P out ? A vector concatenation operation a(?) An attention operation ? A query projection layer in attention operations ? A value projection layer in attention operations g i A centroid-aware voxel feature ? rel A discretized positional encoding layer ? abs A continuous positional encoding layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>mIoU vs. model size. Under reduced number of network parameters, Fast Point Transformer shows little performance drop while MinkowskiNet<ref type="bibr" target="#b5">[6]</ref> gradually degrades. We color green for the positive changes and red for the negative changes w.r.t. the base model. We use ScanNet<ref type="bibr" target="#b6">[7]</ref> validation set for the experiment.</figDesc><table><row><cell>Method</cell><cell cols="2"># Param. (M) Rel. (%)</cell><cell>mIoU (%) ?</cell></row><row><cell>Voxel size: 10cm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinkowskiNet42  ?</cell><cell>37.9</cell><cell>?0.0</cell><cell>60.5?0.2 ?0.0</cell></row><row><cell>MinkowskiNet (small)</cell><cell>21.7</cell><cell>? 42.7</cell><cell>59.9?0.6 ? 0.6</cell></row><row><cell>MinkowskiNet (smaller)</cell><cell>11.6</cell><cell>? 69.4</cell><cell>58.2?0.9 ? 2.3</cell></row><row><cell>FastPointTrans. (ours)</cell><cell>37.9</cell><cell>?0.0</cell><cell>65.9?0.6 ?0.0</cell></row><row><cell>FastPointTrans. (small)</cell><cell>20.2</cell><cell>? 46.7</cell><cell>66.0?0.3 ? 0.1</cell></row><row><cell cols="2">FastPointTrans. (smaller) 10.8</cell><cell>? 71.5</cell><cell>65.7?0.1 ? 0.2</cell></row><row><cell>Voxel size: 5cm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinkowskiNet42  ?</cell><cell>37.9</cell><cell>?0.0</cell><cell>66.7?0.3 ?0.0</cell></row><row><cell>MinkowskiNet (small)</cell><cell>21.7</cell><cell>? 42.7</cell><cell>66.0?0.1 ? 0.7</cell></row><row><cell>MinkowskiNet (smaller)</cell><cell>11.6</cell><cell>? 69.4</cell><cell>64.2?0.4 ? 2.5</cell></row><row><cell>FastPointTrans. (ours)</cell><cell>37.9</cell><cell>?0.0</cell><cell>70.0?0.1 ?0.0</cell></row><row><cell>FastPointTrans. (small)</cell><cell>20.2</cell><cell>? 46.7</cell><cell>70.3?0.2 ? 0.3</cell></row><row><cell cols="2">FastPointTrans. (smaller) 10.8</cell><cell>? 71.5</cell><cell>69.7?0.2 ? 0.3</cell></row><row><cell cols="4">from our model, we have observed a large performance drop</cell></row><row><cell cols="4">since the model does not adopt continuous position infor-</cell></row><row><cell cols="4">mation. Removing either positional encodings of centroid-</cell></row><row><cell cols="4">aware voxelization or devoxelization from our network also</cell></row><row><cell cols="4">degrades the performance. These results indicate that the two</cell></row><row><cell cols="4">proposed voxelization and devoxelization effectively main-</cell></row><row><cell cols="4">tain continuous geometric information of the input point</cell></row><row><cell cols="4">cloud. Moreover, the proposed positional encodings also</cell></row><row><cell cols="4">improve the performance of MinkowskiNet42</cell></row></table><note>? although the total number of parameters becomes much bigger than Fast Point Transformer. However, additional usage of ? abs does not improve the performance of MinkowskiNet42 [6], which</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the proposed positional encodings. Note that Mink42 ? and FastPointTrans. denote MinkowskiNet42 ? and Fast Point Transformer, respectively. We use ScanNet validation dataset<ref type="bibr" target="#b6">[7]</ref> with voxel size 10cm.</figDesc><table><row><cell></cell><cell># Param. (M)</cell><cell cols="2">? enc Vox Devox</cell><cell cols="2">? abs mIoU (%)</cell></row><row><cell>Mink42  ?</cell><cell>37.9 38.0 38.0 51.6</cell><cell>? ? ?</cell><cell>? ?</cell><cell>?</cell><cell>60.4 63.2 65.1 65.0</cell></row><row><cell>FastPointTrans.</cell><cell>27.3 27.3 37.8 27.3 37.8 37.9</cell><cell>? ? ? ?</cell><cell>? ?</cell><cell>? ? ?</cell><cell>59.1 61.3 62.1 62.7 63.4 65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A2 .</head><label>A2</label><figDesc>Color reconstruction results.</figDesc><table><row><cell></cell><cell>We compare RGB color</cell></row><row><cell cols="2">reconstruction quality in PSNR with the same backbone architec-</cell></row><row><cell cols="2">ture as MinkowskiNet [6] except the vox/devoxelization modules.</cell></row><row><cell cols="2">We conduct the experiments on ScanNet [7] validation set (10cm).</cell></row><row><cell>Method</cell><cell>PSNR (?)</cell></row><row><cell>Conventional [6]</cell><cell>21.76</cell></row><row><cell>Our centroid-aware</cell><cell>23.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A3 .</head><label>A3</label><figDesc>Comparison of robustness to voxel size.</figDesc><table><row><cell></cell><cell></cell><cell>We compare</cell></row><row><cell cols="3">mIoU scores of MinkowskiNet42  ? and Fast Point Transformer</cell></row><row><cell cols="3">using a larger voxel size (5cm) for inference than the voxel size</cell></row><row><cell cols="3">(4cm) used for training on S3DIS [2] dataset. Note that both models</cell></row><row><cell cols="2">are trained with voxel size as 4cm.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">mIoU (4cm) mIoU (5cm)</cell></row><row><cell>MinkowskiNet42  ?</cell><cell>67.2</cell><cell>64.0 (? 3.2)</cell></row><row><cell>FastPointTransformer</cell><cell>68.7</cell><cell>67.5 (? 1.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A4 .</head><label>A4</label><figDesc>Effect of the decomposition on memory usage. k denotes the local window size which defines the maximum number of neighbor points, K := k 3 , within the kernel volume. We conduct the experiments on ScanNet<ref type="bibr" target="#b6">[7]</ref> validation set (2cm).</figDesc><table><row><cell>k</cell><cell cols="2">Peak Memory Usage (GB) Decomposition (ours) Exact -parallel</cell></row><row><cell>3</cell><cell>3.613</cell><cell>9.519</cell></row><row><cell>5</cell><cell>3.892</cell><cell>23.245</cell></row><row><cell>7</cell><cell cols="2">4.494 Out of Memory</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A5 .</head><label>A5</label><figDesc>Sequential computation vs. Decomposition.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>We con-</cell></row><row><cell cols="4">duct the experiments on ScanNet [7] validation set (2cm).</cell></row><row><cell>Method</cell><cell cols="3">Memory (GB) Latency (sec) mIoU (%)</cell></row><row><cell>Exact -parallel</cell><cell>9.52</cell><cell>0.15</cell><cell>72.1</cell></row><row><cell>Exact -sequential</cell><cell>3.40</cell><cell>0.48</cell><cell>72.1</cell></row><row><cell>Decomposition</cell><cell>3.61</cell><cell>0.17</cell><cell>72.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A6 .</head><label>A6</label><figDesc>mIoU vs. model size. mIoU vs. model size. We provide additional results with voxel size 2cm inTable A6. Since Fast Point Transformer shows its robustness to the number of parameters with voxel size as 5cm and 10cm, Fast Point Transformer (smaller) still achieves 70.5% of mIoU score while MinkowskiNet (smaller) only shows 68.6% with voxel size as 2cm. Interestingly, both MinkowskiNet and Fast Point Transformer show the largest performance drop with voxel size 2cm.</figDesc><table><row><cell>Method</cell><cell cols="2"># Param. (M) Rel. (%)</cell><cell>mIoU (%) ?</cell></row><row><cell>Voxel size: 10cm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinkowskiNet42  ?</cell><cell>37.9</cell><cell>?0.0</cell><cell>60.5?0.2 ?0.0</cell></row><row><cell>MinkowskiNet (small)</cell><cell>21.7</cell><cell>? 42.7</cell><cell>59.9?0.6 ? 0.6</cell></row><row><cell>MinkowskiNet (smaller)</cell><cell>11.6</cell><cell>? 69.4</cell><cell>58.2?0.9 ? 2.3</cell></row><row><cell>FastPointTrans. (ours)</cell><cell>37.9</cell><cell>?0.0</cell><cell>65.9?0.6 ?0.0</cell></row><row><cell>FastPointTrans. (small)</cell><cell>20.2</cell><cell>? 46.7</cell><cell>66.0?0.3 ? 0.1</cell></row><row><cell cols="2">FastPointTrans. (smaller) 10.8</cell><cell>? 71.5</cell><cell>65.7?0.1 ? 0.2</cell></row><row><cell>Voxel size: 5cm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinkowskiNet42  ?</cell><cell>37.9</cell><cell>?0.0</cell><cell>66.7?0.3 ?0.0</cell></row><row><cell>MinkowskiNet (small)</cell><cell>21.7</cell><cell>? 42.7</cell><cell>66.0?0.1 ? 0.7</cell></row><row><cell>MinkowskiNet (smaller)</cell><cell>11.6</cell><cell>? 69.4</cell><cell>64.2?0.4 ? 2.5</cell></row><row><cell>FastPointTrans. (ours)</cell><cell>37.9</cell><cell>?0.0</cell><cell>70.0?0.1 ?0.0</cell></row><row><cell>FastPointTrans. (small)</cell><cell>20.2</cell><cell>? 46.7</cell><cell>70.3?0.2 ? 0.3</cell></row><row><cell cols="2">FastPointTrans. (smaller) 10.8</cell><cell>? 71.5</cell><cell>69.7?0.2 ? 0.3</cell></row><row><cell>Voxel size: 2cm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinkowskiNet42  ?</cell><cell>37.9</cell><cell>?0.0</cell><cell>71.9?0.2 ?0.0</cell></row><row><cell>MinkowskiNet (small)</cell><cell>21.7</cell><cell>? 42.7</cell><cell>71.2?0.2 ? 0.7</cell></row><row><cell>MinkowskiNet (smaller)</cell><cell>11.6</cell><cell>? 69.4</cell><cell>68.6?0.6 ? 3.3</cell></row><row><cell>FastPointTrans. (ours)</cell><cell>37.9</cell><cell>?0.0</cell><cell>72.1?0.3 ?0.0</cell></row><row><cell>FastPointTrans. (small)</cell><cell>20.2</cell><cell>? 46.7</cell><cell>71.3?0.1 ? 0.8</cell></row><row><cell cols="2">FastPointTrans. (smaller) 10.8</cell><cell>? 71.5</cell><cell>70.5?0.1 ? 1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A7 .</head><label>A7</label><figDesc>Voxel-based vs. Hybrid vs. Fast Point Transformer.</figDesc><table><row><cell cols="4">We compare MinkowskiNet42 [6], SPVCNN [34] and our Fast</cell></row><row><cell cols="4">Point Transfomer on ScanNet [7] validation with voxel size 10cm.</cell></row><row><cell>Method</cell><cell cols="3">Memory (GB) Latency (sec) mIoU (%)</cell></row><row><cell>MinkowskiNet42  ?</cell><cell>1.93</cell><cell>0.04</cell><cell>60.4</cell></row><row><cell>SPVCNN</cell><cell>3.62</cell><cell>0.07</cell><cell>62.8</cell></row><row><cell>Ours</cell><cell>2.73</cell><cell>0.08</cell><cell>65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A9 .</head><label>A9</label><figDesc>Detailed experimental results on S3DIS [2] Area 5 test. Note that the latency of each method denotes the per-scene walltime latency normalized by that of Fast Point Transformer. Numbers except the latency means percentage values (%). We denote MinkowskiNet42 ? and Fast Point Transformer as MinkNet42 ? and FastPointTrans., respectively. Method Latency mAcc mIoU ceil. floor wall beam col. wind. door table chair sofa book. board clut.</figDesc><table><row><cell>PointNet [27]</cell><cell>129.71</cell><cell>49.0</cell><cell>41.1</cell><cell>88.8 97.3 69.8</cell><cell>0.1</cell><cell>3.9</cell><cell>46.3 10.8 59.0 52.6</cell><cell>5.9</cell><cell>40.3</cell><cell cols="2">26.4 33.2</cell></row><row><cell>SPGraph [18]</cell><cell>130.57</cell><cell>66.5</cell><cell>58.0</cell><cell>89.4 96.9 78.1</cell><cell>0.0</cell><cell cols="4">42.8 48.9 61.6 84.7 75.4 69.8 52.6</cell><cell>2.1</cell><cell>52.2</cell></row><row><cell>PointWeb [49]</cell><cell>83.00</cell><cell>66.6</cell><cell>60.3</cell><cell>92.0 98.5 79.4</cell><cell>0.0</cell><cell cols="4">21.1 59.7 34.8 76.3 88.3 46.9 69.3</cell><cell cols="2">64.9 52.5</cell></row><row><cell>KPConv deform [38]</cell><cell>751.07</cell><cell>72.8</cell><cell>67.1</cell><cell>92.8 97.3 82.4</cell><cell>0.0</cell><cell cols="4">23.9 58.0 69.0 81.5 91.0 75.4 75.3</cell><cell cols="2">66.7 58.9</cell></row><row><cell>PAConv [42]</cell><cell>200.93</cell><cell>73.0</cell><cell>66.6</cell><cell>94.6 98.6 82.4</cell><cell>0.0</cell><cell cols="4">26.4 58.0 60.0 80.4 89.7 69.8 74.3</cell><cell cols="2">73.5 57.7</cell></row><row><cell>PointTransformer [50]</cell><cell>129.07</cell><cell>76.5</cell><cell>70.4</cell><cell>94.0 98.5 86.3</cell><cell>0.0</cell><cell cols="4">38.0 63.4 74.3 89.1 82.4 74.3 80.2</cell><cell cols="2">76.0 59.3</cell></row><row><cell>MinkNet42  ? (5cm)</cell><cell>0.50</cell><cell>73.3</cell><cell>66.0</cell><cell>93.2 97.0 84.0</cell><cell>0.0</cell><cell cols="4">25.7 63.9 66.4 76.9 88.9 58.4 70.1</cell><cell cols="2">78.0 54.9</cell></row><row><cell>+ rotation average</cell><cell>4.07</cell><cell>73.5</cell><cell>67.1</cell><cell>93.9 97.1 85.2</cell><cell>0.1</cell><cell cols="4">28.3 64.5 70.3 76.8 90.0 57.2 70.9</cell><cell cols="2">81.1 56.7</cell></row><row><cell>FastPointTrans. (5cm)</cell><cell>0.93</cell><cell>74.7</cell><cell>67.5</cell><cell>91.5 97.4 86.0</cell><cell>0.2</cell><cell cols="4">40.4 60.8 66.7 79.6 87.7 58.6 73.7</cell><cell cols="2">77.2 57.3</cell></row><row><cell>+ rotation average</cell><cell>7.50</cell><cell>75.5</cell><cell>68.5</cell><cell>90.0 96.0 86.2</cell><cell>0.0</cell><cell cols="4">47.1 61.3 69.7 81.1 88.2 60.9 74.2</cell><cell cols="2">78.2 57.3</cell></row><row><cell>MinkNet42  ? (4cm)</cell><cell>0.57</cell><cell>73.6</cell><cell>67.2</cell><cell>93.1 97.6 84.9</cell><cell>0.0</cell><cell cols="4">35.9 57.5 74.5 80.0 88.2 55.6 72.9</cell><cell cols="2">77.1 56.9</cell></row><row><cell>+ rotation average</cell><cell>4.71</cell><cell>74.3</cell><cell>68.3</cell><cell>93.8 97.6 85.9</cell><cell>0.0</cell><cell cols="4">38.9 58.8 75.3 81.1 88.8 53.3 74.6</cell><cell cols="2">80.0 59.8</cell></row><row><cell>FastPointTrans. (4cm)</cell><cell>1.00</cell><cell>77.1</cell><cell>68.7</cell><cell>93.8 97.8 85.5</cell><cell>0.6</cell><cell cols="4">49.9 60.5 72.9 80.2 88.7 56.0 71.4</cell><cell cols="2">78.0 58.1</cell></row><row><cell>+ rotation average</cell><cell>8.07</cell><cell>77.9</cell><cell>70.3</cell><cell>94.2 98.0 86.0</cell><cell>0.2</cell><cell cols="4">53.8 61.2 77.3 81.3 89.4 60.1 72.8</cell><cell cols="2">80.4 58.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported by Qualcomm and the IITP grant (2021-0-02068: AI Innovation Hub and 2019-0-01906: AI Grad. School Prog.) funded by the Korea government (MSIT) and the NRF grant (NRF-2020R1C1C1015260).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time parallel hashing on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Dan A Alcantara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhabrata</forename><surname>Abbasinejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amenta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia 2009 papers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Torch-points3d: A modular multi-task framework for reproducible deep learning on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chaulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofiane</forename><surname>Horache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A unified point-based framework for 3d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yueh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">R</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Live semantic 3d perception for immersive augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2012" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bidirectional projection network for cross dimension scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="14373" to="14382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="518" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on ?-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dycuckoo: dynamic hash tables on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianling</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="744" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Point-net++ deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">32</biblScope>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Radu Alexandru Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimized spatial hashing for collision detection of deformable objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Teschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danat</forename><surname>Pomerantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vmv</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<editor>I. Guyon, U. V</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06076</idno>
		<title level="m">Pvt: Point-voxel transformer for 3d deep learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deformable {detr}: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
