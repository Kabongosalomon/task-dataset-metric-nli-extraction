<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ML-Decoder: Scalable and Versatile Classification Head</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ML-Decoder: Scalable and Versatile Classification Head</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce ML-Decoder, a new attentionbased classification head. ML-Decoder predicts the existence of class labels via queries, and enables better utilization of spatial data compared to global average pooling. By redesigning the decoder architecture, and using a novel group-decoding scheme, ML-Decoder is highly efficient, and can scale well to thousands of classes. Compared to using a larger backbone, ML-Decoder consistently provides a better speed-accuracy trade-off. ML-Decoder is also versatile -it can be used as a drop-in replacement for various classification heads, and generalize to unseen classes when operated with word queries. Novel query augmentations further improve its generalization ability. Using ML-Decoder, we achieve state-of-the-art results on several classification tasks: on MS-COCO multi-label, we reach 91.4% mAP; on NUS-WIDE zero-shot, we reach 31.1%</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification is a vital computer-vision task, that requires assigning a label or multiple labels to an image, according to the objects present in it. With single-label classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b45">46]</ref>, we assume that the image contains only one object, hence we can apply a softmax operation on the output logits. However, natural images usually contain multiple objects and concepts, highlighting the importance of multi-label classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>, where we predict each class separately and independently, in a similar fashion to multi-task problems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>. Notable success in the field of multi-label classification was reported by exploiting label correlation via graph neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and improving loss functions, pretrain methods and backbones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="bibr">*</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal contribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot</head><p>Single-label . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Cost</head><p>Transformer-decoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO mAP [%]</head><p>GAP ML-Decoder <ref type="figure">Figure 1</ref>. Our proposed classification head. ML-Decoder is versatile, and provides a unified solution for several classification tasks, with state-of-the-art results. Unlike transformer-decoder, it is also scalable, and can handle efficiently thousands of classes.</p><p>In a regime of extreme classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>, we need to predict the existence of a large number of classes (usually thousands or more), forcing our model and training scheme to be efficient and scalable. Multi-label zero-shot learning (ZSL) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref> is an extension of multi-label classification, where during inference the network tries to recognize unseen labels, i.e., labels from additional categories that were not used during training. This is usually done by sharing knowledge between the seen classes (that were used for training) and the unseen classes via a text model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Classification networks usually contain a backbone, and a classification head <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. The backbone outputs a spatial embedding tensor, and the classification head transforms the spatial embeddings into prediction logits. In single-label classification, this is commonly done by globalaverage-pooling (GAP) , followed by a fully connected layer <ref type="bibr" target="#b13">[14]</ref>. GAP-based heads are also used also for multilabel classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42]</ref>. However, the need to identify several objects, with different locations and sizes, can make the usage of average pooling sub-optimal. Recently, several works proposed attention-based heads for multi-label classification. <ref type="bibr" target="#b12">[13]</ref> offered a two-stream attention framework to recognize multi-category objects from global image to local regions. <ref type="bibr" target="#b48">[49]</ref> suggested simple spatial attention scores, and then combined them with class-agnostic average pooling features. <ref type="bibr" target="#b24">[25]</ref> presented a pooling transformer with learnable queries for multi-label classification, achieving top results.</p><p>GAP-based classification heads are simple and efficient, and scale well with the number of classes, since they have a fixed spatial pooling cost. However, they provide suboptimal results, and are not directly applicable to ZSL. Attention-based classification heads do improve results, but are often costly, even for datasets with a small number of classes, and practically infeasible to extreme classification scenarios. They also have no natural extension to ZSL.</p><p>In this paper, we introduce a new classification head, called ML-Decoder, that provides a unified solution for single-label, multi-label, and zero-shot classification, with state-of-the-art results (see <ref type="figure">Figure 1</ref>). ML-Decoder design is based on the original transformer-decoder <ref type="bibr" target="#b36">[37]</ref>, with two major modifications, that significantly improve its scalability and efficiency. First, it reduces the quadratic dependence of the decoder in the number of input queries to a linear one, by removing the redundant self-attention block. Second, ML-Decoder uses a novel group-decoding scheme, where instead of assigning a query per class, it uses a fixed number of queries, that are interpolated to the final number of classes via a new architectural block called group fully-connected. Using group-decoding, ML-Decoder also enjoys a fixed spatial pooling cost, and scales well to thousands of classes.</p><p>ML-Decoder is flexible and efficient. It can be trained equally well with learnable or fixed queries, and can use different queries during training and inference (see <ref type="figure" target="#fig_1">Figure  2</ref>). These key features make ML-Decoder suitable for ZSL tasks. When we assign a query per class and train MLdecoder with word queries, it generalizes well to unseen queries, and significantly improves previous state-of-the-art ZSL results. We also show that the group-decoding scheme can be extended to the ZSL scenario, and introduce novel query augmentations during training to further encourage generalization.</p><p>The paper's contributions can be summarized as follows:</p><p>? We propose a new classification head called ML-Decoder, which provides a unified solution for multilabel, zero-shot, and single-label classification, with state-of-the-art results.</p><p>? ML-Decoder can be used as a drop-in replacement for global average pooling. It is simple and efficient, and provides improved speed-accuracy trade-off compared to larger backbones, or other attention-based heads.</p><p>? ML-Decoder novel design makes it scalable to classification with thousands of classes. Complementary queryaugmentation technique improves its generalizability to unseen classes as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we will first review the baseline classification heads. Then we will present our novel ML-Decoder, discuss its advantages, and show its applicability to several computer-vision tasks, such as multi-label, ZSL, and singlelabel classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline Classification Heads</head><p>A typical classification network is comprised of a backbone, and a classification head. The network's backbone outputs a spatial embedding tensor, E ? R H?W ?D , and the classification head transforms the spatial embeddings tensor into N logits, {l n } N n=1 , where N is the number of classes. There are two baseline approaches for processing the spatial embeddings: GAP-based, and attention-based.</p><p>GAP-based: with a GAP-based classification head, we first reduce the spatial embeddings to a one-dimensional vector via simple global averaging operation on the spatial dimensions, outputting a vector z ? R D?1 . Then, a fully connected layer transforms the embedding vector into N output logits: l = W z, where W ? R N ?D is a learnable linear projections matrix. GAP is commonly used for single-label classification tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, and has some generalizations, for example <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>. GAP was also adopted as a baseline approach for mutli-label classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> Attention-based: Unlike single-label classification, in multi-label classification several objects can appear in the image, in different locations and sizes. Several works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49]</ref> have noticed that the GAP operation, which eliminates the spatial dimension via simple averaging, can be sub-optimal for identifying multiple objects with different sizes. Instead they suggested using attention-based classification heads, which enable more elaborate usage of the spatial data, with improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recap -Attention and Transformer-Decoder</head><p>Among the attention-based classification heads proposed, a simple approach based on a transformer-decoder, similar to the one used by DETR for object detection <ref type="bibr" target="#b3">[4]</ref>, has achieved top results on multi-label classification <ref type="bibr" target="#b24">[25]</ref>.</p><p>A transformer-decoder unit relies on the multi-head attention module, introduced in <ref type="bibr" target="#b36">[37]</ref>. A multi-head attention has three inputs: Q, K, V . If we define the attention operation to be:</p><formula xml:id="formula_0">Attention(Q, K, V ) = Softmax( QK T ? d k )V<label>(1)</label></formula><p>A multi-head module output is: </p><formula xml:id="formula_1">MultiHeadAttn(Q, K, V ) = Concat(head 1 , ..., head h )W O where head i = Attention(QW Q i , KW K i , V W V i ) Self-Attention N x D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Queries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attention</head><formula xml:id="formula_2">W Q i , W K i , W V i , W O are learnable projections</formula><p>matrices. An illustration of a transformer-decoder classification head is given in <ref type="figure" target="#fig_2">Figure 3</ref> (left side). The transformerdecoder has two inputs: The spatial embedding tensor, E, and a set of N learnable queries, Q, one for each class. The transformer-decoder processes the inputs via four consecutive stages called self-attention, cross-attention, feedforward and token-pool: self-attn:</p><formula xml:id="formula_3">Q 1 ? ? MultiHeadAttn(Q, Q, Q)</formula><p>cross-attn:</p><formula xml:id="formula_4">Q 2 ? ? MultiHeadAttn(Q 1 , E, E) feed-forward: Q 3 ? ? FF(Q 2 ) token-pool: Logits ? ? Pool(Q 3 )<label>(2)</label></formula><p>FF is a feed-forward fully connected layer, as defined in <ref type="bibr" target="#b36">[37]</ref>. The token pooling stage is a simple pooling on the token embeddings' dimension D, to produce N output logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ML-Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Motivation</head><p>On multi-label datasets with small number of classes, such as MS-COCO <ref type="bibr" target="#b23">[24]</ref> and Pascal-VOC <ref type="bibr" target="#b11">[12]</ref> (80 and 20 classes respectively), transformer-decoder classification head works well, and achieves state-of-the-art results <ref type="bibr" target="#b24">[25]</ref>, with small additional computational overhead. However, it suffers from a critical drawback -the computational cost is quadratic with the number of classes. Hence, for datasets with large number of classes, such as Open Images <ref type="bibr" target="#b20">[21]</ref> (9600 classes), using transformer-decoder is practically infeasible in terms of computational cost, as we will show in Section 3.2. For real-world applications, a large number of classes is imperative to provide a complete and comprehensive description of an input image. Hence, a more scalable and efficient attention-based classification head is needed. In addition, transformer-decoder as a classification head is suitable for multi-label classification only. A more general attention-based head, that can also address other tasks such as single-label and ZSL, will be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">ML-Decoder Design</head><p>We will now describe our proposed classification head, ML-Decoder. Illustration of ML-Decoder flow is given in <ref type="figure">Fig</ref> (1) Self-attention removal: We start by observing that during inference, the self-attention module of transformerdecoder provides a fixed transformation on the input queries. However, as the queries are entering the crossattention module, they are subjected to a projection layer, before going through the attention operation (Eq. 1). In practice, the projection layer can transform the queries to any desired output, making the self-attention module redundant. Hence, we can remove the self-attention layer, while still maintaining the same expressivity of the classification head, and without degrading the results. We will validate this empirically in Section 3.1. By removing the selfattention, we avoid a costly module, and relax the quadratic dependence of ML-Decoder in the number of input queries to a linear one, making it more practical and efficient.</p><p>(2) Group-decoding: In an extreme classification scenario, even linear dependency of the classification head with the number of classes can be costly. We want to break this coupling, and make the cross-attention module, and the feed-forward layer after it, independent of the number of classes, same as GAP operation. To this end, instead of assigning a query per class, we use as inputs a fixed number of group queries, K (see <ref type="figure" target="#fig_2">Figure 3</ref>). After the feed-forward layer, we transform the group queries into output logits via a novel layer called group fully-connected. This layer performs simultaneously two tasks -(1) expand each group query to N K outputs; (2) pool the embeddings' dimension. If we define the group-factor to be g = N K , group fully-connected generates an output logit L i with the following operation:</p><formula xml:id="formula_5">L i = (W k ? Q k ) j where: k = i div g, j = i mod g (3)</formula><p>Q k ? R D is the k th query, and W k ? R g?D is the k th learnable projection matrix. An illustration of group fullyconnected layer is given in <ref type="figure" target="#fig_5">Figure 4</ref>, and a pseudo-code appears in appendix L.  The full flow of ML-Decoder with group-decoding is depicted in Eq. 4, where G q are the input group queries:</p><formula xml:id="formula_6">cross-attn: G q1 ? ? MultiHeadAttn(G q , E, E) feed-forward: G q2 ? ? FF(G q1 ) group FC: Logits ? ? Group-FC(G q2 )<label>(4)</label></formula><p>Some additional observations and insights into the groupdecoding scheme:</p><p>? With full-decoding (g = 1), each query checks the existence of a single class. With group-decoding, each query checks the existence of several classes. We chose to divide the classes into groups in a random manner. Clustering the classes via semantic proximity is an alternative, but will require a cumbersome clustering process, with extra hyper-parameters that might need tuning per dataset. In Section 3.2 we will show that random group clustering is enough to provide results comparable to a full-decoding scheme. ? In terms of flops, the group fully-connected layer is equivalent to a fully-connected layer in a GAP-based head (N ? D multiplications). Both are linearly dependent in the number classes, but in practice they have a small computational overhead, even for thousands of classes.</p><p>In terms of memory consumption, performing the two tasks of group fully-connected together, in a single operation, is more efficient than doing them consecutively, since there is no need to store large intermediate maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? The only component in ML-Decoder that depends on the</head><p>input image size is the cross-attention module. We can think of the cross-attention layer as doing spatial pooling, similar to GAP. With group-decoding, ML-Decoder has fixed spatial pooling cost, independent of N.</p><p>(3) Non-learnable queries: <ref type="bibr" target="#b24">[25]</ref> argued that transformerdecoder for multi-label classification achieves top results only with learnable queries. However, we observe that the queries are always fed into a multi-head attention layer, that applies a learnable projection on them (Eq. 1). Hence, setting the queries weights as learnable is redundant -a learnable projection can transform any fixed-value query to any value obtained by a learnable query. We will validate this empirically in Section 3.1, showing that the same accuracies are obtained when training ML-Decoder with learnable or fixed queries. In addition to simplifying the training process, using fixed queries will enable us to do ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">ML-Decoder for ZSL</head><p>Next, we will present the adaptations needed in order to use ML-Decoder in a multi-label ZSL scenario, and discuss key features of ML-Decoder that make it suitable for the task.</p><p>We will also show that the group-decoding scheme can be extended to ZSL, and present novel query augmentations that further improve ML-Decoder generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLP-based queries:</head><p>We begin by presenting a version of ML-Decoder for ZSL with a full-decoding scheme (each label has a corresponding query). As discussed in the previous section, the input queries can be either learnable or fixed. For ZSL, we use fixed NLP-based queries -for each label, a word embedding vector is extracted using a language model, and set as the input query. We also use shared projection matrix in the group fully-connected layer (setting W k = W in Eq. 3). With NLP-based queries and a shared projection matrix, semantic information can propagate from the seen (training) classes to the unseen (test) classes during inference, enabling generalization.</p><p>ML-Decoder features: ML-Decoder contains several favorable features which make it well suited for ZSL. Firstly, its attention mechanism is based on dot-product similarity between vectors (Eq. 1). Since NLP word embeddings preserve this dot-product semantic similarity <ref type="bibr" target="#b14">[15]</ref>, the unseen labels are more likely to be matched with the most similar keys and values within the decoder. In addition, ML-Decoder with a shared projection matrix allows a variable number of input queries, and is not sensitive to the order of queries. This is beneficial since in ZSL we perform training and testing on different sets of classes, and therefore different sets of queries. For ZSL we train exclusively on the seen labels, and perform inference on the unseen classes, while for Generalized ZSL (GZSL), we perform inference on the union of the unseen and seen sets of labels.</p><p>Group-decoding: Group-decoding (with K &lt; N ) requires modifications in order to work in a ZSL setting.</p><p>In appendix B we thoroughly detail our variant of groupdecoding for ZSL.</p><p>Query augmentations With NLP-based queries, ML-Decoder naturally extends to the task of ZSL. Yet, we want to apply dedicated training tricks that further improve its generalizability. It is a common practice in computer-vision to apply augmentations on the input images to prevent overfitting, and improve generalizability to new unseen images. Similarly, we introduce query augmentations to encourage generalization to unseen class queries. The first augmentation is random-query, which adds additional randomvalued queries to the set of input queries, and assigns a positive ground truth label signifying "random" for these added queries. The second augmentation is query-noise, where we add each batch a small random noise to the input queries. See <ref type="figure">Figure 8</ref> in the appendix for illustration of the augmentations. In Section 3.3 we will show that query augmentations encourage the model to recognize novel query vectors which it hasn't encountered before, and improve ZSL scores. We also tried query-cutout augmentation, where random parts of the queries are deleted each batch. However, this technique was not beneficial in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">ML-Decoder for Single-label Classification</head><p>Most attention-based classification heads previously proposed were purposed for multi-label classification ( <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49]</ref>, for example), and this was also the primary task we focused on in our work. However, our design of ML-Decoder enables it to be used as a drop-in replacement for GAPbased heads on other computer-vision tasks, such as singlelabel classification, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The main motivation for attention-based heads in multilabel classification was the need to identify several objects, with different locations and sizes <ref type="bibr" target="#b24">[25]</ref>. We will show in Section 4.3 that the benefit from ML-Decoder is more general, and fully applies also to single-label problems, where the image usually contains a single object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Study</head><p>In this section, we will bring ablation tests and inner comparisons for our proposed ML-Decoder classification head. First, we will test ML-Decoder with different types of input queries. Then we will compare ML-Decoder to other classification heads, such as transformer-decoder and GAP-based. Finally, we will provide an ablation study for ZSL with augmentation queries and group-decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Comparing Query Types</head><p>As discussed in Section 2.3.2, due to the linear projection in the attention module, ML-Decoder retains the same expressivity when it uses learnable or fixed queries. In <ref type="table" target="#tab_8">Table  7</ref> in the appendix we compare results for ML-Decoder with different types of queries, on MS-COCO multi-label dataset (see appendix C for full training details on MS-COCO).</p><p>Indeed we see that learnable, fixed random, and fixed NLP-based word queries all lead to the same accuracy, 88.1% mAP, as expected. For reducing the number of learned parameters, we shall use fixed queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparing Different Classification Heads</head><p>In <ref type="table">Table 1</ref>  From <ref type="table">Table 1</ref> we learn the following observations:</p><p>? ML-Decoder (and transformer-decoder) provide a significant improvement (more than 1% mAP), compared to GAP-based classification head. ? When using the same number of input queries (80), transformer-decoder and ML-Decoder reach the same accuracy, demonstrating that indeed the self-attention module provides a redundant transformation (see Section 2.3.2), and removing it in ML-Decoder reduces the computational cost without impacting the results. ? Using group decoding with a ratio of N K = 4 has a minimal impact on the results -a reduction of only 0.1% mAP. However, since MS-COCO dataset has a small number of classes, the additional flops from an attentionbased classification head are minimal, so reducing the number of queries is not essential in this case.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we repeat the same comparison on Open Images multi-label dataset, which has significantly more classes -9600 instead of 80. Full training details on Open Images are given in appendix H.</p><p>On Open Images, using transformer-decoder classification head is not feasible -due to the large number of classes, the additional computational cost is very high, and even with a batch size of 1 and input resolution of 224, our training is out-of-memory (see <ref type="table" target="#tab_10">Table 9</ref> for full specifications). In contrast, ML-Decoder with group-decoding increases the flops count by only 10% ? 20%, while significantly improving the mAP score on this challenging extreme classification dataset, compared to GAP. We also see from Table 2 that group decoding with a ratio of N K = 48 already Classification Head gives the full score benefit, and further increasing the ratio to N K = 96 reduces the score by only 0.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Num of Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Num of Queries</head><p>In <ref type="figure" target="#fig_6">Figure 5</ref> we compare on MS-COCO the mAP score vs. flops for GAP-based and ML-Decoder classification head, with three different architectures -TResNet-S, TResNet-M, TResNet-L (equivalent in runtime to ResNet34, ResNet50 and ResNet101 <ref type="bibr" target="#b32">[33]</ref>). We see from <ref type="figure" target="#fig_6">Figure 5</ref> that using ML-decoder provides a better flops-accuracy trade-off compared to using GAP-based classification head with a larger backbone. In addition to the flops-accuracy measurements, in <ref type="table" target="#tab_10">Table  9</ref> in the appendix we provide full speed-accuracy comparisons of different classification heads, measuring inference speed, training speed, maximal batch size and flops. This table can aid in future comparisons of our work for different speed-accuracy metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zero-shot Learning</head><p>This section presents an ablation study of ML-Decoder for ZSL, on NUS-WIDE dataset <ref type="bibr" target="#b8">[9]</ref>. NUS-WIDE is the most widely used benchmark for the multi-label ZSL task, and therefore we focus on this dataset. It is comprised of 925 seen labels, and 81 unseen labels. Full training and dataset details are given in appendix F.</p><p>In <ref type="table">Table 3</ref>  It is evident from the table that both random-query and additive noise contribute to the model's ability to generalize to unseen classes. When applying both of them, we see an increase of 1.2% in the mAP score of the unseen classes. We also tested the impact of query augmentations on the seen classes. Both on MS-COCO and on NUS-WIDE, adding query augmentations had no impact on the seen classes' mAP score (88.1% on MS-COCO, 22.7% on NUS-WIDE). This is in agreement with our results in Section 3.1, where learnable and fixed queries provided the same results on the seen classes. The benefit from using query augmentation is by better generalization to the unseen classes.</p><p>In <ref type="table">Table 4</ref> we compare group-decoder to full-decoder for the ZSL task. The modifications to the group-decoding scheme for ZSL are described in appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Num of Classes</head><p>Num of Queries We see from the <ref type="table">Table 4</ref> that the group-decoding scheme works well also for the ZSL scenario, with a small decrease in the mAP scores compared to full-decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we will evaluate our ML-Decoder-based solution on popular multi-label, ZSL, and single-label classification datasets, and compare results to known state-ofthe-art techniques. <ref type="bibr" target="#b23">[24]</ref> is a commonly-used dataset to evaluate multi-label image classification. It contains 122, 218 im-ages from 80 different categories, divided to a training set of 82, 081 images and a validation set of 40, 137 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-label Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">MS-COCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO</head><p>In <ref type="table" target="#tab_5">Table 5</ref>   TResNet-L has equivalent runtime to ResNet101, with design tricks that lead to improved results <ref type="bibr" target="#b32">[33]</ref>.</p><p>Since previous works did not always report their computational cost, and released a reproducible code, we cannot provide a full speed-accuracy comparison to them. Still, we see that with ML-Decoder we improve results on MS-COCO dataset, with minimal additional computational cost compared to plain GAP-based solution (see also <ref type="table">Table 1</ref>). We hope that future works will use our results as a baseline for a more complete comparison, including computational cost and accuracy. For that matter, in <ref type="table" target="#tab_11">Table 10</ref> in the appendix we report our results and flops count for different input resolutions. Note that with input resolution of 640, we reach on MS-COCO with TResNet-L and TResNet-XL new state-of-the-art results of 91.1% and 91.4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Additional Multi-label Datasets</head><p>Pascal-VOC: In <ref type="table">Table 11</ref> in the appendix we present results on another popular multi-label dataset -Pascal-VOC <ref type="bibr" target="#b11">[12]</ref>. With ML-Decoder we reach new state-of-the-art result on Pascal-VOC -96.6% mAP. Open Images: In <ref type="table" target="#tab_1">Table 12</ref> in the appendix we present results on an extreme classification multi-label dataset -Open Images <ref type="bibr" target="#b20">[21]</ref>, which contains 9600 classes. Also on this dataset ML-Decoder outperforms previous methods, achieving 86.8% mAP. Notice that due the large number of classes, some attention-based methods are not feasible for this dataset. Hence, no results for them are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero-Shot Learning</head><p>In <ref type="table" target="#tab_7">Table 6</ref> we present a SotA comparison on NUS-WIDE multi-label zero-shot dataset <ref type="bibr" target="#b8">[9]</ref>. Similar to previous works, we use F1 score at top-K predictions and mAP as evaluation metrics. mAP is measured both on ZSL (unseen classes only) and GZSL (seen+unseen classes). Full training details appear in appendix F.  We see from <ref type="table" target="#tab_7">Table 6</ref> that our approach significantly outperforms the previous top solution by 4.8% mAP (ZSL), setting a new SotA in this task. Note that previous methods were mostly aimed at optimizing the task of zero-shot, at the expense of the seen classes (ZSL vs. GZSL trade-off). SDL <ref type="bibr" target="#b2">[3]</ref>, for example, proposed to use several principal embedding vectors, and trained them using a tailored loss function for ZSL. In contrast to previous methods, ML-Decoder offers a simple unified solution for plain and zero-shot classification, and achieves top results for both ZSL and GZSL. ML-Decoder sets a new SotA score also on the GZSL task, outperforming SDL with a significant improvement (from 12.1% to 19.9%). This clearly demonstrates that ML-Decoder generalizes well to unseen classes, while maintaining high accuracy on the seen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Single-label Classification</head><p>To test ML-Decoder effectiveness for single-label classification, we used ImageNet dataset <ref type="bibr" target="#b9">[10]</ref>, with the highquality training code suggested in <ref type="bibr" target="#b40">[41]</ref> (A2 configuration). Comparison of various ResNet architectures, with different classification heads, appears in <ref type="figure">Figure 6</ref>. As can be seen, when replacing the baseline GAP-based head with ML-Decoder, we significantly improve the models' accuracy. ML-Decoder also provides a better speed-accuracy trade-off compared to using GAP with a larger backbone.</p><p>Notice that we used ML-Decoder without introducing any change or tuning any hyper-parameter in the training configuration. The fact that ML-Decoder can serve as a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet18</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet34</head><p>ResNet50 ML-Decoder GAP <ref type="figure">Figure 6</ref>. Speed-accuracy comparison for different classification heads on ImageNet dataset. For ML-Decoder, we used group-decoding (100 groups). Training configuration -A2 <ref type="bibr" target="#b40">[41]</ref>.</p><p>drop-in replacement for GAP-based classification head, in a highly-optimized single-label training setting (baseline ResNet50 achieves 79.7%), and still provides an additional boost, demonstrate its effectiveness and versatility. Also, note that following <ref type="bibr" target="#b40">[41]</ref>, our training configuration for Im-ageNet is using a multi-label loss (sigmoid with BCE loss instead of softmax and CE loss). That's the default mode we present in <ref type="figure" target="#fig_1">Figure 2</ref>. In <ref type="table">Table 13</ref> in the appendix we validate that also with softmax, ML-Decoder provides a benefit compared to plain GAP.</p><p>When increasing the number of training epochs to 600 (A1 configuration in <ref type="bibr" target="#b40">[41]</ref>), with ML-Decoder and vanilla ResNet50 backbone we reach 80.7% accuracy. To the best of our knowledge, this is the top results so far achieved with ResNet50 (without extra data or distillation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Additional Results</head><p>In appendix J we bring additional results for single-label classification, showing that with ML-Decoder we achieve top results on two other prominent single-label datasets -CIFAR-100 <ref type="bibr" target="#b19">[20]</ref> and Stanford-Cars <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we introduced ML-Decoder, a new attention-based classification head. By removing the redundant self-attention layer and using a novel group-decoding scheme, ML-Decoder scales well to thousands of classes, and provides a better speed-accuracy trade-off than using a larger backbone. ML-Decoder can work equally well with fixed or random queries, and use different queries during training and inference. With word-based queries and novel query augmentations, ML-Decoder also generalizes well to unseen classes. Extensive experimental analysis shows that ML-Decoder outperforms GAP-based heads on several classification tasks, such as multi-label, single-label and zero-shot, and achieves new state-of-the-art results.</p><p>Our future work will focus on extending the usage of ML-Decoder to other computer-vision tasks that include classification, such as object detection and video recognition. We will also explore using the group-decoding scheme more generally, for processing any spatial embedding tensor, and will try to apply it to other fields and tasks, such as segmentation, pose estimation and NLP-related problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Query Types Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ZSL Group Decoder</head><p>We will now describe how to incorporate groupdecoding in the ZSL setting in order to improve the ML-Decoder's scalability with respect to the number of classes. Group-decoding cannot be trivially applied for ZSL, since in group-decoding each query is associated with a group of labels, while in the ZSL setting we assume that each query is associated with a specific word embedding. To alleviate this issue, we propose to construct each query by concatenating linear projections of all the word embeddings assigned to its group (See <ref type="figure" target="#fig_8">Figure 7)</ref>. Formally, the k th group query is given by</p><formula xml:id="formula_7">q k = concat({W a ? N i } i?Gj )<label>(5)</label></formula><p>where G k is the set of labels assigned to the k th group,</p><formula xml:id="formula_8">{N i } is the set of word embeddings (N i ? R dw ), W a ? R d g ?dw</formula><p>is the parameter matrix, and g = N K . In addition, as can be seen in <ref type="table">Table 8</ref>, the group fullyconnected head, even with shared weights, does not generalize well to unseen classes in a group-decoding setting. Therefore, we implemented a different pooling strategy for ZSL group-decoding: we decompose the group fullyconnected parameter matrix (W k from Eq. 3) into two components: {N i } -the set of word embeddings which is labelspecific, and W b ? R d?dw -a learned parameter matrix which is shared for all labels. Formally, the parameter matrix W k of the k th group-query (Eq. 3) is constructed by the following:</p><formula xml:id="formula_9">W k = W b ? M k<label>(6)</label></formula><p>where M k ? R dw?g is constructed by stacking the word embedding vectors {N i } i?G k of group k. Inserting Eq. 6 into Eq. 3, we get the output logits for the ZSL groupdecoder. (see <ref type="figure" target="#fig_8">Fig. 7</ref>, and pseudo-code in appendix M). <ref type="table">Table 8</ref> shows ablation experiments for the different modifications. . . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attention + Feed-Forward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MS-COCO Training Details</head><p>Unless stated explicitly otherwise, for MS-COCO we used the following training procedure: We trained our models for 40 epochs using Adam optimizer and 1-cycle policy, with maximal learning rate of 2e-4. For regularization, we used Cutout factor of 0.5, True-weight-decay of 1e-4 and auto-augment. We found that the common Im-ageNet statistics normalization does not improve results, and instead used a simpler normalization -scaling all the RGB channels to be between 0 and 1. Our input resolution was 448. For ML-Decoder, our baseline was full-decoding (K = N = 80). Similar to <ref type="bibr" target="#b1">[2]</ref>, we used Open Images pretraining for our models' backbone. ML-Decoder weights were used with random initialization. The number of token embeddings was D = 768. We adjusted the backbone embedding output to D via a 1 ? 1 depth-wise convolution. As a loss function, we used ASL with ? ? = 4, ? + = 0 and Classification Head </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Queries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speed Comparison for Different Classification Heads</head><p>In <ref type="table" target="#tab_10">Table 9</ref> we provide full speed-accuracy measurements. We can see that in practice, using ML-Decoder on TResNet-M architecture reduces inference speed by 15%, independent with the number of classes, while transformerdecoder classification head scales badly with the number of classes, reducing the inference speed (and other throughput metrices) by orders of magnitudes. The 925 and 81 labels are used as seen and unseen classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. MS-COCO Results for Different Input Resolutions</head><p>To be compatible with previous works <ref type="bibr" target="#b2">[3]</ref>, as a loss function we used CE, our backbone was TResNet-M, and our input resolution is 224. Unless stated otherwise, our baseline ML-Decoder for ZSL was full-decoding, with K = N , and shared projection matrix, as discussed in Section 2.3.3. Other training details for ZSL NUS-WIDE were similar to the one used for MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pascal-VOC Training Details and Results</head><p>Pascal Visual Object Classes Challenge (VOC 2007) is another popular dataset for multi-label recognition. It contains images from 20 object categories, with an average of 2.5 categories per image. Pascal-VOC is divided to a trainval set of 5,011 images and a test set of 4,952 images. As a backbone we used TResNet-L, with input resolution of 448. For ML-Decoder, our baseline was full-decoding (K = N = 20). Other training details are similar to the ones used for MS-COCO. Results appear in <ref type="table">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mAP [%] RNN <ref type="bibr" target="#b38">[39]</ref> 91.9 FeV+LV <ref type="bibr" target="#b43">[44]</ref> 92.0 ML-GCN <ref type="bibr" target="#b7">[8]</ref> 94.0 SSGRL <ref type="bibr" target="#b5">[6]</ref> 95.0 BMML <ref type="bibr" target="#b22">[23]</ref> 95.0 ASL <ref type="bibr" target="#b1">[2]</ref> 95.8 Q2L <ref type="bibr" target="#b24">[25]</ref> 96.1 ML-Decoder 96.6 <ref type="table">Table 11</ref>. Comparison of ML-Decoder to known state-of-theart models on Pascal-VOC dataset. For ML-Decoder, we used TResNet-L backbone, input resolution 448.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Open-Images Training Details and Results</head><p>Open Images (v6) <ref type="bibr" target="#b20">[21]</ref> is a large-scale dataset, which consists of 9 million training images, 41, 620 validation images and 125, 436 test images. It is partially annotated with human labels and machine-generated labels. For dealing with the partial labeling methodology of Open Images dataset, we set all untagged labels as negative, with reduced weights. Due to the large the number of images, we trained our network for 25 epochs on input resolution of 224. We used TResNet-M as a backbone. Since the level of positive-negative imbalancing is significantly higher than MS-COCO, we increased the level of loss asymmetry: For ASL, we trained with ? ? = 7, ? + = 0. For ML-Decoder, our baseline was group-decoding with K = 100. Other training details are similar to the ones used for MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mAP [%] CE <ref type="bibr" target="#b1">[2]</ref> 84.8 Focal Loss <ref type="bibr" target="#b1">[2]</ref> 84.9 ASL <ref type="bibr" target="#b1">[2]</ref> 86.3 ML-Decoder 86.8  <ref type="table">Table 13</ref>. ImageNet classification scores for different classification heads and logits activations. For ML-Decoder, we used group-decoding with 100 groups. Our training configuration is A2 <ref type="bibr" target="#b40">[41]</ref>. Backbone -ResNet50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Comparison of ML-Decoder to State-of-theart Models on Single-label Transfer Learning Datasets</head><p>In this section, we will compare our ML-Decoder based models to known state-of-the-art models from the literature, on two prominent single-label datasets -CIFAR-100 <ref type="bibr" target="#b19">[20]</ref> and Stanford-Cars <ref type="bibr" target="#b18">[19]</ref>. The comparison is based on 2 and 3 .</p><p>2 https://paperswithcode.com/sota/image-classification-on-cifar-100 3 https://paperswithcode.com/sota/fine-grained-image-classificationon-stanford</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Model Top-1 Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>CvT We can see from <ref type="table" target="#tab_13">Table 14</ref> that our ML-Decoder based solution is highly competitive, achieving 1st and 2nd place on Stanford-Cars and CIFAR-100 datasets respectivly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Query Augmentations Illustration</head><p>. . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-Decoder</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Versatility -ML-Decoder module is applicable to various classification tasks, such as multi-label, zero-shot, and single-label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Scalability -baseline transformer-decoder vs. our proposed ML-Decoder. N -number of classes, K -number of group queries, D -tokens length. Removing the redundant self-attention block relaxes the quadratic dependence in the number of queries to a linear one, while retraining the same expressivity. When using group queries with fixed number of queries K &lt; N , ML-Decoder becomes fully scalable, with spatial pooling cost independent in the number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ure 3 (right-side). Compared to transformer-decoder, ML-Decoder includes the following modifications:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Scheme of a group fully-connected layer (with g = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>mAP score vs. Flops comparison, on MS-COCO datase, for different classification heads. For ML-Decoder, we used K = N = 80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Group decoding scheme for ZSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 . 2 ''' 3 - 4 - 6 - 2 ''' 3 - 4 - 5 - 6 -</head><label>8234623456</label><figDesc>Query augmentations rand-query: adding random queries which are assigned label "noise". additive noise: adding random noise to the input queries.L. Group Fully-connected Pseudo Code 1 def GroupFullyConnected(G, group_weights, output, num_of_groups): G is the group queries tensor. G.shape = [groups, embeddings] group_weights are learnable (group) fully connected weights.5 group_weights.shape = [groups, embeddings, classes//groups] output is the interpolated queries tensor. output.shape = [groups, classes//groups] [i, :] # [1,embeddings] 10 w_i = group_weights[i, :, :] # [embeddings, classes//groups] 11 output_i = matmul(g_i, w_i) # [1, classes//groups] 12 output[i, :] = output_i 13 logits = output.flatten(1)[:self.num_classes] # [1, classes] 14 return logits Notice that the loop implementation is very efficient in terms of memory consumption during training. Implementing the group-fully-connected in a single vectoric operation (without a loop) is possible, but reduces the possible batch size. Also, the proposed implementation is fully suitable for compile-time acceleration (@torch.jit.script) M. Group Fully-connected ZSL Pseudo-Code 1 def GroupFullConnectedZSL(G, wordvecs, output, W, num_groups, num_classes): G is the group queries tensor. G.shape = [num_groups, embeddings_dim] wordvecs is the word-embedding tensor [num_classes, word_embedding_dim] W is a learnable projection matrix [embeddings_dim, word_embedding_dim] output is the interpolated queries tensor. output.shape = [num_groups, num_classes//num_groups] (num_groups, embedding_dim, labels_per_group) 10 for i in range(num_classes): 11 group_weights[i // labels_per_group, :, i % labels_per_group] = W * wordvecs[i, :] 12 13 logits = GroupFullyConnected(G, group_weights, output, num_groups) 14 return logits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we compare MS-COCO results for training with different classification heads.</figDesc><table><row><cell>Classification</cell><cell>Num of</cell><cell>Num of</cell><cell>Flops</cell><cell>mAP</cell></row><row><cell>Head</cell><cell>Classes</cell><cell>Queries</cell><cell>[G]</cell><cell>[%]</cell></row><row><cell>GAP</cell><cell>80</cell><cell>-</cell><cell>23.0</cell><cell>87.0</cell></row><row><cell>Transformer-Decoder</cell><cell>80</cell><cell>80</cell><cell>24.1</cell><cell>88.1</cell></row><row><cell>ML-Decoder</cell><cell>80</cell><cell>20</cell><cell>23.6</cell><cell>88.0</cell></row><row><cell>ML-Decoder</cell><cell>80</cell><cell>80</cell><cell>23.9</cell><cell>88.1</cell></row></table><note>Table 1. Comparison of multi-label MS-COCO mAP score for different classification heads. Architecture -TResNet-M [33].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Open Images mAP score for different classification heads. Architecture -TResNet-M.</figDesc><table><row><cell>Flops</cell><cell>mAP</cell></row><row><cell>[G]</cell><cell>[%]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>we compare the different types of query augmentations presented in Section 2.3.3.</figDesc><table><row><cell>Augmentation</cell><cell>mAP [%]</cell></row><row><cell>Type</cell><cell>(ZSL)</cell></row><row><cell>None</cell><cell>29.9</cell></row><row><cell>Additive noise</cell><cell>30.6</cell></row><row><cell>Random-query</cell><cell>30.7</cell></row><row><cell>Both</cell><cell>31.1</cell></row><row><cell cols="2">Table 3. Comparison of NUS-WIDE ZSL mAP scores for ML-</cell></row><row><cell cols="2">Decoder with different query augmentations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>we compare ML-Decoder results to top known solutions from the literature. Full training details appear in appendix C.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input Resolution</cell><cell>mAP [%]</cell></row><row><cell>ML-GCN [8]</cell><cell>ResNet101</cell><cell>448x448</cell><cell>83.0</cell></row><row><cell cols="2">KSSNET [26] ResNet101</cell><cell>448x448</cell><cell>83.7</cell></row><row><cell>SSGRL [6]</cell><cell>ResNet101</cell><cell>576x576</cell><cell>83.8</cell></row><row><cell cols="2">MS-CMA [45] ResNet101</cell><cell>448x448</cell><cell>83.8</cell></row><row><cell>ASL [2]</cell><cell>ResNet101</cell><cell>448x448</cell><cell>85.0</cell></row><row><cell>ASL [2]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>88.4</cell></row><row><cell>Q2L [25]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>89.2</cell></row><row><cell>ML-Decoder</cell><cell>ResNet101</cell><cell>448x448</cell><cell>87.1</cell></row><row><cell>ML-Decoder</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>90.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>State-of-the-art comparison on MS-COCO dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table /><note>State-of-the-art comparison for multi-label ZSL and GZSL tasks on NUS-WIDE dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Comparison of MS-COCO mAP scores for ML-Decoder with different query types. We used K = N = 80.</figDesc><table><row><cell>Type of Queries</cell><cell>mAP [%]</cell></row><row><cell>Learnable</cell><cell>88.1</cell></row><row><cell>Fixed NLP-based</cell><cell>88.1</cell></row><row><cell>Fixed Random</cell><cell>88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Comparison of throughput indices for different classification heads. All measurements were done on Nvidia V100 16GB machine, with mixed precision. We used TResNet-M as a backbone, with input resolution 224. Training and inference speed were measured with 80% of maximal batch size.</figDesc><table><row><cell>Training Speed [img/sec]</cell><cell>Inference Speed [img/sec]</cell><cell>Maximal Training Batch Size</cell><cell>Flops [G]</cell></row></table><note>m = 0.05. All results were averaged over three seeds for better consistency. TResNet-L model is V2 1 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Comparison of MS-COCO mAP scores for different input resolutions.F. NUS-WIDE ZSL Dataset and Training DetailsNUS-WIDE is a multi-label ZSL dataset, comprised of nearly 270K images with 81 human-annotated categories, in addition to the 925 labels obtained from Flicker user tags.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input Resolution</cell><cell>Flops [G]</cell><cell>mAP [%]</cell></row><row><cell>ML-Decoder</cell><cell>TResNet-L</cell><cell>224x224</cell><cell>9.3</cell><cell>85.5</cell></row><row><cell>ML-Decoder</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>36.2</cell><cell>90.0</cell></row><row><cell>ML-Decoder</cell><cell>TResNet-L</cell><cell>640x640</cell><cell>73.5</cell><cell>91.1</cell></row><row><cell cols="2">ML-Decoder TResNet-XL</cell><cell>640x640</cell><cell cols="2">103.8 91.4</cell></row></table><note>1 see: https://github.com/Alibaba-MIIL/TResNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>Comparison of ML-Decoder to known state-of-theart results on Open Images dataset.I. Single-label Classification with DifferentLogit Activations</figDesc><table><row><cell cols="3">Logit Activation Classification Head Top1 Acc. [%]</cell></row><row><cell>Sigmoid</cell><cell>GAP ML-Decoder</cell><cell>79.7 80.3</cell></row><row><cell>Softmax</cell><cell>GAP ML-Decoder</cell><cell>79.3 80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Comparison top of state-of-the-art models.</figDesc><table><row><cell></cell><cell>-W24</cell><cell>94.05</cell></row><row><cell></cell><cell>ViT-H</cell><cell>94.55</cell></row><row><cell></cell><cell>EffNet-L2 (SAM)</cell><cell>96.08</cell></row><row><cell></cell><cell>Swin-L + ML-Decoder</cell><cell>95.1</cell></row><row><cell></cell><cell>EffNet-L2 (SAM)</cell><cell>95.95</cell></row><row><cell>Stanford-</cell><cell>ALIGN</cell><cell>96.13</cell></row><row><cell>Cars</cell><cell>DAT</cell><cell>96.2</cell></row><row><cell></cell><cell cols="2">TResNet-L + ML-Deocder 96.41</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? We verify the effectiveness of ML-Decoder with comprehensive experiments on commonly-used classification datasets: MS-COCO, Open Images, NUS-WIDE, PASCAL-VOC, and ImageNet.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Asaf Noy, and Lihi Zelnik-Manor. Multi-label classification with partial annotations using class-aware selective loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic diversity learning for zero-shot multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05926</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with joint class-aware map disentangling and label correlation embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2007 (voc2007) results. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to discover multi-class attentional regions for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adgap: Advanced global average pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arna</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biswarup</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somnath Basu Roy</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A shared multi-attention framework for multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8776" to="8786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07932</idno>
		<title level="m">Bilinear attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in cnns: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="863" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bi-modal learning with channel-wise attention for multilabel image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Query2label: A simple transformer way to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10834</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Extreme classification in log memory using count-min sketch: A case study of amazon search with 50m products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tharun</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13830</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative region-based multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8731" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finetuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="162" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Large-scale multi-label learning with missing labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast zeroshot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5985" to="5994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual attention: A simple but effective method for multi-label recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
