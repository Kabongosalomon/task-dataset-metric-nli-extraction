<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Projected GANs Converge Faster</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>M?ller</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Vision and Learning Lab</orgName>
								<address>
									<country>University Heidelberg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Projected GANs Converge Faster</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr?chet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.</p><p>35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2111.01007v1 [cs.CV] 1 Nov 2021 and natural language processing <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. While combining pretrained perceptual networks <ref type="bibr" target="#b72">[73]</ref> with GANs for image-to-image translation has led to impressive results <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b80">81]</ref>, this idea has not yet materialized for unconditional noise-to-image synthesis. Indeed, we confirm that a na?ve application of this idea does not lead to state-of-the-art results (Section 4) as strong pretrained features enable the discriminator to dominate the two-player game, resulting in vanishing gradients for the generator <ref type="bibr" target="#b1">[2]</ref>. In this work, we demonstrate how these challenges can be overcome and identify two key components for exploiting the full potential of pretrained perceptual feature spaces for GAN training: feature pyramids to enable multi-scale feedback with multiple discriminators and random projections to better utilize deeper layers of the pretrained network.</p><p>We conduct extensive experiments on small and large datasets with a resolution of up to 1024 2 pixels. Across all datasets, we demonstrate state-of-the-art image synthesis results at significantly reduced training time <ref type="figure">(Fig. 1)</ref>. We also find that Projected GANs increase data efficiency and avoid the need for additional regularization, rendering expensive hyperparameter sweeps unnecessary. Code, models, and supplementary videos can be found on the project page https://sites.google.com/view/ projected-gan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We categorize related work into two main areas: pretraining for GANs and discriminator design.</p><p>Pretrained Models for GAN Training. Work on leveraging pretrained representations for GANs can be divided into two categories: First, transferring parts of a GAN to a new dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b87">88]</ref> and, second, using pretrained models to control and improve GANs. The latter is advantageous as pretraining does not need to be adversarial. Our work falls into this second category. Pretrained models can be used as a guiding mechanism to disentangle causal generative factors <ref type="bibr" target="#b68">[69]</ref>, for text-driven image manipulation <ref type="bibr" target="#b57">[58]</ref>, matching the generator activations to inverted classifiers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b70">71]</ref>, or to generate images via gradient ascent in the latent space of a generator <ref type="bibr" target="#b54">[55]</ref>. The non-adversarial approach of [68] learns generative models with moment matching in pretrained models; however, the results remain far from competitive to standard GANs. An established method is the combination of adversarial and perceptual losses <ref type="bibr" target="#b27">[28]</ref>. Commonly, the losses are combined additively <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b80">81]</ref>. Additive combination, however, is only possible if a reconstruction target is available, e.g., in paired image-toimage translation settings <ref type="bibr" target="#b91">[92]</ref>. Instead of providing the pretrained network with a reconstruction target, Sungatullina et al. [75]  propose to optimize an adversarial loss on frozen VGG features <ref type="bibr" target="#b72">[73]</ref>. They show that their approach improves CycleGAN [92] on image translation tasks. In a similar vein, [63] recently proposed a different perceptual discriminator. They utilize a pretrained VGG and connect its features with the prediction of a pretrained segmentation network. The combined features are fed into multiple discriminators at different scales. The two last approaches are specific to the image-toimage translation task. We demonstrate that these methods do not work well for the more challenging unconditional setting where the entire image content is synthesized from a random latent code.</p><p>Discriminator Design. Much work on GANs focuses on novel generator architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b85">86]</ref>, while the discriminator often remains close to a vanilla convolutional neural network or mirrors the generator. Notable exceptions are <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b86">87]</ref> which utilize an encoder-decoder discriminator architecture. However, in contrast to us, they neither use pretrained features nor random projections. A different line of work considers a setup with multiple discriminators, applied to either the generated RGB image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> or low-dimensional projections thereof <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">54]</ref>. The use of several discriminators promises improved sample diversity, training speed, and training stability. However, these approaches are not utilized in current state-of-the-art systems because of diminishing returns compared to the increased computational effort. Providing multi-scale feedback with one or multiple discriminators has been helpful for both image synthesis <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and image-to-image translation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b80">81]</ref>. While these works interpolate the RGB image at different resolutions, our findings indicate the importance of multi-scale feature maps, showing parallels to the success of pyramid networks for object detection <ref type="bibr" target="#b47">[48]</ref>. Lastly, to prevent overfitting of the discriminator, differentiable augmentation methods have recently been proposed <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref>. We find that adopting these strategies helps exploit the full potential of pretrained representations for GAN training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>StyleGAN2-ADA Projected GAN <ref type="figure">Figure 1</ref>: Convergence with Projected GANs. Evolution of samples for a fixed latent code during training on the AFHQ-Dog dataset <ref type="bibr" target="#b6">[7]</ref>. We find that discriminating features in the projected feature space speeds up convergence and yields lower FIDs. This finding is consistent across many datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Generative Adversarial Network (GAN) consists of a generator and a discriminator. For image synthesis, the generator's task is to generate an RGB image; the discriminator aims to distinguish real from fake samples. On closer inspection, the discriminator's task is two-fold: First, it projects the real and fake samples into a meaningful space, i.e., it learns a representation of the input space. Second, it discriminates based on this representation. Unfortunately, training the discriminator jointly with the generator is a notoriously hard task. While discriminator regularization techniques help to balance the adversarial game <ref type="bibr" target="#b42">[43]</ref>, standard regularization methods like gradient penalties <ref type="bibr" target="#b49">[50]</ref> are susceptible to hyperparameter choices <ref type="bibr" target="#b32">[33]</ref> and can lead to a substantial decrease in performance <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we explore the utility of pretrained representations to improve and stabilize GAN training. Using pretrained representations has become ubiquitous in computer vision <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref> 3 Projected GANs GANs aim to model the distribution of a given training dataset. A generator G maps latent vectors z sampled from a simple distribution P z (typically a normal distribution) to corresponding generated samples G(z). The discriminator D then aims to distinguish real samples x ? P x from the generated samples G(z) ? P G(z) . This basic idea results in the following minimax objective</p><formula xml:id="formula_0">min G max D E x [log D(x)] + E z [log(1 ? D(G(z)))]<label>(1)</label></formula><p>We introduce a set of feature projectors {P l } which map real and generated images to the discriminator's input space. Projected GAN training can thus be formulated as follows</p><formula xml:id="formula_1">min G max {D l } l?L E x [log D l (P l (x))] + E z [log(1 ? D l (P l (G(z)))))]<label>(2)</label></formula><p>where {D l } is a set of independent discriminators operating on different feature projections. Note that we keep {P l } fixed in <ref type="bibr" target="#b2">(3)</ref> and only optimize the parameters of G and {D l }. The feature projectors {P l } should satisfy two necessary conditions: they should be differentiable and provide sufficient statistics of their inputs, i.e., they should preserve important information. Moreover, we aim to find feature projectors {P l } which turn the (difficult to optimize) objective in <ref type="bibr" target="#b0">(1)</ref> into an objective more amenable to gradient-based optimization. We now show that a projected GAN indeed matches the distribution in the projected feature space, before specifying the details of our feature projectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Consistency</head><p>The projected GAN objective in <ref type="bibr" target="#b2">(3)</ref> no longer optimizes directly to match the true distribution P T . To understand the training properties under ideal conditions, we consider a more generalized form of the consistency theorem of <ref type="bibr" target="#b53">[54]</ref>:</p><p>Theorem 1. Let P T denote the density of the true data distribution and P G the density of the distribution the Generator G produces. Let P l ? T and P l ? G be the functional composition of the differentiable and fixed function P l and the true/generated data distribution, and y be the transformed input to the discriminator. For a fixed G, the optimal discriminators are given by D * l,G (y) = P P l ?T (y) P P l ?T (y) + P P l ?G (y) for all l ? L. In this case, the optimal G under (3) is achieved iff P P l ?T = P P l ?G for all l ? L.</p><p>A proof of the theorem is provided in the appendix. From the theorem, we conclude that a feature projector P l with its associated discriminator D l encourages the generator to match the true distribution along the marginal through P l . Therefore, at convergence, G matches the generated and true distributions in feature space. The theorem also holds when using stochastic data augmentations <ref type="bibr" target="#b31">[32]</ref> before the deterministic projections P l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>Projecting to and training in pretrained feature spaces opens up a realm of new questions which we address below. This section will provide an overview of the general system and is followed by extensive ablations of each design choice. As our feature projections affect the discriminator, we focus on P l and D l in this section and postpone the discussion of generator architectures to Section 5.</p><p>Multi-Scale Discriminators. We obtain features from four layers L l of a pretrained feature network F at resolutions (L 1 = 64 2 , L 2 = 32 2 , L 3 = 16 2 , L 4 = 8 2 ). We associate a separate discriminator D l with the features at layer L l , respectively. Each discriminator D l uses a simple convolutional architecture with spectral normalization <ref type="bibr" target="#b50">[51]</ref> at each convolutional layer. We observe better performance if all discriminators output logits at the same resolution (4 2 ). Accordingly, we use fewer down-sampling blocks for lower resolution inputs. Following common practice, we sum all logits for computing the overall loss. For the generator pass, we sum the losses of all discriminators. More complex strategies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> did not improve performance in our experiments.</p><p>Random Projections. We observe that features at deeper layers are significantly harder to cover, as evidenced by our experiments in Section 4. We hypothesize that a discriminator can focus on a subset of the feature space while wholly disregarding other parts. This problem might be especially prominent in the deeper, more semantic layers. Therefore, we propose two different strategies to dilute prominent features, encouraging the discriminator to utilize all available information equally. Common to both strategies is that they mix features using differentiable random projections which are fixed, i.e., after random initialization, the parameters of these layers are not trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real/Generated Image</head><formula xml:id="formula_2">L 1 L 2 L 3 L 4 D 1 D 2 D 3 D 4</formula><p>Feature Network Cross-Channel Mixing (CCM). Empirically, we found two properties to be desirable: (i) the random projection should be information preserving to leverage the full representational power of F , and (ii) it should not be trivially invertible. The easiest way to mix across channels is a 1?1 convolution. A 1?1 convolution with an equal number of output and input channels is a generalization of a permutation <ref type="bibr" target="#b37">[38]</ref> and consequently preserves information about its input. In practice, we find that more output channels lead to better performance as the mapping remains injective and therefore information preserving. Kingma et al. <ref type="bibr" target="#b37">[38]</ref> initialize their convolutional layers as a random rotation matrix as a good starting point for optimization. We do not find this to improve GAN performance (see Appendix), arguably since it violates (ii). We therefore randomly initialize the weights of the convolutional layer via Kaiming initialization <ref type="bibr" target="#b21">[22]</ref>. Note that we do not add any activation functions. We apply this random projection at each of the four scales and feed the transformed feature to the discriminator as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Real/Generated Image</p><formula xml:id="formula_3">+ + + L 1 L 2 L 3 L 4 D 1 D 2 D 3 D 4</formula><p>Feature Network Cross-Scale Mixing (CSM). To encourage feature mixing across scales, CSM extends CCM with random 3?3 convolutions and bilinear upsampling, yielding a U-Net <ref type="bibr" target="#b64">[65]</ref> architecture, see <ref type="figure" target="#fig_2">Fig. 3</ref>. However, our CSM block is simpler than a vanilla U-Net <ref type="bibr" target="#b64">[65]</ref>: we only use a single convolutional layer at each scale. As for CCM, we utilize Kaiming initialization for all weights.</p><p>Pretrained Feature Networks. We ablate over varying feature networks. First, we investigate different versions of EfficientNets, which allow for direct control over model size versus performance. Effi-cientNets are image classification models trained on ImageNet <ref type="bibr" target="#b9">[10]</ref> and designed to provide favorable accuracy-compute tradeoffs. Second, we use ResNets of varying sizes. To analyze the dependency on ImageNet features (Section 4.3), we also consider R50-CLIP <ref type="bibr" target="#b59">[60]</ref>, a ResNet optimized with a contrastive language-image objective on a dataset of 400 million (image, text) pairs. Lastly, we utilize a vision transformer architecture (ViT-Base) <ref type="bibr" target="#b13">[14]</ref> and its efficient follow-up (DeiT-small distilled) <ref type="bibr" target="#b77">[78]</ref>. We do not choose an inception network <ref type="bibr" target="#b75">[76]</ref> to avoid strong correlations with the evaluation metric FID <ref type="bibr" target="#b22">[23]</ref>. In the appendix, we also evaluate several other neural and non-neural metrics to rule out correlations. These additional metrics reflect the rankings obtained by FID.</p><p>In the following, we conduct a systematic ablation study to analyze the importance and best configuration of each component in our Projected GAN model, before comparing it to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ablation Study</head><p>To determine the best configuration of discriminators, mixing strategy, and pretrained feature network, we conduct experiments on LSUN-Church <ref type="bibr" target="#b83">[84]</ref>, which is medium-sized (126k images) and reasonably visually complex, using a resolution of 256 2 pixels. For the generator G we use the generator architecture of FastGAN <ref type="bibr" target="#b48">[49]</ref>, consisting of several upsampling blocks, with additional skip-layerexcitation blocks. Using a hinge loss <ref type="bibr" target="#b46">[47]</ref>, we train with a batch size of 64 until 1 million real images have been shown to the discriminator, a sufficient amount for G to reach values close to convergence. If not specified otherwise, we use an EfficientNet-Lite1 <ref type="bibr" target="#b76">[77]</ref> feature network in this section. We found that discriminator augmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref> consistently improves the performance of all methods, and is required to reach state-of-the-art performance. We leverage differentiable data-augmentation <ref type="bibr" target="#b88">[89]</ref> which we found to yield the best results in combination with the FastGAN generator.  <ref type="table">Table 1</ref>: Feature Space Fr?chet Distances. We aim to find the best combination of discriminators and random projections to fit the distributions in feature network F . We show the relative FD at different layers of F (rel-F D i ) between 50k generated and real images on LSUN-Church. rel-F D i is normalized using the baseline Fr?chet Distances for a model with a standard single RGB image discriminator. Hence, values &gt; 1 indicate worse performance than the RGB baseline. We report rel-F D for four layers of an EfficientNet (L 1 , L 2 , L 3 and L 4 from shallow to deep), as well as relative Fr?chet Inception Distance (FID) <ref type="bibr" target="#b22">[23]</ref>. Note that rel-F D i should not be compared between different feature spaces, i.e., only within-column comparisons are meaningful. Blue boxes highlight the layers which we supervise via independent discriminators. The green box corresponds to a perceptual discriminator <ref type="bibr" target="#b74">[75]</ref>, which takes in all feature maps at once.</p><formula xml:id="formula_4">Discriminator(s) rel-F D1 ? rel-F D2 ? rel-F D3 ? rel-F D4 ? rel ? F ID ? No Projection</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Which feature network layers are most informative?</head><p>We first investigate the relevance of independent multi-scale discriminators. For this experiment, we do not use feature mixing. To measure how well G fits a particular feature space, we employ the Fr?chet Distance (FD) <ref type="bibr" target="#b16">[17]</ref> on the spatially pooled features denoted as F D i for layer i. FDs across different feature spaces are not directly comparable. Therefore, we train a GAN baseline with a standard RGB discriminator, record F D RGB i at each layer and quantify the relative improvement via the fraction rel-F D i = F D i /F D RGB i . We also investigate a perceptual discriminator <ref type="bibr" target="#b74">[75]</ref>, where feature maps are fed into different layers of the same discriminator to predict a single logit.</p><p>The results in <ref type="table">Table 1</ref> (No Projection) show that two discriminators are better than one and improve over the vanilla RGB baseline. Surprisingly, adding discriminators at deep layers hurts performance. We conclude that these more semantic features do not respond well to direct adversarial losses. We also experimented with discriminators at resized versions of the original image, but could not find a setting of hyperparameters and architectures that improves over the single image baseline. Omitting the discriminators on the shallow features decreases performance, which is anticipated, as these layers contain most of the information about the original image. A similar effect has been observed for feature inversion <ref type="bibr" target="#b15">[16]</ref> -the deeper the layer, the harder it is to reconstruct its input. Lastly, we observe that independent discriminators outperform the perceptual discriminator by a significant margin.  <ref type="table">Table 2</ref>: Pretrained Feature Networks Study. We train the projected GAN with different pretrained feature networks. We find that compact EfficientNets outperform both ResNets and Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EfficientNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How can we best utilize the pretrained features?</head><p>Given the insights from the previous section, we aim to improve the utilization of deep features. For this experiment, we only investigate configurations that include discriminators at high resolutions. <ref type="table">Table 1</ref> (CCM and CCM + CSM) presents the results for both mixing strategies. CCM moderately decreases the FDs across all settings, confirming our hypothesis that mixing channels results in better feedback for the generator. When adding CSM, we achieve another notable improvement across all configurations. Especially rel-F D i at deeper layers are significantly decreased, demonstrating CSM's usefulness to leverage deep semantic features. Interestingly, we observe that the best performance is now obtained by combining all four discriminators. A perceptual discriminator is again inferior to multiple discriminators. We remark that integrating the original image, via an independent discriminator or CCM or CSM always resulted in worse performance. This failure suggests that na?vely combining non-projected with projected adversarial optimization impairs training dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Which feature network architecture is most effective?</head><p>Using the best setting determined by the experiments above (CCM + CSM with four discriminators), we study the effectiveness of various perceptual feature network architectures for Projected GAN training. To ensure convergence, also for larger architectures, we train for 10 million images. <ref type="table">Table  2</ref> reports the FIDs achieved on LSUN-Church. Surprisingly, we find that there is no correlation with ImageNet accuracy. On the contrary, we observe lower FIDs for smaller models (e.g., EfficientNetslite). This observation indicates that a more compact representation is beneficial while at the same time reducing computational overhead and consequently training time. R50-CLIP slightly outperforms its R50 counterpart, indicating that ImageNet features are not required to achieve low FID. For the sake of completeness, we also train with randomly initialized feature networks, which, however, converge to much higher FID values (see Appendix). In the following, we thus use EfficientNet-Lite1 as our feature network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to State-of-the-Art</head><p>This section conducts a comprehensive analysis demonstrating the advantages of Projected GANs with respect to state-of-the-art models. Our experiments are structured into three sections: evaluation of convergence speed and data efficiency (5.1), and comparisons on large (5.2) and small (5.3) benchmark datasets. We cover a wide variety of datasets in terms of size (hundreds to millions of samples), resolution (256 2 to 1024 2 ), and visual complexity (clip-art, paintings, and photographs).</p><p>Evaluation Protocol. We measure image quality using the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b22">[23]</ref>. Following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, we report the FID between 50k generated and all real images. We select the snapshot with the best FID for each method. In addition to image quality, we include a metric to evaluate convergence. As in <ref type="bibr" target="#b31">[32]</ref>, we measure training progress based on the number of real images shown to the discriminator (Imgs). We report the number of images required by the model for the FID to reach values within 5% of the best FID over training. In the appendix, we also report other metrics that are less benchmarked in GAN literature: KID <ref type="bibr" target="#b3">[4]</ref>, SwAV-FID <ref type="bibr" target="#b52">[53]</ref>, precision and recall <ref type="bibr" target="#b65">[66]</ref>. Unless otherwise specified, we follow the evaluation protocol of <ref type="bibr" target="#b25">[26]</ref> to facilitate fair comparisons. Specifically, we compare all approaches given the same fixed number of images (10 million). With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100, for more details we refer to the appendix.  differentiable data-augmentation <ref type="bibr" target="#b88">[89]</ref> and adaptive discriminator augmentation <ref type="bibr" target="#b31">[32]</ref>. We select the better performing augmentation strategy per model. For all baselines and datasets, we perform data amplification through x-flips. Projected GANs use the same generator and discriminator architecture and training hyperparameters (learning rate and batch size) for all experiments. For high-resolution image generation, additional upsampling blocks are included in the generator to match the desired output resolution. We carefully tune all hyper-parameters for both baselines for best results: we find that FastGAN is sensitive to the choice of batch size, and StyleGAN2-ADA to the learning rate and R1 penalty. The appendix documents additional implementation details used in each of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Convergence Speed and Data Efficiency</head><p>Following <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b84">[85]</ref>, we analyze the training properties of Projected GANs on LSUN-Church at an image resolution of 256 2 pixels and on the 70k CLEVR dataset <ref type="bibr" target="#b28">[29]</ref>. In this section, we also train longer than 10 M images if necessary, as we are interested in convergence properties.</p><p>Convergence Speed. We apply projected GAN training for both the style-based generator of Style-GAN2 and the standard generator with a single input noise vector of FastGAN. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>   For Projected GAN, we directly observe the emergence of structure which becomes more detailed over time. Interestingly, the Projected GAN latent space appears to be very volatile, i.e., for fixed z the images undergo significant perceptual changes during training. In the non-projected cases, these changes are more gradual. We hypothesize that this induced volatility might be due to the discriminator providing more semantic feedback compared to conventional RGB losses. Such semantic feedback could introduce more stochasticity during training which in turn improves convergence and performance. We also observed that the signed real logits of the discriminator remain at the same level throughout training (see Appendix). Stable signed logits indicate that the discriminator does not suffer from overfitting.</p><p>Sample Efficiency. The use of pretrained models is generally linked to improved sample efficiency.</p><p>To evaluate this property, we also created two subsets of the 70k CLEVR dataset by randomly subsampling 10k and 1k images from it, respectively. As depicted in <ref type="figure" target="#fig_3">Fig. 4 (right)</ref>, our Projected GAN significantly improves over both baselines across all dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large Datasets</head><p>Besides CLEVR and LSUN-Church, we benchmark Projected GANs against various state-of-the-art models on three other large datasets: LSUN-Bedroom <ref type="bibr" target="#b83">[84]</ref> (3M indoor bedroom scenes), FFHQ <ref type="bibr" target="#b32">[33]</ref> (70k images of faces) and Cityscapes <ref type="bibr" target="#b8">[9]</ref> (25k driving scenes captured from a vehicle). For all datasets, we use an image resolution of 256 2 pixels. As Cityscapes and CLEVR images are not of aspect ratio 1:1 we resize them to 256 2 for training. Besides StyleGAN2-ADA and FastGAN, we compare against SAGAN <ref type="bibr" target="#b85">[86]</ref> and GANsformers <ref type="bibr" target="#b25">[26]</ref>. All models were trained for 10 M images.</p><p>For the large datasets, we also report numbers for StyleGAN2 trained for more than 10 M images to report the lowest FID values achieved in previous literature (denoted as StyleGAN2*). In the appendix, we report results on nine more large datasets.  <ref type="table" target="#tab_3">Table 3</ref>. Interestingly, when training longer on FFHQ (39 M Imgs), we observe further improvements of Projected GAN to an FID of 2.2. Note that all five datasets represent very different objects in various scenes. This demonstrates that the performance gain is robust to the choice of the dataset, although the feature network is trained only on ImageNet. It is important to note that the main improvements are based on improved sample diversity as indicated by recall which we report in the appendix. The improvement in diversity is most notable on large datasets, e.g., LSUN church, where the image fidelity appears to be similar to StyleGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Small Datasets</head><p>To further evaluate our method in the few-shot setting, we compare against StyleGAN2-ADA and FastGAN on art paintings from WikiArt (1000 images; wikiart.org), Oxford Flowers (1360 images) <ref type="bibr" target="#b55">[56]</ref>, photographs of landscapes (4319 images; flickr.com), AnimalFace-Dog (389 images) <ref type="bibr" target="#b71">[72]</ref> and Pokemon (833 images; pokemon.com). Further, we report results on high-resolution versions of Pokemon and Art-Painting (1024 2 ). Lastly, we evaluate on AFHQ-Cat, -Dog and -Wild at 512 2 <ref type="bibr" target="#b6">[7]</ref>. The AFHQ datasets contain ?5k closeups per category cat, dog, or wildlife. We do not have a license to re-distribute these datasets, but we provide the URLs to enable reproducibility, similar to <ref type="bibr" target="#b48">[49]</ref>.</p><p>Projected GAN outperforms all baselines in terms of FID values by a significant margin on all datasets and all resolutions as shown in <ref type="table" target="#tab_3">Table 3</ref>. Remarkably, our model beats the prior state-of-the-art on all datasets (256 2 ) after observing fewer than 600k images. For AnimalFace-Dog, the Projected GAN surpasses the previously best FID after only 20k images. One might argue that the EfficientNet used as feature network facilitates data generation for the animal datasets as EfficientNet is trained on ImageNet which contains many animal classes (e.g., 120 classes for dog breeds). However, it is interesting to observe that Projected GANs also achieve state-of-the-art FID on Pokemon and Art Painting though these datasets differ significantly from ImageNet. This evidences the generality of ImageNet features. For the high-resolution datasets, Projected GANs achieve the same FID value many times faster than the best baselines, e.g., ten times faster than StyleGAN2-ADA on AFHQ-Cat or four times faster than FastGAN on Pokemon. We remark that F and D l generalize to any resolution as they are fully convolutional.    While we achieve low FID on all datasets, we also identify two systematic failure cases: As depicted in <ref type="figure">Fig. 7</ref>, we sometimes observe "floating heads" on AFHQ. In a few samples, the animals appear in high quality but resemble cutouts on blurry or bland backgrounds. We hypothesize that generating a realistic background and image composition is less critical when a prominent object is already depicted. This hypothesis follows from the fact that we used image classification models for the projection, which have been shown to only marginally reduce in accuracy when applied on images of objects with removed background <ref type="bibr" target="#b82">[83]</ref>. On FFHQ, projected GAN sometimes produces poor-quality samples with wrong proportions and artifacts, even at state-of-the-art FID, see <ref type="figure">Fig. 8</ref>.</p><p>In terms of generators, StyleGAN is more challenging to tune and does not profit as much from projected training. The FastGAN generator is fast to optimize but simultaneously produced unrealistic samples in some parts of the latent space -a problem that could be solved by a mapping network similar to StyleGAN. Hence, we speculate that unifying the strengths of both architectures in combination with projected training might improve performance further. Moreover, our study of different pretrained networks indicates that efficient models are especially suitable for projected GAN training. Exploring this connection in-depth, and in general, determining desirable feature space properties opens up exciting new research opportunities. Lastly, our work advances efficiency for generative models. More efficient models lower the barrier of computational effort needed for generating realistic images. A lower barrier facilitates malignant use of generative models (e.g., "deep fakes") while simultaneously also democratizing research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Proofs</head><p>As described in the paper, Projected GAN training can be formulated as follows</p><formula xml:id="formula_5">min G max {D l } l?L E x [log D l (P l (x))] + E z [log(1 ? D l (P l (G(z))))]<label>(3)</label></formula><p>where {D l } is a set of independent discriminators operating on different feature projections. In the following, we first prove Theorem 1 for a deterministic projection. The second proof demonstrates the theorem's validity when training with stochastic differentiable augmentations.</p><p>Proof (deterministic). The following proof follows the consistency proofs in <ref type="bibr" target="#b53">[54]</ref> and <ref type="bibr" target="#b19">[20]</ref>. Let {P l } l?L be a set of fixed feature projectors. Furthermore, let P T be the density of the true data distribution and P G the density of the distribution the generator G produces. As in Theorem 1, P l ? T and P l ? G are functional compositions of P l and the true/generated data distribution. The minimax objective in <ref type="formula" target="#formula_5">(3)</ref> is then defined via</p><formula xml:id="formula_6">min G max {D l } l?L V l (D l , G) (4) where V l (D l , G) = E x?P T [log D l (P l (x))] + E x?P G [log (1 ? D l (P l (x)))] = E y?P P l ?T [log D l (y)] + E y?P P l ?G [log (1 ? D l (y))] = y P P l ?T (y) log(D l (y)) + P P l ?G (y) log(1 ? D l (y))dy<label>(5)</label></formula><p>In the following we derive the optimal discriminator for a fixed G. For any (a, b) ? R 2 \ {(0, 0)}, the function t ? a log(t)+b log(1?t) obtains its maximum in [0, 1] at a a+b <ref type="bibr" target="#b19">[20]</ref>. Since the discriminators do not need to be defined outside supp (P</p><formula xml:id="formula_7">P l ?T ) ? supp (P P l ?G ), the maximum max {D l } V l (D l , G) is achieved for D * l,G (y) = P P l ?T (y) P P l ?T (y) + P P l ?G (y)<label>(6)</label></formula><p>where G is fixed. Assuming a perfect discriminator, the minimax objective can be reformulated as</p><formula xml:id="formula_8">C(G) = max {D l } l V l (G, D l ) = l V l (G, D * l,G )<label>(7)</label></formula><p>Following the arguments of <ref type="bibr" target="#b19">[20]</ref> and in <ref type="bibr" target="#b53">[54]</ref>, we obtain</p><formula xml:id="formula_9">C(G) = ?2|L| log(2) + 2 l JSD(P P l ?T ||P P l ?G )<label>(8)</label></formula><p>where JSD is the Jensen-Shannon divergence. Since the Jensen-Shannon divergence is non-negative and zero only in case of equality, the minimum is achieved iff P P l ?T = P P l ?G for all l. This shows, that we achieve min G C(G) iff P P l ?T = P P l ?G for all l.</p><p>Proof (stochastic). We now show that the result above still holds when applying stochastic differentiable augmentations before the feature projections. Utilizing a stochastic augmentation f ?,l before the projection through P l , can be viewed as a functional composition, i.e. P ?,l = P l ? f ?,l . The parameter ? ? P ? encompasses both the probability of applying the augmentation and its parameters, e.g., translation direction and magnitude. As in the deterministic case, the minimax objective is defined as</p><formula xml:id="formula_10">min G max {D l } l?L V l (D l , G)<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">V l (D l , G) = E x?P T [E ??P? [log D l (P ?,l (x))]] + E x?P G [E ??P? [log (1 ? D l (P ?,l (x)))]] = E ??P? E x?P T [log D l (P ?,l (x))] + E x?P G [log (1 ? D l (P ?,l (x)))] = E ??P? E y?P P ?,l ?T [log D l (y)] + E y?P P ?,l ?G [log (1 ? D l (y))]</formula><p>= E ??P? y P P ?,l ?T (y) log(D l (y)) + P P ?,l ?G (y) log(1 ? D l (y))dy = y E ??P? [P P ?,l ?T (y)] log(D l (y)) + E ??P? [P P ?,l ?G (y)] log(1 ? D l (y))dy</p><p>Using the same arguments as above, we obtain that the maximum max {D l } V l (D l , G) is achieved for</p><formula xml:id="formula_13">D * l,G (y) = E ??P? [P P ?,l ?T (y)] E ??P? [P P ?,l ?T (y)] + E ??P? [P P ?,l ?G (y)]<label>(11)</label></formula><p>where G is fixed. Note that E ??P? [P P ?,l ?T ] and E ??P? [P P ?,l ?G ] are densities. Similar to above, we</p><formula xml:id="formula_14">obtain min G C(G) iff E ??P ?,l [P P l ?T ] = E ??P? [P P ?,l ?G ] for all l.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Additional Metrics and Datasets</head><p>Metrics. In addition to FID and Imgs reported in the paper, we compute the following metrics:</p><p>? Kernel Inception Distance (KID) <ref type="bibr" target="#b3">[4]</ref>. KID is an unbiased alternative to FID, hence, especially suitable for small datasets. ? Precision and Recall <ref type="bibr" target="#b65">[66]</ref>. Precision measures the quality of samples, and recall measures image diversity. Generally, GANs produce high-quality samples while being prone to mode collapse (high precision, low recall), compared to VAEs <ref type="bibr" target="#b38">[39]</ref>, which generate lower quality but more diverse samples (low precision, high recall). This observation is evidenced empirically in <ref type="bibr" target="#b65">[66]</ref>. For our evalution, we use the improved formulation by <ref type="bibr" target="#b43">[44]</ref>. ? SwAV-FID <ref type="bibr" target="#b52">[53]</ref>. Instead of utilizing an image classifier feature space, SwAV-FID uses selfsupervised representations. More specifically, SwAV-FID computes the Fr?chet distance in the penultimate layer of a ResNet-50 trained with the contrastive SwAV objective <ref type="bibr" target="#b5">[6]</ref>. Morozov et al. <ref type="bibr" target="#b52">[53]</ref> show that in some cases, SwAV-FID is more consistent with human judgment of visual quality than FID. ? CLIP-FID &amp; Virtex-FID. FID uses an Inception network trained on ImageNet. Our feature network F has also been trained on ImageNet. To rule out training data as a confounding factor between F and the evaluation metric, we propose to use FID with non-ImageNet features. We evaluate CLIP-FID (using a ResNet50 trained with the CLIP objective on the dataset collected by <ref type="bibr" target="#b59">[60]</ref>) and VirTex-FID (using a ResNet50 trained on COCO Captions with the VirTex objective <ref type="bibr" target="#b10">[11]</ref>). ? Sliced Wasserstein Distance (SWD). SWD is a non-neural metric that computes the Wasserstein distance between local image patches drawn from a Laplacian pyramid. We follow the evaluation protocol proposed by <ref type="bibr" target="#b30">[31]</ref>.</p><p>In addition to the metrics above, we conduct a human preference study with 28 participants unfamiliar with our method. The structure of the study is as follows:</p><p>? For each dataset, we first show samples of the real dataset for context. ? We then present two sample sheets and ask the participants to rank the sheets relative to each other based on sample fidelity and diversity. We define these as follows: (i) Fidelity is the degree to which the generated samples resemble the real ones. (ii) Diversity measures whether the generated samples cover the full variability of the real samples. Results. On all datasets, KID, SwaV-FID, CLIP-FID, VirTex-FID, and SWD mirror the ranking obtained via FID. The low SwAV-FIDs indicate that Projected GAN's low FIDs are not due to correlations between the feature network used for projection and the inception network used in FID.</p><p>On the large datasets, the baselines only outperform Projected GAN in precision on FFHQ, Cityscapes, and LSUN Church <ref type="table" target="#tab_7">(Table 4</ref>). However, when comparing recall in these cases, it is apparent that the baselines suffer from mode collapse. The high recall generally obtained by Projected GAN hints at the reason for its superior performance on FID, KID, and SwAV-FID: the generated images are very diverse. Hence, we conclude that Projected GANs alleviate mode collapse. The sample diversity is also evident in the qualitative comparisons in Section C. On small datasets <ref type="table" target="#tab_8">(Table 5)</ref>, Projected GAN is outperformed in precision on Art Painting by FastGAN; however, FastGANs very low recall of 0.044 hints at mode collapse. Only on flowers, Projected GAN appears to cover fewer modes than the baselines as indicated by lower recall.</p><p>At high resolutions <ref type="table" target="#tab_9">(Table 6)</ref>, Projected GAN performs slightly worse in precision. It appears that Projected GAN incurs small losses in image quality while obtaining a better mode coverage, which can be observed in the quantitative comparisons, e.g., on AFHQ-Cat, some samples exhibit artifacts. These artifacts indicate that projected GAN training at higher resolutions warrants closer inspection.</p><p>On small datasets, overfitting is a problem that is not detected well by FID and other metrics <ref type="bibr" target="#b63">[64]</ref>. Therefore, it is instructive to inspect latent interpolations for which we refer the reader to the supplementary videos. Projected GAN generates smooth interpolations between random samples on all datasets suggesting that it generalizes rather than memorizing training samples.</p><p>The results of the human preference study are shown in <ref type="table" target="#tab_10">Table 7</ref>. The study results largely agree with the results obtained via FID. On FFHQ, the study demonstrates our reported failure case for projected GANs. Interestingly, on AnimalFace projected GAN outperforms real data. We hypothesize that this is because for AnimalFace there is a significant portion of low-quality images (blurry, compression artifacts) in the dataset, and possibly projected GAN generates fewer of those. Of course, human studies are not optimal, as it is not straightforward to evaluate sample diversity -which is a strong suit of projected GANs -given only a few samples. <ref type="table" target="#tab_11">Table 8</ref> reports the FID achieved by Projected GAN for nine more datasets, all at a resolution of 256 2 . We compare on LSUN cat and horse <ref type="bibr" target="#b83">[84]</ref>, ADE indoor (a subset of ADE <ref type="bibr" target="#b90">[91]</ref> proposed in <ref type="bibr" target="#b2">[3]</ref>), the full Oxford flowers dataset with 8k images <ref type="bibr" target="#b55">[56]</ref>, KITTI-fisheye (a subset of KITTI-360 <ref type="bibr" target="#b45">[46]</ref>, consisting of fisheye images), STL-10 <ref type="bibr" target="#b7">[8]</ref>, CUB200 <ref type="bibr" target="#b79">[80]</ref>, Stanford Dogs <ref type="bibr" target="#b35">[36]</ref>, and Stanford Cars <ref type="bibr" target="#b41">[42]</ref>. We do not change the hyperparameters of Projected GAN. On each dataset, we report the lowest FID achieved in previous literature. We train FastGAN as a baseline for ADE indoor and KITTI-fisheye.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Qualitative Comparisons</head><p>We show generated images for CLEVR ( <ref type="figure" target="#fig_10">Fig. 9</ref>), FFHQ ( <ref type="figure">Fig. 10)</ref>, Cityscapes <ref type="figure">(Fig. 11</ref>), Bedroom ( <ref type="figure" target="#fig_1">Fig. 12)</ref>, Church <ref type="figure" target="#fig_2">(Fig. 13</ref>), Art Painting <ref type="figure" target="#fig_3">(Fig. 14, 19</ref>), Landscape <ref type="figure" target="#fig_4">(Fig. 15</ref>), AnimalFace Dog ( <ref type="figure" target="#fig_6">Fig. 16</ref>), Flowers <ref type="figure" target="#fig_18">(Fig. 17)</ref>, Pokemon <ref type="figure" target="#fig_1">(Fig. 18, 20)</ref>, AFHQ-Cat ( <ref type="figure" target="#fig_1">Fig. 21</ref>), AFHQ-Dog <ref type="figure" target="#fig_1">(Fig. 22)</ref>, and AFHQ-Wild ( <ref type="figure" target="#fig_1">Fig. 23</ref>). Following <ref type="bibr" target="#b31">[32]</ref>, we select a global seed per dataset. We do not perform truncation on any of the models. Projected GAN produces convincing results on all datasets. The sample diversity in particular is apparent in comparison to the baselines, e.g., on AFHQ-Cat or AFHQ-Wild, all baselines generate high-quality samples, but Projected GAN captures more variability of the training data.</p><p>StyleGAN2-ADA                . The images are selected randomly given one global random seed. We recommend zooming in for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Additional Experiments</head><p>This section presents additional experiments referenced in the paper. The experiments explore alternative setups to our final configuration entailing a pretrained, fixed feature network F , fixed 1x1 convolutions for cross-channel mixing (CCM), and fixed convolutions for cross-scale mixing (CSM). We follow the setup of Section 4 of the paper: training on LSUN church at a resolution of 256 2 , with a batch size of 64 for 1 million Imgs, four discriminators, and a EfficientNet-Lite1 as feature network F . Again, we report FID normalized by the FID obtained by a model with a standard single RGB image discriminator. Values &gt; 1 indicate worse performance than the RGB baseline. The results are summarized in <ref type="table" target="#tab_13">Table 9</ref>.   <ref type="table" target="#tab_13">Table 9</ref> demonstrate that randomly initializing F results in much higher FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration FID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Projection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCM.</head><p>We explore three different options for CCM. The first option, Feature Norm, does not mix features; rather, it normalizes the features to exhibit zero mean and a standard deviation of 1. This option investigates the importance of input scaling. The two other options utilize random convolution with different initializations. CCM-rotation is initialized with a random rotation matrix, CCM-Kaiming utilizes Kaiming initialization. As shown in <ref type="table" target="#tab_13">Table 9</ref>, CCM-Kaiming (0.77) improves over the baselines (Feature Norm (1.27), CCM-rotation (1.27)), over the RGB baseline (1.0), and pretrained F without projection <ref type="bibr">(1.15</ref>). This supports our hypothesis, that we need a sufficient amount of channel mixing.</p><p>CSM. We investigate if training the random projections P rand in CCM and CSM is advantageous for FID. We consider two options: training P rand before or during GAN training. First, we train P rand before GAN training, the feature network F remains fixed. For this purpose, we add a head on the last CSM layer to map back to full resolution and three output channels, forming an autoencoder architecture. This model trains with a denoising autoencoder loss on ImageNet: the input image is augmented with gaussian blur, JPEG compression, coarse and fine region dropout, and conversion to grayscale. All augmentations are applied with a probability of 0.5. The target for reconstruction is the nonaugmented image. We keep four models, at the beginning of training (t 0 ), at convergence (t 3 ), and two in between with equal spacing in terms of reconstruction loss. After autoencoder training, we use the model (without the additional head) for projected GAN training, denoted as Denoising AE (t i ). Interestingly, the longer the AE trained, the higher the FID, see <ref type="table" target="#tab_13">Table 9</ref>. For the second ablation, different parts of the projection stay fixed during GAN training: (i) training both F and P rand , (ii) fixed F , training only P rand , (iii) both F and P rand stay fixed. Again, the results in <ref type="table" target="#tab_13">Table 9</ref> suggest that training any part of the projection results in worse performance. We conclude that training the random projection is not advantageous, neither before nor during GAN training.</p><p>The signed real logits of the discriminator sign(D(x)) are the portion of the training set that gets positive discriminator outputs. Karras et al. <ref type="bibr" target="#b31">[32]</ref> find this a helpful heuristic for quantifying discriminator overfitting. The signed logits should remain constant, which they achieve via adapting the augmentation probability during training. <ref type="figure" target="#fig_1">Fig. 24</ref> shows that the logits of the RGB baseline steadily increase throughout training, whereas the logits remain mostly constant for Projected GAN training. This observation coincides with the finding that adaptive augmentation is unnecessary for Projected GANs as the logits are already stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imgs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Implementation Details</head><p>This section highlights the codebases we used and details hyperparameters and training configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Code and Compute</head><p>For dataset preparation, training, and evaluation, we build on top of the Stylegan2-ADA codebase <ref type="bibr" target="#b0">1</ref> . The evaluation employs the official pretrained Inception network to compute FID and KID. For SwAV-FID, we integrate the model of the SwaV-FID codebase 2 using weights by <ref type="bibr" target="#b5">[6]</ref>. SAGAN <ref type="bibr" target="#b85">[86]</ref> and Gansformers <ref type="bibr" target="#b25">[26]</ref> are trained in the Gansformers codebase 3 , we use their default hyperparameters. For the feature networks' implementation and weights, we use timm 4 , except for R50-CLIP 5 . Lastly, we utilize official implementations for differentiable augmentation 6 and FastGAN 7 .</p><p>We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX 6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Wall Clock-Time</head><p>Model sec/kimg STYLEGAN2-ADA 5.6 FASTGAN 7.2 PROJECTED GAN 10.1 On images at resolution 256 2 , the wall-clock training times measured in sec/kimg using 8 Quadro RTX 6000 are shown in <ref type="table" target="#tab_14">Table 10</ref>. StyleGAN2 is the fastest overall, which is expected as we enable mixed-precision and use the custom CUDA kernels provided by the authors. These are not available for the FastGAN generator; hence, Projected GAN can only be compared to FastGAN in a fair manner. FastGANs wall-clock times are higher because it uses a reconstruction loss on the discriminator features. This reconstruction loss adds computational overhead. In contrast, projected GANs exhibit lower wall-clock times as we do not need any regularization other than spectral normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Hyperparameters</head><p>Below, we describe the hyperparameter search for each method. For optimization, we always use Adam <ref type="bibr" target="#b36">[37]</ref> with ? 1 = 0, ? 2 = 0.99, and = 10 ?8 . All models employ exponential moving average for the generator weights <ref type="bibr" target="#b30">[31]</ref>.</p><p>entailing several heuristics for different hyperparameters. We find the automatic option very robust for both architecture and most hyperparameters. The exception is the R1 gradient penalty <ref type="bibr" target="#b49">[50]</ref>, which is highly dependent on the dataset. For all large datasets, the Gansformer codebase suggests suitable values for StyleGAN2. For the small datasets at 256 2 and 1024 2 , we perform a grid search over ? ? {1, 10, 20, 50}. We first train for 1 M Imgs, then continue training only the best one. For all AFHQ datasets, we use the same setting as <ref type="bibr" target="#b31">[32]</ref>. All experiments employ adaptive discriminator augmentation with the default target value of 0.6.</p><p>FastGAN. We replicate the generator and discriminator architecture of the official FastGAN codebase. FastGAN is robust to most hyperparameters; we always use a learning rate of 0.0002 and train with a hinge loss. The only sensitive hyperparameter with direct impact on performance is the batch size. Interestingly, FastGAN profits of smaller batch sizes. The default suggested by <ref type="bibr" target="#b48">[49]</ref> is a batch size of 8. We conduct a search over 8, 16, 32, and 64, a batch size of 16 further improves the results, while larger batch sizes decrease performance and even result in divergence in some cases. We employ differentiable augmentation <ref type="bibr" target="#b88">[89]</ref> of color, translation, and cutout.</p><p>Projected GAN. We use the same architecture, learning rate (0.0002), batch size <ref type="formula" target="#formula_7">(64)</ref>  <ref type="bibr" target="#b7">8</ref> . For all EfficientNets and ResNets, we use features at spatial resolutions r = {64 2 , 32 2 , 16 2 , 8 2 }, for DeiT and ViT, we use layers l = {3, 6, 9, 12}. The CSM blocks follow a residual design typically used in architectures for dense prediction <ref type="bibr" target="#b61">[62]</ref>. The lower-resolution feature is passed through a residual 3x3 convolution block; the higher-resolution feature is added, followed by a second residual block and bilinear upsampling, followed by a 1x1 convolution.</p><p>The discriminator architectures are shown in <ref type="table" target="#tab_16">Table 11</ref> where n i are the channels of the different feature network stages, c in and c out are the input and output channels of the DownBlock DB. A DownBlock consists of a convolution with k size k = 4 and stride s = 2, BatchNorm, and LeakyReLU with a slope of 0.2. We apply spectral normalization on all convolution layers Conv.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CCM (dashed blue arrows) employs 1?1 convolutions with random weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CSM (dashed red arrows) adds random 3?3 convolutions and bilinear upsampling, yielding a U-Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training Properties. Left: Projected FastGAN surpasses the best FID of StyleGAN2 (at 88 M images) after just 1.1 M images on LSUN-Church. Right: Projected FastGAN yields significantly improved FID scores, even when using subsets of CLEVR with 1k and 10k samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Training progress on LSUN church at 256 2 pixels. Shown are samples for a fixed noise vector z over k images. From top to bottom: FastGAN, StyleGAN2-ADA, Projected GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>shows samples for a fixed noise vector z during training on LSUN-Church. For both FastGAN and StyleGAN, patches of texture gradually morph into a global structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Real samples (top rows) vs. samples by Projected GAN (bottom rows). Datasets (top left to bottom right): CLEVR (256 2 ), LSUN church (256 2 ), Art Painting (256 2 ), Landscapes (256 2 ), AFHQ-wild (512 2 ), Pokemon (256 2 ), AFHQ-dog (512 2 ), AFHQ-cat (512 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6Figure 7 :Figure 8 :</head><label>78</label><figDesc>Discussion and Future Work "Floating Heads" Artifacts on FFHQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>?</head><label></label><figDesc>Each sheet contains nine random samples; we present three sample sheet pairs per dataset. ? Samples and comparison pairs are randomized per participant. We make sure that all possible pairings are equally represented. ? Evaluated Models: StyleGAN2-ADA, FastGAN, Projected GAN, and real data (for control) ? Evaluated datasets: all 256x256 datasets ? We count how often a given model wins a comparison and report the relative amount of wins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FID 10 . 17 -FastGANFID 3 . 24 -</head><label>1017324</label><figDesc>Recall 0.569 Recall 0.650 Projected GAN (ours) FID 0.89 -Recall 0.735</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Uncurated Results for CLEVR (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.StyleGAN2-ADA FID 7.32 -Recall 0.445 FastGAN FID 12.69 -Recall 0.184 Projected GAN (ours) FID 3.39 -Recall 0.464</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :FID 3 . 41 -</head><label>10341</label><figDesc>Uncurated Results for FFHQ (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.361</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :FID 1 . 52 -</head><label>11152</label><figDesc>Uncurated Results for Cityscapes (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.346</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :FastGANFID 8 . 43 -</head><label>12843</label><figDesc>Uncurated Results for LSUN bedroom (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. StyleGAN2-ADA FID 5.85 -Recall 0.416 Recall 0.207 Projected GAN (ours) FID 1.59 -Recall 0.438</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Uncurated Results for LSUN church (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.StyleGAN2-ADA FID 43.07 -Recall 0.218 FastGAN FID 44.02 -Recall 0.044 Projected GAN (ours) FID 27.96 -Recall 0.239</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :FID 6 . 92 -</head><label>14692</label><figDesc>Uncurated Results for Art Painting (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.258</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Uncurated Results for Landscape (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.StyleGAN2-ADA FID 60.90 -Recall 0.036 FastGAN FID 62.11 -Recall 0.015 Projected GAN (ours) FID 17.88 -Recall 0.095</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :FID 13 . 86 -</head><label>161386</label><figDesc>Uncurated Results for AnimalFace-Dog (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.058</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :</head><label>17</label><figDesc>Uncurated Results for Flowers (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.StyleGAN2-ADA FID 40.38 -Recall 0.197 FastGAN FID 81.86 -Recall 0.004 Projected GAN (ours) FID 26.36 -Recall 0.259</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :</head><label>18</label><figDesc>Uncurated Results for Pokemon (256 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 :</head><label>19</label><figDesc>Uncurated Results for Art Painting (1024 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison.StyleGAN2-ADA FID 56.76 -Recall 0.053 FastGAN FID 56.46 -Recall 0.080 Projected GAN (ours) FID 33.96 -Recall 0.215</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 :FID 2 . 16 -</head><label>20216</label><figDesc>Uncurated Results for Pokemon (1024 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.565</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 :FID 4 . 52 -</head><label>21452</label><figDesc>Uncurated Results for AFHQ-Cat (512 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.643</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 :FID 2 . 17 -</head><label>22217</label><figDesc>Uncurated Results for AFHQ-Dog (512 2 ). The images are selected randomly given one global random seed. We recommend zooming in for comparison. Recall 0.292</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 23 :</head><label>23</label><figDesc>Uncurated Results for AFHQ-Wild (512 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 24 :</head><label>24</label><figDesc>Signed Discriminator Logits. For this experiment, we project through F and train with up to four discriminators; we leave the augmentation probability constant. (|D i | = 1: red, |D i | = 2: blue, |D i | = 3: pink, |D i | = 4: green, RGB baseline: orange). For projected GAN training, the logits remain stable throughout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>= c1, cout = 64) DB(cin = c2, cout = 128) DB(cin = c3, cout = 256) DB(cin = c4, cout = 512) DB(cin = 64, cout = 128) DB(cin = 128, cout = 256) DB(cin = 256, cout = 512) Conv(cin = 512, cout = 1, k = 4) DB(cin = 128, cout = 256) DB(cin = 256, cout = 512) Conv(cin = 512, cout = 1, k = 4) DB(cin = 256, cout = 512) Conv(cin = 512, cout = 1, k = 4) Conv(cin = 512, cout = 1, k = 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(left), FastGAN converges quickly but saturates at a high FID. StyleGAN2 converges more slowly (88 M images) but reaches a lower FID. Projected GAN training improves both generators. Particularly for FastGAN, improvements in both convergence speed and final FID are significant while improvements for StyleGAN2 are less pronounced. Remarkably, Projected FastGAN reaches the previously best FID of StyleGAN2 after experiencing only 1.1 M images as compared to 88 M of StyleGAN2. In wall clock time, this corresponds to less than 3 hours instead of 5 days. Hence, from now on, we utilize the FastGAN generator and refer to this model simply as Projected GAN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>shows that the Projected GAN outperforms all state-of-the-art models in terms of FID values on all datasets by a large margin. For example, on LSUN-Bedroom, it achieves an FID value of 1.52 compared to 6.15 by GANsformer, the previously best model in this setting. Projected GAN achieves state-of-the-art FID values remarkably fast, e.g., on LSUN-church, it achieves an FID value of 3.18 after 1.1 M Imgs. StyleGAN2 has obtained the previously lowest FID value of 3.39 after 88 M Imgs, 80 times as many as needed by Projected GAN. Similar speed-ups are also realized for all other large datasets as shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ADA [32] 43.07 3.2 M 15.99 6.3 M 60.90 2.2 M 21.66 3.8 M 40.38 3.4 M FASTGAN [49] 44.02 0.7 M 16.44 1.8 M 62.11 0.2 M 26.23 0.8 M 81.86 2.5 M PROJECTED GAN 27.96 0.8 M 6.92 3.5 M 17.88 10 M 13.86 1.8 M 26.36 0.8 M PROJECTED GAN* 40.22 0.2 M 14.99 0.6 M 58.07 0.02 M 21.60 0.2 M 36.57 0.3 M</figDesc><table><row><cell></cell><cell cols="4">Large Datasets (256 2 )</cell></row><row><cell>CLEVR</cell><cell>FFHQ</cell><cell cols="2">Cityscapes</cell><cell>Bedroom</cell><cell>Church</cell></row><row><cell>SAGAN [86]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 M</cell></row><row><cell cols="2">STYLEGAN2* [32, 33, 85] 5.05 25 M 3.62 25 M</cell><cell>-</cell><cell>-</cell><cell cols="2">2.65 70 M 3.39 88 M</cell></row><row><cell></cell><cell cols="4">Small Datasets (256 2 )</cell></row><row><cell cols="2">Art Painting Landscape</cell><cell cols="2">AnimalFace</cell><cell>Flowers</cell><cell>Pokemon</cell></row><row><cell cols="2">STYLEGAN2-1024 2</cell><cell></cell><cell></cell><cell>512 2</cell></row><row><cell>Art Painting</cell><cell>Pokemon</cell><cell cols="2">AFHQ-Cat</cell><cell cols="2">AFHQ-Dog AFHQ-Wild</cell></row><row><cell cols="6">STYLEGAN2-ADA [32] 41.69 1.0 M 56.76 0.6 M 3.55 10 M 7.40 10 M 3.05 10 M</cell></row><row><cell>FASTGAN [49]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>).FID Imgs FID Imgs FID Imgs FID Imgs FID Imgs26.04 10 M 16.21 10 M 12.81 10 M 14.06 10 M 6.15 10 M STYLEGAN2-ADA [32] 10.17 10 M 7.32 10 M 8.35 10 M 11.53 10 M 5.85 10 M GANSFORMERS [26] 9.24 10 M 7.42 10 M 5.23 10 M 6.15 10 M 5.47 10 M FASTGAN [49] 3.24 10 M 12.69 10 M 8.78 1.8 M 8.24 4.8 M 8.43 8.9 M PROJECTED GAN 0.89 4.5 M 3.39 7.1 M 3.41 1.7 M 1.52 5.2 M 1.59 9.2 M PROJECTED GAN* 3.39 0.5 M 3.56 7.0 M 4.60 1.1 M 2.58 1.5 M 3.18 1.46.71 0.8 M 56.46 0.8 M 4.69 1.1 M 13.09 1.6 M 3.14 1.6 M PROJECTED GAN 32.07 0.9 M 33.96 1.3 M 2.16 3.7 M 4.52 3.8 M 2.17 5.4 M PROJECTED GAN* 40.33 0.2 M 53.74 0.2 M 3.53 1.0 M 7.10 0.9 M 3.03 1.6 M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Quantitative Results. Projected GAN* reports the point where our approach surpasses the state-of-the-art. StyleGAN2* obtains the lowest FID in previous literature if trained long enough.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Metrics on Large Datasets (256 2 ). Projected GAN compares favorably on most metrics. Exceptions are precision on FFHQ, Cityscapes, and LSUN Church. As argued by<ref type="bibr" target="#b34">[35]</ref>, shifting from precision to recall is generally desirable, since recall can be traded into precision via truncation.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Small Datasets (256 2 )</cell><cell></cell></row><row><cell></cell><cell>Art Painting</cell><cell cols="3">Landscape AnimalFace Flowers</cell><cell>Pokemon</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>43.07</cell><cell>15.99</cell><cell>60.90</cell><cell>21.66</cell><cell>40.38</cell></row><row><cell>FASTGAN [49]</cell><cell>44.02</cell><cell>16.44</cell><cell>62.11</cell><cell>26.23</cell><cell>81.86</cell></row><row><cell>PROJECTED GAN</cell><cell>27.96</cell><cell>6.92</cell><cell>17.88</cell><cell>13.86</cell><cell>26.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell>KID ? 10 3 ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>10.23</cell><cell>4.39</cell><cell>22.52</cell><cell>3.56</cell><cell>13.49</cell></row><row><cell>FASTGAN [49]</cell><cell>13.00</cell><cell>3.40</cell><cell>22.11</cell><cell>6.61</cell><cell>80.30</cell></row><row><cell>PROJECTED GAN</cell><cell>1.25</cell><cell>1.30</cell><cell>0.03</cell><cell>0.38</cell><cell>1.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P recision ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>0.691</cell><cell>0.709</cell><cell>0.841</cell><cell>0.731</cell><cell>0.735</cell></row><row><cell>FASTGAN [49]</cell><cell>0.858</cell><cell>0.768</cell><cell>0.849</cell><cell>0.611</cell><cell>0.731</cell></row><row><cell>PROJECTED GAN</cell><cell>0.762</cell><cell>0.774</cell><cell>0.998</cell><cell>0.816</cell><cell>0.809</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Recall ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>0.218</cell><cell>0.213</cell><cell>0.036</cell><cell>0.095</cell><cell>0.197</cell></row><row><cell>FASTGAN [49]</cell><cell>0.044</cell><cell>0.160</cell><cell>0.015</cell><cell>0.100</cell><cell>0.004</cell></row><row><cell>PROJECTED GAN</cell><cell>0.239</cell><cell>0.258</cell><cell>0.095</cell><cell>0.058</cell><cell>0.259</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SwAV ? F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>3.32</cell><cell>2.98</cell><cell>16.26</cell><cell>5.02</cell><cell>6.71</cell></row><row><cell>FASTGAN [49]</cell><cell>3.29</cell><cell>2.42</cell><cell>15.07</cell><cell>7.45</cell><cell>9.25</cell></row><row><cell>PROJECTED GAN</cell><cell>2.25</cell><cell>1.42</cell><cell>4.22</cell><cell>2.70</cell><cell>2.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CLIP ? F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>44.13</cell><cell>24.89</cell><cell>46.18</cell><cell>26.30</cell><cell>13.96</cell></row><row><cell>FASTGAN [49]</cell><cell>40.47</cell><cell>19.84</cell><cell>54.69</cell><cell>40.12</cell><cell>87.65</cell></row><row><cell>PROJECTED GAN</cell><cell>22.91</cell><cell>13.71</cell><cell>16.89</cell><cell>15.83</cell><cell>9.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell>V irT ex ? F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>4.15</cell><cell>2.78</cell><cell>8.83</cell><cell>3.25</cell><cell>3.69</cell></row><row><cell>FASTGAN [49]</cell><cell>5.72</cell><cell>3.86</cell><cell>9.41</cell><cell>4.08</cell><cell>17.49</cell></row><row><cell>PROJECTED GAN</cell><cell>3.53</cell><cell>1.98</cell><cell>3.79</cell><cell>2.19</cell><cell>2.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SW D ? 10 ?3 ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>25.55</cell><cell>19.06</cell><cell>22.31</cell><cell>14.04</cell><cell>14.73</cell></row><row><cell>FASTGAN [49]</cell><cell>21.94</cell><cell>29.87</cell><cell>29.23</cell><cell>17.39</cell><cell>46.81</cell></row><row><cell>PROJECTED GAN</cell><cell>11.44</cell><cell>15.38</cell><cell>14.34</cell><cell>9.61</cell><cell>11.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Metrics on Small Datasets (256 2 ). Projected GAN performs best on most metrics.</figDesc><table><row><cell></cell><cell cols="2">1024 2</cell><cell></cell><cell>512 2</cell><cell></cell></row><row><cell></cell><cell>Art Painting</cell><cell>Pokemon</cell><cell>AHFQ-Cat</cell><cell>AFHQ-Dog</cell><cell>AFHQ-Wild</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>41.69</cell><cell>56.76</cell><cell>3.55</cell><cell>7.40</cell><cell>3.05</cell></row><row><cell>FASTGAN [49]</cell><cell>46.71</cell><cell>56.46</cell><cell>4.69</cell><cell>13.09</cell><cell>3.14</cell></row><row><cell>PROJECTED GAN</cell><cell>32.07</cell><cell>33.96</cell><cell>2.16</cell><cell>4.52</cell><cell>2.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>KID ? 10 3 ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>26.59</cell><cell>15.31</cell><cell>0.63</cell><cell>1.21</cell><cell>0.47</cell></row><row><cell>FASTGAN [49]</cell><cell>12.70</cell><cell>29.40</cell><cell>1.72</cell><cell>5.51</cell><cell>0.74</cell></row><row><cell>PROJECTED GAN</cell><cell>1.70</cell><cell>7.76</cell><cell>0.16</cell><cell>0.80</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P recision ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>0.619</cell><cell>0.791</cell><cell>0.767</cell><cell>0.753</cell><cell>0.765</cell></row><row><cell>FASTGAN [49]</cell><cell>0.776</cell><cell>0.777</cell><cell>0.784</cell><cell>0.746</cell><cell>0.761</cell></row><row><cell>PROJECTED GAN</cell><cell>0.706</cell><cell>0.780</cell><cell>0.693</cell><cell>0.718</cell><cell>0.705</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Recall ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>0.168</cell><cell>0.053</cell><cell>0.411</cell><cell>0.470</cell><cell>0.137</cell></row><row><cell>FASTGAN [49]</cell><cell>0.033</cell><cell>0.080</cell><cell>0.305</cell><cell>0.380</cell><cell>0.201</cell></row><row><cell>PROJECTED GAN</cell><cell>0.235</cell><cell>0.215</cell><cell>0.565</cell><cell>0.643</cell><cell>0.292</cell></row><row><cell></cell><cell></cell><cell cols="2">SwAV ? F ID ?</cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>3.68</cell><cell>5.03</cell><cell>1.23</cell><cell>1.98</cell><cell>1.89</cell></row><row><cell>FASTGAN [49]</cell><cell>3.41</cell><cell>5.14</cell><cell>1.73</cell><cell>3.07</cell><cell>1.77</cell></row><row><cell>PROJECTED GAN</cell><cell>2.28</cell><cell>3.83</cell><cell>0.68</cell><cell>1.12</cell><cell>1.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Metrics on Small Datasets (512 2 and 1024 2 ). The results are in line with the findings at a resolution of 256 2 . Only with respect to precision, the baselines slightly outperform Projected GAN.</figDesc><table><row><cell></cell><cell cols="11">CLEVR FFHQ Cityscapes Bedroom Church ArtPainting Landscape AnimalFace Flowers Pokemon All Datasets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fidelity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>14 %</cell><cell>32 %</cell><cell>17 %</cell><cell>5 %</cell><cell>16 %</cell><cell>16 %</cell><cell>21 %</cell><cell>4 %</cell><cell>17 %</cell><cell>10 %</cell><cell>15 %</cell></row><row><cell>FASTGAN [49]</cell><cell>7 %</cell><cell>2 %</cell><cell>15 %</cell><cell>3 %</cell><cell>9 %</cell><cell>0 %</cell><cell>8 %</cell><cell>7 %</cell><cell>4 %</cell><cell>0 %</cell><cell>6 %</cell></row><row><cell>PROJECTED GAN</cell><cell>42 %</cell><cell>5 %</cell><cell>15 %</cell><cell>16 %</cell><cell>9 %</cell><cell>28 %</cell><cell>17 %</cell><cell>55 %</cell><cell>25 %</cell><cell>38 %</cell><cell>25 %</cell></row><row><cell>DATA</cell><cell>37 %</cell><cell>61 %</cell><cell>53 %</cell><cell>76 %</cell><cell>66 %</cell><cell>56 %</cell><cell>54 %</cell><cell>34 %</cell><cell>54 %</cell><cell>52 %</cell><cell>54 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Diversity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STYLEGAN2-ADA [32]</cell><cell>13 %</cell><cell>24 %</cell><cell>9 %</cell><cell>10 %</cell><cell>19 %</cell><cell>11 %</cell><cell>25 %</cell><cell>9 %</cell><cell>17 %</cell><cell>6 %</cell><cell>14 %</cell></row><row><cell>FASTGAN [49]</cell><cell>13 %</cell><cell>4 %</cell><cell>11 %</cell><cell>2 %</cell><cell>0 %</cell><cell>4 %</cell><cell>13 %</cell><cell>9 %</cell><cell>17 %</cell><cell>0 %</cell><cell>7 %</cell></row><row><cell>PROJECTED GAN</cell><cell>31 %</cell><cell>12 %</cell><cell>21 %</cell><cell>21 %</cell><cell>14 %</cell><cell>27 %</cell><cell>16 %</cell><cell>49 %</cell><cell>29 %</cell><cell>40 %</cell><cell>27 %</cell></row><row><cell>DATA</cell><cell>43 %</cell><cell>60 %</cell><cell>59 %</cell><cell>67 %</cell><cell>67 %</cell><cell>58 %</cell><cell>46 %</cell><cell>33 %</cell><cell>37 %</cell><cell>54 %</cell><cell>52 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Human Preference Study. The obtained results largely agree with the rankings of other metrics.</figDesc><table><row><cell></cell><cell>LSUN cat</cell><cell>LSUN horse</cell><cell>ADE indoor</cell><cell>Flowers Full</cell><cell>KITTI fisheye</cell></row><row><cell>Prev. SotA</cell><cell>5.57</cell><cell>2.57</cell><cell>30.33</cell><cell>19.60</cell><cell>6.64</cell></row><row><cell>(Approach)</cell><cell>(ADM [12])</cell><cell>(ADM [12])</cell><cell>(FastGAN [49])</cell><cell>(MSG-StyleGAN [30])</cell><cell>(FastGAN [49])</cell></row><row><cell>Projected GAN</cell><cell>3.89 STL-10</cell><cell>CUB200</cell><cell>Stanford Dogs</cell><cell>Stanford Cars</cell><cell></cell></row><row><cell>Prev. SotA</cell><cell>25.32</cell><cell>11.25</cell><cell>25.66</cell><cell>16.03</cell><cell></cell></row><row><cell>(Approach)</cell><cell>(TransGAN [27])</cell><cell>(FineGAN [74])</cell><cell>(FineGAN [74])</cell><cell>(FineGAN [74])</cell><cell></cell></row><row><cell>Projected GAN</cell><cell>13.68</cell><cell>2.79</cell><cell>11.75</cell><cell>2.09</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>FID on Additional Datasets (256 2 ) Without any hyperparameter changes, Projected GAN outperforms the previous state-of-the art on all evaluated datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Ablations.No Projection. In all experiments, we utilize pretrained representations. As a sanity check, it is instructive to test if the architectural bias of F alone is helpful. The results in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Training Speed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>, and hinge loss for all experiments at all resolutions. Compared to FastGAN, we see a slight improvement when increasing model capacity; we double the channel count in each dimension, from a base value of 64 to 128. The multipliers for the base value are as follows (resolution: multiplier): 4 2 : 16, 8 2 : 8, 16 2 : 4, 32 2 : 2, 64 2 : 2, 128 2 : 1, 256 2 : 0.5, 512 2 : 0.25, 1024 2 : 0.125. We did not observe similar improvements for FastGAN when increasing capacity. We employ differentiable augmentation<ref type="bibr" target="#b88">[89]</ref> of color, translation, and cutout. Lastly, to extract feature maps of intermediate layers of the feature networks, both CNNs and visual transformers, we follow the protocol presented in the MiDAS<ref type="bibr" target="#b61">[62]</ref> codebase</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Discriminator Architectures.</figDesc><table /><note>8 https://github.com/intel-isl/MiDaS</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">.17 6.70 3.86 2.72</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Baselines. We use StyleGAN2-ADA <ref type="bibr" target="#b31">[32]</ref> and FastGAN <ref type="bibr" target="#b48">[49]</ref> as baselines. StyleGAN2-ADA is the strongest model on most datasets in terms of sample quality, whereas FastGAN excels in training speed. We implement these baselines and our Projected GANs within the codebase provided by the authors of StyleGAN2-ADA <ref type="bibr" target="#b31">[32]</ref>. For each model, we ran two kinds of data augmentation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We acknowledge the financial support by the BMWi in the project KI Delta Learning (project number 19A19013O). Andreas Geiger was supported by the ERC Starting Grant LEGO-3D (850533). Kashyap Chitta was supported by the German Federal Ministry of Education and Research (BMBF): T?bingen AI Center, FKZ: 01IS18039B and the International Max Planck Research School for Intelligent Systems (IMPRS-IS). Jens M?ller received funding by the Heidelberg Collaboratory for Image Processing (HCI). We thank the Center for Information Services and High Performance Computing (ZIH) at Dresden University of Technology for generous allocations of computation time. Lastly, we would like to thank Vanessa Sauer for her general support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-objective training of generative adversarial networks with multiple discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Considine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<title level="m">Semantic bottleneck scene generation. arXiv.org</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Demystifying MMD gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments. arXiv.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno>2021. 21</idno>
		<title level="m">Diffusion models beat gans on image synthesis. arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online adaptative curriculum learning for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The fr?chet distance between multivariate normal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multivariate analysis</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unbalanced gans: Pre-training the generator of generative adversarial network using variational autoencoder. arXiv.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Generative adversarial transformers. arXiv.org</title>
		<imprint>
			<date type="published" when="1209" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv.org, 2021. 21</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Msg-gan: Multi-scale gradients for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. arXiv.org, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV) Workshops</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The gan landscape: Losses, architectures, regularization, and normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML) Workshops</title>
		<meeting>of the International Conf. on Machine learning (ICML) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>2109.13410</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<title level="m">Geometric gan. arXiv.org, 1705.02894</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards faster and stabilized gan training for highfidelity few-shot image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Freeze the discriminator: a simple baseline for fine-tuning gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On self-supervised image representations for GAN evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<title level="m">Stabilizing gan training with multiple random projections. arXiv.org, 1705.0783</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno>1612.00005</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Indian Conf. on Computer Vision, Graphics &amp; Image Processing</title>
		<meeting>Indian Conf. on Computer Vision, Graphics &amp; Image essing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Styleclip: Text-driven manipulation of stylegan imagery. arXiv.org</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>arXiv.org, 2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Enhancing photorealism enhancement. arXiv.org, 2105.04619</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Few-shot adaptation of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning implicit generative models by matching perceptual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dognin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Counterfactual generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A u-net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Semantic pyramid for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning hybrid image templates (hit) by information projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Image manipulation with perceptual discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Training dataefficient image transformers &amp; distillation through attention. arXiv.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">On data augmentation for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">17</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Transferring gans: generating images from limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno>2021. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno>1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv.org</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<title level="m">Dual contrastive loss and attention for gans. arXiv.org</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">On leveraging pretrained gans for generation with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Image augmentations for gan training. arXiv.org</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The official codebase supplies standard configurations of architectures and hyperparameters for different resolutions. Furthermore, an automatic configuration option is available</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ada</forename><surname>Stylegan2</surname></persName>
		</author>
		<ptr target="https://github.com/NVlabs/stylegan2-ada-pytorch2https://github.com/stanis-morozov/self-supervised-gan-eval/3https://github.com/dorarad/gansformer4https://github.com/rwightman/pytorch-image-models5https://github.com/openai/CLIP6https://github.com/mit-han-lab/data-efficient-gans" />
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
