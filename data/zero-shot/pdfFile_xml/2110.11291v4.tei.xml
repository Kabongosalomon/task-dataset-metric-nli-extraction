<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIKELIHOOD TRAINING OF SCHR?DINGER BRIDGE USING FORWARD-BACKWARD SDES THEORY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrong</forename><surname>Chen</surname></persName>
							<email>tianrong.chen@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
							<email>ghliu@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><forename type="middle">A</forename><surname>Theodorou</surname></persName>
							<email>evangelos.theodorou@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIKELIHOOD TRAINING OF SCHR?DINGER BRIDGE USING FORWARD-BACKWARD SDES THEORY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Schr?dinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory -a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE.</p><p>Published as a conference paper at ICLR 2022 two problems (i.e. both involve transforming distributions) is evident, and the additional flexibility from SB is also attractive. To enable SB-inspired generative training, however, previous works require either ad-hoc multi-stage optimization or retreat to traditional SB algorithms, e.g. Iterative Proportional Fitting (IPF; <ref type="bibr" target="#b13">Kullback (1968)</ref>). The underlying relation between the optimization principle of SB and modern generative training, in particular SGM, remains relatively unexplored, despite their intimately related problem formulations. More importantly, with the recent connection between SGM and log-likelihood computation <ref type="bibr" target="#b38">(Song et al., 2021)</ref>, it is crucial to explore whether there exists an alternative way of training SB that better respects, or perhaps generalizes, modern training of SGM, so as to solidify the suitability of SB as a principled generative model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head> <ref type="figure">Figure 1</ref><p>: Both Score-based Generative Model (SGM) and Schr?dinger Bridge (SB) transform between two distributions. While SGM requires pre-specifying the data-to-noise diffusion, SB instead learns the process.</p><p>Score-based Generative Model (SGM; ) is an emerging generative model class that has achieved remarkable results in synthesizing high-fidelity data <ref type="bibr" target="#b10">Kong et al., 2020a;</ref><ref type="bibr">b)</ref>. Like many deep generative models, SGM seeks to find nonlinear functions that transform simple distributions (typically Gaussian) into complex, often intractable, data distributions. In SGM, this is done by first diffusing data to noise through a stochastic differential equation (SDE); then learning to reverse this diffusion process by regressing a network to the score function (i.e. the gradient of the log probability density) at each time step <ref type="bibr">(Hyv?rinen &amp; Dayan, 2005)</ref>. This reversed process thereby defines the generation (see <ref type="figure">Fig. 1</ref>).</p><p>Despite its empirical successes, SGM admits few limitations. First, the diffusion process has to obey a simple form (e.g. linear or degenerate drift) in order to compute the analytic score function for the regression purpose. Secondly, the diffusion process needs to run to sufficiently large time steps so that the end distribution is approximate Gaussian <ref type="bibr" target="#b11">(Kong &amp; Ping, 2021)</ref>. For these reasons, SGM often takes a notoriously long time in generating data <ref type="bibr">(Jolicoeur-Martineau et al., 2021)</ref>, thereby limiting their practical usages compared to e.g. GANs or flow-based models <ref type="bibr" target="#b29">(Ping et al., 2020;</ref><ref type="bibr">Karras et al., 2020b)</ref>.</p><p>In the attempt to lift these restrictions, a line of recent works inspired by Schr?dinger Bridge (SB; <ref type="bibr" target="#b33">Schr?dinger (1932)</ref>) has been proposed <ref type="bibr" target="#b43">Wang et al., 2021;</ref><ref type="bibr" target="#b42">Vargas et al., 2021)</ref>. SB -as an entropy-regularized optimal transport problem -seeks two optimal policies that transform back-and-forth between two arbitrary distributions in a finite horizon. The similarity between the In this work, we present a fundamental connection between solving SB and training SGM. The difficulty arises immediately as one notices that the optimality condition of SB and the likelihood objective of SGM are represented by merely two distinct mathematical objects. While the former is characterized by two coupled partial differential equations (PDEs) <ref type="bibr" target="#b14">(L?onard, 2013)</ref>, the latter integrates over a notably complex SDE that resembles neither its diffusion nor reversed process <ref type="bibr" target="#b38">(Song et al., 2021)</ref>. Nevertheless, inspired by the recent advance on understanding deep learning through the optimal control perspective <ref type="bibr" target="#b15">(Li &amp; Hao, 2018;</ref><ref type="bibr" target="#b16">Liu et al., 2021a;</ref><ref type="bibr">b)</ref>, we show that Forward-Backward SDEs -a mathematical methodology appearing in stochastic optimal control for solving nonlinear PDEs <ref type="bibr">(Han et al., 2018)</ref> -paves an elegant way to connect the two objectives. The implication of our findings is nontrivial: It yields an exact log-likelihood expression of SB that precisely generalizes the one of SGM <ref type="bibr" target="#b38">(Song et al., 2021)</ref> to fully nonlinear diffusion, thereby providing novel theoretical connections between the two model classes. Algorithmically, our framework suggests rich training procedures that resemble the joint optimization for diffusion flow-based models <ref type="bibr" target="#b45">(Zhang &amp; Chen, 2021)</ref> or more traditional IPF approaches <ref type="bibr" target="#b13">(Kullback, 1968;</ref>. This allows one to marry the best of both worlds by improving the SB training with e.g. a SGM-inspired Langevin corrector <ref type="bibr" target="#b35">(Song &amp; Ermon, 2019)</ref>. The resulting method, SB-FBSDE, generates encouraging images on MNIST, CelebA, and CIFAR10 and outperforms prior optimal transport models by a large margin.</p><p>Our method differs from the concurrent SB methods <ref type="bibr" target="#b42">Vargas et al., 2021)</ref> in various aspects. First, while both prior methods rely on solving SB with mean-matching regression, our SB-FBSDE instead utilizes a divergence-based objectives (see ?3.2). Secondly, neither of the prior methods focuses on log-likelihood training, which is the key finding in SB-FBSDE to bridge connections to SGM and adopt modern training improvements. Indeed, due to the difference in the underlying SDE classes, 1 their connections to SGM can only be made after time discretization by carefully choosing each step size . In contrast, our theoretical connection is derived readily in continuous-time; hence unaffected by the choice of numerical discretization.</p><p>In summary, we present the following contributions.</p><p>? We present a novel computational framework, grounded on Forward-Backward SDEs theory, for computing the log-likelihood objectives of Schr?dinger Bridge (SB) and solidifying their theoretical connections to Score-based Generative Model (SGM). ? Our framework suggests a new training principle that retains the mathematical flexibility from SB while enjoying advanced techniques from the modern generative training of SGM. ? We show that the resulting method -named SB-FBSDE -outperforms previous optimal transportinspired baselines on synthesizing high-fidelity images and is comparable to other existing models.</p><p>Notation. We denote p SDE t (X t ) as the marginal density driven by some SDE process X(t) ? X t until the time step t ? [0, T ]. The time direction is aligned throughout this article such that p 0 and p T respectively correspond to the data and prior distributions. The gradient, divergence, and Hessian of a function f (x), where x ? R n , will be denoted as ?</p><formula xml:id="formula_0">x f ? R n , ? x ? f ? R, and ? 2 x f ? R n?n . 2 PRELIMINARIES 2.1 SCORE-BASED GENERATIVE MODEL (SGM)</formula><p>Given a data point X 0 ? R n sampled from an unknown data distribution p data , SGM first progressively diffuses the data towards random noise with the following forward SDE:</p><formula xml:id="formula_1">dX t = f (t, X t )dt + g(t)dW t , X 0 ? p data ,<label>(1)</label></formula><p>where s t ? s(t, x; ?) and the expectation is taken over the SDE (1) given a data point X 0 = x 0 . This objective (3) suggests a principled choice of ?(t) := g(t) 2 . After training, SGM simply substitutes the score function with the learned score network s(t, x; ?) to generate data from p prior ,</p><formula xml:id="formula_2">dX t = [f ? g 2 s(t, X t ; ?)]dt + g dW t , X T ? p prior .<label>(4)</label></formula><p>It is important to notice that p prior needs not equal p (1) T in practice, and the approximation is close only through a careful design of (1). Notably, designing the diffusion g(t) can be particularly problematic as it affects both the approximation p (1) T ? p prior and the training via the weighting ?(t); hence can lead to unstable training <ref type="bibr" target="#b23">(Nichol &amp; Dhariwal, 2021)</ref>. In contrast, Schr?dinger Bridge considers a more flexible framework for designing the forward diffusion that requires minimal manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SCHR?DINGER BRIDGE (SB)</head><p>Following the dynamic expression of SB <ref type="bibr" target="#b26">(Pavon &amp; Wakolbinger, 1991;</ref><ref type="bibr" target="#b8">Dai Pra, 1991)</ref>, consider min Q?P(pdata,pprior)</p><formula xml:id="formula_3">D KL (Q || P),<label>(5)</label></formula><p>where Q ? P(p data , p prior ) belongs to a set of path measure with p data and p prior as its marginal densities at t = 0 and T . On the other hand, P denotes a reference measure, which we will set to the path measure of (1) for later convenience. The optimality condition to (5) is characterized by two PDEs that are coupled through their boundary conditions. We summarize the related result below. Theorem 1 (SB optimality; ; <ref type="bibr" target="#b26">Pavon &amp; Wakolbinger (1991)</ref>; <ref type="bibr" target="#b2">Caluya &amp; Halder (2021)</ref>). Let ?(t, x) and ?(t, x) be the solutions to the following PDEs:</p><formula xml:id="formula_4">?? ?t = ?? x ? T f ? 1 2 Tr(g 2 ? 2 x ?) ? ? ?t = ?? x ? ( ?f )+ 1 2 Tr(g 2 ? 2 x ?) s.t. ?(0, ?) ?(0, ?) = p data , ?(T, ?) ?(T, ?) = p prior (6)</formula><p>Then, the solution to the optimization (5) can be expressed by the path measure of the following forward (7a), or equivalently backward (7b), SDE:</p><formula xml:id="formula_5">dX t = [f + g 2 ? x log ?(t, X t )]dt + g dW t , X 0 ? p data ,<label>(7a)</label></formula><formula xml:id="formula_6">dX t = [f ? g 2 ? x log ?(t, X t )]dt + g dW t , X T ? p prior ,<label>(7b)</label></formula><p>where ? x log ?(t, X t ) and ? x log ?(t, X t ) are the optimal forward and backward drifts for SB.  <ref type="figure">Figure 2</ref>: Schematic diagram of the our stochastic optimal control interpretation, and how it connects the objective of SGM (3) and optimality of SB (6) through Forward-Backward SDEs theory.</p><p>Similar to the forward/backward processes in SGM, the stochastic processes of SB in <ref type="formula" target="#formula_5">(7a)</ref> and <ref type="formula" target="#formula_6">(7b)</ref> are also equivalent in the sense that ?t</p><formula xml:id="formula_7">? [0, T ], p (7a) t ? p (7b) t ? p SB t .</formula><p>In fact, its marginal density obeys a factorization principle:</p><formula xml:id="formula_8">p SB t (X t ) = ?(t, X t ) ?(t, X t ).</formula><p>To construct the generative pipeline from (7b), one requires solving the PDEs in (6) to obtain ?. Unfortunately, these PDEs are hard to solve even for low-dimensional systems <ref type="bibr" target="#b31">(Renardy &amp; Rogers, 2006)</ref>; let alone for generative applications. Indeed, previous works either have to replace the original Schr?dinger Bridge (p data p prior ) with multiple stages, p data p middle p prior , so that each segment admits an analytic solution <ref type="bibr" target="#b43">(Wang et al., 2021)</ref>, or consider the following half-bridge (p data ? p prior vs. p data ? p prior ) optimization <ref type="bibr" target="#b42">Vargas et al., 2021)</ref>,</p><formula xml:id="formula_9">Q (1) := arg min Q?P(?,pprior) D KL (Q || Q (0) ), Q (0) := arg min Q?P(pdata,?) D KL (Q || Q (1) )</formula><p>which can be solved with IPF algorithm <ref type="bibr" target="#b13">(Kullback, 1968)</ref> starting from Q (0) := P. In the following section, we will present a scalable computational framework for solving the optimality PDEs in (6) and show that it paves an elegant way connecting the optimality principle of SB (6) to the parameterized log-likelihood of SGM (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We motivate our approach starting from some control-theoretic observation (see <ref type="figure">Fig. 2</ref>). Notice that both SGM and SB consist of forward and backward SDEs with similar structures. From the stochastic control perspective, these SDEs belong to the class of control-affine SDEs with additive noise:</p><formula xml:id="formula_10">dX t = A(t, X t )dt + B(t, X t )u(t, X t )dt + C(t) dW t .<label>(8)</label></formula><p>It is clear that the control-affine SDE (8) includes all SDEs (1,2,4,7) appearing in ?2 by considering (A, B, C) := (f, I, g) and different interpretations of the control variables u(t, X t ). This implies that the optimization processes of both SGM and SB can be aligned through the lens of stochastic optimal control (SOC). Indeed, both problems can be interpreted as seeking some time-varying control policy, either the score function ? x log p t|x0 in SGM or ? x log ? in SB, that minimizes some objectives, (3) vs. (5), while subjected to some control-affine SDEs, (1,2) vs. <ref type="bibr">(7)</ref>. In what follows, we will show that a specific mathematical methodology in nonlinear SOC literature -called Forward-Backward SDEs theory (FBSDEs; see <ref type="bibr" target="#b19">Ma et al. (1999)</ref>) -links the optimality condition of SB (6) to the log-likelihood objectives of SGM (3). All proofs are left to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FORWARD-BACKWARD SDES (FBSDES) REPRESENTATION FOR SB</head><p>The theory of FBSDEs establishes an innate connection between different classes of PDEs and forward-backward SDEs. Below we introduce the following connection related to our problem. Lemma 2 (Nonlinear Feynman-Kac; 3 Exarchos &amp; Theodorou (2018)). Consider the coupled SDEs</p><formula xml:id="formula_11">dX t = f (t, X t )dt + G(t, X t )dW t , X 0 = x 0 dY t = ?h(t, X t , Y t , Z t )dt + Z(t, X t ) T dW t , Y T = ?(X T )<label>(9a)</label></formula><p>where the functions f , G, h, and ? satisfy proper regularity conditions 4 so that there exists a pair of unique strong solutions satisfying (9). Now, consider the following second-order parabolic PDE and suppose v(t, x) ? v is once continuously differentiable in t and twice in x, i.e. v ? C 1,2 ,</p><formula xml:id="formula_13">?v ?t + 1 2 Tr(? 2 x v GG T ) + ? x v T f + h(t, x, v, G T ? x v) = 0, v(T, x) = ?(x),<label>(10)</label></formula><p>then the solution to (9) coincides with the solution to (10) along paths generated by the forward SDE (9a) almost surely, i.e., the following stochastic representation (known as the nonlinear Feynman-Kac relation) is valid:</p><formula xml:id="formula_14">v(t, X t ) = Y t and G(t, X t ) T ? x v(t, X t ) = Z t .<label>(11)</label></formula><p>Lemma 2 states that solutions to a certain class of nonlinear (via the function h in (10)) PDEs can be represented by solutions to a set of forward-backward SDEs (9) through the transformation (11), and this relation can be extended to the viscosity case <ref type="bibr" target="#b24">(Pardoux &amp; Peng (1992)</ref>; see also Appendix B). Note that Y t is the solution to the backward SDE (9b) whose randomness is driven by the forward SDE (9a). Indeed, it is clear from (11) that Y t (hence also Z t ) is a time-varying function of X t . Since the v appearing in the nonlinear Feynman-Kac relation (11) takes the random vector X t as its argument, v(t, X t ) shall also be understood as a random variable. Finally, it is known that the original (deterministic) PDE solution v(t, x) can be recovered by taking conditional expectation, i.e.</p><formula xml:id="formula_15">v(t, x) = E Xt?(9a) [Y t |X t = x] and G(t, x) T ? x v(t, x) = E Xt?(9a) [Z t |X t = x].<label>(12)</label></formula><p>Since it is often computationally favorable to solve SDEs rather than PDEs, Lemma 2 has been widely used as a scalable method for solving high-dimensional PDEs <ref type="bibr">(Han et al., 2018;</ref><ref type="bibr" target="#b27">Pereira et al., 2019)</ref>. Take SOC applications for instance, their PDE optimality condition can be characterized by (11) under proper conditions, with the optimal control given in the form of Z t . Hence, one can adopt Lemma 2 to solve the underlying FBSDEs, rather than the original PDE optimality, for the optimal control. Despite seemingly attractive, whether these principles can be extended to SB, whose optimality conditions are given by two coupled PDEs in (6), remains unclear. Below we derive a similar FBSDEs representation for SB. Theorem 3 (FBSDEs to SB optimality (6)). Consider the following set of coupled SDEs,</p><formula xml:id="formula_16">? ? ? ? ? ? ? ? ? ? ? ? ? dX t = (f + gZ t ) dt + gdW t dY t = 1 2 Z T t Z t dt + Z T t dW t d Y t = 1 2 Z T t Z t + ? x ? (g Z t ? f ) + Z T t Z t dt + Z T t dW t (13a) (13b)<label>(13c)</label></formula><p>where f and g satisfy the same regularity conditions in Lemma 2 (see Footnote 4), and the boundary conditions are given by X(0) = x 0 and Y T + Y T = log p prior (X T ). Suppose ?, ? ? C 1,2 , then the nonlinear Feynman-Kac relations between the FBSDEs (13) and PDEs (6) are given by</p><formula xml:id="formula_17">Y t ? Y(t, X t ) = log ?(t, X t ), Z t ? Z(t, X t ) = g? x log ?(t, X t ), Y t ? Y(t, X t ) = log ?(t, X t ), Z t ? Z(t, X t ) = g? x log ?(t, X t ).<label>(14)</label></formula><p>Furthermore, (Y t , Y t ) obey the following relation:</p><formula xml:id="formula_18">Y t + Y t = log p SB t (X t ).</formula><p>The FBSDEs for SB (13) share a similar forward-backward structure as in (9), where (13a) and (13b,13c) respectively represent the forward and backward SDEs. One can verify that the forward SDE (13a) coincides with the optimal forward SDE (7a) with the substitution Z t = g? x log ?.</p><p>In other words, these FBSDEs provide a local representation of log ? and log ? evaluated on the optimal path governed by (7a). Since Z t and Z t can be understood as the forward/backward policies, in a similar spirit of policy-based methods <ref type="bibr" target="#b28">(Pereira et al., 2020;</ref><ref type="bibr" target="#b34">Schulman et al., 2015)</ref>, that guide the SDE processes of SB, they sufficiently characterize the SB model. Hence, our next step is to derive a proper training objective to optimize these policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LOG-LIKELIHOOD COMPUTATION OF SB</head><p>Theorem 3 has an important implication: It suggests that given a path sampled from the forward SDE (13a), the solutions to the backward SDEs (13b,13c) at t = 0 provide an unbiased estimation of the log-likelihood of the data point</p><formula xml:id="formula_19">x 0 , i.e. E Y 0 + Y 0 |X 0 = x 0 = log p SB 0 (x 0 ) = log p data (x 0 ), where X t is sampled from (13a)</formula><p>. We now state our main result, which makes this observation formal: Theorem 4 (Log-likelihood of SB model). Given the solution satisfying the FBSDE system in (13), the log-likelihood of the SB model (Z t , Z t ), at a data point x 0 , can be expressed as</p><formula xml:id="formula_20">log p SB 0 (x 0 ) = E [log p T (X T )] ? T 0 E 1 2 Z t 2 + 1 2 Z t ? g? x log p SB t + Z t 2 ? 1 2 g? x log p SB t ? Z t 2 ?? x ? f dt (15) = E [log p T (X T )] ? T 0 E 1 2 Z t 2 + 1 2 Z t 2 + ? x ? (g Z t ? f ) + Z T t Z t dt, (16)</formula><p>where the expectation is taken over the forward SDE (13a) with the initial condition X 0 = x 0 .</p><p>Similar to <ref type="formula">(3)</ref>, Theorem 4 suggests a parameterized lower bound to the log-likelihoods, i.e.</p><formula xml:id="formula_21">log p SB 0 (x 0 ) ? L SB (x 0 ; ?, ?) where L SB (x 0 ; ?, ?) shares the same expression in (16) except that Z t ? Z(t, x; ?) and Z t ? Z(t, x; ?) are approximated with some parameterized models (e.g. DNNs). Note that ? x log p SB t is intractable in practice for any nontrivial (Z t , Z t ).</formula><p>Hence, we use the divergence-based objective in (16) as our training objective of both policies.</p><p>Connection to score-based models. Recall <ref type="figure">Fig. 2</ref> and compare the parameterized log-likelihoods of SB (16) to SGM (3); one can verify that L SB collapses to L SGM when (Z t , Z t ) := (0, g s t ). From the SB perspective, this occurs only when p (1) T = p prior . Since no effort is required in the forward process to reach p prior , the optimal forward control Z t , by definition, degenerates; thereby making the backward control Z t collapses to the score function. However, in any case when p (1) T = p prior , for instance when the diffusion SDEs are improperly designed, the forward policy Z t steers the diffusion process back to p prior , while its backward counterpart Z t compensates the reversed process accordingly. From this view, SB alleviates the problematic design in SGM by enlarging the class of diffusion processes to accept nonlinear drifts and providing an optimization principle on learning these processes. Moreover, Theorem 4 generalizes the log-likelihood training from SGM to SB.</p><p>Connection to flow-based models. Interestingly, the log-likelihood computation in Theorem 4, where we use a path {X t } t?[0,T ] sampled from a data point X 0 to parameterize its log-likelihood, resembles modern training of (deterministic) flow-based models <ref type="bibr">(Grathwohl et al., 2018)</ref>, which have recently been shown to admit a close relation to SGM <ref type="bibr">Gong &amp; Li, 2021)</ref>. The connection is built on the concept of probability flow -which suggests that the marginal density of an SDE can be evaluated through an ordinary differential equation (ODE). Below, we provide a similar flow representation for SB, further strengthening their connection to modern generative models. Corollary 5 (Probability flow for SB). The following ODE characterizes the probability flow of the optimal processes of SB (7) in the sense that ?t, p (17)</p><formula xml:id="formula_22">t ? p (7) t ? p SB t . dX t = f + gZ(t, X t ) ? 1 2 g (Z(t, X t ) + Z(t, X t )) dt<label>(17)</label></formula><p>One can verify (see Remark 10 in ?B) that computing the log-likelihood of this ODE model <ref type="formula" target="#formula_1">(17)</ref> using flow-based training techniques indeed recovers the training objective of SB derived in (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PRACTICAL IMPLEMENTATION</head><p>In this section, we detail the implementation of our FBSDE-inspired SB model, named SB-FBSDE.</p><p>Training process. We treat the log-likelihood in (16) as our training objective, where the divergence can be can be estimated efficiently following <ref type="bibr">Hutchinson (1989)</ref>. This immediately distinguishes SB-FBSDE from prior SB models <ref type="bibr" target="#b42">Vargas et al., 2021)</ref>, which instead rely on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Likelihood training of SB-FBSDE</head><p>Input: boundary distributions pdata and pprior, parameterized policies Z(?, ?; ?) and Z(?, ?; ?) repeat if memory resource is affordable then run Algorithm 2. else run Algorithm 3. end if until converges Algorithm 2 Joint (diffusion flow-based) training</p><formula xml:id="formula_23">for k = 1 to K do Sample X t?[0,T ] from (13a) where x0 ? pdata (computational graph retained). Compute L SB (x0; ?, ?) with (16). Update (?, ?) with ? ?,? L SB (x0; ?, ?). end for Algorithm 3 Alternate (IPF-based) training Input: Caching frequency M for k = 1 to K do if k%M == 0 then Sample X t?[0,T ] from (13a) where x0 ? pdata (computational graph discarded). end if Compute L SB (x0; ?) with (18). Update ? with gradient ? ? L SB (x0; ?). end for for k = 1 to K do if k%M == 0 then Sample X t?[0,T ] from (7b) where xT ? pprior (computational graph discarded). end if Compute L SB (xT ; ?) with (19). Update ? with gradient ? ? L SB (xT ; ?). end for</formula><p>regression-based objectives. 5 For low-dimensional datasets, we simply perform joint optimization, max L SB (x 0 ; ?, ?), to train the parameterized policies Z(?, ?; ?) and Z(?, ?; ?). For higher-dimensional (e.g. image) datasets, however, it can be prohibitively expensive to keep the entire computational graph. In these cases, we follow De Bortoli et al. <ref type="formula" target="#formula_1">(2021)</ref> by caching the sampled trajectories in a reply buffer and refreshing them in a lower frequency basis (around 1500 iterations). Although this implies that the gradient path w.r.t. ? will be discarded, we can leverage the symmetric structure of SB and re-derive the log-likelihood for the sampled noise, i.e. L SB (x T ), based on the backward trajectories sampled from (7b). We leave the derivation to Theorem 11 in ?B due to space constraint. This results in an alternate training between the following two objectives after dropping all unrelated terms,</p><formula xml:id="formula_24">L SB (x 0 ; ?) = ? T 0 E Xt?(7a) 1 2 Z(t, X t ; ?) 2 + g? x ? Z(t, X t ; ?) + Z T t Z(t, X t ; ?) dt, (18) L SB (x T ; ?) = ? T 0 E Xt?(7b) 1 2 Z(t, X t ; ?) 2 + g? x ? Z(t, X t ; ?) + Z T t Z(t, X t ; ?) dt. (19)</formula><p>Our training process is summarized in Alg. Generative process. While the generative processes for SB can be performed as simply as propagating (7b) given the trained policy Z(?, ?; ?), it has been constantly observed that adopting Langevin sampling to the generative process greatly improves performance . This procedure, often referred to as the Langevin corrector, requires knowing the score function ? x log p t . For SB, we can estimate its value by recalling (see ?2.2) that Z t + Z t = g? x log p SB t . This results in the following predictor-corrector sampling procedure (see Alg. 4 in Appendix D for more details).</p><formula xml:id="formula_25">Predict step: X t ? X t + g Z t ?t + g?t (20) Correct step: X t ? X t + ?t g (Z t + Z t ) + ? 2? t<label>(21)</label></formula><p>where ? N (0, I) and ? t is the pre-specified noise scales (see (59) in Appendix D).</p><p>Limitations &amp; efficiency. The main computational burden of our method comes from the computation of the divergence and maintaining two distinct networks. Despite it typically increases the memory by 2?2.5 times compared to SGM, we empirically observe that the divergence-based training converges much faster per iteration than standard regression.   Setups. We testify SB-FBSDE on two toy datasets and three image datasets, i.e. MNIST, CelebA, 6 and CIFAR10. p prior is set to a zero-mean Gaussian whose variance varies for each task and can be computed according to . We parameterize Z(?, ?; ?) and Z(?, ?; ?) with residual-based networks for toy datasets and consider Unet <ref type="bibr" target="#b32">(Ronneberger et al., 2015)</ref> and NCSN++  respectively for MNIST/CelebA and CIFAR10. All networks adopt position encoding and are trained with AdamW <ref type="bibr" target="#b18">(Loshchilov &amp; Hutter, 2017</ref>) on a TITAN RTX. We adopt VE-SDE (i.e. f := 0; see ) as our SDE backbone, which implies that in order to achieve reasonable performance, SB must learn a proper data-to-noise diffusion process. On all datasets, we set the horizon T =1.0 and solve the SDEs via the Euler-Maruyama method. The interval [0, T ] is discretized into 200 steps for CIFAR10 and 100 steps for all other datasets, which are much fewer than the ones in SGM (?1000 steps). Other details are left in Appendix D.</p><p>Toy datasets. We first validate our joint optimization (i.e. Alg 2) on generating a mixture of Gaussian and checkerboard (adopted from Grathwohl et al. <ref type="formula" target="#formula_1">(2018)</ref>) as the representatives of continuous and discontinuous distributions. <ref type="figure" target="#fig_0">Figure 3</ref> shows how the learned policies, i.e. Z(?, ?; ?) and Z(?, ?; ?), construct the vector fields that progressively transport samples back-and-forth between p prior and p data . The vector fields can be highly nonlinear and dissimilar to each other. This resembles neither SGMs, whose forward vector field must obey linear structure, nor flow-based models, whose vector fields are simply with opposite directions. We highlight this as a distinct feature arising from SB models.</p><p>Image datasets. Next, we validate our alternate training (i.e. Alg 3) on high-dimensional image generation. The generated images for MNIST, CelebA, and CIFAR10 are presented in <ref type="figure" target="#fig_1">Fig. 4</ref>, which clearly suggest that our SB-FBSDE is able to synthesize high-fidelity images. More uncurated images can be founded in Appendix E. Regarding the quantitative evaluation, <ref type="table" target="#tab_3">Table 1</ref> summarizes the negative log-likelihood (NLL; measured in bits/dim) and the Fr?chet Inception Distance score (FID; Heusel et al. <ref type="formula" target="#formula_1">(2017)</ref>) on CIFAR10. For our SB-FBSDE, we compute the NLL on the test set using Corollary 5, in a similar vein to SGMs and flow-based models, and report the FID over 50k  <ref type="bibr" target="#b38">(Song et al., 2021)</ref> 2.74 5.7 VDM <ref type="bibr">(Kingma et al., 2021)</ref> 2.49 4.00 LSGM <ref type="bibr" target="#b41">(Vahdat et al., 2021)</ref> 3.43 2.10  and outperforms prior Optimal Transport (OT) methods <ref type="bibr" target="#b43">(Wang et al., 2021;</ref><ref type="bibr" target="#b39">Tanaka, 2019</ref>) by a large margin in terms of the sample quality. More importantly, it enables log-likelihood computations that are otherwise infeasible in prior OT methods. We note that the quantitative comparisons on MNIST and CelebA are omitted as the scores on these two datasets are not widely reported and different pre-processing (e.g. resizing of CelebA) can lead to values that are not directly comparable.</p><p>Validity of SB forward diffusion. Our theoretical analysis in ?3.2 suggests that the forward policy Z(?, ?; ?) ? Z ? plays an essential role in governing samples towards p prior . Here, we validate this conjecture by computing the KL divergence between the terminal distribution induced by Z ? , i.e. p (13a) T , and the designated prior p prior . We refer readers to Appendix D for the actual computation. <ref type="figure">Figure 5</ref> reports these values over MNIST training. For both degenerate (f := 0) and linear (f := ? t X t ) base drifts, our SB-FBSDE generates terminal distributions that are much closer to p prior . Note that the values of SGM remain unchanged throughout training since SGM relies on pre-specified diffusion. This is in contrast to our SB-FBSDE whose forward policy Z ? gradually shortens the KL gap to p prior , thereby providing a better forward diffusion for training the backward policy.</p><p>Effect of Langevin corrector. In practice, we observe that the Langevin corrector greatly affects the generative performance. As shown in <ref type="figure">Fig. 6</ref>, including these corrector steps uniformly improves the sample quality (FID) on both CelebA and CIFAR10 throughout training. Since the SDEs are often solved via the Euler-Maruyama method, their propagation can be subjected to discretization errors accumulated over time. These Langevin steps thereby help re-distributing the samples at each time step t towards the desired density p SB t . We emphasize this improvement as the benefit gained from applying modern generative training techniques based on the solid connection between SB and SGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we present a novel computational framework, grounded on Forward-Backward SDEs theory, for computing the log-likelihood of Schr?dinger Bridge (SB) -a recently emerging model that adopts entropy-regularized optimal transport for generative modeling. Our findings provide new theoretical insights by generalizing previous theoretical results for Score-based Generative Model, and facilitate applications of modern generative training for SB. We validate our method on various image generative tasks, e.g. MNIST, CelebA, and CIFAR10, showing encouraging results in synthesizing high-fidelity samples while retaining the rigorous mathematical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Ioannis Exarchos and Oswin So for their generous involvement and helpful supports during the rebuttal. The authors would also like to thank Marcus A Pereira and Ethan N Evans for their participation and kind discussion in the early stage of project exploration. This research was supported by the ARO Award # W911NF2010151.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>The original idea of solving the PDE optimality of SB with FBSDEs theory was initiated by Tianrong. Later, Guan derived the main theories (i.e. Theorem 3, 4, 11, and Corollary 5) presented in Section 3.1, 3.2 and Appendix B with few helps from Tianrong. Tianrong designed the practical algorithms (e.g. stage-wise optimization and Langevin-corrector) in Section 3.3 and conducted most experiments with few helps from Guan. Guan wrote the main paper except for Section 4, which were written by both Tianrong and Guan. Both Guan and Tianrong contributed to code development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Our training algorithms are detailed in Alg. 1, 2, and 3, with the training objectives given in the same section (see <ref type="bibr">(16,</ref><ref type="bibr">18,</ref><ref type="bibr">19)</ref>). Other implementation details (e.g. data pre-processing) are left in Appendix D. This shall provide sufficient information for readers of interests to reproduce our results. As we strongly believe in the merit of open sourcing, we intend to release our implementation upon publication. On the theoretical side, all proofs are left to Appendix B due to space constraint. We provide the assumptions in the same section.</p><p>Ioannis Exarchos and Evangelos A Theodorou. Stochastic optimal control via forward and backward stochastic differential equations and importance sampling. Magdalena Kobylanski. Backward stochastic differential equations and partial differential equations with quadratic growth. Annals of probability, pp. 558-602, 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A INTRODUCTION OF SCHR?DINGER BRIDGE</head><p>In this subsection, we provide a brief review for Schr?dinger Bridge (SB) and some reasonings for Theorem 1. The SB problem, at its classical form, considers the following optimization <ref type="bibr" target="#b8">(Dai Pra, 1991;</ref><ref type="bibr" target="#b26">Pavon &amp; Wakolbinger, 1991)</ref>,</p><formula xml:id="formula_26">min u?U E T 0 1 2 u(t, X t ) 2 s.t. dX t = u(t, X t )dt + ? 2 dW t X 0 ? p 0 (X), X T ? p T (X) ,<label>(22)</label></formula><p>where U := {u : [0, T ] ? R n ? R n | u, u &lt; ?}. The optimization (22) characterizes a standard stochastic optimal control (SOC) programming with energy (i.e. 1 2 u 2 ) minimization except with an additional terminal boundary condition. The optimality conditions to (22) are given by</p><formula xml:id="formula_27">? ? ? ? ? ? ? ?? ?t = ? 1 2 ? x ? 2 ? ??, ?p * ?t = ? x ? (p * ? x ?) + ?p * ,<label>(23a)</label></formula><p>where ?(t, x) ? C 1,2 is known as the value function in SOC literature and p * (t, x) ? C 1,2 is the associated optimal marginal density. ? denotes the Laplace operator. Equations <ref type="formula" target="#formula_27">(23a)</ref> and <ref type="formula" target="#formula_28">(23b)</ref> are respectively the Kolmogorov's backward and forward PDEs, also known as Hamilton-Jacobi-Bellman and Fokker-Planck equations. The SB system can be obtained by applying the Hopf-Cole <ref type="bibr">(Hopf, 1950;</ref><ref type="bibr" target="#b7">Cole, 1951)</ref> </p><formula xml:id="formula_29">transformation (?, p * ) ? (?, ?), ?? ?t = ? ?? ? ? ?t = ? ? s.t. ?(0, ?) ?(0, ?) = p 0 , ?(T, ?) ?(T, ?) = p T .<label>(24)</label></formula><p>In this work, we consider a recent generalization of <ref type="formula" target="#formula_26">(22)</ref> and <ref type="formula" target="#formula_2">(24)</ref> to an SDE class with nonlinear drift, affine control, and time-varying diffusion. We synthesize their results below. Theorem 6 (SB optimality; <ref type="bibr" target="#b2">Caluya &amp; Halder (2021)</ref>). Consider the following optimization</p><formula xml:id="formula_30">min u E T 0 1 2 u(t, X t ) 2 s.t. dX t = [f (t, X t ) + g(t) u(t, X t )]dt + ? 2 g(t) dW t X 0 ? p 0 (X), X T ? p T (X) ,<label>(25)</label></formula><p>where g(t) is uniformly lower-bounded and f (t, X t ) satisfies Lipschitz conditions with at most linear growth in x. Then, the Hopf-Cole transformation to (25) becomes</p><formula xml:id="formula_31">?? ?t = ?? x ? T f ? Tr(g 2 ? 2 x ?) ? ? ?t = ?? x ? ( ?f )+ Tr(g 2 ? 2 x ?) ,<label>(26)</label></formula><p>with the same boundary conditions ?(0, ?) ?(0, ?) = p 0 , ?(T, ?) ?(T, ?) = p T . The optimal control to (25) is thereby given by</p><formula xml:id="formula_32">u * (t, X t ) = 2 g(t)? x log ?(t, X t ).<label>(27)</label></formula><p>Proof. See Section III and Theorem 2 inf <ref type="bibr" target="#b2">Caluya &amp; Halder (2021)</ref>.</p><p>Theorem 6 is particularly attractive to us since its SDE corresponds exactly to the one appearing in score-based generative models. One can recover Theorem 1 by (i) Following <ref type="bibr" target="#b26">Pavon &amp; Wakolbinger (1991)</ref>, we know that the objective in (25) is equivalent to D KL (Q || P) by an application of Girsanov's Theorem.</p><p>(ii) Equation <ref type="formula" target="#formula_31">(26)</ref> is exactly (6) with = 1 2 . Furthermore, substituting the optimal control (27) to the stochastic process in (25) yields the optimal forward SDE in (7a). (iii) Finally, reversing the SDE (7a) from forward to backward following <ref type="bibr" target="#b0">Anderson (1982)</ref>,</p><formula xml:id="formula_33">dX t = [f + g 2 ? x log ?(t, X t ) ? g 2 ? x log p SB t ]dt + g dW t ,<label>(28)</label></formula><p>and recalling the factorization principle, log p SB t (?) = log ?(t, ?) + log ?(t, ?), from Equation (4.15) in  yield the optimal backward SDE in (7b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOFS AND REMARKS IN SECTION 3</head><p>In this section, we provide proofs for all of our theorems. We following the same notation by denoting p SDE t (X t ) as the marginal density driven by some SDE process X(t) ? X t until the time step t ? [0, T ]. Gradient and Hessian of a function f (x), where x ? R n , will respectively be denoted as ? x f ? R n and ? 2</p><p>x f ? R n?n . Divergence and Laplace operators will respectively be denoted as ?? and ?. Note that ? = ? ? ?. For notational brevity, we will only keep the subscript x for multivariate functions. Finally, Tr(A) denotes the trace of a square matrix A.</p><p>We first restate the celebrated It? lemma, which is known as the extension of the chain rule of ordinary calculus to the stochastic setting. It relies on the fact that dW t 2 and dt are of the same scale and keeps the expansion up to O(dt). Lemma 7 (It? formula; It? <ref type="formula" target="#formula_1">(1951)</ref>). Let v ? C 1,2 and let X t be the stochastic process satisfying</p><formula xml:id="formula_34">dX t = f (t, X t )dt + G(t, X t )dW t .</formula><p>Then, the stochastic process v(t, X t ) is also an It? process satisfying</p><formula xml:id="formula_35">dv(t, X t ) = ?v(t, X t ) ?t dt + ? x v(t, X t ) T f + 1 2 Tr G T ? 2 x v(t, X t )G dt + ? x v(t, X t ) T G(t, X t ) dW t .<label>(29)</label></formula><p>Next, the following lemma will be useful in proving Theorem 3. Lemma 8. The following equality holds at any point x ? R n such that p(x) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">p(x)</head><p>Tr ? 2 p(x) = ? log p(x) 2 + ? log p(x)</p><p>Proof.</p><p>Tr ? 2 p(x) = ?p(x) = ? ? ?p(x)</p><formula xml:id="formula_36">= ? ? (p(x)? log p(x)) = ?p(x) T ? log p(x) + p(x)? log p(x) = p(x)? log p(x) T ? log p(x) + p(x)? log p(x) = p(x) ? log p(x) 2 + ? log p(x)</formula><p>Assumptions Before stating our proofs, we provide the assumptions used throughout the paper. These assumptions are adopted from stochastic analysis for SGM <ref type="bibr" target="#b38">(Song et al., 2021;</ref><ref type="bibr" target="#b44">Yong &amp; Zhou, 1999;</ref><ref type="bibr" target="#b0">Anderson, 1982)</ref>, SB <ref type="bibr" target="#b2">(Caluya &amp;</ref><ref type="bibr" target="#b2">Halder, 2021), and</ref><ref type="bibr">FBSDE (Exarchos &amp;</ref><ref type="bibr">Theodorou, 2018;</ref><ref type="bibr">Gorodetsky et al., 2015)</ref>.</p><p>(i) p prior , p data ? C 2 with finite second-order moment.</p><p>(ii) f and g are continuous functions, and |g(t)| 2 &gt; 0 is uniformly lower-bounded w.r.t. t.</p><p>(iii) ?t ? [0, T ], we have f (t, x), ? x log p t (x), ? x log ?(t, x), ? x log ?(t, x), Z(t, x; ?), and Z(t, x; ?) Lipschitz and at most linear growth w.r.t. x.</p><p>(iv) ?, ? ? C 1,2 . h, and ? are continuous functions. h satisfies quadratic growth w.r.t. x uniformly in t.</p><p>(</p><formula xml:id="formula_37">v) ?k &gt; 0 : p SB t (x) = O(exp ? x 2 k ) as x ? ?.</formula><p>Assumptions (i) (ii) (iii) are standard conditions in stochastic analysis to ensure the existenceuniqueness of the SDEs; hence also appear in SGM analysis <ref type="bibr" target="#b38">(Song et al., 2021)</ref>. Assumption (iv) allows applications of It? formula and properly defines the backward SDE in FBSDE theory. Finally, assumption (v) assures the exponential limiting behavior when performing integration by parts.</p><p>Now, let us begin the proofs of Theorem 3, 4, and Corollary 5.</p><p>Theorem 3 (FBSDEs to SB optimality <ref type="formula">(6)</ref>). Consider the following set of coupled SDEs,</p><formula xml:id="formula_38">? ? ? ? ? ? ? ? ? ? ? ? ? dX t = (f + gZ t ) dt + gdW t dY t = 1 2 Z T t Z t dt + Z T t dW t d Y t = 1 2 Z T t Z t + ? x ? (g Z t ? f ) + Z T t Z t dt + Z T t dW t (30a) (30b)<label>(30c)</label></formula><p>where f and g satisfy the same regularity conditions in Lemma 2 (see Footnote 4), and the boundary conditions are given by X(0) = x 0 and Y T + Y T = log p prior (X T ). Suppose ?, ? ? C 1,2 , then nonlinear Feynman-Kac relations between the FBSDEs (13) and PDEs (6) are given by</p><formula xml:id="formula_39">Y t ? Y(t, X t ) = log ?(t, X t ), Z t ? Z(t, X t ) = g? x log ?(t, X t ), Y t ? Y(t, X t ) = log ?(t, X t ), Z t ? Z(t, X t ) = g? x log ?(t, X t ).<label>(31)</label></formula><p>Furthermore, (Y t , Y t ) obey the following relation:</p><formula xml:id="formula_40">Y t + Y t = log p SB t (X t ).</formula><p>Proof. Similar to how the original nonlinear Feynman-Kac (i.e. Lemma 2) can be carried out by an application of It? lemma <ref type="bibr" target="#b19">(Ma et al., 1999)</ref>. We can apply It? lemma 7 to the stochastic process log ?(t, X t ) w.r.t. the optimal forward SDE (7a).</p><formula xml:id="formula_41">d log ? = ? log ? ?t dt + ? x log ? T (f + g 2 ? x log ?) + 1 2 g 2 Tr ? 2 x log ? dt + g? x log ? T dW t .<label>(32)</label></formula><p>From the PDE dynamics (6), we know that</p><formula xml:id="formula_42">? log ? ?t = 1 ? ?? x ? T f ? 1 2 Tr(g 2 ? 2 x ?) = ?? x log ? T f ? 1 2 g 2 Tr( 1 ? ? 2 x ?).</formula><p>The first term in the RHS can be readily canceled out with the related f -term in (32). The second term can also be canceled out using the fact that ? 2</p><formula xml:id="formula_43">x log ? = 1 ? ? 2 x ? ? 1 ? 2 ? x ?? x ? T . Hence, we are left with d log ? = g? x log ? 2 ? 1 2 g 2 Tr 1 ? 2 ? x ?? x ? T dt + g? x log ? T dW t = 1 2 g? x log ? 2 dt + g? x log ? T dW t .<label>(33)</label></formula><p>Likewise, applying It? lemma to log ?(t, X t ), where X t follows the SDE in (7a),</p><formula xml:id="formula_44">d log ? = ? log ? ?t dt + ? x log ? T (f + g 2 ? x log ?) + 1 2 g 2 Tr ? 2 x log ? dt + g? x log ? T dW t ,<label>(34)</label></formula><p>but now noticing that the dynamics of ? log ? ?t become</p><formula xml:id="formula_45">? log ? ?t = 1 ? ?? x ? ( ?f ) + 1 2 Tr(g 2 ? 2 x ?) = ?? x log ? T f ? ? x ? f + 1 2 g 2 Tr( 1 ? ? 2 x ?).</formula><p>Only the first term in the RHS will be canceled out in (34). Hence, we are left with</p><formula xml:id="formula_46">d log ? = ?? x ? f + 1 2 g 2 Tr 1 ? ? 2 x ? dt + g 2 ? x log ? T ? x log ? + 1 2 g 2 Tr ? 2 x log ? dt + g? x log ? T dW t .<label>(35)</label></formula><p>Notice that the trace terms above can be simplified to</p><formula xml:id="formula_47">1 2 Tr 1 ? ? 2 x ? + ? 2 x log ? = Tr 1 ? ? 2 x ? ? 1 2 ? x log ? 2 = 1 2 ? x log ? 2 + ? x log ?,</formula><p>where the last equality follows by Lemma 8. Substituting this result back to (35), we get</p><formula xml:id="formula_48">d log ? = ?? x ? f + 1 2 g? x log ? 2 + g 2 ? x log ? + g 2 ? x log ? T ? x log ? dt + g? x log ? T dW t = ? x ? g 2 ? x log ??f + 1 2 g? x log ? 2 + g 2 ? x log ? T ? x log ? dt + g? x log ? T dW t<label>(36)</label></formula><p>Finally, by rewriting <ref type="formula" target="#formula_43">(33)</ref> and <ref type="formula" target="#formula_48">(36)</ref> with the nonlinear Feynman-Kac in (31) yields</p><formula xml:id="formula_49">dX t = (f + gZ t ) dt + gdW t dY t = 1 2 Z T t Z t dt + Z T t dW t d Y t = 1 2 Z T t Z t + ? x ? (g Z t ? f ) + Z T t Z t dt + Z T t dW t</formula><p>This concludes the proof.</p><p>Remark 9 (Viscosity solutions). These FBSDE results can be extended to viscosity solutions in the case when the classical solution does not exist <ref type="bibr" target="#b24">(Pardoux &amp; Peng, 1992)</ref>. For the completeness, one shall understand them in the sense of v(t, x) = lim ?? v (t, x) uniformly in (t, x) over a compact set.</p><p>Here v (t, x) is the classical solution to (10) with (f , G , h , ? ) converge uniformly toward (f, G, h, ?) over the compact set. We refer readers of interests to Exarchos &amp; Theodorou (2018); <ref type="bibr" target="#b22">Negyesi et al. (2021)</ref>, and their references therein.</p><p>Theorem 4 (Log-likelihood for SB models). Given the solution satisfying the FBSDE system in (13), the log-likelihood of the SB model (Z t , Z t ), at a data point x 0 , can be expressed as</p><formula xml:id="formula_50">L SB (x 0 ) = E [log p T (X T )] ? T 0 E 1 2 Z t 2 + 1 2 Z t ? g? x log p SB t + Z t 2 ? 1 2 g? x log p SB t ? Z t 2 ?? x ? f dt (38) = E [log p T (X T )] ? T 0 E 1 2 Z t 2 + 1 2 Z t 2 + ? x ? (g Z t ? f ) + Z T t Z t dt,<label>(39)</label></formula><p>where the expectation is taken over the forward SDE (13a) with the initial condition X 0 = x 0 .</p><p>Remark 10 (Connection between SB-FBSDE and flow-based models). To demonstrate how applying flow-based training techniques to the probability ODE flow of SB (43) recovers the same loglikelihood objective in (39), recall that given an ODE dX t = F (t, X t )dt with X 0 = x 0 ? p data , flow-based models compute the change in log-density using the instantaneous change of variables formula <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>:</p><formula xml:id="formula_51">? log p(X t ) ?t = ?? x ? F,</formula><p>which implies that the log-likelihood of x 0 can be computed as</p><formula xml:id="formula_52">log p(X T ) = log p(x 0 ) ? T 0 ? x ? F dt.<label>(45)</label></formula><p>Now, consider the probability ODE flow of SB in (44),</p><formula xml:id="formula_53">F SB := f + gZ t ? 1 2 g(Z t + Z t ) = f + 1 2 g(Z t ? Z t ).</formula><p>Substituting this vector field F SB to (45) yields</p><formula xml:id="formula_54">log p T (X T ) = log p 0 (x 0 ) ? T 0 ? x ? f + 1 2 g(Z t ? Z t ) dt ? E [log p 0 (x 0 )] = E [log p T (X T )] + T 0 E ? x ? f + 1 2 g(Z t ? Z t ) dt = E [log p T (X T )] ? T 0 E ? x ? (g Z t ? f ) ? 1 2 g? x ? (Z t + Z t ) dt ( * ) = E [log p T (X T )] ? T 0 E ? x ? (g Z t ? f ) + 1 2 g(Z t + Z t ) T ? x log p SB t dt ( * * ) = E [log p T (X T )] ? T 0 E ? x ? (g Z t ? f ) + 1 2 (Z t + Z t ) 2 dt,<label>(46)</label></formula><p>where (*) is due to integration by parts (recall (41)) and (**) again uses the factorization principle Z t + Z t = g? x log p SB t . One can verify that (46) indeed recovers (39). Theorem 11 (FBSDE computation for L SB (x T ) in SB models). With the same regularity conditions in Theorem 3, the following FBSDEs also satisfy the nonlinear Feynman-Kac relations in (31).</p><formula xml:id="formula_55">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? dX t = f ? g Z t dt + gdW t dY t = ? 1 2 Z T t Z t + ? x ? (gZ t + f ) + Z T t Z t dt + Z T t dW t d Y t = ? 1 2 Z T t Z t dt + Z T t dW t (47a) (47b)<label>(47c)</label></formula><p>Given a backward trajectory sampled from (47a), where X T = x T and x T ? p prior , the log-likelihood of x T is given by log p prior (</p><formula xml:id="formula_56">x T ) = E Y T + Y T |X T = x T := L SB (x T ).</formula><p>In particular,</p><formula xml:id="formula_57">L SB (x T ) = E [log p T (X 0 )] ? T 0 E 1 2 Z t 2 + 1 2 Z t 2 + ? x ? (gZ t + f ) + Z T t Z t dt,<label>(48)</label></formula><p>Proof. Due to the symmetric structure of SB, we can consider a new time coordinate</p><formula xml:id="formula_58">s T ? t.</formula><p>Under this transformation, the base reference P appearing in (5) is equivalent to</p><formula xml:id="formula_59">dX s = ?f (s, X s )ds + g dW s .</formula><p>The corresponding PDE optimality becomes</p><formula xml:id="formula_60">?? ?s = ? x ? T f ? 1 2 Tr(g 2 ? 2 x ?) ? ? ?s = ? x ? ( ?f )+ 1 2 Tr(g 2 ? 2 x ?) s.t. ?(0, ?) ?(0, ?) = p prior , ?(T, ?) ?(T, ?) = p data ,<label>(49)</label></formula><p>and the optimal forward/backward policies are given by</p><formula xml:id="formula_61">dX s = [?f + g 2 ? x log ?(s, X s )]ds + g dW s , X 0 ? p prior ,<label>(50a)</label></formula><formula xml:id="formula_62">dX s = [?f ? g 2 ? x log ?(s, X s )]ds + g dW s , X T ? p data .<label>(50b)</label></formula><p>By comparing (50) with <ref type="formula">(7)</ref>, one can notice that the new SB system (?, ?) s corresponds to the original system (?, ?) t via</p><formula xml:id="formula_63">?(s, X s ) = ?(T ? t, X T ?t ) and ?(s, X s ) = ?(T ? t, X T ?t ).<label>(51)</label></formula><p>Equation <ref type="formula" target="#formula_1">(51)</ref> shall be understood as the forward policy in t-coordinate system corresponds to the backward policy in s-coordinate system, and vise versa. Following similar derivations in the proof of Theorem 3, we can apply It? lemma to expend the stochastic processes d log ? and d log ? w.r.t. (50a). This yields the following FBSDE system.</p><formula xml:id="formula_64">dX s = (gZ s ? f ) ds + gdW s (52a) dY s = 1 2 Z s 2 ds + Z T s dW s (52b) d Y s = 1 2 Z s 2 + ? x ? (g Z s + f ) + Z T s Z s ds + Z T s dW s (52c) Similar to (51), (Y s , Y s , Z s , Z s ) relate to the original FBSDE system (30) by (Y s , Y s , Z s , Z s ) = ( Y T ?t , Y T ?t , Z T ?t , Z T ?t ).<label>(53)</label></formula><p>Changing the coordinate from s to t and applying (53) readily yield (47). Finally, the expression in (48) can be carried out similar to <ref type="formula" target="#formula_2">(40)</ref>:</p><formula xml:id="formula_65">L SB (x T ) =E Y T + Y T |X T = x T =E Y 0 ? T 0 1 2 Z t 2 dt + Y 0 ? T 0 1 2 Z t 2 + ? ? (gZ t + f ) + Z T t Z t dt X T = x T =E Y 0 + Y 0 |X T = x T ? T 0 E 1 2 Z t 2 + 1 2 Z t 2 + ? ? (gZ t + f ) + Z T t Z t X T = x T dt =E[log p 0 (X 0 )] ? T 0 E 1 2 Z t 2 + 1 2 Z t 2 + ? ? (gZ t + f ) + Z T t Z t dt.<label>(54)</label></formula><p>We conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISON WITH PRIOR SB WORKS</head><p>Our method is closely related to two concurrent SB models <ref type="bibr" target="#b42">Vargas et al., 2021)</ref>, yet differs in various aspects. Below we enumerate some of the differences.</p><p>Training loss. Both concurrent methods rely on solving SB mean-matching regression between the current drift and (estimated) optimal drift. This is in contrast to our SB-FBSDE, which instead utilizes a divergence-based objective (16). However, the regression objectives are in fact captured by Theorem 4. To see that, recall the forward and backward transition models considered in De Bortoli et al. <ref type="formula" target="#formula_1">(2021)</ref>,</p><formula xml:id="formula_66">X k+1 ? N (F k (X k ), 2? k+1 I), and X k ? N (B k+1 (X k+1 ), 2? k+1 I),</formula><p>where F k (x) := x + ? k+1 f k (x) and B k+1 (x) := x + ? k+1 b k+1 (x) are solved alternately via</p><formula xml:id="formula_67">B k+1 ? arg min B k+1 E B k+1 (X k+1 ) ? (X k+1 + F k (X k ) ? F k (X k+1 )) 2 (55a) F k ? arg min F k E F k (X k+1 ) ? (X k + B k+1 (X k ) ? B k+1 (X k )) 2 .<label>(55b)</label></formula><p>In what follows, we focus mainly on the connection between (55a) and Theorem 4, yet similar analysis can be applied to (55b). Now, expanding (55a) with the definition of (B k+1 , F k )</p><formula xml:id="formula_68">B k+1 (X k+1 ) ? (X k+1 + F k (X k ) ? F k (X k+1 ) 2 = (X k+1 + ? k+1 b k+1 (X k+1 )) ? (X k+1 + X k + ? k+1 f k (X k ) ? X k+1 ? ? k+1 f k (X k+1 )) 2 = ? k+1 f k (X k+1 ) 1 + ? k+1 b k+1 (X k+1 ) 2 ? (X k + ? k+1 f k (X k ) ? X k+1 ) 3 2 ,<label>(56)</label></formula><p>which resembles the term Z t + Z t ? g? x log p SB t 2 in (15). While 2 indeed corresponds to our backward policy Z(k+1, X k+1 ) after time discretization, 1 slightly differs from Z(k+1, X k+1 ) in how time is integrated, ? k+1 f (k, X k+1 ) vs. Z(k+1, X k+1 ). On the other hand, 3 may be seen as an approximation of g? x log p SB t , which, crucially, is not utilized in SB-FBSDE training. Since ? x log p SB t is often intractable (nor does SB-FBSDE try to approximate it), SB-FBSDE instead uses the divergence-based objective (16), which does not appear in their practical training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDE model class.</head><p>It is important to recognize that both concurrent methods are rooted in the classical SB formulation with the following SDE model,</p><formula xml:id="formula_69">dX t = f (t, X t )dt + 2? dW t ,</formula><p>which, crucially, differs from the SDE concerned by both our SB-FBSDE and SGM, </p><formula xml:id="formula_70">dX t = f (t, X t )dt + g(t)dW t ,<label>(57)</label></formula><formula xml:id="formula_71">X k+1 = X k + ? k+1 f (k, X k ) + 2? k+1 ,<label>(58)</label></formula><p>where ? N (0, I). In order for (58) to match the discretization of (57), where g(t) is often a monotonically increasing function, the step sizes {? k } N k=1 must also increase monotonically. However, since ? x log p SB t is approximated in De Bortoli et al. (2021) using the states from two consecutive steps (see <ref type="formula" target="#formula_3">(55)</ref>), this may also affect the accuracy of the regression targets.</p><p>In contrast, our SB-FBSDE is grounded on the recent SB theory <ref type="bibr" target="#b2">(Caluya &amp; Halder, 2021)</ref>, which considers the same SDE model as in <ref type="formula" target="#formula_3">(57)</ref>. As such, connection between SB-FBSDE and SGM is made directly in continuous-time (and can be extended to discrete-time flawlessly); hence unaffected by the choice of numerical discretization or step sizes.  Sampling. The sampling procedure is summarized in Alg. 4. Given some pre-defined signal-to-noise ratio r (we set r =0.05 for all experiments), the Langevin noise scale ? t,i at each time step t and each corrector step i is computed by ? t,i = 2r 2 g 2 i 2 (Z(t, X t,i ) + Z(t, X t,i )) 2 ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENT DETAILS</head><p>Network architectures. <ref type="table">Table 8</ref> summarizes the network architecture used for each dataset. For toy datasets, we parameterize Z(?, ?; ?) and Z(?, ?; ?) with the architectures shown in <ref type="figure">Fig. 9</ref>. Specifically, FCBlock represents a fully connected layer followed by a swish nonlinear activation <ref type="bibr" target="#b30">(Ramachandran et al., 2017)</ref>. As for MNIST, we consider a smaller version of Unet (Ho et al., 2020) by reducing the numbers of residual block, attention heads, and channels respectively to 1, 2, and 32. Unet and NCSN++ respectively correspond to the architectures appeared in Ho et al. (2020) and .</p><p>Remarks on <ref type="table" target="#tab_3">Table 1</ref>. We note that the values of our SB-FBSDE reported in <ref type="table" target="#tab_3">Table 1</ref> are computed without the Langevin corrector due to the computational constraint. For all other experiments, we adopt the Langevin corrector as it generally improves the performance (see <ref type="figure">Fig. 6</ref>). This implies that our results on CIFAR10, despite already being encouraging, may be further improved with the Langevin corrector. <ref type="figure">Fig. 5</ref>. To estimating KL(p T , p prior ), we first compute the pixel-wise first and second moments given the generated samples X T at the end of the forward diffusion. After fitting a diagonal Gaussian to {X T }, we can apply the analytic formula for computing the KL divergence between two multivariate Gaussians. <ref type="figure">Fig. 6</ref>. To accelerate the sampling process with the Langevin corrector, for this experiment we consider a reduced Unet (see <ref type="table">Table 8</ref>) for CelebA. The FID scores on both datasets are computed with 10k samples. We stress, however, that the performance improvement using the Langevin corrector remains consistent across other (larger) architectures and if one increases the FID samples.  <ref type="bibr">(2021)</ref>. Specifically, we adopt the same network architecture (reduced U-net), image preprocessing (center-cropping 140 pixel and resizing to 32 ? 32), step sizes (N =50), and horizon (0.5 second) for fair comparison. Comparing our <ref type="figure">Fig. 10b</ref> to De Bortoli et al. (2021) (see their <ref type="figure">Fig. 6)</ref>, it is clear that images generated by our model have higher diversity (e.g. color skin, facing angle, background, etc) and better visual quality. We conjecture that our performance difference may come from (i) the (in)sensitivity to numerical discretization between our divergence objectives and their mean-matching regression, and (ii) the foundational differences in how diffusion coefficients are designed.  <ref type="figure">Fig. 11a</ref> with a full SB-FBSDE stage using <ref type="bibr">(18,</ref><ref type="bibr">19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL EXPERIMENTS</head><p>SGM regression training + SB divergence-based training. <ref type="table">Table 3</ref> reports the FID (using 10k samples, without corrector steps) at different stages of CIFAR10 training. We first train the backward policy with SGM's regression loss for a sufficient long iterations (50k) until the FID roughly converges. Then, we switch to our alternate training (Alg. 3) using the divergence-based objectives. Crucially, <ref type="table">Table 3</ref>: SGM regression training + SB divergence-based training. We denote "SGM/50k" as "training 50k steps using SGM loss", and "SB/{f,b}/5k" as "training forward/backward policy with 5k steps using our divergence loss", and etc. with only 5k iterations of our divergence-based training, we drop the FID dramatically down to 13.35 from 33.68. With a full stage of training (last column), the FID decreases even lower to 11.85. The qualitative results are provided in <ref type="figure">Fig. 11</ref>. Comparing <ref type="figure">Fig. 11a</ref> (corresponds to "SGM/50k" in <ref type="table">Table 3</ref>) and <ref type="figure">Fig. 11b</ref> (corresponds to "SGM/50k + SB/b/5k" in <ref type="table">Table 3</ref>), it can be seen that the visible flaw and noise have been substantially improved.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Validation of our SB-FBSDE model on two synthetic toy datasets that represent continuous and discontinuous distributions. Upper: Generation (p data ? p prior ) process with the backward vector field Z(?, ?; ?). Bottom: Diffusion (p data ? p prior ) process with the forward vector field Z(?, ?; ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Uncurated samples from our SB-FBSDE models trained on MNIST (left), resized CelebA (middle) and CIFAR10 (right). More images can be found in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Validation of our SB-FBSDE on learning forward diffusions that are closer (in KL sense) to p prior compared to SGM. Ablation analysis where we show that adding Langevin corrector to SB-FBSDE uniformly improves the FID scores on both CelebA and CIFAR10 training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>samples w.r.t the training set. Notably, our SB-FBSDE achieves 2.98 bits/dim and 3.18 FID score on CIFAR10, which is comparable to the top existing methods from other model classes (e.g. SGMs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in that the diffusion g(t) is a time-varying function. This implies that the connection between classical SB models and SGM can only be made in discrete-time after choosing proper step sizes. For instance, De Bortoli et al. (2021) considers the Euler-Maruyama discretization (see their ?C.3),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Comparison between images generated by ground truth and SB-FBSDE on reduced CelebA. Our SB-FBSDE is trained under the same data pre-processing, network architecture and stepsizes implemented in De Bortoli et al. (2021). (a) SGM/50k (b) SGM/50k + SB/b/5k (c) SGM/50k + SB/f/5k + SB/b/5k Qualitative results at the different stages of training. (a) Results after 50k training iterations using SGM's regression loss. (b) Refine the results of Fig. 11a by training the backward policy using (18) with 5k iterations. (c) Refine the results of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Uncurated samples generated by our SB-FBSDE on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Uncurated samples generated by our SB-FBSDE on resized CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Uncurated samples generated by our SB-FBSDE on CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. While the joint training scheme in Alg. 2 resembles recent diffusion flow-based models<ref type="bibr" target="#b45">(Zhang &amp; Chen, 2021)</ref>, the alternate training in Alg. 3 relates to the classical IPF, despite differing in the underlying objectives. Empirically, the joint training scheme can converge faster yet at the cost of introducing memory complexity. We highlight these flexible training procedures arising from the unified viewpoint provided in Theorem 4. Hereafter, we refer to each cycle, i.e. 2K training steps, in Alg. 3 as a training stage of SB-FBSDE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>CIFAR10 evaluation using negative log-likelihood (NLL; bits/dim) on the test set and sample quality (FID score) w.r.t. the training set. Our SB-FBSDE outperforms other optimal transport baselines by a large margin and is comparable to existing generative models.</figDesc><table><row><cell>Model Class</cell><cell>Method</cell><cell cols="2">NLL ? FID ?</cell></row><row><cell></cell><cell>SB-FBSDE (ours)</cell><cell>2.96</cell><cell>3.01</cell></row><row><cell>Optimal Transport</cell><cell>DOT (Tanaka, 2019) Multi-stage SB (Wang et al., 2021)</cell><cell>--</cell><cell>15.78 12.32</cell></row><row><cell></cell><cell>DGflow (Ansari et al., 2020)</cell><cell>-</cell><cell>9.63</cell></row><row><cell></cell><cell>SDE (deep, sub-VP; Song et al. (2020))</cell><cell>2.99</cell><cell>2.92</cell></row><row><cell></cell><cell>ScoreFlow</cell><cell></cell><cell></cell></row><row><cell>SGMs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Automatica, 87:159-165, 2018. Wenbo Gong and Yingzhen Li. Interpreting diffusion score matching using normalizing flow. arXiv preprint arXiv:2107.10072, 2021. Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. Springer Science &amp; Business Media, 2012. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020a. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b.</figDesc><table><row><cell>Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang. Autogan: Neural architecture search</cell></row><row><cell>for generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on</cell></row><row><cell>Computer Vision, pp. 3224-3234, 2019.</cell></row><row><cell>Alex A Gorodetsky, Sertac Karaman, and Youssef M Marzouk. Efficient high-dimensional stochastic</cell></row><row><cell>optimal motion control using tensor-train decomposition. In Robotics: Science and Systems, 2015.</cell></row><row><cell>Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud. Ffjord:</cell></row><row><cell>Free-form continuous dynamics for scalable reversible generative models. arXiv preprint</cell></row><row><cell>arXiv:1810.01367, 2018.</cell></row><row><cell>Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations</cell></row><row><cell>using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018.</cell></row><row><cell>(3):1059-1076,</cell></row><row><cell>1989.</cell></row><row><cell>Aapo Hyv?rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.</cell></row><row><cell>Journal of Machine Learning Research, 6(4), 2005.</cell></row></table><note>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020. Eberhard Hopf. The partial differential equation ut+ uux= ?xx. Communications on Pure and Applied mathematics, 3(3):201-230, 1950. Chin-Wei Huang, Laurent Dinh, and Aaron Courville. Augmented normalizing flows: Bridging the gap between generative flows and latent variable models. arXiv preprint arXiv:2002.07101, 2020. Chin-Wei Huang, Jae Hyun Lim, and Aaron Courville. A variational perspective on diffusion-based generative models and score matching. arXiv preprint arXiv:2106.02808, 2021. Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18Kiyosi It?. On stochastic differential equations, volume 4. American Mathematical Soc., 1951.Alexia Jolicoeur-Martineau, Ke Li, R?mi Pich?-Taillefer,Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXiv preprint arXiv:2107.00630, 2021.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc>with other models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>CIFAR10 evaluation.</figDesc><table><row><cell>Model Class</cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">NLL ? FID ?</cell></row><row><cell></cell><cell cols="2">SB-FBSDE (ours)</cell><cell></cell><cell>2.96</cell><cell>3.01</cell></row><row><cell>Optimal Transport</cell><cell cols="3">DOT (Tanaka, 2019) Multi-stage SB (Wang et al., 2021)</cell><cell>--</cell><cell>15.78 12.32</cell></row><row><cell></cell><cell cols="2">DGflow (Ansari et al., 2020)</cell><cell></cell><cell>-</cell><cell>9.63</cell></row><row><cell></cell><cell cols="3">SDE (deep, sub-VP; Song et al. (2020))</cell><cell>2.99</cell><cell>2.92</cell></row><row><cell></cell><cell cols="2">ScoreFlow (Song et al., 2021)</cell><cell></cell><cell>2.74</cell><cell>5.7</cell></row><row><cell>SGMs</cell><cell cols="2">VDM (Kingma et al., 2021)</cell><cell></cell><cell>2.49</cell><cell>4.00</cell></row><row><cell></cell><cell cols="2">LSGM(Vahdat et al., 2021)</cell><cell></cell><cell>3.43</cell><cell>2.10</cell></row><row><cell></cell><cell cols="2">VDVAE (Child, 2020)</cell><cell></cell><cell>2.87</cell><cell>-</cell></row><row><cell>VAEs</cell><cell cols="3">NVAE (Vahdat &amp; Kautz, 2020) BIVA (Maal?e et al., 2019)</cell><cell>2.91 3.08</cell><cell>23.49 -</cell></row><row><cell></cell><cell cols="3">FFJORD (Grathwohl et al., 2018)</cell><cell>3.40</cell><cell>-</cell></row><row><cell>Flows</cell><cell cols="2">VFlow (Chen et al., 2020)</cell><cell></cell><cell>2.98</cell><cell>-</cell></row><row><cell></cell><cell cols="2">ANF (Huang et al., 2020)</cell><cell></cell><cell>3.05</cell><cell>-</cell></row><row><cell></cell><cell cols="2">AutoGAN (Gong et al., 2019)</cell><cell></cell><cell>-</cell><cell>12.42</cell></row><row><cell>GANs</cell><cell cols="3">StyleGAN2-ADA (Karras et al., 2020a)</cell><cell>-</cell><cell>2.92</cell></row><row><cell></cell><cell cols="2">LeCAM (Park &amp; Kim, 2021)</cell><cell></cell><cell>-</cell><cell>2.47</cell></row><row><cell></cell><cell cols="3">Figure 7: Training Hyper-parameters</cell><cell></cell></row><row><cell cols="6">Dataset learning rate time steps batch size variance of p prior</cell></row><row><cell>Toy</cell><cell>2e-4</cell><cell>100</cell><cell>400</cell><cell cols="2">1.0</cell></row><row><cell>Mnist</cell><cell>2e-4</cell><cell>100</cell><cell>200</cell><cell cols="2">1.0</cell></row><row><cell>CelebA</cell><cell>2e-4</cell><cell>100</cell><cell>200</cell><cell cols="2">900.0</cell></row><row><cell>CIFAR10</cell><cell>1e-5</cell><cell>200</cell><cell>64</cell><cell cols="2">2500.0</cell></row><row><cell></cell><cell cols="3">Figure 8: Network Architectures</cell><cell></cell></row><row><cell cols="6">Dataset Z t (?, ?; ?) and # of parameters Z t (?, ?; ?) and # of parameters</cell></row><row><cell>Toy</cell><cell cols="2">FC-ResNet (0.76M)</cell><cell cols="2">FC-ResNet (0.76M)</cell></row><row><cell>Mnist</cell><cell cols="2">reduced Unet (1.95M)</cell><cell cols="3">reduced Unet (1.95M)</cell></row><row><cell>CelebA</cell><cell>Unet (39.63M)</cell><cell></cell><cell cols="2">Unet (39.63M)</cell></row><row><cell>CIFAR10</cell><cell cols="2">NCSN++ (62.69M)</cell><cell cols="2">Unet (39.63M)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Comparison to De Bortoli et al. (2021) under same setup. To demonstrate the superior performance of our model, we conduct experiments with the exact same setup implemented in De Bortoli et al.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We adopt the recent advance in SB theory<ref type="bibr" target="#b2">(Caluya &amp; Halder, 2021</ref>) that extends classical SB models (used in prior works) to the exact SDE class appearing in SGM. See Appendices A and C for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Hereafter, we will sometimes drop f ? f (t, Xt) and g ? g(t) for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Lemma 2 can be viewed as the nonlinear extension of the celebrated Feynman-Kac formula (Karatzas &amp; Shreve, 2012), which characterizes the connection between linear PDEs and forward SDEs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" target="#b44">Yong &amp; Zhou (1999)</ref>;Kobylanski (2000)  require f , G, h, and ? to be continuous, f and G to be uniformly Lipschitz in x, and h to satisfy quadratic growth condition in z.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In fact, their regression targets may be recovered from(15)under proper transformation; see Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We follow a similar setup of prior SB models and resize the image size to 32.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">(40)</ref> <p>which recovers <ref type="bibr">(39)</ref>. Finally, notice that with integration by part, we have</p><p>where we adopt common practice and assume the limiting behavior of p SB t ; in other words, ?k &gt; 0 :</p><p>With <ref type="formula">(41)</ref>, we can rewrite the related parts in (40) as</p><p>Hence, we also recover (38).</p><p>Corollary 5 (Probability flow for SB). The following ODE characterizes the probability flow of the optimal processes of SB <ref type="formula">(7)</ref> in the sense that ?t, p (17)</p><p>Proof. The probability ODE flow <ref type="bibr" target="#b21">Maoutsa et al., 2020)</ref> suggests that the equivalent ODE model for the SDE (1) is given by</p><p>We can adopt this result to the SDEs of SB (7a) by considering f ? f + gZ t and p (1) t ? p SB t . This yields</p><p>Applying the the factorization principle  with g log p SB t = Z t + Z t concludes the proof.</p><p>Training. We use Exponential Moving Average (EMA) with the decay rate of 0.99. <ref type="table">Table 7</ref> details the hyper-parameters used for each dataset. As mentioned in De , the alternate training scheme may substantially accelerate the convergence under proper initialization. Specifically, when Z t is initialized with degenerate outputs (e.g. by zeroing out its last layer), training Z t at the first K steps can be made in a similar SGM fashion since p SB t now admits analytical expression. As for the proceeding stages, we resume to use (18, 19) since (Z t , Z t ) no longer have trivial outputs.</p><p>Data pre-processing. MNIST is padded from 28?28 to 32?32 to prevent degenerate feature maps through Unet. CelebA is resized to 3?32?32 to accelerate training. Both CelebA and CIFAR10 are augmented with random horizontal flips to enhance the diversity.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Refining deep generative models via discriminator gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Abdul Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Liang Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00780</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein proximal algorithms for the schr?dinger bridge problem: Density control with nonlinear drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Caluya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Halder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6572" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schr?dinger bridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tryphon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="313" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="arXiv">arXiv:2011.10650</idno>
		<title level="m">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On a quasi-linear parabolic equation occurring in aerodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of applied mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A stochastic control approach to reciprocal diffusion processes. Applied mathematics and Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Dai Pra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="313" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Diffusion schr?dinger bridge with applications to score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01357</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05646</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probability densities with given marginals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1236" to="1243" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A survey of the schr?dinger problem and some of its connections with optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>L?onard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0215</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An optimal control approach to deep learning and applications to discrete-weight neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01299</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ddpnopt: Differential dynamic programming neural optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos A</forename><surname>Theodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic game theoretic neural optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos A</forename><surname>Theodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Forward-backward stochastic differential equations and their applications. Number 1702</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongmin</forename><surname>Yong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interacting particle solutions of fokkerplanck equations through gradient-log-density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">802</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balint</forename><surname>Negyesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristoffer</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelis W</forename><surname>Oosterlee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05421</idno>
		<title level="m">The one step malliavin scheme: new discretization of bsdes implemented with deep learning regressions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backward stochastic differential equations and quasilinear parabolic partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Pardoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shige</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic partial differential equations and their applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="200" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Styleformer: Transformer based generative adversarial networks with style vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeeseung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younggeun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07023</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On free energy, stochastic control, and schr?dinger processes. In Modeling, Estimation and Control of Systems with Uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Pavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Wakolbinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="334" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural network architectures for stochastic control using the nonlinear feynman-kac lemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos A</forename><surname>Theodorou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03986</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feynman-kac neural network architectures for stochastic control using second-order fbsde theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Theodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning for Dynamics and Control</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="728" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Waveflow: A compact flow-based model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7706" to="7716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An introduction to partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Renardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sur la th?orie relativiste de l&apos;?lectron et l&apos;interpr?tation de la m?canique quantique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Schr?dinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;institut Henri Poincar?</title>
		<imprint>
			<date type="published" when="1932" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="269" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05600</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06832</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Discriminator optimal transport. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">Nvae: A deep hierarchical variational autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05931</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Solving schr?dinger bridges via maximum likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Thodoroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austen</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamacraft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02081</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep generative learning via schr?dinger bridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gefei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuling</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10410</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stochastic controls: Hamiltonian systems and HJB equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongmin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun Yu</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07579</idno>
		<title level="m">Diffusion normalizing flow</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">While Vargas et al. (2021) utilizes non-parametric models, e.g. Gaussian processes (hence are not directly comparable)</title>
		<imprint/>
	</monogr>
	<note>Model parametrization. both De Bortoli et al. (2021) and SB-FBSDE use DNNs to approximate the SB policies</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">While SB-FBSDE can also be trained with IPF (see Alg. 3), we stress that it is also possible to train both policies jointly whenever the computational budget permits. Interestingly, this joint optimization -which is not presented in concurrent methods -resembles the training scheme of the recently-proposed diffusion flow-based model (Zhang &amp; Chen, 2021)</title>
	</analytic>
	<monogr>
		<title level="m">Both concurrent methods rely on solving SB with IPF algorithm, which performs alternate training between the forward/backward policies</title>
		<imprint/>
	</monogr>
	<note>We highlight these flexible training procedures arising from the unified viewpoint provided in Theorem 4. Finally, with the close relation between Alg. 3 and IPF (despite with different objectives and SDE model classes). convergence analysis from classical IPF can be applied with few efforts. We leave it as a promising future work</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2021) and SB-FBSDE implement corrector sampling, they corresponds to different quantities. Specifically, our SB-FBSDE relies on the same predictor-corrector scheme proposed in SGM (see Sec 4.2 in Song et al. (2020)), where the &quot;corrector&quot; part is made with a Langevin sampling using the desired optimal density &quot;? log p t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corrector</forename><surname>Sampling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>While Both De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bortoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SB, this term corresponds exactly to adding the outputs of our networks &quot;Z + Z&quot;. This computation differs from the corrector sampling appearing in De Bortoli et</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>which relies on single network (i.e. either Z or Z). Crucially, this implies that the two methods is approaching different target distributions</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">hence leading to different training results. Notably, it has been reported in De Bortoli et al. (2021) that corrector sampling only gives negligible improvement (see ?J.2 in De Bortoli et al. (2021)), yet in our case we observe major quantitative improvement</title>
		<imprint/>
	</monogr>
	<note>up to 4 FID</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
