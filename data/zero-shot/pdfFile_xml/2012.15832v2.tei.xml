<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shortformer: Better Language Modeling Using Shorter Inputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
							<email>ofirp@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shortformer: Better Language Modeling Using Shorter Inputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scaling up transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> language models <ref type="bibr" target="#b12">Lewis et al., 2019;</ref><ref type="bibr" target="#b18">Raffel et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref> has been an important driver of progress in NLP. Language models require data to be segmented into subsequences for both training and inference: memory constraints limit a language model to handling at most a few thousand tokens at once, while many training and evaluation datasets are much longer.</p><p>Recent work focuses on increasing the length of input subsequences, which determines the maximum number of tokens a model can attend to <ref type="bibr" target="#b1">(Baevski and Auli, 2018;</ref><ref type="bibr" target="#b21">Sukhbaatar et al., 2019;</ref><ref type="bibr" target="#b11">Kitaev et al., 2020;</ref><ref type="bibr" target="#b19">Roy et al., 2020)</ref>.</p><p>We challenge the assumption that longer input subsequences are always better by showing that existing transformers do not always effectively use them. We then introduce new methods based on shorter input subsequences that improve runtime, memory efficiency, and perplexity.</p><p>We first investigate how input subsequence length affects transformer language models ( ?3). Na?ve evaluation-where we split a large evaluation set into multiple nonoverlapping subsequences, each evaluated independently-initially supports the commonly-held belief that models that train and do inference on longer subsequences achieve better perplexity <ref type="table" target="#tab_0">(Table 1, col. 3)</ref>.</p><p>However, when we evaluate each model with a sliding window <ref type="bibr" target="#b1">(Baevski and Auli, 2018)</ref>, outputting one token at a time using the maximal amount of context, we find-surprisingly-that models using subsequences exceeding 1,024 tokens do not further improve performance <ref type="table" target="#tab_0">(Table 1</ref>, col. 5).</p><p>We conclude that the performance gains (using na?ve evaluation) of models that use longer subsequences occur not only because of their better modeling ability, but partly because they divide the evaluation set into longer subsequences. This division helps because of an issue we call the early token curse: by default, early tokens in a subsequence will have short histories to attend to. Using longer subsequences means fewer tokens will suffer from the early token curse. For example, when using inputs of length 1,024, about 94% of tokens get to attend to more than 64 preceding tokens. If we use inputs of length 128, only 50% of tokens get to attend to 64 or more preceding tokens.</p><p>Based on this analysis, we explore how to improve models by using shorter inputs. We introduce two techniques.</p><p>Staged Training ( ?4) First, we show that initially training on shorter subsequences (before moving to longer ones) leads not only to much faster and more memory-efficient training, but it surprisingly also greatly improves perplexity, suggesting that longer inputs are harmful early in training.</p><p>Position-Infused Attention ( ?5) Second, we consider a natural way to avoid the early token curse during training and inference: attending to cached representations from the previously evaluated subsequence <ref type="bibr" target="#b6">(Dai et al., 2019)</ref>. This approach interferes with conventional absolute position embeddings in a way that forced <ref type="bibr">Dai et al.</ref> to use relative position embeddings, which are computationally expensive. We introduce a fast, simple alternative: instead of adding absolute position embeddings to word embeddings-thereby entangling a word's content and positional information-we add them to the keys and queries in the self-attention mechanism (but not to the values). This does not increase parameter count or runtime. Token representations can then be cached and reused in subsequent computations. We show that when using this method, shorter subsequence models outperform longer ones.</p><p>Finally, we show additive gains from combining staged training and position-infused attention (Shortformer, ?6), resulting in a model that trains much quicker and achieves better perplexity on WikiText-103. We also show that these results transfer to language modeling on the Toronto Book Corpus ( ?A.5, appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Experimental Setup</head><p>Transformer language models map a list of tokens x n?L:n?1 to a probability distribution over the next token x n . We refer to the list of tokens as the current input subsequence (whose length is L). Causal masking lets us make L predictions at once, with the prediction for token i + 1 conditioned on the ith token and all previous inputs x n?L:i?1 , but not on future inputs. We define the number of tokens the model can attend to at each timestep as its effective context window. Note that L is not to be confused with the (typically much greater) length of a training or evaluation dataset.</p><p>During inference, language models can be used for two distinct tasks: generation and evaluation. In order to define these tasks, we first define nonoverlapping and sliding window inference.</p><p>Nonoverlapping Inference To evaluate a string longer than L, we can evaluate each subsequence of L tokens independently. This fast approach is commonly used during training; if used, tokens in one subsequence cannot condition on those in the previous subsequence, giving rise to the early token curse discussed in ?1. See <ref type="figure">Figure 1(a)</ref>.</p><p>Sliding Window Inference An alternative to the above is to use a sliding window during inference.</p><p>Here, we choose a stride S between 1 and L ? 1 and advance the window by S tokens after each forward pass. 2 This means that L ? S tokens from the previous block are re-encoded, and only S new tokens are outputted. The advantage is that all outputs in each subsequence after the first have at least L ? S previous tokens to condition on. However, since tokens must be re-encoded multiple times, this approach is much slower. When S = 1, we output one token every inference pass, each using the maximal context window, but this is the slowest approach. See <ref type="figure">Figure 1</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal and Maximal Effective Context Window Sizes</head><p>In the nonoverlapping approach, the min. and max. effective context window sizes are 1 and L, respectively. In the sliding window approach, the max. context window size is still L, but the min. context window size is now L ? S + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation vs. Generation</head><p>In evaluation, a model assigns a perplexity score to a given sequence. Evaluation is done using either nonoverlapping inference or with a sliding window of any stride; since we already have the target sequence we can simultaneously make predictions for multiple timesteps using causal masking. In generation, a model generates a new sequence, as in demonstrations of GPT-3 <ref type="bibr">(Brown et al., 2020)</ref>. Generation is done only with a sliding window with stride S = 1, which we refer to as token-by-token generation. During generation, we append to the input a single new token, get a prediction from the model about the next token (e.g., using beam search or picking the token with the highest probability); the process is then repeated. 3 (a)</p><formula xml:id="formula_0">a 1 b 2 c 3 d 1 e 2 f 3 (b) a 1 b 2 c 3 b 1 c 2 d 3 c 1 d 2 e 3 d 1 e 2 f 3 (c) a 1 b 2 c 3 d 4 e 5 f 6</formula><p>Figure 1: Language model modes for generating or evaluating 6 tokens (a, b, . . . , f ) when subsequence length L = 3. The numbers denote the position embeddings (P.E.). (a) Nonoverlapping ( ?2). (b) Sliding window, stride S = 1 . Here, after the first inference pass we ignore all outputs other than the last ( ?2). (c) Caching ( ?5.2) where each subsequence attends to representations of the previous one. (In the next iteration, tokens d, e and f become the cache, with P.E. 1, 2 and 3, the three new tokens get P.E. 4, 5, and 6.)</p><p>Experimental Setup Our baseline is the Baevski and Auli (2018) model, henceforth B&amp;A, trained and evaluated on WikiText-103 <ref type="bibr" target="#b13">(Merity et al., 2016)</ref>. We use this baseline because of its prominent role in recent language modeling developments <ref type="bibr" target="#b10">(Khandelwal et al., 2020;</ref><ref type="bibr" target="#b14">Press et al., 2020)</ref>. The training set contains 103.2 million tokens from English Wikipedia. The B&amp;A model has 16 transformer layers of dimension 1,024, with 8 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4,096. This model ties the word embedding and softmax matrices <ref type="bibr" target="#b15">(Press and Wolf, 2017;</ref><ref type="bibr" target="#b8">Inan et al., 2017)</ref> and uses sinusoidal position embeddings. It has a subsequence length of 3,072 tokens and achieves a perplexity of 18.65 ? 0.24 (std. dev.) on the development set. In our experiments, other than varying the subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs (205).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">How Does Context Window Size Affect Transformers?</head><p>Segmenting a corpus into subsequences results in different effective context windows for different timesteps depending on where they fall in a segment. Subsequence length L is an upper bound on the effective context window at each timestep. When making the first prediction, the model attends only to the first input token. When making the second prediction, the model attends to the first two inputs, and so on, up to the Lth timestep where the model can attend to all input tokens when making the Lth prediction. of tokens in each batch to 9,216 but vary the subsequence length L and batch size (so the product of the batch size and subsequence length remains at 9,216). We report results for both nonoverlapping inference and sliding window inference with stride S = 1, which generates only one new token per forward pass; it thus has the maximal effective context window for each generated token. We find that performance increases as S decreases until it reaches a peak and then stops improving (not shown in <ref type="table" target="#tab_0">Table 1</ref>). <ref type="bibr">5</ref> We derive the following conclusions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Window Size Matters</head><p>Training on long sequences is expensive. Models trained on subsequences of length 256 are twice as fast as models trained on subsequences of 3,072 tokens, but gains for even shorter lengths are negligible (Tab. 1, col. 2).</p><p>Long subsequence lengths can improve results. When using the na?ve approach, nonover-L = 512 to run slowly (in N.o. eval.), although during batched N.o. eval. they are slightly faster than the L = 512 model. <ref type="bibr">5</ref> For example, the L = 3,072 model's performance peaked at S = 512 (used in <ref type="bibr" target="#b1">Baevski and Auli (2018)</ref>) and then stopped improving. Thus, the result shown in <ref type="table" target="#tab_0">Table 1</ref> for that model with S = 1 can also be achieved with S = 512 even though that runs 500 times faster, at 2.5k tok./sec. lapping evaluation, we see a monotonic decrease in dev. perplexity when increasing L (Tab. 1, col. 3).</p><p>Increasing the minimum effective context window size is more important than increasing the maximum one. Using a sliding window for token-by-token evaluation substantially improves results for all models (Tab. 1, col. 5). Here, we see negligible improvement between the models trained with subsequence lengths of 1,024 and 3,072 tokens (0.05 perplexity). This approach improves results by increasing the minimum amount of context available at each timestep which indicates that long contexts may not be beneficial to transformer models, but very short contexts are harmful. However, sliding window inference can be expensive since each token is encoded many times. For example, token-by-token inference for the L = 3,072 model is almost 300 times slower than nonoverlapping inference.</p><p>4 Training Subsequence Length ?3 results show that models trained on shorter subsequences can be effective at test time, and are much faster to train. We further explore this below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Staged Training</head><p>We propose a two-stage training routine that initially uses short input subsequences followed by long subsequences. 6 This method was previously applied to speed up the training of BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, but we show that it also improves perplexity.</p><p>We use sinusoidal position embeddings; learned position embeddings, which we do not consider, create a dependency between the parameterization and subsequence length. In our experiments, we neither modify nor reset the state of the optimization algorithm between the two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>Our experimental setup is described in ?2. We do not change any hyperparameters other than reducing subsequence length while correspondingly increasing batch size to keep the number of tokens per batch constant. As in the baseline, all models are trained for 205 epochs.</p><p>All models are trained in two stages; the second stage always uses a subsequence length of 3,072, since that lead to the best performance (discussed at end of this subsection).</p><p>Appendix <ref type="table" target="#tab_8">Table 6</ref> shows the time each training routine takes to match the baseline model's performance on the validation set of  Many configurations match this performance in less than half the time it takes to train the baseline itself; some reach baseline performance in only 37% of the time needed to train the baseline.</p><p>Although all models take less time to train than the baseline, <ref type="table" target="#tab_1">Table 2</ref> shows that many outperform it. For example, the best model-trained with subsequence length L = 128 until epoch 50outperforms the baseline by 1.1 perplexity despite completing training in 87% of the time the baseline takes to do so. The model that trains with L = 128 until epoch 100 achieves similarly strong results (17.62 perplexity) and finishes training in 74% of the time it takes the baseline. <ref type="bibr">8</ref> These results are very robust to the choice of initial stage subsequence length and number of epochs. <ref type="table" target="#tab_1">Table 2</ref> shows that all models with an initial stage of L = 1,024 tokens or less that switch to the second stage at epoch 125 or before beat the baseline by a large margin at the end of training.</p><p>Additionally, Appendix <ref type="table" target="#tab_8">Table 6</ref> shows that those models match the baseline's perplexity in at most 71% of the time it takes to train the baseline.</p><p>When we use nonoverlapping evaluation, the B&amp;A baseline obtains 18.65 perplexity on the development set; our best model obtains 17.52. When we use sliding window evaluation (following Baevski &amp; Auli, we use stride S = 512), our best 7 <ref type="table" target="#tab_9">Table 7</ref> in the appendix shows the epoch at which every model matched the baseline's performance.</p><p>8 <ref type="table" target="#tab_10">Table 8</ref> in the appendix shows the total time it took to train each model. model obtains 16.89 perplexity, a large improvement on the 17.92 B&amp;A result in that setting. On the test set, using the same sliding window evaluation, our model obtains 17.56 perplexity, a substantial gain over the baseline's 18.70 test-set perplexity. Appendix <ref type="table" target="#tab_0">Table 10</ref> shows that our best model uses almost five times less memory during the first stage than the baseline.</p><p>We also found that setting L to less than 3,072 tokens in the second stage degraded performance. (Appendix <ref type="table" target="#tab_12">Table 9</ref> shows staged training results with an initial stage length of 128 for 50 epochs (as in the best model) and varying lengths for the second stage. We found this to also be true for other initial stage lengths and epochs.) Unlike results in <ref type="table" target="#tab_0">Table 1</ref>, where we show that models with L larger than 1,024 do not substantially improve token-bytoken generation perplexity, models trained using staged training improve when given longer inputs (Appendix <ref type="table" target="#tab_12">Table 9</ref>). Further, we explored using more than two stages (up to six), but this did not outperform our two-stage curriculum.</p><p>Finally, Appendix A.5 shows that staged training substantially improves on the Toronto Book Corpus <ref type="bibr" target="#b24">(Zhu et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Repositioning Position Embeddings</head><p>Sliding window inference substantially improves performance by increasing the minimum effective context window size. But it is very slow. We could solve this by letting the model attend to representations of the previous subsequence during inference on the current one.</p><p>In this case, the same token representations would be used in different positions since a token generated near the end of one subsequence would be cached and reused near the start of the next one. However, transformer model representations entangle positional and content information, so a cached token representation would encode an incorrect position when reused in a new position.</p><p>TransformerXL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> uses relative position embeddings to solve this problem. However, that approach is slower and uses more parameters and memory than the baseline transformer. <ref type="bibr">9</ref> We solve this using no extra parameters, memory, or runtime. We also show that our method can use much shorter input subsequences and still achieve superior performance.</p><p>Transformer Language Models The baseline transformer LM, given a token list T of length L and a tensor P containing the first L position embeddings, produces L next-token predictions using the following procedure:</p><p>1. Embed each token in T , producing tensor X.</p><p>2. Add the position embedding of each index to the token at that index: X = X + P.</p><p>3. Feed X through each transformer layer. The self-attention sublayer in each transformer layer is invoked as follows: self-attention <ref type="bibr">(key=X, query=X, value=X)</ref> 4. Transform the outputs of the last transformer layer using the softmax layer, giving L nexttoken probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Position-Infused Attention (PIA)</head><p>We propose to let the model reuse previous outputs by making each output contain no explicit positional information. To do this, we modify the 9 The self-attention coefficients between q queries and k keys in TransformerXL are the sum of two dot products of size q ? k; the unmodified attention sublayer and our PIA method both compute only one dot product of size q ?k. We also benchmarked the TransformerXL model using its publicly released code and found that their relative position embeddings slow inference by 22% and require 26% more parameters than their implementation of the unmodified self-attention sublayer. model so that it does not add position embeddings at the beginning of the computation (step 2), but rather adds them to the query and key vectors at each layer (but not to the value vectors). The outputs at each layer are the transformed, weighted sums of the value vectors, and, since the value vectors in our model do not contain explicit positional information, the outputs also do not.</p><p>Formally, steps 1 and 4 do not change, step 2 is omitted, and step 3 is modified to invoke the self-attention sublayer as follows: Although PIA sublayer outputs contain no explicit positioning information, the attention mechanism can still compute position-dependent outputs because positional information is added to the query and key vectors. Our method is implementable in just a few lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PIA Enables Caching</head><p>In the unmodified transformer, to generate a string whose length exceeds L, it would have to be split into separate subsequences, and the model would be unable to attend to the previous subsequence when generating the current one.</p><p>Using PIA, we can store and attend to representations of the previous subsequence since they no longer contain any explicit positioning information.</p><p>Therefore, all our PIA models use a cache, where representations from the previous forward pass are stored and attended to in the next forward pass.</p><p>Caching makes generation faster. The complexity of the attention mechanism is O(q?k) where q is the number of queries (outputs) and k is the number of key-value pairs (inputs). To generate a sequence whose length exceeds L using tokenby-token generation in the unmodified transformer (with subsequence length L), attention takes O(L 2 ) time (since there are L queries and L keys). Using PIA and caching, we can reuse L ? 1 of the previous outputs at every layer. Thus, our attention sublayer takes O(L) time (because now there is a single query and L keys).</p><p>Our approach is useful in scenarios where we need to evaluate or generate sequences that are longer than the model's subsequence length. Therefore, it would not be applicable to sequence-to-sequence tasks such as sentence-level translation, where sequence lengths are short.</p><p>Most language models, including B&amp;A, train on their data as nonoverlapping subsequences. This means that training subsequences can be shuffled at each epoch and consumed in random order. However, when using PIA, we would like the cache to contain the previous subsequence. We therefore do not shuffle the data, making the cached subsequence the previously occurring one. <ref type="figure">Figure 1(c)</ref> depicts training with a cache that contains representations of the previous subsequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We use the experimental setup described in ?2.</p><p>The B&amp;A baseline achieves 18.65 on the development set. We train two additional baselines, the first uses PIA without caching and the second uses caching but no PIA. If just PIA is used (without caching), performance degrades to 19.35 perplexity, but the model's speed and memory usage do not change. Using caching without PIA severely hurts performance, obtaining 41.59 perplexity. Disabling data shuffling in the PIA-only model achieves similar performance to that model when it does use data shuffling, at 19.44 perplexity. Not shuffling the data is necessary for recurrent-style training that caches previously computed subsequence representations.</p><p>Our next experiments use the recurrent-style training of <ref type="bibr" target="#b6">Dai et al. (2019)</ref>, where we receive L new tokens at every training iteration and attend to L cached representations (of the subsequence of tokens that came immediately prior to the L new tokens). As before, we output L predictions at every training iteration. This means that the maximal and minimal effective context window sizes are L + L and L + 1, respectively.</p><p>In all our models with PIA and caching, we set L = L because a manual exploration of different models where L = L did not yield better results. <ref type="table" target="#tab_3">Table 3</ref> compares the results of our models that use PIA and caching to the baseline on the WikiText-103 dev. set. Evaluation and generation speeds are shown in the nonoverlapping (N.o.) and sliding window (S.W., with stride S = 1) speed columns, respectively. 10 Unlike in the baseline, token-by-token evaluation in our model achieves the same perplexity as nonoverlapping evaluation  since in both cases, the predictions for each input subsequence are conditioned not only on the current input, but also on the previous input, making the context window the same in both inference modes (in both cases, at every timestep, the context window is all tokens up to that timestep). <ref type="table" target="#tab_3">Table 3</ref> shows that as we increase subsequence length, perplexity improves, peaking at 512 before starting to degrade. Our best model obtains 17.85 perplexity, which is multiple standard deviations better than the baseline <ref type="bibr">(18.65, N.o.)</ref>. <ref type="table" target="#tab_7">Table 5</ref> in ?6 shows a similar gain on the test set. The best model runs 1% slower than the baseline during N.o. eval. (since caching reduces the speed gain from smaller attention matrices in this mode). <ref type="table" target="#tab_0">Table 10</ref> (appendix) shows that it uses less than half of the memory the baseline does during training. Our best model trains 55% faster than the baseline.</p><p>Our best model, with subsequence length 512, has attention matrices of size 512 ? 1,024 (since we have 512 queries-one per every new token-and 1,024 keys and 1,024 values-one per every new token and every cached token). In the baseline, all attention matrices are of size 3,072 ? 3,072.</p><p>Caching previously computed representations lets us do token-by-token generation efficiently when generating more than L tokens. Our model is nine times faster than the baseline at token-bytoken generation even as it achieves better perplexity and uses much less memory (Tab. 3, col. 5).  PIA and caching also greatly improve perplexity on the Toronto Book Corpus; see A.5 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Shortformer Results</head><p>To assess whether the gains from staged training, PIA and caching are additive, we take our best caching PIA model, with subsequence length 512, and apply staged training to it, training it with a subsequence length of between 32 to 256 for the first half of training. 11 <ref type="table" target="#tab_5">Table 4</ref> shows the results. As in ?4.2, where staged training was applied to the unmodified baseline, the results are very robust to the choice of initial stage subsequence length, with all the different choices improving perplexity over the model that does not use staged training.</p><p>The best model (with initial subsequence length 128), which we call Shortformer, achieves 17.47 dev. set perplexity and trains 65% faster than the baseline. Since its attention matrices are of dimension 512 ? 1,024 (the baseline's are 3,072 ? 3,072), our model uses less memory <ref type="bibr">( ?A.4, appendix)</ref>. It has the same number of parameters as the baseline. <ref type="figure">Figure 3</ref> (appendix) compares our best models using each method we presented (and their combination) to the baseline. It shows that combining caching, PIA and staged training (Shortformer) yields the quickest training and best perplexity when using nonoverlapping evaluation. Evaluation speed is similar for all of these models.</p><p>Finally, <ref type="table" target="#tab_7">Table 5</ref> compares our best models on the test set of WikiText-103 to the state of the art. <ref type="bibr">12</ref> Shortformer is almost twice as fast to train as the baseline and achieves superior results. Like the  best model from ?5.3, it is nine times faster than the baseline for token-by-token generation.</p><p>Since it uses a cache, sliding window evaluation does not increase Shortformer's performance. By training the baseline with staged training (and no PIA or caching), we obtain a model (our best model from ?4.2) that, with sliding window eval., obtains even better results, but that model is much slower than Shortformer <ref type="table" target="#tab_7">(Table 5</ref>, second-to-last row).</p><p>Shortformer outperforms the baseline's perplexity and performs within a standard deviation of the Sandwich Transformer <ref type="bibr" target="#b14">(Press et al., 2020)</ref> and TransformerXL. It does not outperform the Compressive Transformer <ref type="bibr" target="#b17">(Rae et al., 2020)</ref>, Routing Transformer <ref type="bibr" target="#b19">(Roy et al., 2020)</ref> and kNN-LM <ref type="bibr" target="#b10">(Khandelwal et al., 2020)</ref>, which make orthogonal improvements that can be applied to any language model, at the price of slower decoding. Combining them with our approach may yield further gains. These results are similar to those we obtain on the Toronto Book Corpus ( ?A.5 in the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Staged Training <ref type="bibr" target="#b7">Devlin et al. (2019)</ref> used a staged training routine for BERT by performing the first 90% of training on short subsequences (of length 128) before moving on to longer ones (of length 512). They use this method to speed training, but we show that also it improves perplexity and analyze different configurations of this method.</p><p>Many recent papers have explored improving transformer efficiency by reducing the quadratic cost of self-attention, motivated by scaling to longer sequences <ref type="bibr" target="#b11">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b19">Roy et al., 2020;</ref><ref type="bibr" target="#b22">Tay et al., 2020)</ref>. We instead demonstrate improved results with shorter sequences, which naturally also improve efficiency. One way to reduce transformer memory usage is to sparsify the attention matrix by letting the model attend only to a subset of nearby tokens at each timestep <ref type="bibr" target="#b2">Beltagy et al., 2020;</ref><ref type="bibr" target="#b19">Roy et al., 2020)</ref>. Training on shorter subsequence lengths is much more efficient: we use multiple, but much smaller, attention matrices. Since attention uses memory and computation in a way that scales quadratically with input size, splitting the inputs into multiple subsequences each processed independently lets us use less memory and run faster. Like our method, <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref> attend at each timestep to a growing number of neighbors as training progresses, but they use five stages, which we found not to be superior to our two-staged method.</p><p>The adaptive attention span model of <ref type="bibr" target="#b21">Sukhbaatar et al. (2019)</ref> learns the maximum effective context window sizes for each head at each layer independently. Like in our method, context window sizes are smaller at the start of training and lengthen as training progresses. We show that a simple approach of manually choosing two subsequence lengths is highly effective. In addition, keeping subsequence lengths equal across all heads and layers lets us save memory and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position-Infused</head><p>Attention TransformerXL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> caches and attends to previous representations using an attention sublayer that uses relative positioning <ref type="bibr" target="#b20">(Shaw et al., 2018)</ref>. It runs much slower than the unmodified attention sublayer, requires extra parameters, and requires internally modifying the self-attention sublayer, while our PIA method ( ?5) does not.</p><p>In parallel with our work, <ref type="bibr" target="#b9">Ke et al. (2020)</ref> compute attention coefficients by summing two attention matrices, one based on position-position interactions and the other on content-content interactions. As in PIA, they do not add position embeddings at the bottom of the model. They present results only for BERT, which uses much smaller subsequences than our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our results challenge the conventional wisdom that longer subsequences are always better. By first training on shorter subsequences and then progressing to longer ones via staged training, we improve perplexity and reduce training time. We additionally propose position-infused attention, which enables caching and efficiently attending to previous outputs; we show that models using this method do not require large input subsequences. We finally show that these two methods can be combined to produce a speedier and more accurate model. <ref type="table" target="#tab_8">Table 6</ref> shows the time each staged training model needs to match baseline performance, as a fraction of the time it takes to train the baseline. The fastest three configurations each match the baseline's performance in just 37% of the time it takes to train the baseline. This result is very robust to hyperparameter changes, as all models trained with initial subsequence length of between 64 and 512, that switch to the second stage at epoch 50 to 150, manage to match the baseline's performance in at most 59% of the time it takes to train it.    <ref type="table" target="#tab_9">Tables 7 and 8</ref> show the epoch at which each model matched the baseline's performance and the total time it took to train each of our staged training models.  <ref type="table" target="#tab_12">Table 9</ref>, we show the results of training with a first stage with L = 128 for 50 epochs, and using varying subsequence lengths for the second stage. The best result is obtained when the second stage uses L = 3,072. In addition, in all of our other experiments (not presented here) with different L and epoch number values for the first stage, we observed that using L = 3,072 for the second stage always achieved the best perplexities. Models trained with staged training and evaluated with a sliding window sometimes perform slightly worse when S is decreased, but this difference is much smaller than the standard deviation. The L = 1536 and L = 3072 models peaked at S = 512, and then as S was decreased perplexity started slightly degrading. 13</p><p>A.3 Training Speed vs. Performance <ref type="figure">Figure 3</ref> compares the validation performance and training speed of the baseline to our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Memory Usage</head><p>To understand how much memory our models and the baseline use during training, we find the largest batch size that we can load into memory for both our models and the baseline. Models that can simultaneously make more predictions are more memory efficient. <ref type="bibr">13</ref> We conjecture that this is because of a train-test mismatch that occurs since the average effective context length during training is 3,072 2 = 1,536 and so the model focuses on learning how to make predictions for the tokens in the center of the input, and does not perform as well when making predictions for tokens at the end of the input (which is what we use when using sliding window evaluation).    <ref type="figure">Figure 3</ref>: Dev. perplexity vs. training speed for the baseline and our best staged training model, our best PIA and caching model, and our best combined model (Shortformer). All models are evaluated using nonoverlapping evaluation. <ref type="table" target="#tab_0">Table 10</ref> shows the memory usage for the baseline model and our models. Since our Shortformer model has much smaller attention matrices, during training it can make more than double the nexttoken predictions than the baseline can in each feedforward pass. During inference, we use a batch size of 1 throughout the paper, following <ref type="bibr" target="#b6">(Dai et al., 2019)</ref>, and in our experiments, the PIA + Caching model, the final staged training model and the baseline all use a similar amount of memory during nonoverlapping evaluation. During token-by-token inference, the maximum number of predictions for the baseline model is 7, whereas our model can fit a batch size of 39 (so 39 predictions are made during token-by-token inference), making our model more than 5 times more memory efficient than the baseline. Using a batch size of one is a realistic benchmarking scenario: in large models such as GPT-3, a batch size of one is used during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Toronto Book Corpus</head><p>To verify that our results transfer to other datasets, we ran our models on the Toronto Book Corpus (TBC) <ref type="bibr" target="#b24">(Zhu et al., 2015)</ref>, a 700M token collection of books that has previously been used in the training corpus of BERT (along with English Wikipedia). We use the same train/development/test split as <ref type="bibr" target="#b10">(Khandelwal et al., 2020)</ref> and <ref type="bibr" target="#b14">(Press et al., 2020)</ref>, as well as their tokenization, which uses BERT's vocabulary of 29K BPE subwords. As in <ref type="bibr" target="#b10">(Khandelwal et al., 2020)</ref> and <ref type="bibr" target="#b14">(Press et al., 2020)</ref>, since the vocabulary is much smaller than WikiText-103's, we use a tied word embedding and softmax matrix <ref type="bibr" target="#b15">(Press and Wolf, 2017;</ref><ref type="bibr" target="#b8">Inan et al., 2017)</ref>, instead of using the adaptive word embeddings <ref type="bibr" target="#b1">(Baevski and Auli, 2018)</ref> as in the WikiText-103 models.</p><p>To fairly compare our models to the ones from <ref type="bibr" target="#b10">(Khandelwal et al., 2020)</ref> and <ref type="bibr" target="#b14">(Press et al., 2020)</ref>, our initial set of experiments on the TBC use a maximum subsequence length of 1,024 (for staged training), train for 59 epochs, and for all other hyperparameters we use the same values as the ones we used for WikiText-103 (see Experiment Setup in Section 2). In this setting, the baseline achieves a perplexity of 15.38 ? 0.39 (standard deviation) on the development set.</p><p>We do not tune the hyperparameters of our methods on the TBC, we simply use the same values as the best ones that we found on the WikiText-103 dataset. For staged training, our best model trained for 50 205 % of the epochs with L = 128 and spent the rest of training with the same subsequence size as the baseline. For the TBC, we again trained the staged training model model with L = 128 for the first 50 205 % of training, and then move on to L = 1,024, to match the Sandwich Transformer <ref type="bibr" target="#b14">(Press et al., 2020)</ref> and kNN-LM (Khandelwal et al., 2020) which used 1,024 as the subsequence length.</p><p>For the PIA + Caching model, we set L = 512, as we did for our best PIA + Caching on the WikiText-103 dataset.</p><p>For the Toronto Book Corpus Shortformer, we trained for the first half of training with L = 128 before moving on to training with L = 512, as in our WikiText-103 models (Section 6).  <ref type="table" target="#tab_0">Table 11</ref>: Comparison of our best models to other strong LMs trained on the Toronto Book Corpus (TBC). Following <ref type="bibr" target="#b10">Khandelwal et al. (2020)</ref> and <ref type="bibr" target="#b14">Press et al. (2020)</ref>, for the baseline and our staged training model, we set L = 1,024 and S = 512 when using sliding window (S.W.) evaluation in the TBC dataset. All models have 261M parameters. * kNN-LM requires a 400GB datastore. <ref type="table" target="#tab_0">Table 11</ref> shows that staged training and the Shortformer improve over the baseline by a wide margin and match the results of the Sandwich Transformer and the kNN-LM. As noted in Section 6, those contributions are orthogonal to ours, and combining them might yield further gains. Since in <ref type="table" target="#tab_0">Table 11</ref> the final stage of the staged training model (and the baseline) both have L = 1,024, Shortformer lacks a speed advantage in this scenario. <ref type="table" target="#tab_0">Table 12</ref> shows results for our staged training model trained with a final stage subsequence length of 3,072 tokens, as in our WikiText-103 experiments in Section 4. This model trains faster than the L = 3,072 baseline and also achieves much better perplexity scores (the baseline in this setting achieves a perplexity of 14.52 ? 0.15 (standard deviation) on the development set). In addition, note that the Shortformer model from <ref type="table" target="#tab_0">Table 11</ref> achieves better perplexity than even the baseline with L = 3,072, although Shortformer is much faster to train and uses much smaller attention matrices during inference (of size 512 ? 1024; the baseline has attention matrices of size 3,072 ? 3,072, as in Section 6).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Inputs to the self-attention sublayer, conventionally (left) and with position-infused attention (right), for L = 3, at timestep 3. The numbers denote the position embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 (b) depicts this method. Although PIA sublayer outputs contain no explicit positioning information, the attention mechanism can still compute position-dependent outputs because positional information is added to the query and key vectors. Our method is implementable in just a few lines of code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>60 0.54 0.53 0.65 0.64 0.71 50 0.53 0.48 0.47 0.54 0.59 0.63 0.81 75 0.51 0.43 0.42 0.48 0.53 0.56 0.79 Initial Stage Epochs 100 0.52 0.40 0.38 0.41 0.47 0.50 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>explores the effect of subsequence length</cell></row><row><cell>in the B&amp;A model on training runtime and on dev.</cell></row><row><cell>set perplexity and runtime. 4 We fix the number</cell></row><row><cell>use the ground truth token. This has the same complexity as</cell></row><row><cell>sampling the token with the highest probability.</cell></row><row><cell>4 For consistency, throughout the paper we run inference</cell></row><row><cell>with a batch size of one. This causes models shorter than</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Each model's perplexity at the end of training (dev. set, nonoverlapping eval.). All models have a subsequence length of 3,072 tokens at the end of training. The B&amp;A baseline achieves 18.65 ? 0.24 perplexity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Dev. perplexity and speed for PIA models</cell></row><row><cell>trained with different subsequence lengths (L). PIA</cell></row><row><cell>models attend to L new and L cached tokens at each in-</cell></row><row><cell>ference pass. N.o. is nonoverlapping eval.; S.W. is slid-</cell></row><row><cell>ing window eval., where we always use S = 1 (token-</cell></row><row><cell>by-token) here. The baseline is evaluated with both</cell></row><row><cell>evaluation methods. We measure speed in tok./sec. per</cell></row><row><cell>GPU and use a batch size of 1 for inference.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Dev. perplexity for models that use PIA, caching, and staged training (with final subseq. length of 512). We measure speed in tok./sec. per GPU. Evaluation speed is the same for all models, at 14.5k tok./sec.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Speed ? Mode Speed ? PPL ?</figDesc><table><row><cell></cell><cell></cell><cell>Train</cell><cell cols="3">Inference (Test)</cell></row><row><cell cols="2">Model Param. ? Baseline 247M</cell><cell>13.9k</cell><cell cols="3">N.o. 14.7k 19.40 S.W. 2.5k 18.70</cell></row><row><cell cols="2">TransformerXL  *  257M</cell><cell cols="4">6.0k N.o. 3.2k 18.30</cell></row><row><cell>Sandwich T.</cell><cell>247M</cell><cell cols="4">13.9k S.W. 2.5k 17.96</cell></row><row><cell>Compressive T.</cell><cell>329M</cell><cell>-</cell><cell>N.o.</cell><cell>-</cell><cell>17.1</cell></row><row><cell>Routing T.</cell><cell>-</cell><cell>-</cell><cell>N.o</cell><cell>-</cell><cell>15.8</cell></row><row><cell>kNN-LM  *  *</cell><cell>247M</cell><cell cols="2">13.9k S.W.</cell><cell cols="2">145 15.79</cell></row><row><cell>PIA + Caching</cell><cell>247M</cell><cell cols="4">21.5k N.o. 14.5k 18.55</cell></row><row><cell cols="2">Staged Training 247M</cell><cell cols="4">17.6k S.W. 2.5k 17.56</cell></row><row><cell>Shortformer</cell><cell cols="5">247M 22.9k N.o. 14.5k 18.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Comparison of our best models to other strong</cell></row><row><cell>LMs (see text for citations and explanations) evaluating</cell></row><row><cell>the WikiText-103 test set, where S = 512. We mea-</cell></row><row><cell>sure speed in tok./sec. per GPU, and use a batch size of</cell></row><row><cell>1 for inference.  *  TransformerXL runs on an older ver-</cell></row><row><cell>sion of PyTorch, which might affect speed.  *  *  kNN-LM</cell></row><row><cell>requires a 400GB datastore.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: Time needed to match baseline performance</cell></row><row><cell cols="3">(dev. set, nonoverlapping eval.) as a fraction of time</cell></row><row><cell cols="3">needed to train the baseline (smaller is better). Mod-</cell></row><row><cell cols="3">els never matching the baseline have empty cells. All</cell></row><row><cell cols="3">models have a subsequence length of 3,072 tokens at</cell></row><row><cell cols="2">the end of training.</cell></row><row><cell></cell><cell cols="2">Initial Stage Subsequence Length</cell></row><row><cell></cell><cell cols="2">32 64 128 256 512 1024 1536</cell></row><row><cell></cell><cell cols="2">25 136 123 122 146 144 155</cell></row><row><cell>Initial Stage Epochs</cell><cell cols="2">50 135 124 122 136 144 149 179 75 143 128 125 136 144 145 181 100 158 135 130 136 145 142 175 125 190 149 141 140 146 144 174 150 176 160 154 153 151 174 175 191 178 177 176 189</cell></row><row><cell></cell><cell>200</cell><cell>202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Epoch at which each model matches the baseline. Some models never match the baseline, and so those cells are empty.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Total time needed to train each model as a fraction of the time needed for baseline training.</figDesc><table><row><cell>A.2 Staged Training with Shorter Final Stage</cell></row><row><cell>L</cell></row><row><cell>In section 4, all models presented used Staged</cell></row><row><cell>Training with a final input subsequence length L</cell></row><row><cell>of 3,072 tokens. In</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Inference perplexity for staged training models trained with an initial stage subsequence length of 128 for 50 epochs and varying second stage subsequence length L (for the second stage's 155 epochs). S is stride. To see how these models perform without staged training, refer toTable 1.</figDesc><table><row><cell></cell><cell></cell><cell>Training</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Max</cell><cell>Max</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Batch Size ? Predictions ?</cell></row><row><cell>Baseline</cell><cell></cell><cell>2</cell><cell>6,144</cell></row><row><cell></cell><cell>Stage 1</cell><cell>230</cell><cell>29,440</cell></row><row><cell>Staged Training</cell><cell>Stage 2</cell><cell>2</cell><cell>6,144</cell></row><row><cell>PIA + Caching</cell><cell></cell><cell>26</cell><cell>13,312</cell></row><row><cell></cell><cell>Stage 1</cell><cell>160</cell><cell>20,480</cell></row><row><cell>Shortformer</cell><cell>Stage 2</cell><cell>26</cell><cell>13,312</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Memory usage of the baseline and our models during WikiText-103 training. For each model we show the maximal batch size that it could fit on one GPU at once during training. The max predictions column denotes the number of tokens predicted at each feedforward pass, which we calculate by multiplying batch size by number of predictions per subsequence (which is equivalent to L). We benchmarked all models on a V100 GPU, with 32GB of memory. Note that the second stage in the staged training model matches the performance of the baseline model, because those architectures are identical. The same is true for the second stage of the Shortformer and the PIA + Caching model.</figDesc><table><row><cell></cell><cell>19.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18.5</cell><cell>Baseline</cell><cell></cell><cell></cell></row><row><cell>Perplexity ( )</cell><cell>17.5 18.0</cell><cell cols="2">Staged Training</cell><cell cols="2">PIA + Cache Shortformer</cell></row><row><cell></cell><cell>13k 17.0</cell><cell>15k</cell><cell cols="2">17k Tokens per Second ( ) 19k</cell><cell>21k</cell><cell>23k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Caching 20.5k N.o. 15.0k 13.86 11.20 Staged Training 25.5k N.o. 19.2k 13.81 11.18 S.W. 9.6k 13.13 10.72 Shortformer 21.3k N.o. 15.5k 13.40 10.88</figDesc><table><row><cell></cell><cell>Train</cell><cell></cell><cell cols="2">Inference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PPL ?</cell></row><row><cell>Model</cell><cell cols="4">Speed? Mode Speed? Dev. Test</cell></row><row><cell>Baseline</cell><cell>24.0k</cell><cell cols="3">N.o. 19.2k 15.38 12.73 S.W. 9.6k 14.75 11.89</cell></row><row><cell>kNN-LM  *</cell><cell cols="2">24.0k S.W.</cell><cell>-</cell><cell>14.20 10.89</cell></row><row><cell>Sandwich T.</cell><cell cols="3">24.0k S.W. 9.6k</cell><cell>-10.83</cell></row><row><cell>PIA +</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Comparison of the staged training model to the baseline, when the subsequence length L is set to 3,072. In this table, S = 512.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ ofirpress/shortformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Nonoverlapping inference can be viewed as sliding window inference with stride L.3 In this paper we do not consider open-ended generation; we generate the dev. set, and for next-token prediction we</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Curriculum learning<ref type="bibr" target="#b3">(Bengio et al., 2009</ref>) trains on easier inputs before progressing to harder ones. Our approach does not change the order in which the training examples are given to the model, but instead modifies their lengths.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Note that<ref type="bibr" target="#b1">Baevski and Auli (2018)</ref> show that the baseline model can also achieve 17.92 during S.W. evaluation, when S = 512, with a speed of 2.5k tokens per second.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We picked 50% of epochs as the length of the first stage since that produced near-optimal results at a fast speed in ?4.12  We benchmarked speed, on V100 GPUs, for all models that had publicly available code.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Tim Dettmers, Jungo Kasai, Gabriel Ilharco, Hao Peng, Sewon Min, Mandar Joshi, Omer Levy, Luke Zettlemoyer, Julian Michael, Edward Misback, Sofia Serrano, Nikolaos Pappas, Jesse Dodge, Myle Ott, and Sam Shleifer for their valuable feedback and fruitful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Staged Training Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>100 18.14 17.67 17.62 18.00 18.10 18.00 18.51 125 18.61 17.88 17.70 18.00 18.13 17.98 18.49 150 19.45 18.37 17.98 18.01 18.15 18.00 18.49 175 21.16 19.51 18.57 18.23 18.20 18.08 18.57 200 35.38 28.03 23.80 21.45 19.63 18.56 18.84</idno>
		<title level="m">Initial Stage Epochs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1809.10853</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/sparse-transformers" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Rethinking positional encoding in language pre-training</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalization through Memorization: Nearest Neighbor Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1032</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<idno>100 0.75 0.75 0.74 0.75 0.77 0.80 0.87 125 0.68 0.68 0.68 0.69 0.71 0.75 0.84 150 0.62 0.62 0.61 0.62 0.65 0.70 0.81 175 0.56 0.56 0.55 0.56 0.59 0.66 0.78 200 0.49 0.49 0.48 0.50 0.53 0.61 0.75</idno>
		<title level="m">Initial Stage Epochs</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
