<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENT-DESC: Entity Description Generation by Exploring Knowledge Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liying</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Design 2 DAMO Academy</orgName>
								<orgName type="department" key="dep2">Alibaba Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekun</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">York University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Design 2 DAMO Academy</orgName>
								<orgName type="department" key="dep2">Alibaba Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
							<email>zhanmingjie@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Design 2 DAMO Academy</orgName>
								<orgName type="department" key="dep2">Alibaba Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Design 2 DAMO Academy</orgName>
								<orgName type="department" key="dep2">Alibaba Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
						</author>
						<title level="a" type="main">ENT-DESC: Entity Description Generation by Exploring Knowledge Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous works on knowledge-to-text generation take as input a few RDF triples or keyvalue pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>KG-to-text generation, automatically converting knowledge into comprehensive natural language, is an important task in natural language processing (NLP) and user interaction studies <ref type="bibr" target="#b6">(Damljanovic et al., 2010)</ref>. Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of * Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design.</p><p>? Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. <ref type="bibr">1</ref> Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bruno Mars</head><p>retro style, <ref type="bibr">funk, rhythm and blues, hip hop music, ... Peter Gene Hernandez (born October 8, 1985)</ref>, known professionally as Bruno Mars, is an American singer, songwriter, multi-instrumentalist, record producer, and dancer. He is known for his stage performances, retro showmanship and for performing in a wide range of musical styles, including <ref type="bibr">R&amp;B, funk, pop, soul, reggae, hip hop, and rock.</ref> knowledge graph  WebNLG <ref type="bibr" target="#b12">(Gardent et al., 2017)</ref>, key-value pairs of WIKIBIO <ref type="bibr" target="#b19">(Lebret et al., 2016)</ref> and E2E <ref type="bibr" target="#b24">(Novikova et al., 2017)</ref>, to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., <ref type="bibr">WIKIBIO and E2E)</ref>, and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and expensive annotation process. More importantly, in practice, the knowledge regarding the main entity could be more than enough, and the description may only cover the most significant knowledge. Thereby, the generation model should have such differentiation capability.</p><p>In this paper, we tackle an entity description generation task by exploring KG in order to work towards more practical problems. Specifically, the aim is to generate a description with one or more sentences for a main entity and a few topic-related entities, which is empowered by the knowledge from a KG for a more natural description. In order to facilitate the study, we introduce a new dataset, namely entity-to-description (ENT-DESC) extracted from Wikipedia and Wikidata, which contains over 110k instances. Each sample is a triplet, containing a set of entities, the explored knowledge from a KG, and the description. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example to generate the description of the main entity, i.e., Bruno Mars, given some relevant keywords, i.e., retro style, funk, etc., which are called topic-related entities of Bruno Mars. We intend to generate the short paragraph below to describe the main entity in compliance with the topic revealed by topic-related entities. For generating accurate descriptions, one challenge is to extract the underlying relations between the main entity and keywords, as well as the peripheral information of the main entity. In our dataset, we use such knowledge revealed in a KG, i.e., the upper right in <ref type="figure" target="#fig_1">Figure 1</ref> with partially labeled triples. Therefore, to some extent, our dataset is a generalization of existing KG-to-text datasets. The knowledge, in the form of triples, regarding the main entity and topic entities is automatically extracted from a KG, and such knowledge could be more than enough and not necessarily useful for generating the output.</p><p>Our dataset is not only more practical but also more challenging due to lack of explicit alignment between the input and the output. Therefore, some knowledge is useful for generation, while others might be noise. In such a case that many different relations from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs <ref type="bibr" target="#b3">(Beck et al., 2018)</ref>. However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information <ref type="bibr" target="#b22">(Marcheggiani and Perez-Beltrachini, 2018)</ref>. As more GCN layers are being stacked, it suffers from information loss from KG <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018)</ref>. In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transformation incorporated with an aggregation layer. Multi-graph transformation is able to represent the original graph information more accurately, while the aggregation layer learns to extract useful information from the KG. Extensive experiments are conducted on both our dataset and benchmark dataset (i.e., WebNLG). MGCN outperforms several strong baselines, which demonstrates the effectiveness of our techniques, especially when using fewer GCN layers.</p><p>Our main contributions include: ? We construct a large-scale dataset ENT-DESC for a more practical task of entity description generation by exploring KG. To the best of our knowledge, ENT-DESC is the largest dataset of KG-to-text generation. ? We propose a multi-graph structure transformation approach that explicitly expresses a more comprehensive and more accurate graph information, in order to overcome limitations associated with Levi graphs. ? Experiments and analysis on our new dataset show that our proposed MGCN model incorporated with aggregation methods outperforms strong baselines by effectively capturing and aggregating multi-graph information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. <ref type="bibr" target="#b12">Gardent et al. (2017)</ref> introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples <ref type="bibr" target="#b11">(Fu et al., 2020b)</ref>. <ref type="bibr" target="#b17">Koncel-Kedziorski et al. (2019)</ref> introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. <ref type="bibr" target="#b19">Lebret et al. (2016)</ref> introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the article's infobox. <ref type="bibr" target="#b24">Novikova et al. (2017)</ref> introduced E2E dataset in the restaurant domain, which aimed to generate restaurant recommendations given 3 to 8 slot-value pairs. These two datasets were only for a single domain, while ours focuses on multiple domains of over 100 categories, including people, event, location, organization, etc. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref>. The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts <ref type="bibr" target="#b20">(Li et al., 2019;</ref><ref type="bibr" target="#b10">Fu et al., 2020a)</ref>. However, they did not explore the knowledge from a KG.</p><p>Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling  and neural machine translation <ref type="bibr" target="#b2">(Bastings et al., 2017)</ref>) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, <ref type="bibr" target="#b28">Velickovic et al. (2017)</ref> introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization.  then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, such graph-to-sequence models without graph transformation will incur information loss and parameter explosion. <ref type="bibr" target="#b3">Beck et al. (2018)</ref> proposed to transform the graph into Levi graph in order to work towards the aforementioned deficiencies, together with gated graph neural network (GGNN) to build graph representation for AMR-to-text problem. However, they face some new limitations brought in by Levi graph transformation: the entityto-entity information is being ignored in Levi transformation, as also mentioned in their paper. Afterwards, deeper GCNs were stacked <ref type="bibr" target="#b13">(Guo et al., 2019)</ref> to capture such ignored information implicitly. In contrast, we intend to use fewer GCN layers to capture more global contextualization by explicitly stating all types of graph information with different transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>In this paper, we tackle a practical problem of entity description generation by exploring KG. In prac- tice, it is difficult to describe an entity in only a few sentences as there are too many aspects for an entity. Now, if we are given a few topic-related entities as topic restrictions to the main entity, the text to be generated could be more concrete, particularly when we are allowed to explore the connections among these entities in KG. As seen in <ref type="figure" target="#fig_1">Figure 1</ref>, when we are asked to use one or two sentences to introduce "Bruno Mars" 2 , his popular singles will first come into some people's minds, while his music genres might be in other people's first thought. With the introduction of topic-related entities, the description will have some focus. In this case, when topic-related entities, i.e., R&amp;B, hip hop, rock, etc., are provided, we are aware of describing Bruno Mars in the direction of music styles on top of their basic information. Formally, given a set of entities e = {E 1 , ..., E n } and a KG G = (V, E), where E 1 is main entity, E 2 , ..., E n are topic-related entities, V is the set of entity nodes and E is the set of directed relation edges. We intend to generate a natural language text y = {y 1 , y 2 , ? ? ? , y T }. Meanwhile, we explore G for useful information to allow a more natural description. Here, the KG G can also be written as a set of RDF triples:</p><formula xml:id="formula_0">G = { V S 1 , P 1 , V O 1 , ..., V SM , P M , V OM }, where M is the total number of triples, V S i , V O i ? V are</formula><p>the subject and object entities respectively, P i is the predicate stating the relation between V S i and V O i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ENT-DESC Dataset</head><p>To prepare our dataset, we first use Nayuki's implementation 3 to calculate the PageRank score for more than 9.9 million Wikipedia pages. We then extract the categories from Wikidata for the top 100k highest scored pages and manually select 90 categories out of the top 200 most frequent ones as  the seed categories. The domains of the categories mainly include humans, events, locations and organizations. The entities from these categories are collected as our candidate set of main entities. We further process their associated Wikipedia pages for collecting the first paragraphs and entities with hyperlink as topic-related entities. We then search Wikidata to gather neighbors of the main entities and 1-hop/2-hop paths between main entities and their associated topic-related entities, which finally results in a dataset consisting of more than 110k entity-text pairs with 3 million triples in the KG. Although more-hop paths might be helpful, we limit to 1-hop/2-hop paths for the first study. The comparison of our dataset with WebNLG, AGENDA and E2E is shown in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>In the comparison of these four datasets, there are some obvious differences. First, our dataset is significantly larger than WebNLG, AGENDA and E2E (i.e., more than twice of their instances). Meanwhile, our vocabulary size and numbers of distinct entities/relations are all much larger. Second, the average number of input triples per instance is much larger than those of the other two. More importantly, our dataset provides a new genre of data for the task. Specifically, WebNLG has a strict alignment between input triples and output text, and accordingly, each input triple roughly corresponds to 8 words. AGENDA is different from WebNLG for generating much longer output, namely paper abstracts, with the paper title also given as input. Moreover, as observed, quite a portion of text information cannot be directly covered by the input triples. E2E focuses on the restaurant domain with relatively simple inputs, including 77 entities and 8 relations in total. Considering the construction details of these 3 datasets, all their input triples provide useful information (i.e., should be used) for generating the output. In contrast, our dataset has a much larger number of input triples, particularly considering the length difference of output texts. Lastly, another unique characteristic of our dataset is that not every input triple is useful for generation, which brings in the challenge that a model should be able to distill the helpful part for generating a better output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our MGCN Model</head><p>Given the explored knowledge, our task can be cast as a problem of generating text from KG. We propose an encoder-decoder architecture with a multi-graph transformation, shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-Graph Encoder</head><p>We first briefly introduce the general flow of multigraph encoder which consists of n MGCN layers. Before the first layer, graph embedding h (0) representing a collection of node embeddings is initialized from input KG after multi-graph transformation. By stacking n MGCN layers accordingly with multi-graph transformation and aggregation, we obtain the final graph representation by aggregating the outputs of n MGCN layers for decoding. We explain the details of an MGCN layer as follows.</p><p>Graph Encoder. Before introducing our multigraph transformation, we first look at our basic graph encoder in each MGCN layer (i.e., Graph Encoder 1 to 6 in <ref type="figure">Figure 3</ref> left). In this paper, we adopt graph convolutional networks (GCNs) <ref type="bibr" target="#b9">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b14">Kearnes et al., 2016;</ref><ref type="bibr" target="#b16">Kipf and Welling, 2017;</ref> as the basic encoder to consider the graph structure and to capture graph information for each node. More formally, given a directed graph G * = (V * , E * ), we define a feature vector x V ? R d for each node V ? V * . In order to capture the information of neighbors N (?), the node representation h V j for each V j ? V * is calculated as:</p><formula xml:id="formula_1">h V j = ReLU V i ?N (V j ) W P(i,j) x V i + b P(i,j) ,</formula><p>where P(i, j) denotes the edge between node V i and V j including three possible directions:  <ref type="figure">Figure 3</ref>: Overview of our model architecture. There are n MGCN layers in the multi-graph encoder, and 2 LSTM layers in the decoder. h (k?1) is the input graph representation at Layer k, and its 6 copies together with the corresponding adjacent matrices A i 's of transformed graphs in the multi graph (refer to <ref type="figure" target="#fig_4">Figure 4</ref>) are fed into individual basic encoders. Finally, we obtain the graph representation h (k) for the next layer by aggregating the representations from these encoders. Multi-Graph Transformation. The basic graph encoder with GCN architecture as described above struggles with the problem of parameter explosion and information loss, as the edges are encoded in the form of parameters. Previous works <ref type="bibr" target="#b3">(Beck et al., 2018;</ref><ref type="bibr" target="#b13">Guo et al., 2019;</ref><ref type="bibr" target="#b17">Koncel-Kedziorski et al., 2019)</ref> deal with this deficiency by transforming the graph into a Levi graph. However, Levi graph transformation also has its limitations, where entity-to-entity information is learned implicitly.</p><formula xml:id="formula_2">(1) V i to V j , (2) V j to V i ,<label>(</label></formula><formula xml:id="formula_3">A 6 A 5 A 4 A 3 A 2 h (k-1) h (k) h g4 (k-1) h g5 (k-1) h g1 (k-1) h g2 (k-1) h g6 (k-1) h g3 (k-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGCN Layer 2</head><p>In order to overcome all the difficulties, we introduce a multi-graph structure transformation. A simple example is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Given such a directed graph, where E 1 , E 2 , E 3 , E 4 represent entities and R 1 , R 2 , R 3 represent relations in the KG, we intend to transform it into multiple graphs which capture different types of information. Similar to Levi graph transformation, all the entities and relations are represented as nodes in our multi-graph structure. By doing such transformation, we are able to represent relations in the same format as entities using embeddings directly, which avoids the risk of parameter explosion. This multi-graph transformation can be generalised for any graph regardless of the complexity and characteristic of the KG, and the transformed graph can be applied to any model architecture.</p><p>In this work, we employ a six-graph structure for our multi-graph transformation as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Firstly, in self graph <ref type="formula" target="#formula_2">(1)</ref>, each node is assigned a self-loop edge namely self label. Secondly, graphs <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> are formed by connecting the nodes representing the entities and their adjacent relations. In addition to connecting them in their original direction using default1 label, we also add a reverse1 label for the inverse direction of their original relations. Thirdly, we create graphs <ref type="formula" target="#formula_2">(4)</ref> and <ref type="formula" target="#formula_2">(5)</ref> by connecting the nodes representing adjacent entities in the input graph, labeled by default2 and reverse2, respectively. These two graphs overcome the deficiency of Levi graph transformation by explicitly representing the entity-to-entity information from the input graph. It also allows us to differentiate entities and relations by adding edges between entities. Finally, in order to consider more global contextualization, we add a global node on top of the graph structure to form graph (6). Each node is assigned with a global edge directed from global node. In the end, the set of transformed graphs can be represented by their edge labels T = {self, default, reverse, default2, reverse2, global}. Given the six transformed graphs mentioned above, we construct six corresponding adjacency matrices: {A 1 , A 2 , ? ? ? , A 6 }. As shown in <ref type="figure">Figure  3</ref> (left), these adjacency matrices are used by six basic graph encoders to obtain the corresponding transformed graph representations (i.e., h g ).</p><p>Aggregation Layer. After learning 6 embeddings of multi graphs from the basic encoders at the current MGCN layer k ? 1, the model goes through an aggregation layer to obtain the graph  embedding for the next MGCN layer k. We can get it by simply concatenating all 6 transformed graph embeddings with different types of edges. However, such simple concatenation of the transformed graphs involves too many features and parameters. In order to address the challenge mentioned above, we propose three aggregation methods for the multigraph structure: sum-based, average-based and CNN-based aggregation.</p><p>Firstly, in sum-based aggregation layer, we compute the representation h (k) at k-th layer as:</p><formula xml:id="formula_4">h (k) = g i ?T h (k?1) g i , where h (k?1) g i</formula><p>represents the i-th graph representation, and T is the set of all transformed graphs. Sum-based aggregation allows a linear approximation of spectral graph convolutions and helps to reduce data sparsity and over-fitting problems.</p><p>Similarly, we apply an average-based aggregation method by normalizing each graph through a mean operation:</p><formula xml:id="formula_5">h (k) = 1 m g i ?T h (k?1) g i ,</formula><p>where m is the number of graphs in T .</p><p>We also try to employ a more complex CNNbased aggregation method. Formally, the representation h (k) at k-th layer is defined as:</p><formula xml:id="formula_6">h (k) = W conv h (k?1) mg + b (k) mg .</formula><p>Here, we use convolutional neural networks (CNN) to convolute the multi-graph representation, where h mg = [h g 1 , ..., h g 6 ] is the representation of multi-graph and b (k) mg is the bias term. By applying these aggregation methods, we obtain the graph representation for the next layer h (k) , which is able to capture different aspects of graph information more effectively by learning different types of edges in each transformed graph.</p><p>Stacking MGCN Layers. With the introduction of MGCN layer as described above, we can capture the information of higher-degree neighbors by stacking multiple MGCN layers. Inspired by <ref type="bibr" target="#b29">Xu et al. (2018)</ref>, we employ a concatenation operation over h (1) , ? ? ? , h (n) to aggregate the graph representations from all MGCN layers <ref type="figure">(Figure 3 right)</ref> to form the final layer h (f inal) , which can be written as follows:</p><formula xml:id="formula_7">h (f inal) = h (1) , ? ? ? h (n) .</formula><p>Such a mechanism allows weight sharing across graph nodes, which helps to reduce overfitting problems. To further reduce the number of parameters and overfitting problems, we apply the softmax weight tying technique <ref type="bibr" target="#b26">(Press and Wolf, 2017)</ref> by tying source embeddings and target embeddings with a target softmax weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Attention-based LSTM Decoder</head><p>We adopt the commonly-used standard attentionbased LSTM as our decoder, where each next word y t is generated by conditioning on the final graph representation h (f inal) and all words that have been predicted y 1 , ..., y t?1 . The training objective is to minimize the negative conditional log-likelihood. Thus, the objective function can be written as:</p><formula xml:id="formula_8">L = ? T t=1 log p ? (y t |y 1 , ..., y t?1 , h (f inal) ),</formula><p>where T represents the length of the output sequence, and p is the probability of decoding each word y t parameterized by ?. As shown in the decoder from <ref type="figure">Figure 3</ref>, we stack 2 LSTM layers and apply a cross-attention mechanism in our decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>We implement our MGCN architecture based on MXNET <ref type="bibr" target="#b4">(Chen et al., 2015)</ref> and Sockeye toolkit. Hidden units and embedding dimensions for both encoder and decoder are fixed at 360. We use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.0003 and update parameters with a batch size of 16. The training phase is stopped when detecting the convergence of perplexity on the validation set.  During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU <ref type="bibr" target="#b25">(Papineni et al., 2002)</ref>, ME-TEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2011)</ref>, TER <ref type="bibr" target="#b27">(Snover et al., 2006)</ref>, ROUGE 1 , ROUGE 2 , ROUGE L <ref type="bibr" target="#b21">(Lin, 2004)</ref>, PARENT <ref type="bibr" target="#b8">(Dhingra et al., 2019)</ref>. We adapt MultEval <ref type="bibr" target="#b5">(Clark et al., 2011)</ref> and Py-rouge for resampling and significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Main Experimental Results</head><p>We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref>, GraphTransformer <ref type="bibr" target="#b17">(Koncel-Kedziorski et al., 2019)</ref>, GRN <ref type="bibr" target="#b3">(Beck et al., 2018)</ref>, GCN <ref type="bibr" target="#b22">(Marcheggiani and Perez-Beltrachini, 2018)</ref> and DeepGCN <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>, as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples following the occurrence of entities in output for S2S model to ease its limitation of not able to capture the graph structure. We also apply sequence-to-sequence models on generating outputs directly from entities without exploring KG by (1) randomly shuffling the order of all input entities (E2S) and (2) randomly shuffling the order of all topic-related entities while keeping the Main Entity at Front (E2S-MEF). Furthermore, we apply a delexicalization technique on our dataset. We delexicalize the main entity and topic-related entities by replacing these entities with tokens indicating the entity types and indices.</p><p>Main results on our ENT-DESC dataset are shown in <ref type="table" target="#tab_5">Table 2</ref>. Here, the numbers of layers in all baseline models and our MGCN models are set to be 6 for fair comparisons. Our models consistently outperform the baseline models on all evaluation metrics. S2S model has poor performance, mainly because the structure of our input triples is complicated as explained earlier. Compared to GRN and GCN models, the BLEU score of MGCN model increases by 1.3 and 0.9, respectively. This result suggests the effectiveness of multi-graph transformation, which is able to capture more comprehensive information compared to the Levi graph transformation used by GCN and GRN (especially entity-to-entity information in the original graph). We then apply multiple methods of aggregation on top of the multi-graph structure. MGCN+CNN and MGCN+SUM report the highest BLEU score of 26.4, followed by MGCN+AVG. By applying our delexicalization technique, the results are further boosted by 3.2 to 3.6 BLEU scores for both baseline and our proposed models. Moreover, our MGCN models and most baseline models outperform E2S and E2S-MEF, suggesting the importance of exploring KG when generating entity descriptions. Compared to E2S and E2S-MEF, there is no further improvement after applying delexicalization (i.e., E2S+delex and E2S-MEF+delex). We speculate it is because the copy mechanism is incorporated in the sequence-to-sequence model. Some useful information in original entities may be lost when further applying the delexicalization.   <ref type="table">Table 3</ref>: Effect of different numbers of input triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis and Discussion</head><p>Effect of different numbers of MGCN layers.</p><p>In order to examine the robustness of our MGCN models, we conduct further experiments by using different numbers of MGCN layers. The results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. We use MGCN to compare with the strongest baseline models using GCN according to the results in <ref type="table" target="#tab_5">Table 2</ref>. More specifically, we compare to GCN on 2 to 9 layers and DeepGCN on 9, 18, 27 and 36 layers. As shown in <ref type="figure" target="#fig_5">Figure  5</ref>, both models perform better initially as more GCN/MGCN layers are being stacked and start to drop afterward. In general, MGCN/DeepMGCN achieves decent performance improvements of 0.3 to 1.0 from 2 to 36 layers, as shown in the line chart. DeepMGCN achieves 26.3 BLEU score at 18 MGCN layers, which is 1.0 higher than deepGCN. It shows that, compared with learning the information implicitly by Levi graph, our multi-graph transformation brings in robust improvements by explicitly representing all types of information in the graph. Another observation is that the BLEU score of MGCN with 3 layers (25.4) is already higher than the best performance of GCN/deepGCN.</p><p>Effect of various numbers of input triples. In order to have a deeper understanding of how multigraph transformation helps the generation, we further explore the model performance under different numbers of triples on the test set. <ref type="table">Table 3</ref> shows the BLEU comparison between MGCN+SUM and GCN when using 6 layers. Both models perform the best when the number of triples is between 31 and 50. They both have a poorer performance  when the number of triples is too small or too large, which should be due to the fact that the models have insufficient or very noisy input information for generation. Another observation is that the improvement of BLEU (?) by our model is greater with a smaller number of input triples. It is plausibly because when the graph is larger, although our transformation techniques still bring in overall BLEU improvements, the increased graph complexity due to the transformation also hinders the generation.</p><p>Ablation Study. To examine the impact of each graph in our multi-graph structure, we show the ablation study in <ref type="table" target="#tab_8">Table 4</ref>. Each transformed graph is removed respectively from MGCN+SUM with 6 layers, except for the g 1 (self ), which is always enforced in the graph <ref type="bibr" target="#b16">(Kipf and Welling, 2017)</ref>. We notice that the result drops after removing any transformed graph from the multigraph. Particularly, we observe the importance of {default2, reverse2} and {default1, reverse1} are equivalent, as the BLEU scores after removing them individually are almost the same. This explains how multi-graph structure addresses the deficiency of Levi graph, i.e., entity-to-entity information is not represented explicitly in Levi graph. Additionally from the results, it is beneficial to represent the edges in the reverse direction for more effective information extraction in directed graphs as there are relatively larger gaps in BLEU drop after removing g 3 (reverse1) or g 5 (reverse2).</p><p>Case Study. <ref type="table" target="#tab_9">Table 5</ref> shows example outputs generated by GCN and MGCN+SUM, as compared to the gold reference. The main entity is highlighted in red, while topic-related entities are highlighted in blue. Given the KG containing all these entities, we intend to generate the description about "New Jersey Symphony Orchestra". Firstly, MGCN+SUM is able to cover the main entity and most topic-related entities correctly, while GCN fails to identify the Gold The New Jersey Symphony Orchestra is an American symphony orchestra based in the state of New Jersey . The NJSO is the state orchestra of New Jersey, performing concert series in six venues across the state, and is the resident orchestra of the New Jersey Performing Arts Center in Newark, New Jersey .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN</head><p>The Newark Philharmonic Orchestra is an American orchestra based in Newark, New Jersey , United States.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGCN +SUM</head><p>The New Jersey Symphony Orchestra is an American chamber orchestra based in Newark, New Jersey . The orchestra performs at the Newark Symphony Center at the Newark Symphony Center in Newark, New Jersey .  main entity. This suggests that without multi-graph transformation or effective aggregation methods, it is hard for GCN to extract useful information given a large number of triples in the KG. Length-wise, the output generated by MGCN+SUM is relatively longer than the one generated by GCN, and thus covers more information. We attribute the reason to GCN's deficiency of information loss, as mentioned earlier.</p><p>Human Evaluation In order to further assess the quality of the generated sentences, we conduct human evaluation by randomly selecting 100 sentences from outputs generated by GCN+delex and MGCN+SUM+delex. We hire 6 annotators to evaluate the quality based on three evaluation metrics: fluency, grammar and authenticity. In terms of authenticity, annotators rate this metric based on the KG (i.e., Wikidata). More specifically, we give our annotators all main entities' neighbors, 1-hop and 2-hop connections between main entities and topic-related entities as references. A full score will be given if the statements in the generated sentences are consistent with the facts shown in the KG. All three metrics take values from 1 to 5, where 5 states the highest score. The results are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Recall that BLEU scores of GCN+delex and MGCN+SUM+delex are 28.4 and 30.0 respectively, we can see from <ref type="figure" target="#fig_6">Figure 6</ref> Models BLEU TILB-SMT <ref type="bibr" target="#b12">(Gardent et al., 2017)</ref> 44.28 MELBOURNE <ref type="bibr" target="#b12">(Gardent et al., 2017)</ref>   that MGCN+SUM+delex only performs slightly better than GCN+delex on the two language quality metrics, namely, fluency and grammar. For authenticity, the improvement is more significant.</p><p>Plausibly it is because the 1.6 BLEU improvement results in more impact on the factual correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Additional Experiments</head><p>To examine our model's efficacy on a dataset of different characteristics, we conduct an auxiliary experiment on WebNLG <ref type="bibr" target="#b12">(Gardent et al., 2017)</ref>, which shares the most similarity with ENT-DESC dataset among those benchmark datasets (e.g., E2E, AGENDA, WIKIBIO, etc.). The experiments on WebNLG dataset are under the same settings as the main experiments on our ENT-DESC dataset. As shown in <ref type="table" target="#tab_11">Table 6</ref>, we observe that our proposed models outperform the state-of-the-art model MELBOURNE. However, the performance improvement is less obvious on this dataset, largely due to different characteristics between WebNLG and ENT-DESC. As mentioned in the dataset comparison, the input graphs in WebNLG dataset are much simpler and smaller, where all the information is useful for generation. Our MGCN model would show stronger advantages when applied to a larger and more complicated dataset (e.g. ENT-DESC dataset), where extracting more useful entities and relations from the input graphs and effectively aggregating them together is more essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We present a practical task of generating sentences from relevant entities empowered by KG, and construct a large-scale and challenging dataset ENT-DESC to facilitate the study of this task. Extensive experiments and analysis show the effectiveness of our proposed MGCN model architecture with multiple aggregation methods. In the future, we will explore more informative generation and consider applying MGCN to other NLP tasks for better information extraction and aggregation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An example showing our proposed task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 https://en.wikipedia.org/wiki/Bruno Mars 3 https://www.nayuki.io/page/ computing-wikipedias-internal-pageranks Length of the output texts in each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Dataset comparison among WebNLG, AGENDA, E2E and our ENT-DESC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>An example of multi-graph transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Effect of different numbers of layers. # Input triples # Instances GCN MGCN+SUM ? (BLEU)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Results for human evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3) V i to itself when i equals to</figDesc><table><row><cell>A 1</cell><cell>Graph Encoder 1</cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell>Graph Encoder 4 Graph Encoder 3 Graph Encoder 2</cell><cell>Aggregation Layer</cell><cell>MGCN Layer 1</cell><cell>+</cell><cell>LSTM Layer LSTM Layer Cross-attention</cell></row><row><cell></cell><cell>Graph</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Encoder 5</cell><cell></cell><cell>MGCN Layer n</cell><cell></cell></row><row><cell></cell><cell>Graph Encoder 6</cell><cell>MGCN Layer k</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Multi-Graph Encoder</cell><cell></cell><cell>Decoder</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ModelsBLEU METEOR TER? ROUGE 1 ROUGE 2 ROUGE L PARENT</figDesc><table><row><cell>S2S (Bahdanau et al., 2014)</cell><cell>06.8</cell><cell>10.8</cell><cell>80.9</cell><cell>38.1</cell><cell>21.5</cell><cell>40.7</cell><cell>10.0</cell></row><row><cell cols="2">GraphTransformer (Koncel-Kedziorski et al., 2019) 19.1</cell><cell>16.1</cell><cell>94.5</cell><cell>53.7</cell><cell>37.6</cell><cell>54.3</cell><cell>21.4</cell></row><row><cell>GRN (Beck et al., 2018)</cell><cell>24.4</cell><cell>18.9</cell><cell>70.8</cell><cell>54.1</cell><cell>38.3</cell><cell>55.5</cell><cell>21.3</cell></row><row><cell>GCN (Marcheggiani and Perez-Beltrachini, 2018)</cell><cell>24.8</cell><cell>19.3</cell><cell>70.4</cell><cell>54.9</cell><cell>39.1</cell><cell>56.2</cell><cell>21.8</cell></row><row><cell>DeepGCN (Guo et al., 2019)</cell><cell>24.9</cell><cell>19.3</cell><cell>70.2</cell><cell>55.0</cell><cell>39.3</cell><cell>56.2</cell><cell>21.8</cell></row><row><cell>MGCN</cell><cell>25.7</cell><cell>19.8</cell><cell>69.3</cell><cell>55.8</cell><cell>40.0</cell><cell>57.0</cell><cell>23.5</cell></row><row><cell>MGCN + CNN</cell><cell>26.4</cell><cell>20.4</cell><cell>69.4</cell><cell>56.4</cell><cell>40.5</cell><cell>57.4</cell><cell>24.2</cell></row><row><cell>MGCN + AVG</cell><cell>26.1</cell><cell>20.2</cell><cell>69.2</cell><cell>56.4</cell><cell>40.3</cell><cell>57.3</cell><cell>23.9</cell></row><row><cell>MGCN + SUM</cell><cell>26.4</cell><cell>20.3</cell><cell>69.8</cell><cell>56.4</cell><cell>40.6</cell><cell>57.4</cell><cell>23.9</cell></row><row><cell>GCN + delex</cell><cell>28.4</cell><cell>22.9</cell><cell>65.9</cell><cell>61.8</cell><cell>45.5</cell><cell>62.1</cell><cell>30.2</cell></row><row><cell>MGCN + CNN + delex</cell><cell>29.6</cell><cell>23.7</cell><cell>63.2</cell><cell>63.0</cell><cell>46.7</cell><cell>63.2</cell><cell>31.9</cell></row><row><cell>MGCN + SUM + delex</cell><cell>30.0</cell><cell>23.7</cell><cell>67.4</cell><cell>62.6</cell><cell>46.3</cell><cell>62.7</cell><cell>31.5</cell></row><row><cell cols="6">The rows below are results of generating from entities only without exploring the KG.</cell><cell></cell><cell></cell></row><row><cell>E2S</cell><cell>23.3</cell><cell>20.4</cell><cell>68.7</cell><cell>58.8</cell><cell>41.9</cell><cell>58.2</cell><cell>27.7</cell></row><row><cell>E2S + delex</cell><cell>21.8</cell><cell>20.5</cell><cell>67.5</cell><cell>59.5</cell><cell>39.5</cell><cell>59.2</cell><cell>23.4</cell></row><row><cell>E2S-MEF</cell><cell>24.2</cell><cell>21.3</cell><cell>65.8</cell><cell>59.8</cell><cell>43.3</cell><cell>60.0</cell><cell>26.3</cell></row><row><cell>E2S-MEF + delex</cell><cell>20.6</cell><cell>20.3</cell><cell>66.5</cell><cell>59.1</cell><cell>40.0</cell><cell>59.3</cell><cell>24.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Main results of models on ENT-DESC dataset. ? indicates lower is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results of the ablation study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>An example of generated sentences.</figDesc><table><row><cell>5</cell><cell>GCN + delex</cell><cell cols="3">MGCN + SUM + delex</cell></row><row><cell>4.82</cell><cell>4.84</cell><cell>4.87</cell><cell>4.88</cell></row><row><cell>4.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.64</cell></row><row><cell>4.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Fluency</cell><cell cols="2">Grammar</cell><cell>Authenticity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Results on WebNLG dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlin</forename><surname>Yutianli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Jiewang</forename><surname>Naiyanwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingxu</forename><surname>Tianjunxiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhang</forename><surname>Chiyuanzhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language interfaces to ontologies: combining syntactic analysis and ontology-based lookup through the user interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Damljanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Agatonovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on The Semantic Web: research and Applications-Volume Part I</title>
		<meeting>international conference on The Semantic Web: research and Applications-Volume Part I</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open domain event text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partially-aligned data-to-text generation with distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
		<meeting>INLG</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to write stories with thematic consistency and wording novelty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
		<meeting>INLG</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The e2e dataset: New challenges for end-toend generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of association for machine translation in the Americas</title>
		<meeting>association for machine translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
