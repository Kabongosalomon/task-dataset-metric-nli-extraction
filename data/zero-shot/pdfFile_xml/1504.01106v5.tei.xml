<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Neural Sentence Modeling by Tree-Based Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<email>zhijin@sei.pku.edu.cnpenghao.pku@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Neural Sentence Modeling by Tree-Based Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or dependency trees of sentences. The treebased convolution process extracts sentences' structural features, and these features are aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, which enables effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-ofthe-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community <ref type="bibr" target="#b0">(Allan et al., 2003;</ref><ref type="bibr" target="#b32">Su and Markert, 2008;</ref><ref type="bibr" target="#b34">Zhao et al., 2015)</ref>. Feature engineering-for example, n-gram features <ref type="bibr" target="#b5">(Cui et al., 2006)</ref>, dependency subtree features <ref type="bibr" target="#b22">(Nakagawa et al., 2010)</ref>, or more dedicated ones <ref type="bibr" target="#b25">(Silva et al., 2011)</ref>-can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in <ref type="bibr" target="#b20">Moschitti (2006)</ref> and <ref type="bibr" target="#b23">Reichartz et al. (2010)</ref> by specifying a certain measure of similarity between sentences, without explicit feature representation.</p><p>Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. <ref type="bibr" target="#b2">Bengio et al. (2003)</ref> and <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. <ref type="bibr" target="#b17">Le and Mikolov (2014)</ref> extend such approaches to learn sentences' and paragraphs' representations. Compared with human engineering, neural networks serve as a way of automatic feature learning <ref type="bibr" target="#b3">(Bengio et al., 2013)</ref>.</p><p>Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words' neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parsing trees). RNNs encode, to some extent, structural information by recursive semantic composition along a parsing tree. However, they may have difficulties in learning deep dependencies because of long propagation paths <ref type="bibr" target="#b7">(Erhan et al., 2009)</ref>. (CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)</p><p>A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).</p><p>In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN). Our models can leverage different sentence parsing trees, e.g., constituency trees and dependency trees. The model variants are denoted as c-TBCNN and d-TBCNN, respectively. The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire parsing tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension. One merit of such  architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.</p><p>TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments. To understand how TBCNNs work, we also visualize the network by plotting the convolution process. We make our code and results available on our project website. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks</head><p>Convolutional neural networks (CNNs), early used for image processing <ref type="bibr" target="#b18">(LeCun et al., 1995)</ref>, turn out to be effective with natural languages as well. <ref type="figure" target="#fig_0">Figure 1a</ref> depicts a classic convolution process on a sentence <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>. A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features. Let t be the window size, and x 1 , ? ? ? , x t ? R ne be n e -dimensional word embeddings. The output of convolution, evaluated at the current position, is</p><formula xml:id="formula_0">y = f (W ? [x 1 ; ? ? ? ; x t ] + b)</formula><p>where y ? R nc (n c is the number of feature detectors). W ? R nc?(t?ne) and b ? R nc are parameters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classification.</p><p>Convolution can extract neighboring information effectively.</p><p>However, the features are "local"-words that are not in a same convolution window do not interact with each other, even though they may be semantically related. <ref type="bibr" target="#b15">Kalchbrenner et al. (2014)</ref> build deep convolutional networks so that local features can mix at high-level layers. Similar deep CNNs include <ref type="bibr" target="#b16">Kim (2014)</ref> and <ref type="bibr" target="#b11">Hu et al. (2014)</ref>. All these models are "flat," by which we mean no structural information is used explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recursive Neural Networks</head><p>Recursive neural networks (RNNs), proposed in <ref type="bibr" target="#b27">Socher et al. (2011b)</ref>, utilize sentence parsing trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by n edimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c 1 and c 2 , vector representations denoted as p, c 1 , and c 2 . The parent's representation is composited by</p><formula xml:id="formula_1">p = f (W ? [c 1 ; c 2 ] + b)<label>(1)</label></formula><p>where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification <ref type="figure" target="#fig_0">(Figure 1b</ref>). Dependency parsing and the combinatory categorical grammar can also be exploited as RNNs' skeletons <ref type="bibr" target="#b9">(Hermann and Blunsom, 2013;</ref><ref type="bibr">Iyyer et al., 2014)</ref>. <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref> build deep RNNs to enhance information interaction. Improvements for semantic compositionality include matrix-vector interaction <ref type="bibr" target="#b29">(Socher et al., 2012)</ref>, tensor interaction <ref type="bibr" target="#b30">(Socher. et al., 2013)</ref>. They are more suitable for capturing logical information in sentences, such as negation and exclamation.</p><p>One potential problem of RNNs is that the long propagation paths-through which leaf nodes are connected to the output layer-may lead to infor-mation loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult <ref type="bibr" target="#b7">(Erhan et al., 2009</ref>). Long short term memory (LSTM), first proposed for modeling time-series data <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref>, is integrated to RNNs to alleviate this problem <ref type="bibr" target="#b33">(Tai et al., 2015;</ref><ref type="bibr" target="#b18">Le and Zuidema, 2015;</ref><ref type="bibr" target="#b35">Zhu et al., 2015)</ref>.</p><p>Recurrent networks. A variant class of RNNs is the recurrent neural network <ref type="bibr" target="#b1">(Bengio et al., 1994;</ref><ref type="bibr" target="#b24">Shang et al., 2015)</ref>, whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tree-based Convolution</head><p>This section introduces the proposed tree-based convolutional neural networks (TBCNNs). <ref type="figure" target="#fig_0">Figure  1c</ref> depicts the convolution process on a tree.</p><p>First, a sentence is converted to a parsing tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tree is represented as a distributed, real-valued vector.</p><p>Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution window. The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in <ref type="figure" target="#fig_0">Figure 1c</ref>. Formally, let us assume we have t nodes in the convolution window, x 1 , ? ? ? , x t , each represented as an n e -dimensional vector. Let n c be the number of feature detectors. The output of the tree-based convolution window, evaluated at the current subtree, is given by the following generic equation.</p><formula xml:id="formula_2">y = f t i=1 W i ?x i + b<label>(2)</label></formula><p>where W i ? R ne?nc is the weight parameter associated with node x i ; b ? R nc is the bias term. Extracted features are thereafter packed into one or more fixed-size vectors by max pooling, that is, the maximum value in each dimension is taken. Finally, we add a fully connected hidden layer, and a softmax output layer.</p><p>From the designed architecture <ref type="figure" target="#fig_0">(Figure 1c</ref>), we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree. Therefore structural feature learning becomes effective.</p><p>Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees? (2) How can we determine weights, W i , for dependency trees, where nodes may have different numbers of children? (3) How can we pool varying sized and shaped features to fixed-size vectors?</p><p>In the rest of this section, we explain model variants in detail. Particularly, Subsections 3.1 and 3.2 address the first and second problems; Subsection 3.3 deals with the third problem by introducing several pooling heuristics. Subsection 3.4 presents our training objective. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase. Sentences are parsed by the Stanford parser; 2 further, constituency trees are binarized for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">c-TBCNN</head><p>One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings. Our strategy is to pretrain the constituency tree with an RNN by Equation 1 <ref type="bibr" target="#b27">(Socher et al., 2011b)</ref>. After pretraining, vector representations of nodes are fixed.</p><p>We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children c l and c r , their vector representations denoted as p, c l , and c r . The convolution equation, specific for c-TBCNN, is</p><formula xml:id="formula_3">y = f W (c) p ?p + W (c) l ?c l + W (c) r ?c r + b (c) where W (c) p , W (c) l , and W (c) r</formula><p>are weights associated with the parent and its child nodes. <ref type="bibr">Superscript (c)</ref> indicates that the weights are for c-TBCNN. For leaf nodes, which do not have children, we set c l and c r to be 0.</p><p>Tree-based convolution windows can be extended to arbitrary depths straightforwardly. The complexity is exponential to the depth of the window, but linear to the number of nodes. Hence, tree-based convolution, compared with "flat" CNNs, does not add to computational cost, provided the same amount of information to process at a time. In our experiments, we use convolution windows of depth 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">d-TBCNN</head><p>Dependency trees are another representation of sentence structures. The nature of dependency representation leads to d-TBCNN's major difference from traditional convolution: there exist nodes with different numbers of child nodes. This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., <ref type="bibr" target="#b4">Collobert and Weston (2008)</ref> or c-TBCNN.</p><p>To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions. We believe this strategy makes much sense because dependency types <ref type="bibr" target="#b6">(de Marneffe et al., 2006)</ref> reflect the relationship between a governing word and its child words. To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes</p><formula xml:id="formula_4">y = f W (d) p ?p + n i=1 W (d) r[c i ] ?c i + b (d) where W (d) p</formula><p>is the weight parameter for the parent p (governing word); W (d) r[c i ] is the weight for child c i , who has grammatical relationship r[c i ] to its parent, p. Superscript (d) indicates the parameters are for d-TBCNN. Note that we keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix.</p><p>Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pooling Heuristics</head><p>As different sentences may have different lengths and tree structures, the extracted features by tree- based convolution also have topologies varying in size and shape. Dynamic pooling <ref type="bibr" target="#b26">(Socher et al., 2011a</ref>) is a common technique for dealing with this problem. We propose several heuristics for pooling along a tree structure. Our generic design criteria for pooling include: (1) Nodes that are pooled to one slot should be "neighboring" from some viewpoint.</p><p>(2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it. Thus, (approximately) equal amount of information is aggregated along different parts of the tree. Following the above intuition, we propose pooling heuristics as follows.</p><p>? Global pooling. All features are pooled to one vector, shown in <ref type="figure" target="#fig_2">Figure 3a</ref>. We take the maximum value in each dimension. This simple heuristic is applicable to any structure, including c-TBCNN and d-TBCNN. ? 3-slot pooling for c-TBCNN. To preserve more information over different parts of constituency trees, we propose 3-slot pooling <ref type="figure" target="#fig_2">(Figure 3b</ref>). If a tree has maximum depth d, we pool nodes of less than ? ? d layers to a TOP slot (? is set to 0.6); lower Task Data samples Label</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis</head><p>Offers that rare combination of entertainment and education. ++ An idealistic love story that brings out the latent 15-year-old romantic in everyone. + Its mysteries are transparently obvious, and it's too slowly paced to be a thriller.</p><p>? Question Classification</p><p>What is the temperature at the center of the earth?</p><p>number What state did the Battle of Bighorn take place in? location <ref type="table">Table 1</ref>: Data samples in sentiment analysis and question classification. In the first task, "++" refers to strongly positive; "+" and "?" refer to positive and negative, respectively.</p><p>nodes are pooled to slots LOWER LEFT or LOWER RIGHT according to their relative position with respect to the root node. For a constituency tree, it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time. Therefore, we regard 3-slot pooling for c-TBCNN is a "hard mechanism" temporarily. Further improvement can be addressed in future work. ? k-slot pooling for d-TBCNN. Different from constituency trees, nodes in dependency trees are one-one corresponding to words in a sentence. Thus, a total order on features (after convolution) can be defined according to their corresponding word orders. For kslot pooling, we can adopt an "equal allocation" strategy, shown in <ref type="figure" target="#fig_2">Figure 3c</ref>. Let i be the position of a word in a sentence (i = 1, 2, ? ? ? , n). Its extracted feature vector is pooled to the j-th slot, if</p><formula xml:id="formula_5">(j ? 1) n k ? i ? j n k</formula><p>We assess the efficacy of pooling quantitatively in Section 4.3.1. As we shall see by the experimental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large. TBCNNs are not very sensitive to pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective</head><p>After pooling, information is packed into one or more fixed-size vectors (slots). We add a hidden layer, and then a softmax layer to predict the probability of each target label in a classification task. The error function of a sample is the standard cross entropy loss, i.e., J = ? c i=1 t i log y i , where t is the ground truth (one-hot represented), y the output by softmax, and c the number of classes. To regularize our model, we apply both 2 penalty and dropout <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref>. Training details are further presented in Section 4.1 and 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we evaluate our models with two tasks, sentiment analysis and question classification. We also conduct quantitative and qualitative model analysis in Subsection 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The Task and Dataset</head><p>Sentiment analysis is a widely studied task for discriminative sentence modeling. The Stanford sentiment treebank 3 consists of more than 10,000 movie reviews. Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative). Some examples are shown in <ref type="table">Table 1</ref>. We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction. Binary classification does not contain the neutral class.</p><p>In the dataset, phrases (sub-sentences) are also tagged with sentiment labels. RNNs deal with them naturally during the recursive process. We regard sub-sentences as individual samples during training, like <ref type="bibr" target="#b15">Kalchbrenner et al. (2014)</ref> and <ref type="bibr" target="#b17">Le and Mikolov (2014)</ref>. The training set therefore has more than 150,000 entries in total. For validating and testing, only whole sentences (root labels) are considered in our experiments.</p><p>Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training Details</head><p>This subsection describes training details for d-TBCNN, where hyperparameters are chosen by validation.</p><p>c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN's settings can be found on our (anonymized) website.  <ref type="table">Table 2</ref>: Accuracy of sentiment prediction (in percentage). For 2-class prediction, " ?" remarks indicate that the network is transferred directly from that of 5-class.</p><p>In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer. Word embeddings are 300 dimensional, pretrained ourselves using word2vec <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref>  To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200. We use ReLU (Nair and Hinton, 2010) as the activation function .</p><p>For regularization, we add 2 penalty for weights with a coefficient of 10 ?5 . Dropout <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref> is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. <ref type="table">Table 2</ref> compares our models to state-of-the-art results in the task of sentiment analysis. For 5class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory <ref type="bibr" target="#b33">(Tai et al., 2015)</ref>. c-TBCNN is slightly worse. It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Performance</head><p>Regarding 2-class prediction, we adopted a simple strategy in <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref>, 4 where the 5-class network is "transferred" directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes. (The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking third in the list.</p><p>In a more controlled comparison-with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)-TBCNNs, of both variants, consistently outperform RNNs <ref type="bibr" target="#b27">(Socher et al., 2011b)</ref> to a large extent (50.4-51.4% versus 43.2%); they also consistently outperform "flat" CNNs by more than 10%. Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs.</p><p>We also observe d-TBCNN achieves higher performance than c-TBCNN. This suggests that compact tree expressiveness is more important than integrating global information in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Classification</head><p>We further evaluate TBCNN models on a question classification task. 5 The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10.</p><p>We also use the standard split, like <ref type="bibr" target="#b25">Silva et al. (2011)</ref>   <ref type="table">Table 1</ref>. We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs' performance when applied to datasets of different sizes. To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively. We do not back-propagate gradient to embeddings in this task. Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%. <ref type="table" target="#tab_3">Table 3</ref> compares our models to various other methods. The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering <ref type="bibr" target="#b25">(Silva et al., 2011)</ref>. Their method utilizes more than 10k features and 60 hand-coded rules. On the contrary, our TBCNN models do not use a single human-engineered feature or rule. Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%. To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.</p><p>The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Analysis</head><p>In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The Effect of Pooling</head><p>The extracted features by tree-based convolution have topologies varying in size and shape. We propose in Section 3.3 several heuristics for pooling. This subsection aims to provide a fair comparison among these pooling methods.   One reasonable protocol for comparison is to tune all hyperparameters for each setting and compare the highest accuracy. This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning. An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the same setting. In this experiment, we chose the latter protocol, where hidden layers are all 300dimensional; no 2 penalty is added. Each configuration was run five times with different random initializations. We summarize the mean and standard deviation in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>As the results imply, complicated pooling is better than global pooling to some degree for both model variants. But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data. In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.</p><p>Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering. Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification. Our TBCNNs are typically trained within 25 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The Effect of Sentence Lengths</head><p>We analyze how sentence lengths affect our models. Sentences are split into 7 groups by length, with granularity 5. A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126  <ref type="figure">Figure 5</ref>: Visualizing how features (after convolution) are related to the sentiment of a sentence. The sample corresponds a sentence in the dataset, "The stunning dreamlike visual will impress even those viewers who have little patience for Euro-film pretension." The numbers in brackets denote the fraction of a node's features that are gathered by the max pooling layer (also indicated by colors). to 457. <ref type="figure" target="#fig_4">Figure 4</ref> presents accuracies versus lengths in TBCNNs. For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in <ref type="bibr" target="#b27">Socher et al. (2011b)</ref>. Thus, we think our reimplementation is fair and that the comparison is sensible.</p><p>We observe that c-TBCNN and d-TBCNN yield very similar behaviors. They consistently outperform the RNN in all scenarios. We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words. This result confirms our theoretical analysis in Section 2-for long sentences, the propagation paths in RNNs are deep, causing RNNs' difficulty in information processing. By contrast, our models explore structural information more effectively with tree-based convolution. As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Visualization</head><p>Visualization is important to understanding the mechanism of neural networks. For TBCNNs, we would like to see how the extracted features (after convolution) are further processed by the max pooling layer, and ultimately related to the supervised task.</p><p>To show this, we trace back where the max pooling layer's features come from. For each dimension, the pooling layer chooses the maximum value from the nodes that are pooled to it. Thus, we can count the fraction in which a node's features are gathered by pooling. Intuitively, if a node's features are more related to the task, the fraction tends to be larger, and vice versa. <ref type="figure">Figure 5</ref> illustrates an example processed by d-TBCNN in the task of sentiment analysis. 6 Here, <ref type="bibr">6</ref> We only have space to present one example in the paper.</p><p>we applied global pooling because information tracing is more sensible with one pooling slot. As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest. The 2-layer windows corresponding to "visual will impress viewers," "the stunning dreamlike visual," say, are discriminative to the sentence's sentiment. Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling. On the other hand, words like the, will, even are known as stop words <ref type="bibr" target="#b8">(Fox, 1989)</ref>. They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered. Such results are consistent with human intuition.</p><p>We further observe that tree-based convolution does integrate information of different words in the window. For example, the word stunning appears in two windows: (a) the window "stunning" itself, and (b) the window of "the stunning dreamlike visual," with root node visual, stunning acting as a child. We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment. In fact, Window a has a larger fraction than the sum of its children's (the windows of "the," "stunning," and "dreamlike").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing structures. Our model can be built upon either constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).</p><p>Both variants have achieved high performance in sentiment analysis and question classification. d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous stateof-the-art results in both tasks. The results show that tree-based convolution can capture sentences' structural information effectively, which is useful for sentence modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A comparison of information flow in the convolutional neural network (CNN), the recursive neural network (RNN), and the tree-based convolutional neural network (TBCNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Tree-based convolution in (a) c-TBCNN, and (b) d-TBCNN. The parsing trees correspond to the sentence "I loved it." The dashed triangles illustrate a shared-weight convolution window sliding over the tree. For clarity, only two positions are drawn in c-TBCNN. Notice that dotted arrows are not part of neural connections; they merely indicate the topologies of tree structures. Specially, an edge a r ? b in the dependency tree refers to a being governed by b with dependency type r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>tree-based convolution in the order of words LOWER LOWER (c) k-slot pooling for d-TBCNN Pooling heuristics. (a) Global pooling. (b) 3-slot pooling for c-TBCNN. (c) k-slot pooling for d-TBCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on the English Wikipedia corpus. 2slot pooling is applied for d-TBCNN. (c-TBCNN uses 3-slot pooling.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Accuracies versus sentence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Accuracy of 6-way question classification.</cell></row><row><cell>bels contain 6 classes, namely abbreviation,</cell></row><row><cell>entity, description, human, location,</cell></row><row><cell>and numeric. Some examples are also shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Accuracies of different pooling methods,</cell></row><row><cell cols="3">averaged over 5 random initializations. We chose</cell></row><row><cell cols="3">sensible hyperparameters manually in advance to</cell></row><row><cell cols="3">make a fair comparison. This leads to performance</cell></row><row><cell cols="3">degradation (1-2%) vis-a-vis Table 2.</cell></row><row><cell>Accuracy (%)</cell><cell>0 10 20 30 40 50</cell><cell>?9 10?14 15?19 20?24 25?29 30?34 ?35 Setence length RNN c-TBCNN d-TBCNN</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/tbcnnsentence/ (This site is properly anonymized, and complies with the double-blind review requirement.)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://nlp.stanford.edu/software/lex-parser.shtml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://nlp.stanford.edu/sentiment/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification. We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model. Our website reviews some related work and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This example was not chosen deliberately. Similar traits can be found through out the entire gallery, available on our website. Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization since we know the "meaning" of every node.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retrieval and novelty detection at the sentence level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">R</forename><surname>Weston2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparative experiments on sentiment classification for online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 21st AAAI Conference on Artificial Intelligence</title>
		<meeting>21st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resource and Evaluation Conference</title>
		<meeting>Language Resource and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A stop list for general text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="19" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">K</forename><surname>Blunsom2013</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Irsoy and Cardie2014</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Iyyer et al.2014</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">Q</forename><surname>Mikolov2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">P</forename><surname>Zuidema2015</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zuidema ; Y. Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sackinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
		<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Compositional distributional semantics with long short term memory</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference of Machine Learning</title>
		<meeting>European Conference of Machine Learning</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">V</forename><surname>Hinton2010</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency tree-based sentiment classification using CRFs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic relation extraction with kernels over typed dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Korte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Annual Conference of the North American Chapter of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<title level="m">Neural responding machine for short-text conversation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From symbolic to sub-symbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Socher et al.2011a</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Socher et al.2012</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From words to senses: a case study of subjectivity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Su and Markert2008</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved semantic representations from treestructured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
	</analytic>
	<monogr>
		<title level="m">to appear in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">to appear in Proceedints of Intenational Joint Conference in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Self-adaptive hierarchical sentence model</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04881</idno>
		<title level="m">Long short-term memory over tree structures</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>to appear in Proceedings of International Conference on Machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
