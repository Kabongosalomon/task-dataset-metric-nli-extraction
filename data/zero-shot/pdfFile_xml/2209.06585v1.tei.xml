<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Metric Learning and Attention Heads For Accurate and Efficient Multilabel Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Intel</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Metric Learning and Attention Heads For Accurate and Efficient Multilabel Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multilabel image classification ? deep learning ? lightweight models ? graph attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kirill Prokofiev [0000?0001?9619?0248]1 and Vladislav Sovrasov [0000?0001?6525?2602]2</p><p>Abstract. Multi-label image classification allows predicting a set of labels from a given image. Unlike multiclass classification, where only one label per image is assigned, such setup is applicable for a broader range of applications. In this work we revisit two popular approaches to multilabel classification: transformer-based heads and labels relations information graph processing branches. Although transformer-based heads are considered to achieve better results than graph-based branches, we argue that with the proper training strategy graph-based methods can demonstrate just a small accuracy drop, while spending less computational resources on inference. In our training strategy, instead of Asymmetric Loss (ASL), which is the de-facto standard for multilabel classification, we introduce its modification acting in the angle space. It implicitly learns a proxy feature vector on the unit hypersphere for each class, providing a better discrimination ability, than binary cross entropy loss does on unnormalized features. With the proposed loss and training strategy, we obtain SOTA results among single modality methods on widespread multilabel classification benchmarks such as MS-COCO, PASCAL-VOC, NUS-Wide and Visual Genome 500. Source code of our method is available as a part of the OpenVINO? Training Extensions 3 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Starting from the impressive AlexNet <ref type="bibr" target="#b19">[19]</ref> breakthrough on the ImageNet benchmark <ref type="bibr" target="#b5">[6]</ref>, deep-learning era has drastically changed approaches to almost every computer vision task. Throughout this process multiclass classification problem was a polygon for developing new architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">30]</ref> and learning paradigms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">17]</ref>. At the same time, multilabel classification has been developing not so intensively, although the presence of several labels on one image is more natural, than having one hard label. Due to lack of specialized multilabel datasets, researchers turned general object detection datasets such as MS-COCO <ref type="bibr" target="#b22">[22]</ref> and PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> into challenging multilabel classification benchmarks by removing bounding boxes from the data annotation and leveraging only their class labels. Conventionally, multilabel classification task is transformed into a set of binary classification tasks, which are solved by optimizing a BCE-like loss function. This formulation leads to the following two key challenges: positive-negative imbalance in each binary subtask and modeling the inter-label dependencies. We tackle the imbalance challenge by applying a metric learning loss inspired by SphereFace2 <ref type="bibr" target="#b34">[34]</ref> loss. By its nature, this loss performs hard sample mining by normalizing features and scaling logits. Combining the metric learning approach with ASL <ref type="bibr" target="#b0">[1]</ref> focal weighting provides even stronger mechanism for mining hard positives and discarding easy negatives.</p><p>Implicit label co-occurrence modeling is also beneficial for multilabel classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b27">27]</ref>. Baseline classification architectures consist of a backbone (CNN or transformer), global pooling layer and linear layer with binary classification losses attached to each logit. This architecture can be augmented by a set of binary classifiers generated by stacked GCNs or graph attention layers, which process a graph built over vector representations of labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">35]</ref>. Applying graph attention layers enables utilizing labels co-occurrences to generate more discriminative classifiers. We incorporate the output of the graph attention subnetwork in a slightly different manner, which allow us to combine this approach with transformer-based classification heads.</p><p>The main contributions of this paper are as follows:</p><p>-We proposed a modification of ML-GCN that adds graph attention operations <ref type="bibr" target="#b32">[32]</ref> and performs graph and CNN features fusion in a more conventional way than generating a set of binary classifiers in the graph branch. -We demonstrated that using a proper training strategy, one can decrease the performance gap between transformer-based heads and label co-occurrence modeling via graph attention. -We studied the importance of per class confidence thresholds tuning for adapting models to real-world applications. -We first applied the metric learning paradigm to multilabel classification task and proposed a modified version of angular margin binary loss <ref type="bibr" target="#b34">[34]</ref>, which adds ASL <ref type="bibr" target="#b0">[1]</ref> mechanism to it. -We verified the effectiveness of our loss and overall training strategy with comprehensive experiments on widespread multilabel classification benchmarks: PASCAL VOC, MS-COCO, Visual Genome <ref type="bibr" target="#b18">[18]</ref> and NUS-WIDE <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Historically multilabel classification was attracting less attention than the multiclass scenario, but nonetheless there is stills a great progress on that field. Notable progress was achieved by developing advanced loss functions <ref type="bibr" target="#b0">[1]</ref>, label co-occurrence modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">35]</ref>, designing advanced classification heads <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b36">36]</ref> and discovering architectures taking into account spatial distribution of objects via exploring attentional regions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">33]</ref>. Each single-class classification subtask in multilabel scenario suffers from hard positives-negatives imbalance. More classes the training dataset contains, more negatives we have for in each of the single-class subtasks, because an image typically contains a tiny fraction of the vast number of all classes. A modified asymmetric loss <ref type="bibr" target="#b0">[1]</ref>, that down-weights and hard-thresholds easy negative samples, showed impressive results, reaching state-of-the-art results on multiple popular multi-label datasets without any sophisticated architecture tricks. These results indicate that a proper choice of a loss function is crucial for multilabel classification performance.</p><p>Another promising direction is designing class-specific classifiers instead of using a fully connected layer on top of a single feature vector produced by a backbone network. This approach also doesn't introduce additional training steps and marginally increases model complexity. Authors of <ref type="bibr" target="#b36">[36]</ref> propose a drop-in replacement of global average pooling layer that generates class-specific features for every category. Leveraging compact transformer heads for generating such features <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b27">27]</ref> turned out to be even more effective. This approach assumes pooling class-specific features by employing learnable embedding queries.</p><p>Taking into account spatial a distribution of objects or label relationships based on prior knowledge requires data pre-processing and additional assumptions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">35]</ref> or sophisticated model architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">33]</ref>. For instance, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">35]</ref> represents labels by word embeddings, then a directed graph is built over these label representations, where each node denotes a label. Then stacked GCNs are learned over this graph to obtain a set of object classifiers. The method relies on an ability to represent labels as words, which is not always possible. Spatial distribution modeling requires placing a RCNN-like <ref type="bibr" target="#b11">[12]</ref> module inside the model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">33]</ref>, which drastically increases complexity of the training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe the overall training pipeline and details of our approach. We aimed not only to achieve competitive results, but also to make the training more friendly to the end user and adaptive to data. Thus, following principles described in <ref type="bibr" target="#b25">[25]</ref>, we use lightweight model architectures, hyperparameters optimization and early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>We chose EfficientNetV2 <ref type="bibr" target="#b31">[31]</ref> and TResNet <ref type="bibr" target="#b26">[26]</ref> as base architectures for performing multilabel image classification. Namely, we conducted all the experiments on TResNet-L, EfficientNetV2 small and large. On top of these backbones we used several different features aggregation approaches. <ref type="figure">Fig. 1</ref>: Re-weighting scheme for incorporating labels semantic and labels cooccurrence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer multilabel classification head</head><p>As a representor of transformer-based feature aggregation methods we use the ML-Decoder <ref type="bibr" target="#b27">[27]</ref> head. It provides up to K feature vectors (where K is the number of classes) as a model output instead of a single class-agnostic vector when using a standard global average pooling (GAP) head. Let's denote x ? R C?H?W as a model input, then the model F with parameters W produces a downscaled multi-channel featuremap</p><formula xml:id="formula_0">f = F W (x) ? R S? H d ? W d ,</formula><p>where S is the number of output channels, d is the spatial downscale factor. That featuremap is then passed to the ML-Decoder head: v = M LD(f ) ? R M ?L , where M is the embedding dimension, L ? K is the number of groups in decoder. Finally, vectors v are projected to K class logits by a fully-connected (if L = K) or group fully-connected projection (if L &lt; K) as it is described in <ref type="bibr" target="#b27">[27]</ref>. In our experiments we set L = min(100, K). Also we normalize the arguments of all dot products in projections in case if we need to operate with features on the unit hypersphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph attention multilabel branch</head><p>The original structure of the graph processing branch from <ref type="bibr" target="#b2">[3]</ref> supposes generating classifiers right in this branch and then applying them directly to the features generated by a backbone. This approach is incompatible with the transformerbased head or any other processing of the raw spatial features f , like CSRA <ref type="bibr" target="#b36">[36]</ref>. To alleviate this limitation, we propose the architecture showed in <ref type="figure">Figure 1</ref>.</p><p>First, we generate the label correlation matrix Z ? R K?K in the same way as in <ref type="bibr" target="#b2">[3]</ref>. Together with the word embeddings G ? R K?N , obtained with GLOVE [24] model, where N = 300 is word embeddings dimension, we utilize Z as an input of the Graph Attention Network <ref type="bibr" target="#b32">[32]</ref>. Unlike <ref type="bibr" target="#b35">[35]</ref>, we use estimations of conditional probabilities to build Z, rather that fully rely on GLOVE and compute cosine similarities.</p><p>We process the input with the graph attention layers and obtain output h ? R S?K . Then we derive the most influential features through the max pooling operation and receive the weights w ? R S for further re-weighting of the CNN spatial features:f = w f . Next, we apply global average pooling and max pooling operations tof in parallel, sum the results and obtain final latent embedding? ? R S . The embedding? is finally passed to the binary classifiers. Instead of applying a simple spatial pooling, we can pass the weighted features f to the ML-Decoder or any other features processing module.</p><p>The main advantage of using the graph attention branch for re-weighting or classification over transformer head is a tiny computational and model complexity overhead at the inference stage. Since graph attention branch has the same input for any image, we can compute the result of its execution only once, before starting the inference of the resulting model. At the same time, the graph attention branch requires a vector representation of labels. Such representations can be generated by a text-to-vec model in case we have a meaningful descriptions for all labels (even single word ones). This condition doesn't always hold: some dataset could have untitled labels. How to generate representations for labels in that case is still an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Angular margin binary classification</head><p>Recently Asymmetric loss <ref type="bibr" target="#b0">[1]</ref> has become a standard loss option for performing multi-label classification. By design it penalizes each logit with a modified binary cross-entropy (BCE) loss. Asymmetric handling of positives and negatives allows ASL to down-weight the negative part of the loss to tackle the positives-negatives imbalance problem. But this approach leaves a room for improvement from the model's discriminative ability perspective.</p><p>Angular margin losses are known for generating more discriminative classification features than the cross-entropy loss, which is a must-have property for recognition tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b34">34]</ref>. We propose joining paradigms from <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b34">[34]</ref> to build even stronger loss for multilabel classification. Denote the result of the dot product between the normalized class embedding v j and the j-th binary classifier W j as cos? j . Then, for a training sample x and corresponding embeddings set v we formulate our asymmetric angular margin loss (AAM) as:  </p><formula xml:id="formula_1">L AAM (v, y) = ? K j=1 L j (cos? j , y) L j (cos? j , y) = k s yp ? ? ? log p + + 1 ? k s (1 ? y)p ? + + log p ? ,<label>(1)</label></formula><formula xml:id="formula_2">p + = ?(s(cos? j ? m)), p ? = ?(?s(cos? j + m)),</formula><p>where s is a scale parameter, m is an angular margin, k is negative-positive weighting coefficient, ? + and ? ? are weighting parameters from ASL. Despite on a large number of hyperparameters, some of them could be safely fixed (like ? + and ? ? from ASL). The effect of varying s saturates when increasing s (see <ref type="figure" target="#fig_1">Figure 2b</ref>), and if the suitable value of this parameter is large enough, we don't need to tune it precisely. Also values of m should be close to 0, because it duplicates to some extent the effect of s and ? and can even bring undesirable increase of the negative part of AAM (see <ref type="figure" target="#fig_1">Figure 2a</ref>). Detailed analysis of the hyperparameters is provided in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Details of the training strategy</head><p>As in our former work <ref type="bibr" target="#b25">[25]</ref>, we aim to make the training pipeline for multilabel classification reliable, fast and adaptive to dataset, so we use the following components:</p><p>-SAM <ref type="bibr" target="#b9">[10]</ref> optimizer with no bias decay <ref type="bibr" target="#b14">[15]</ref> is a default optimizer; -EMA weights averaging for an additional protection from overfitting; -Initial learning rate estimation process from <ref type="bibr" target="#b25">[25]</ref>; -OneCycle <ref type="bibr" target="#b28">[28]</ref> learning rate scheduler; -Early stopping heuristic: if the best result on the validation subset hasn't been improved during 5 epochs, and evaluation results stay below the EMA averaged sequence of the previous best results, the training process stops; -Random flip, pre-defined Randaugment <ref type="bibr" target="#b4">[5]</ref> strategy and Cutout <ref type="bibr" target="#b7">[8]</ref> data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct a comparison of our approach with the state-of-theart solutions on popular multilabel benchmarks. Besides the mAP metric, we present the GFLOPs for each method to consider the speed/accuracy trade-off. Also, we show the results of the ablation study to reveal the importance of each training pipeline component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To compare performance of different methods we picked up widespread datasets for multilabel classification, that are listed in <ref type="table" target="#tab_0">Table 1</ref>. According to the table, all of them have a noticeable positives-negatives imbalance on each image: average number of presented labels per images is significantly lower than the number of classes. Also, we use a precisely annotated subset of OpenImages V6 <ref type="bibr" target="#b20">[20]</ref> for pretraining purposes. We join the original training and testing parts into one train subset and use the original validation subset for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation protocol</head><p>We adopt commonly used metrics for evaluation of multilabel classification models: mean average precision (mAP) over all categories, overall precision (OP), recall (OR), F1-measure (OF1) and per-category precision (CP), recall (CR), F1-measure (CF1). We use the mAP as a main metric, others are provided when an advanced comparison of approaches is conducted. In every operation where the confidence thresholding is required, threshold 0.5 is substituted. The exact formulas of the mentioned metrics can be found in <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pretraining</head><p>Although the multilabel classification task is similar to multiclass classification, a multilabel model should pay attention to several objects on an image, instead of concentrating its attention at one object. Thus, additional task specific pretraining a on large-scale dataset looks beneficial to the standard ImageNet pretraining, and it was shown in recent works <ref type="bibr" target="#b27">[27]</ref>. In this work we utilize OpenImages V6 for pretraining. According to our experiments, the precisely-annotated subset containing 1.8M images is enough to get a substantial improvement over the ImageNet weights.</p><p>To obtain pretrained weights on OpenImages for our models, we use the ML-Decoder head, 224 ? 224 input resolution, ASL loss with ? + = 0 and ? ? = 7, learning rate 0.001, and 50 epochs of training with OneCycle scheduler.</p><p>For TResNet-L we use the weights provided with source code in <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>In all cases where we use EfficientNet-V2-s as a backbone we also use our training strategy from Section 3.5 for a fair comparison. For ASL loss <ref type="bibr" target="#b0">[1]</ref> we set lr = 0.0001, ? ? = 4, ? + = 0 as suggested in the original paper <ref type="bibr" target="#b0">[1]</ref>. We share the following hyperparameters across all the experiments: m = 0.0, k = 0.7, EMA decay factor equals to 0.9997, ? = 0.05 for the SAM optimizer. Other hyperperameters for particular datasets were found empirically or via coarse grid search.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> results on MS-COCO are presented. For this dataset we set s = 23, lr = 0.007, ? ? = 1, ? + = 0 (see Sec. 4.6). With our AAM loss, we can achieve a state-of-the-art result using TResNet-L as a backbone. At the same time, combination of EfficientNetV2-s with ML-Decoder and AAM loss outperforms TResNet-L with ASL, while consuming 3.5x less FLOPS. GCN/GAN branches performs slightly worse than ML-Decoder, but still improves results over EfficientNetV2-s + ASL with a marginal computational cost at inference. Results on Pascal-VOC can be found in <ref type="table" target="#tab_2">Table 3</ref>. We set s = 17, lr = 0.005, ? ? = 2, ? + = 1 to train our models on this dataset. Our modification of the GAN branch outperforms ML-Decoder when using EfficientNet-V2-s, while AAM loss gives a small performance boost and allows achieving SOTA with TResNet-L. Also, on Pascal-VOC EfficientNet-V2-s with all of considered additional graph branches or heads demonstrates a great speed/accuracy trade-off outperforming TResNet-L with ASL.  <ref type="table" target="#tab_4">Tables 4 and 5</ref> show results on NUS and VG500 datasets. For NUS dataset we set s = 23, lr = 0.009, ? ? = 2, ? + = 1. For VG500 the hyperparameters are s = 25, lr = 0.005, ? ? = 1, ? + = 0. ML-Decoder clearly outperforms GCN and GAN branches on NUS-WIDE dataset. Also on NUS EfficientNet-V2-s with the AAM loss and GAN or ML-decoder head performs significantly better than the original implementations of Q2L and ASL with TResNet-L.</p><p>On VG500 we don't provide results of applying GCN or GAN branches, because this dataset has unnamed labels, so we can not apply a text-to-vec model to generate representations of graph nodes. Applying AAM with ML-Decoder on VG500 together with increased resolution allows achieving SOTA performance on this dataset as well.</p><p>As a result of the experiments, we can conclude that the combination of ML-Decoder with AAM loss gives an optimal performance on all of the considered datasets, while use the of the GAN-based branch could lead to better inference speed at the price of a small accuracy drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Confidence threshold tuning</head><p>Conventionally, threshold a value equals to 0.5 is used for calculating OP, OR, OF1 and CP, CR, CF1. But to apply a classification model in the wild under <ref type="table">Table 4</ref>: Comparison with the state-of-the-art on NUS-WIDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Input resolution mAP GFLOPs GATN <ref type="bibr" target="#b35">[35]</ref> ResNeXt-101 448x448 59.80 36.00 ASL <ref type="bibr" target="#b0">[1]</ref> TResNet-L 448x448 65.20 43.50 Q2L <ref type="bibr" target="#b23">[23]</ref> TResNet-L 448x448 66.30 60.40 ML-GCN * EfficientNet-V2-s 448x448 66. <ref type="bibr" target="#b30">30</ref>   TResNet-L 576x576 43.10 59.63 * Trained by us using our training strategy the variety of input data and presented classes, one need to estimate per-class confidence thresholds. When we don't have an access to the target real-world data, we can at least take into account per-class threshold variance, which arises because of training data distribution and loss function modifications. To do that, we propose finding per class thresholds by maximizing F1 score on each class via grid search on the train subset. Then we can evaluate the obtained thresholds on the validation subset and check if the precision-recall balance was improved.</p><p>In <ref type="table" target="#tab_5">Table 6</ref> the results of thresholds tuning for EfficientNetV2-s model trained with AAM loss on several datasets are shown. Precision-recall balance was improved by the mentioned procedure under train-validation distribution shift conditions for all of the considered datasets excepting Visual Genome 500. There thresholds tuning on train slightly decreased both OF1 and CF1 scores. This fact indicates that VG500 has a big distribution gap between the train and validation subsets, which the model can't handle (mAP &lt; 50%). Thus, if the trained model has low accuracy, estimating confidence threshold on the training subset may not work as expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation study</head><p>Contribution of algorithm's components To demonstrate the impact of each component on the whole pipeline we add them to a baseline one by one. As a baseline we take EfficientNetV2-s backbone and ASL loss with SGD optimizer. We set all the hyperparameters of the ASL loss and learning rate as in <ref type="bibr" target="#b0">[1]</ref>. We use the training strategy described in Section 3.5 for all the experiments. In <ref type="table" target="#tab_6">Table 7</ref> we can see that each component brings an improvement, except adding the GAN branch. ML-Decoder has enough capacity to learn labels correlation information, so further hints, that provides the GAN branch, don't improve the result. Also, we can see that tuning of the ? parameters is beneficial for the AAM loss, but the metric learning approach itself brings an improvement even without it. Hyperparameters impact <ref type="figure" target="#fig_2">Figure 3</ref> shows the scale parameter s impact on the training pipeline. We fix margin parameter at 0.0 value. We use our full training pipeline with AAM loss and ML-Decoder on 224x224 resolution for faster training. We can conclude that the value of s is correlated with the number of classes. For a small number of classes, the optimal value lies in the range 10-20. Otherwise, the 20-30 range will be a good choice. <ref type="figure">Figure 4</ref> shows margin influence from AAM loss. We see that this parameter brings unnecessary complexity to the choice of extra hyperparameter. We can exclude this parameter and simplify applying of the loss function.  <ref type="table" target="#tab_8">Table 8</ref> shows the impact of the different asymmetry parameters values ? + and ? ? . As the authors of ASL <ref type="bibr" target="#b0">[1]</ref> state in their work, a larger ? ? is required to handle a larger positive-negative imbalance. They also set ? + to 0 for all experiments, but we observe that there are datasets where non-zero ? + in AAM loss could bring an improvement as well.</p><p>We also notice that in the case of AAM loss, we need just 0-2 values ranges, as we have additional global loss scale parameter 1?k s in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph attention branch and transformer head comparison</head><p>We want to compare our proposed new re-weighting scheme and the GCN method. If we refer to ML-GCN work <ref type="bibr" target="#b2">[3]</ref>, we see that the training strategy the authors chose for their method is rather simple. We argued that if we add a modern bag of training tricks to the method, the accuracy potentially could increase. We train backbone with ML-GCN head as described in <ref type="bibr" target="#b2">[3]</ref>, but with our training strategy as described in Section 3.5. <ref type="table" target="#tab_9">Table 9</ref> shows that GAN with a re-weighting scheme is more sophisticated in incorporating inter-label correlation. At the same time, the global attention ability of the transformer is sufficient for seeking internal  dependencies. Even if we combine two methods and add to the model apriori information about the conditional probability of appearing labels, we will not obtain better quality. The next point is to check the ability of the transformer and GAN model to handle case of the high level of label noise. A high level of label noise can be achieved by simply downsizing the image resolution. Most of the small objects (especially in COCO and NUS-WIDE datasets) will disappear because of resize artifacts. <ref type="table" target="#tab_0">Table 10</ref> shows the obtained results on all data with resolution 224x224. We also present the results of the model without both ML Decoder and GAN. We can conclude, that while the GAN branch is sufficient to help the ordinary model to better obtain global information on label distribution and co-occurrence. The transformer-based head can derive this information implicitly from the CNN features and in a more efficient way. Again, adding GAN as an additional branch doesn't improve the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we revisited two popular approaches to multilabel classification: transformer-based heads and labels graph branches. We refined the performance of these approaches by applying our training strategy with the modern bug of tricks and introducing a novel loss for multilabel classification called AAM. The loss combines properties of the ASL loss and metric learning approach and allows achieving competitive results on popular multilabel benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Plots of positive and negative parts of AAM with varying hyperparameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The performance curves of EfficientNetV2-s+MLD+AAM model with different values of the s parameter. Margin parameter in AAM loss is set to zero. Training resolution is 224x224.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image classification datasets which were used for training.</figDesc><table><row><cell>Dataset</cell><cell># of classes</cell><cell cols="2"># of Images</cell><cell>Avg # of</cell></row><row><cell></cell><cell></cell><cell cols="3">Train Validation labels per image</cell></row><row><cell>Pascal VOC 2007 [9]</cell><cell>20</cell><cell>5011</cell><cell>4952</cell><cell>1.58</cell></row><row><cell>MS-COCO 2014 [22]</cell><cell>80</cell><cell>117266</cell><cell>4952</cell><cell>2.92</cell></row><row><cell>NUS-WIDE [4]</cell><cell>81</cell><cell>119103</cell><cell>50720</cell><cell>2.43</cell></row><row><cell>Visual Genome [18]</cell><cell>500</cell><cell>82904</cell><cell>10000</cell><cell>13.61</cell></row><row><cell>OpenImages V6 [20]</cell><cell>601</cell><cell cols="2">1866950 41151</cell><cell>5.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on MS-COCO dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input resolution mAP GFLOPs</cell></row><row><cell>ASL [1]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>88.40 43.50</cell></row><row><cell>Q2L [23]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>89.20 60.40</cell></row><row><cell>GATN [35]</cell><cell>ResNeXt-101</cell><cell>448x448</cell><cell>89.30 36.00</cell></row><row><cell>Q2L [23]</cell><cell>TResNet-L</cell><cell>640x640</cell><cell>90.30 119.69</cell></row><row><cell>ML-Decoder [27]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>90.00 36.15</cell></row><row><cell>ML-Decoder [27]</cell><cell>TResNet-L</cell><cell>640x640</cell><cell>91.10 73.42</cell></row><row><cell>ASL  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>87.05 10.83</cell></row><row><cell>ML-GCN  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>87.50 10.83</cell></row><row><cell>Q2L  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>87.35 16.25</cell></row><row><cell>ML-Decoder  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>88.25 12.28</cell></row><row><cell>GAN re-weighting (ours)</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>87.70 10.83</cell></row><row><cell>GAN re-weighting (ours)</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>89.95 35.20</cell></row><row><cell cols="2">ML-Decoder + AAM (ours) EfficientNet-V2-s</cell><cell>448x448</cell><cell>88.75 12.28</cell></row><row><cell cols="2">ML-Decoder + AAM (ours) EfficientNet-V2-L</cell><cell>448x448</cell><cell>90.10 49.92</cell></row><row><cell>ML-Decoder + AAM (ours)</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>90.30 36.15</cell></row><row><cell>ML-Decoder + AAM (ours)</cell><cell>TResNet-L</cell><cell>640x640</cell><cell>91.30 73.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Trained by us using our training strategy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art on Pascal-VOC dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input resolution mAP GFLOPs</cell></row><row><cell>ASL [1]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>94.60 43.50</cell></row><row><cell>Q2L [23]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>96.10 57.43</cell></row><row><cell>GATN [35]</cell><cell>ResNeXt-101</cell><cell>448x448</cell><cell>96.30 36.00</cell></row><row><cell>ML-Decoder [27]</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>96.60 35.78</cell></row><row><cell>ASL  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>94.24 10.83</cell></row><row><cell>Q2L  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>94.94 15.40</cell></row><row><cell>ML-GCN  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>95.25 10.83</cell></row><row><cell>ML-Decoder  *</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>95.54 12.00</cell></row><row><cell>GAN re-weighting (ours)</cell><cell>EfficientNet-V2-s</cell><cell>448x448</cell><cell>96.00 10.83</cell></row><row><cell>GAN re-weighting (ours)</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>96.67 35.20</cell></row><row><cell cols="2">ML-Decoder + AAM (ours) EfficientNet-V2-s</cell><cell>448x448</cell><cell>95.86 12.00</cell></row><row><cell cols="2">ML-Decoder + AAM (ours) EfficientNet-V2-L</cell><cell>448x448</cell><cell>96.05 49.92</cell></row><row><cell>ML-Decoder + AAM (ours)</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>96.70 35.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Trained by us using our training strategy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art on VG500.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Input resolution mAP GFLOPs</cell></row><row><cell>C-Tran [21]</cell><cell>ResNet101</cell><cell>576x576</cell><cell>38.40</cell><cell>-</cell></row><row><cell>Q2L [23]</cell><cell>TResNet-L</cell><cell>512x512</cell><cell cols="2">42.50 119.37</cell></row><row><cell>ASL  *</cell><cell>EfficientNet-V2-s</cell><cell>576x576</cell><cell cols="2">38.84 17.90</cell></row><row><cell>Q2L  *</cell><cell>EfficientNet-V2-s</cell><cell>576x576</cell><cell cols="2">40.35 32.81</cell></row><row><cell>ML-Decoder  *</cell><cell>EfficientNet-V2-s</cell><cell>576x576</cell><cell cols="2">41.20 20.16</cell></row><row><cell cols="2">ML-Decoder + AAM (ours) EfficientNet-V2-s</cell><cell>576x576</cell><cell cols="2">42.00 20.16</cell></row><row><cell>ML-Decoder + AAM (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of confidence thresholds calibration for EfficientNetV2-s+MLD model trained with AAM.</figDesc><table><row><cell>Dataset</cell><cell cols="4">OF1 OF1-adaptive CF1 CF1-adaptive</cell></row><row><cell cols="2">Pascal-VOC 91.14</cell><cell>91.94</cell><cell>92.60</cell><cell>93.21</cell></row><row><cell>COCO</cell><cell>82.86</cell><cell>84.39</cell><cell>85.03</cell><cell>86.06</cell></row><row><cell cols="2">NUS-WIDE 71.88</cell><cell>75.52</cell><cell>72.90</cell><cell>75.04</cell></row><row><cell>VG500</cell><cell>56.04</cell><cell>54.09</cell><cell>55.15</cell><cell>53.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Algorithm's components contribution.</figDesc><table><row><cell>Step</cell><cell cols="4">VOC-2007 COCO NUS-WIDE VG500</cell></row><row><cell>baseline</cell><cell>93.58</cell><cell>85.90</cell><cell>63.85</cell><cell>37.85</cell></row><row><cell>+ SAM</cell><cell>94.00</cell><cell>86.75</cell><cell>65.20</cell><cell>38.50</cell></row><row><cell>+ OI pretraining</cell><cell>94.24</cell><cell>87.05</cell><cell>65.54</cell><cell>38.84</cell></row><row><cell>+ MLD</cell><cell>95.54</cell><cell>88.30</cell><cell>67.07</cell><cell>41.20</cell></row><row><cell>+ AM loss  *</cell><cell>95.80</cell><cell>88.60</cell><cell>67.30</cell><cell>41.90</cell></row><row><cell>+ AAM loss</cell><cell>95.86</cell><cell>88.75</cell><cell>67.60</cell><cell>42.00</cell></row><row><cell>+ GAN branch</cell><cell>95.85</cell><cell>88.70</cell><cell>67.20</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Angular Margin loss with ?+ = ?? = 0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Asymmetry influence on the training pipeline.</figDesc><table><row><cell>Gamma</cell><cell cols="4">VOC-2007 COCO NUS-WIDE VG500</cell></row><row><cell>? ? = 0, ? + = 0</cell><cell>95.80</cell><cell>88.60</cell><cell>67.20</cell><cell>41.90</cell></row><row><cell>? ? = 1, ? + = 0</cell><cell>95.80</cell><cell>88.75</cell><cell>67.30</cell><cell>42.00</cell></row><row><cell>? ? = 2, ? + = 0</cell><cell>95.77</cell><cell>88.68</cell><cell>67.47</cell><cell>41.90</cell></row><row><cell>? ? = 3, ? + = 0</cell><cell>95.78</cell><cell>88.62</cell><cell>67.27</cell><cell>41.80</cell></row><row><cell cols="2">? ? = 2, ? + = 1 95.86</cell><cell>88.64</cell><cell>67.60</cell><cell>41.95</cell></row><row><cell>? ? = 3, ? + = 1</cell><cell>95.80</cell><cell>88.52</cell><cell>67.60</cell><cell>41.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of GAN vs GCN method and ML Decoder.</figDesc><table><row><cell>Step</cell><cell cols="3">VOC-2007 COCO NUS-WIDE</cell></row><row><cell>ML-GCN EfficientNetV2-s</cell><cell>95.25</cell><cell>87.50</cell><cell>66.30</cell></row><row><cell>GAN re-weighting EfficientNetV2-s</cell><cell>96.00</cell><cell>87.70</cell><cell>66.85</cell></row><row><cell>EfficientNetV2-s + MLD</cell><cell>95.86</cell><cell>88.75</cell><cell>67.60</cell></row><row><cell>GAN re-weighting EfficientNetV2-s + MLD</cell><cell>95.85</cell><cell>88.70</cell><cell>67.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparison of graph-based method and transformer based in low resolution setup.</figDesc><table><row><cell>Step</cell><cell cols="3">VOC-2007 COCO NUS-WIDE</cell></row><row><cell>EfficientNetV2-s</cell><cell>92.85</cell><cell>82.25</cell><cell>65.60</cell></row><row><cell>GAN re-weighting EfficientNetV2-s</cell><cell>93.15</cell><cell>82.55</cell><cell>65.87</cell></row><row><cell>EfficientNetV2-s + MLD</cell><cell>93.77</cell><cell>83.03</cell><cell>66.74</cell></row><row><cell>GAN re-weighting EfficientNetV2-s + MLD</cell><cell>93.71</cell><cell>83.03</cell><cell>66.60</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/openvinotoolkit/deep-object-reid/tree/multilabel arXiv:2209.06585v1 [cs.CV] 14 Sep 2022</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Asymmetric loss for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5172" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18613" to="18624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno>ArXiv abs/2010.01412</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discover multi-class attentional regions for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5920" to="5932" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Searching for mobilenetv3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F., Burges, C.J.C., Bottou, L., Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dataset v 4 unified image classification , object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16473" to="16483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Query2label: A simple transformer way to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>ArXiv abs/2107.10834</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Makarenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<title level="m">Language models with pre-trained (glove) word embeddings. arXiv: Computation and Language</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards efficient and data agnostic image classification training pipeline for embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prokofiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sovrasov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing -ICIAP 2022: 21st International Conference</title>
		<meeting><address><addrLine>Lecce, Italy; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="476" to="488" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpudedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1399" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<title level="m">Ml-decoder: Scalable and versatile classification head</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1803.09820</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building computationally efficient and well-generalizing person re-identification models with metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sovrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidnev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="639" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>ArXiv abs/1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>ArXiv abs/2104.00298</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sphereface2: Binary classification is all you need for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<idno>ArXiv abs/2108.01513</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph attention transformer network for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<idno>ArXiv abs/2203.04049</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Residual attention: A simple but effective method for multi-label recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
