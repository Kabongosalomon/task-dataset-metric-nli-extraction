<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VQ-GNN: A Universal Framework to Scale-up Graph Neural Networks using Vector Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucong</forename><surname>Ding</surname></persName>
							<email>mcding@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<email>kong@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
							<email>jingling@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
							<email>chenzhu@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
							<email>furongh@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VQ-GNN: A Universal Framework to Scale-up Graph Neural Networks using Vector Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art Graph Neural Networks (GNNs) can be defined as a form of graph convolution which can be realized by message passing between direct neighbors or beyond. To scale such GNNs to large graphs, various neighbor-, layer-, or subgraph-sampling techniques are proposed to alleviate the "neighbor explosion" problem by considering only a small subset of messages passed to the nodes in a mini-batch. However, sampling-based methods are difficult to apply to GNNs that utilize many-hops-away or global context each layer, show unstable performance for different tasks and datasets, and do not speed up model inference. We propose a principled and fundamentally different approach, VQ-GNN, a universal framework to scale up any convolution-based GNNs using Vector Quantization (VQ) without compromising the performance. In contrast to sampling-based techniques, our approach can effectively preserve all the messages passed to a mini-batch of nodes by learning and updating a small number of quantized reference vectors of global node representations, using VQ within each GNN layer. Our framework avoids the "neighbor explosion" problem of GNNs using quantized representations combined with a low-rank version of the graph convolution matrix. We show that such a compact low-rank version of the gigantic convolution matrix is sufficient both theoretically and experimentally. In company with VQ, we design a novel approximated message passing algorithm and a nontrivial back-propagation rule for our framework. Experiments on various types of GNN backbones demonstrate the scalability and competitive performance of our framework on large-graph node classification and link prediction benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise of Graph Neural Networks (GNNs) has brought the modeling of complex graph data into a new era. Using message-passing, GNNs iteratively share information between neighbors in a graph to make predictions of node labels, edge labels, or graph-level properties. A number of powerful GNN architectures <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr">[4]</ref> have been widely applied to solve down-stream tasks such as recommendation, social analysis, visual recognition, etc.</p><p>With the soaring size of realistic graph datasets and the industrial need to model them efficiently, GNNs are hindered by a scalability problem. An L-layer GNN aggregates information from all L-hop neighbors, and standard training routines require these neighbors to all lie on the GPU at once. This prohibits full-batch training when facing a graph with millions of nodes <ref type="bibr">[5]</ref>.  <ref type="figure">Figure 1</ref>: In our framework, VQ-GNN, each mini-batch message passing (left) is approximated by a VQ codebook update (middle) and an approximated message passing (right). All the messages passed to the nodes in the current mini-batch are effectively preserved. Circles are nodes, and rectangles are VQ codewords. A double circle indicates nodes in the current mini-batch. Color represents codeword assignment. During VQ codebook update, codeword assignment of nodes in the mini-batch is refreshed (node 1), and codewords are updated using the assigned nodes. During approximated message passing, messages from out-of-mini-batch nodes are approximated by messages from the corresponding codewords, messages from nodes assigned to the same codeword are merged (a and b), and intra-mini-batch messages are not changed <ref type="bibr">(c and d)</ref>.</p><p>A number of sampling-based methods have been proposed to accommodate large graphs with limited GPU resources. These techniques can be broadly classified into three categories: (1) Neighborsampling methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> sample a fixed-number of neighbors for each node; (2) Layer-sampling methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> sample nodes in each layer independently with a constant sample size; (3) Subgraphsampling methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> sample a subgraph for each mini-batch and perform forward and backpropagation on the same subgraph across all layers. Although these sampling-based methods may significantly speed up the training time of GNNs, they suffer from the following three major drawbacks: (1) At inference phase, sampling methods require all the neighbors to draw non-stochastic predictions, resulting in expensive predictions if the full graph cannot be fit on the inference device;</p><p>(2) As reported in <ref type="bibr">[5]</ref> and in Section 6, state-of-the-art sampling-baselines fail to achieve satisfactory results consistently across various tasks and datasets; (3) Sampling-based methods cannot be universally applied to GNNs that utilize many-hop or global context in each layer, which hinders the application of more powerful GNNs to large graphs.</p><p>This paper presents VQ-GNN, a GNN framework using vector quantization to scale most state-ofthe-art GNNs to large graphs through a principled and fundamentally different approach compared with the sampling-based methods. We explore the idea of using vector quantization (VQ) as a means of dimensionality reduction to learn and update a small number of quantized reference vectors (codewords) of global node representations. In VQ-GNN, mini-batch message passing in each GNN layer is approximated by a VQ codebook update and an approximated form of message passing between the mini-batch of nodes and codewords; see <ref type="figure">Fig. 1</ref>. Our approach avoids the "neighbor explosion" problem and enables mini-batch training and inference of GNNs. In contrast to samplingbased techniques, VQ-GNN can effectively preserve all the messages passed to a mini-batch of nodes. We theoretically and experimentally show that our approach is efficient in terms of memory usage, training/inference time, and convergence speed. Experiments on various GNN backbones demonstrate the competitive performance of our framework compared with the full-graph training baseline and sampling-based scalable algorithms.</p><p>Paper organization. The remainder of this paper is organized as follows. Section 2 summarizes GNNs that can be re-formulated into a common framework of graph convolution. Section 3 defines the scalability challenge of GNNs and shows that dimensionality reduction is a potential solution.</p><p>In Section 4, we describe our approach, VQ-GNN, from theoretical framework to algorithm design and explain why it solves the scalability issue of most GNNs. Section 5 compares our approach to the sampling-based methods. Section 6 presents a series of experiments that validate the efficiency, robustness, and universality of VQ-GNN. Finally, Section 7 concludes this paper with a summary of limitations and broader impacts. </p><formula xml:id="formula_0">C = D ?1/2 A D ?1/2 SAGE-Mean 2 [2] Message Passing Fixed 2 C (1) = I n C (2) = D ?1 A GAT 3 [3] Self-Attention Learnable # of heads ? ? ? ? ? C (s) = A + I n and h (s) a (l,s) (X (l) i,: , X (l) j,: ) = exp LeakyReLU( (X (l) i,: W (l,s) X (l) j,: W (l,s) ) ? a (l,s) ) 1 Where A = A + In, D = D + In. 2 C (2) represents mean aggregator. Weight matrix in [2] is W (l) = W (l,1) W (l,2) . 3 Need row-wise normalization. C (l,s) i,j</formula><p>is non-zero if and only if Ai,j = 1, thus GAT follows direct-neighbor aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries: GNNs defined as Graph Convolution</head><p>Notations. Consider a graph with n nodes and m edges (average degree d = m/n). Connectivity is given by the adjacency matrix A ? {0, 1} n?n and features are defined on nodes by X ? R n?f0 with f 0 the length of feature vectors. Given a matrix C, let C i,j , C i,: , and C :,j denote its (i, j)-th entry, i-th row, j-th column, respectively. For a finite sequence i b : i 1 , . . . , i b , we use C i b ,: to denote the matrix whose rows are the i b -th rows of matrix C. We use to denote the element-wise (Hadamard) product. ? p denotes the entry-wise p norm of a vector and ? F denotes the Frobenius norm. We use I n ? R n?n to denote the identity matrix, 1 n ? R n to denote the vector whose entries are all ones, and e i n to denote the unit vector in R n whose i-th entry is 1. The 0-1 indicator function is 1{?}. We use diag(c) to denote a diagonal matrix whose diagonal entries are from vector c. And represents concatenation along the last axis. We use superscripts to refer to different copies of same kind of variable. For example, X (l) ? R n?f l denotes node representations on layer l. A Graph Neural Network (GNN) layer takes the node representation of a previous layer X (l) as input and produces a new representation X (l+1) , where X = X (0) is the input features.</p><p>A common framework for generalized graph convolution. Finally, use a table to summarize popular types of GNN models into the general graph convolution framework and refer readers to the appendix for more details. Although many GNNs are designed following different guiding principles including neighborhood aggregation (GraphSAGE <ref type="bibr" target="#b1">[2]</ref>, PNA <ref type="bibr" target="#b10">[11]</ref>), spatial convolution (GCN <ref type="bibr" target="#b0">[1]</ref>), spectral filtering (ChebNet <ref type="bibr" target="#b11">[12]</ref>, CayleyNet <ref type="bibr" target="#b12">[13]</ref>, ARMA <ref type="bibr" target="#b13">[14]</ref>), self-attention (GAT <ref type="bibr" target="#b2">[3]</ref>, Graph Transformers <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>), diffusion (GDC <ref type="bibr" target="#b17">[18]</ref>, DCNN <ref type="bibr" target="#b18">[19]</ref>), Weisfeiler-Lehman (WL) alignment (GIN <ref type="bibr">[4]</ref>, 3WL-GNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>), or other graph algorithms ( <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>). Despite these differences, nearly all GNNs can be interpreted as performing message passing on node features, followed by feature transformation and an activation function. As pointed out by Balcilar et al. <ref type="bibr" target="#b23">[24]</ref>, GNNs can typically be written in the form</p><formula xml:id="formula_1">X (l+1) = ? s C (s) X (l) W (l,s) ,<label>(1)</label></formula><p>where C (s) ? R n?n denotes the s-th convolution matrix that defines the message passing operator, s ? Z + denotes index of convolution, and ?(?) denotes the non-linearity. W (l,s) ? R f l ?f l+1 is the learnable linear weight matrix for the l-th layer and s-th filter.</p><p>Within this common framework, GNNs differ from each other by choice of convolution matrices C (s) , which can be either fixed or learnable. A learnable convolution matrix relies on the inputs and learnable parameters and can be different in each layer (thus denoted as C (l,s) ):</p><formula xml:id="formula_2">C (l,s) i,j = C (s) i,j fixed ? h (s) ? (l,s) (X (l) i,: , X (l) j,: ) learnable<label>(2)</label></formula><p>where C (s) denotes the fixed mask of the s-th learnable convolution, which may depend on the adjacency matrix A and input edge features </p><formula xml:id="formula_3">E i,j . While h (s) (?, ?) : R f l ? R f l ? R</formula><formula xml:id="formula_4">i,j ? C (l,s) i,j / j C (l,s)</formula><p>i,j , for example in GAT <ref type="bibr" target="#b2">[3]</ref>. We stick to Eq. (2) in the main paper and discuss row-wise normalization in Appendices A and E. The receptive field of a layer of graph convolution (Eq. (1)) is defined as a set of nodes R 1 i whose features {X</p><formula xml:id="formula_5">(l) j,: | j ? R i } determines X (l+1) i,:</formula><p>. We re-formulate some popular GNNs into this generalized graph convolution framework; see <ref type="table" target="#tab_0">Table 1</ref> and Appendix A for more.</p><p>The back-propagation rule of GNNs defined by Eq. (1) is as follows,</p><formula xml:id="formula_6">? X (l) = s C (l,s) T ? X (l+1) ? ? ?1 X (l+1) W (l,s) T ,<label>(3)</label></formula><p>which can also be understood as a form of message passing. ? and ? ?1 are the derivative and inverse of ? respectively and ? X (l+1) ? ? ?1 (X (l+1) ) is the gradients back-propagated through the non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalability Problem and Theoretical Framework</head><p>When a graph is large, we are forced to mini-batch the graph by sampling a subset of b n nodes in each iteration. Say the node indices are i 1 , . . . , i b and a mini-batch of node features is denoted by X B = X i b ,: . To mini-batch efficiently for any model, we hope to fetch ?(b) information to the training device, spend ?(Lb) training time per iteration while taking (n/b) iterations to traverse through the entire dataset. However, it is intrinsically difficult for most of the GNNs to meet these three scalability requirements at the same time. The receptive field of L layers of graph convolution (Eq. (1)) is recursively given by</p><formula xml:id="formula_7">R L i = j?R 1 i R L?1 j (starting with R 1 i ? {i} ? N i )</formula><p>, and its size grows exponentially with L. Thus, to optimize on a mini-batch of b nodes, we require ?(bd L ) inputs and training time per iteration. Sampling a subset of neighbors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> for each node in each layer does not change the exponential dependence on L. Although layer- <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> and subgraph-sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> may require only ?(b) inputs and ?(Lb) training time per iteration, they are only able to consider an exponentially small proportion of messages compared with full-graph training. Most importantly, all existing sampling methods do not support dense convolution matrices with O(n 2 ) non-zero terms. Please see Section 5 for a detailed comparison with sampling-based scalable methods after we introduce our framework.</p><p>Idea of dimensionality reduction. We aim to develop a scalable algorithm for any GNN models that can be re-formulated as Eq. (1), where the convolution matrix can be either fixed or learnable, and either sparse or dense. The major obstacle to scalability is that, for each layer of graph convolution, to compute a mini-batch of forward-passed features X</p><formula xml:id="formula_8">(l+1) B = X (l+1) i b ,: , we need O(n) entries of C (l,s) B = C (l,s) i b ,:</formula><p>and X (l) , which will not fit in device memory. Our goal is to apply a dimensionality reduction to both convolution and node feature matrices, and then apply convolution using compressed "sketches" of C (l,s) B and X (l) . More specifically, we look for a projection matrix R ? R n?k with k n, such that the product of low-dimensional sketches C</p><formula xml:id="formula_9">(l,s) B = C (l,s) B R ? R b?k and X (l) = R T X (l) ? R k?f l is approximately the same as C (l,s) B X (l) .</formula><p>The approximated product (of all nodes) C (l,s) X (l) = C (l,s) RR T X (l) can also be regarded as the result of using a low-rank approximation C (l,s) RR T ? R n?n of the convolution matrix such that rank C (l,s) RR T ? k. The distributional Johnson-Lindenstrauss lemma <ref type="bibr" target="#b25">[26]</ref> (JL for short) shows the existence of such projection R with m = ?(log(n)), and the following result by Kane and Nelson <ref type="bibr" target="#b26">[27]</ref> shows that R can be chosen to quite sparse: Theorem 1. For any convolution matrix C ? R n?n , any column vector X :,a ? R n of the node feature matrix X ? R n?f (where a = 1, . . . , f ) and ? &gt; 0, there exists a projection matrix R ? R n?k (drawn from a distribution) with only an O(?)-fraction of entries non-zero, such that Pr CRR T X :,a ? CX :,a 2 &lt; ? CX :,a 2 &gt; 1 ? ?,</p><p>with k = ?(log(n)/? 2 ) and ? = O(1/n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, the sketches C (l,s) B</head><p>and X (l) take up O(b log(n)) and ?(f l log(n)) memory respectively and can fit into the training and inference device. The sparsity of projection matrix R is favorable because:(1) if the convolution matrix C (l,s) is sparse (e.g., direct-neighbor message passing where only O(d/n)-fraction of entries are non-zero), only an O(?d)-fraction of entries are non-zero in the sketch C (l,s) ; (2) During training, X (l) is updated in a "streaming" fashion using each minibatch's inputs X (l) B , and a sparse R reduces the computation time by a factor of O(?). However, the projection R produced following the sparse JL-lemma <ref type="bibr" target="#b26">[27]</ref> is randomized and requires O(log 2 (n)) uniform random bits to sample. It is difficult to combine this with the deterministic feed-forward and back-propagation rules of neural networks, and there is no clue when and how we should update the projection matrix. Moreover, randomized projections destroy the "identity" of each node, and for learnable convolution matrices (Eq. (2)), it is impossible to compute the convolution matrix only using the sketch of features X <ref type="bibr">(l)</ref> . For this idea to be useful, we need a deterministic and identity-preserving construction of the projection matrix R ? R n?k to avoid these added complexities.</p><p>4 Proposed Method: Vector Quantized GNN Dimensionality reduction using Vector Quantization (VQ). A natural and widely-used method to reduce the dimensionality of data in a deterministic and identity-preserving manner is Vector Quantization <ref type="bibr" target="#b27">[28]</ref> (VQ), a classical data compression algorithm that can be formulated as the following optimization problem:</p><formula xml:id="formula_11">min R?{0,1} n?k , X?R k?f X ? R X F s.t. R i,: ? {e 1 k , . . . , e k k },<label>(5)</label></formula><p>which is classically solved via k-means <ref type="bibr" target="#b27">[28]</ref>. Here the sketch of features X is called the feature "codewords." R is called the codeword assignment matrix, whose rows are unit vectors in R k , i.e., R i,v = 1 if and only if the i-th node is assigned to the v-th cluster in k-means. The objective in Eq. (5) is called Within-Cluster Sum of Squares (WCSS), and we can define the relative error of VQ as = X ? R X F / X F . The rows of X are the k codewords (i.e., centroids in k-means), and can be computed as X = diag ?1 (R T 1 n )R T X, which is slightly different from the definition in Section 3 as a row-wise normalization of R T is required. The sketch of the convolution matrix C can still be computed as C = CR. In general, VQ provides us a principled framework to learn the low-dimensional sketches X and C, in a deterministic and node-identity-preserving manner. However, to enable mini-batch training and inference of GNNs using VQ, three more questions need to be answered:</p><p>? How to approximate the forward-passed mini-batch features of nodes using the learned codewords?</p><p>? How to back-propagate through VQ and estimate the mini-batch gradients of nodes? ? How to update the codewords and assignment matrix along with the training of GNN?</p><p>In the following part of this section, we introduce the VQ-GNN algorithm by answering all the three questions and presenting a scalability analysis.</p><p>Approximated forward and backward message passing. To approximate the forward pass through a GNN layer (Eq. (1)) with a mini-batch of nodes i b , we can divide the messages into two categories: intra-mini-batch messages, and messages from out-of-mini-batch nodes; see the right figure of <ref type="figure">Fig. 1</ref>. Intra-mini-batch messages C</p><formula xml:id="formula_12">(l,s) in X (l) B can always be computed exactly, where C (l,s) in = (C (l,s) B ) :, i b ? R b?b ,</formula><p>because they only rely on the previous layer's node features of the current mini-batch. Equipped with the codewords X (l) and the codeword assignment of all nodes R (l) , we can approximate the messages from out-of-mini-batch nodes as C (l,s) out . In general, we can easily approximate the forward-passed mini-batch features X</p><formula xml:id="formula_13">out X (l) , where X (l) = diag ?1 (R T 1 n )R T X (l</formula><formula xml:id="formula_14">(l+1) B by X (l+1) B = ? s (C (l,s) in X (l) B + C (l,s) out X (l) )W (l,s) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Both forward and backward</head><p>Used only in forward-pass Used only in back-propagation <ref type="figure">Figure 2</ref>: Three types of messages contribute to the mini-batch features and gradients. We only need "red" and "green" messages for the forward-pass. However, "blue" messages are required for back-propagation. The "red", "blue", and "green" messages are characterized by Cout, ( C T )out, and Cin respectively (Eqs. <ref type="formula" target="#formula_19">(6)</ref> and <ref type="formula" target="#formula_21">(7)</ref>). <ref type="figure">Figure 3</ref>: For each layer, VQ-GNN estimates the forward-passed mini-batch features using the previous layer's mini-batch features and the feature codewords through approximated forward message-passing (Eq. <ref type="formula" target="#formula_19">(6)</ref>). The back-propagated mini-batch gradients are estimated in a symmetric manner with the help of gradient codewords (Eq. <ref type="formula" target="#formula_21">(7)</ref>).</p><formula xml:id="formula_15">VQ Forward Message-Passing Coodbook Mini-Batch Features Gradient Codewords Feature Codewords Back-Propagated Mini-Batch Gradients -th Layer ? Forward-Passed Mini-Batch Features Backward Message-Passing ? Mini-Batch Gradients</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>However, the above construction of X</head><formula xml:id="formula_16">(l+1) B</formula><p>does not allow us to back-propagate through VQ straightforwardly using chain rules. During back-propagation, we aim at approximating the previous layer's mini-batch gradients ? X (l) B given the gradients of the (approximated) output ? X (l+1) B (Eq. <ref type="formula" target="#formula_6">(3)</ref>).</p><p>Firstly, we do not know how to compute the partial derivative of C (l,s) out and X (l) with respect to X (l) B , because the learning and updating of VQ codewords and assignment are data dependent and are usually realized by an iterative optimization algorithm. Thus, we need to go through an iterative computation graph to evaluate the partial derivative of R (l) with respect to X (l) B , which requires access to many historical features and gradients, thus violating the scalability constraints. Secondly, even if we apply some very rough approximation during back-propagation as in <ref type="bibr" target="#b28">[29]</ref>, that is, assuming that the partial derivative of R (l) with respect to X (l) B can be ignored (i.e., the codeword assignment matrix is detached from the computation graph, known as "straight through" back-propagation), we are not able to evaluate the derivatives of codewords X (l) because they rely on some node features out of the current mini-batch and are not in the training device. Generally speaking, designing a back-propagation rule for VQ under the mini-batch training setup is a challenging new problem.</p><p>It is helpful to re-examine what is happening when we back-propagate on the full graph. In Section 2, we see that back-propagation of a layer of convolution-based GNN can also be realized by message passing (Eq. <ref type="formula" target="#formula_6">(3)</ref>). In <ref type="figure">Fig. 2</ref>, we show the messages related to a mini-batch of nodes can be classified into three types. The "green" and "red" messages are the intra-mini-batch messages and the messages from out-of-mini-batch nodes, respectively. Apart from them, although the "blue" messages to out-of-mini-batch nodes do not contribute to the forward-passed mini-batch features, they are used during back-propagation and are an important part of the back-propagated mini-batch gradients. Since both forward-pass and back-propagation can be realized by message passing, can we approximate the back-propagated mini-batch gradients ? X (l) B in a symmetric manner? We can introduce a set of gradient codewords</p><formula xml:id="formula_17">G (l+1) = diag ?1 (R T 1 n )R T G (l+1) using the same assignment matrix, where G (l+1) = ? X (l+1) ? ? ?1 (X (l+1) )</formula><p>is the gradients back-propagated through non-linearity. Each gradient codeword corresponds one-to-one with a feature codeword since we want to use only one assignment matrix R. Each pair of codewords are concatenated together during VQ updates. Following this idea, we define the approximated forward and backward message passing as follows:</p><formula xml:id="formula_18">X (l+1) B ? = ? s C (l,s) in C (l,s) out ( C (l,s)T )out 0 approx. message passing weight matrix C (l,s) X (l) B X (l)</formula><p>mini-batch features and feat. codewords</p><formula xml:id="formula_19">W (l,s) ,<label>(6)</label></formula><formula xml:id="formula_20">? ? ? X (l) B ? ? ? = s C (l,s) T G (l+1) B G (l+1)</formula><p>mini-batch gradients and grad. codewords</p><formula xml:id="formula_21">W (l,s) T ,<label>(7)</label></formula><p>where C (l,s) ? R (b+m)?(b+m) is the approximated message passing weight matrix and is shared during the forward-pass and back-propagation process. The lower halves of the left-hand side vectors of Eqs. <ref type="formula" target="#formula_19">(6)</ref> and <ref type="formula" target="#formula_21">(7)</ref> are used in neither the forward nor the backward calculations and are never calculated during training or inference. The approximated forward and backward message passing enables the end-to-end mini-batch training and inference of GNNs and is the core of our VQ-GNN framework.</p><p>Error-bounds on estimated features and gradients. We can effectively upper bound the estimation errors of mini-batch features and gradients using the relative error of VQ under some mild conditions. For ease of presentation, we assume the GNN has only one convolution matrix in the following theorems.</p><formula xml:id="formula_22">Theorem 2. If the VQ relative error of l-th layer is (l) , the convolution matrix C (l) is either fixed or learnable with the Lipschitz constant of h ? (l) (?) : R 2f l ? R upper-bounded by Lip(h ? (l) )</formula><p>, and the Lipschitz constant of the non-linearity is Lip(?), then the estimation error of forward-passed mini-batch features satisfies,</p><formula xml:id="formula_23">X (l+1) B ? X (l+1) B F ? (l) ? (1 + O(Lip(h ? (l) )))Lip(?) C (l) F X (l) F W (l) F .<label>(8)</label></formula><p>Corollary 3. If the conditions in Theorem 2 hold and the non-linearity satisfies |? (z)| ? ? max for any z ? R, then the estimation error of back-propagated mini-batch gradients satisfies,</p><formula xml:id="formula_24">? X (l) B ? ? X (l) B F ? (l) ? (1 + O(Lip(h ? (l) ))? max C (l) F ? X (l+1) F W (l) F . (9)</formula><p>Note that the error bounds rely on the Lipschitz constant of h(?) when the convolution matrix is learnable. In practice, we can Lipshitz regularize GNNs like GAT <ref type="bibr" target="#b2">[3]</ref> without affecting their performance; see Appendix E.</p><p>VQ-GNN: the complete algorithm and analysis of scalability. The only remaining question is how to update the learned codewords and assignments during training? In this paper, we use the VQ update rule proposed in <ref type="bibr" target="#b28">[29]</ref>, which updates the codewords as exponential moving averages of the mSeini-batch inputs; see Appendix E for the detailed algorithm. We find such an exponential moving average technique suits us well for the mini-batch training of GNNs and resembles the online k-means algorithm. See <ref type="figure">Fig. 3</ref> for the schematic diagram of VQ-GNN, and the complete pseudo-code is in Appendix E.</p><p>With VQ-GNN, we can mini-batch train and perform inference on large graphs using GNNs, just like a regular neural network (e.g., MLP). We have to maintain a small codebook of k codewords and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this section, we review some of the recent scalable GNN methods and analyze their theoretical memory and time complexities, with a focus on scalable algorithms that can be universally applied to a variety of GNN models (like our VQ-GNN framework), including NS-SAGE 2 <ref type="bibr" target="#b1">[2]</ref>, Cluster-GCN <ref type="bibr" target="#b8">[9]</ref>, and GraphSAINT <ref type="bibr" target="#b9">[10]</ref>. We consider GCN here as the simplest benchmark. For a GCN with L layers and f -dimensional (hidden) features in each layer, when applied to a sparse graph with n nodes and m edges (i.e., average degree d = m/n) for "full-graph" training and inference: the memory usage is O(Lnf + Lf 2 ) and the training/inference time is O(Lmf + Lnf 2 ). We further assume the graph is large and consider the training and inference device memory is O(b) where b is the mini-batch </p><formula xml:id="formula_25">NS-SAGE O(br L f + Lf 2 ) - O(nr L f + nr L?1 f 2 ) O(nd L f + nd L?1 f 2 ) Cluster-GCN O(Lbf + Lf 2 ) O(m) O(Lmf + Lnf 2 ) GraphSAINT-RW O(L 2 bf + Lf 2 ) - O(L 2 nf + L 2 nf 2 ) VQ-GNN (Ours) O(Lbf + Lf 2 + Lkf ) - O(Lbdf + Lnf 2 + Lnkf ) O(Lbdf + Lnf 2 )</formula><p>size (i.e., the memory bottleneck limits the mini-batch size), and generally d b n m holds. We divide sampling baselines into three categories, and the complexities of selected methods are in <ref type="table" target="#tab_3">Table 2</ref>. See Appendix D for more related work discussions.</p><p>Neighbor-sampling. Neighbor sampling scheme chooses a subset of neighbors in each layer to reduce the amount of message passing required. NS-SAGE <ref type="bibr" target="#b1">[2]</ref> samples r neighbors for each node and only aggregates the messages from the sampled node. For a GNN with L layers, O(br L ) nodes are sampled in a mini-batch, which leads to the complexities growing exponentially with the number of layers L; see <ref type="table" target="#tab_3">Table 2</ref>. Therefore, NS-SAGE is not scalable on large graphs for a model with an arbitrary number of layers. NS-SAGE requires all the neighbors to draw non-stochastic predictions in the inference phase, resulting in a O(d L ) inference time since we cannot fit O(n) nodes all at once to the device. VR-GCN <ref type="bibr" target="#b5">[6]</ref> proposes a variance reduction technique to further reduce the size r of sampled neighbors. However, VR-GCN requires a O(Lnf ) side memory of all the nodes' hidden features and suffers from this added memory complexity.</p><p>Layer-sampling. These methods perform node sampling independently in each layer, which results in a constant sample size across all layers and limits the exponential expansion of neighbor size. FastGCN <ref type="bibr" target="#b6">[7]</ref> applies importance sampling to reduce variance. Adapt <ref type="bibr" target="#b24">[25]</ref> improves FastGCN by an additional sampling network but also incurs the significant overhead of the sampling algorithm.</p><p>Subgraph-sampling. Some proposed schemes sample a subgraph for each mini-batch and perform forward and backward passes on the same subgraph across all layers. Cluster-GCN <ref type="bibr" target="#b8">[9]</ref> partitions a large graph into several densely connected subgraphs and samples a subset of subgraphs (with edges between clusters added back) to train in each mini-batch. Cluster-GCN requires O(m) precomputation time and O(bd) time to recover the intra-cluster edges when loading each mini-batch. GraphSAINT <ref type="bibr" target="#b9">[10]</ref> samples a set of nodes and takes the induced subgraph for mini-batch training. We consider the best-performing variant, GraphSAINT-RW, which uses L steps of random walk to induce subgraph from b randomly sampled nodes. O(Lb) nodes and edges are covered in each of the n/b mini-batches. Although O(Ln) nodes are sampled with some repetition in an epoch, the number of edges covered (i.e., messages considered in each layer of a GNN) is also O(Ln) and is usually much smaller than m. GraphSAINT-Node, which randomly samples nodes for each mini-batch, does not suffer from this L factor in the complexities. However, its performance is worse than GraphSAINT-RW's. Like NS-SAGE and some other sampling methods, Cluster-GCN and GraphSAINT-RW cannot draw predictions on a randomly sampled subgraph in the inference phase. Thus they suffer from the same O(d L ) inference time complexity as NS-SAGE; see <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we verify the efficiency, robustness, and universality of VQ-GNN using a series of experiments. See Appendix F for implementation details and Appendix G for ablation studies and more experiments.</p><p>Scalability and efficiency: memory usage, convergence, training and inference time. We summarize the memory usage of scalable methods and our VQ-GNN framework in <ref type="table" target="#tab_4">Table 3</ref>. Based on the implementations of the PyG library <ref type="bibr" target="#b29">[30]</ref>, memory consumption of GNN models usually grows linearly with respect to both the number of nodes and the number of edges in a mini-batch. On the ogbn-arxiv benchmark, we fix the number of gradient-descended nodes and the number of messages passed in a mini-batch to be 85K and 1.5M respectively for fair comparisons among the sampling  methods and our approach. VQ-GNN might require some small extra memory when provided with the same amount of nodes per batch, which is the cost to retain all the edges from the original graph. However, our VQ-GNN framework can effectively preserve all the edges connected to a mini-batch of nodes (i.e., never drop edges); see <ref type="figure">Fig. 1</ref>. Thus when we fix the number of messages passed per batch, our method can show significant memory efficiency compared with the sampling baselines. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the convergence comparison of various scalability methods, where we see VQ-GNN is superior in terms of the convergence speed with respect to the training time. When training GCN and SAGE-Mean on the ogbn-arxiv benchmark for a specific amount of time (e.g., 100 s), the validation performance of VQ-GNN is always the highest. The training time in <ref type="figure" target="#fig_2">Fig. 4</ref> excludes the time for data loading, pre-processing, and validation set evaluation.</p><p>Our VQ-GNN approach also leads to compelling inference speed-ups. Despite the training-efficiency issues of GNNs, conducting inference on large-scale graphs suffers from some unique challenges. According to our discussions in Section 5, and following the standard implementations provided by the Open Graph Benchmark (OGB) <ref type="bibr">[5]</ref>, the three sampling-based baselines (which share the same inference procedure) require all of the L-hop neighbors of the mini-batch nodes to lie on the device at once during the inference phase. The inference time of SAGE-Mean trained with sampling-methods on the ogbn-arxiv benchmark is 1.61 s, while our method can accelerate inference by an order of magnitude and reduce the inference time to 0.40 s.</p><p>Performance comparison across various datasets, settings, and tasks. We validate the efficacy of our method on various benchmarks in <ref type="table" target="#tab_5">Table 4</ref>. The four representative benchmarks are selected because they have very different types of datasets, settings, and tasks. The ogbn-arxiv benchmark is a common citation network of arXiv papers, while Reddit is a very dense social network of Reddit posts, which has much more features per node and larger average node degree; see <ref type="table" target="#tab_9">Table 6</ref> in Appendix F for detailed statistics of datasets. PPI is a node classification benchmark under the inductive learning setting, i.e., neither attributes nor connections of test nodes are present during training, while the other benchmarks are all transductive. VQ-GNN can be applied under the inductive setting with only one extra step: during the inference stage, we now need to find the codeword assignments (i.e., the nearest codeword) of the test nodes before making predictions since we have no access to the test nodes during training. Neither the learned codewords nor the GNN parameters are updated during inference. ogbl-collab is a link prediction benchmark where the labels and loss are intrinsically different.</p><p>It is very challenging for a scalable method to perform well on all benchmarks. In <ref type="table" target="#tab_5">Table 4</ref>, we confirm that VQ-GNN is more robust than the three sampling-based methods. Across the four benchmarks, 2 "OOM" refers to "out of memory". The "full-graph" training on the Reddit benchmark requires more than 11 GB of memory. <ref type="bibr" target="#b2">3</ref> The PPI benchmark comes with multiple labels per node, and the evaluation metric is F1 score instead of accuracy.</p><p>VQ-GNN can always achieve performance similar with or better than the oracle "full-graph" training performance, while the other scalable algorithms may suffer from performance drop in some cases. For example, NS-SAGE fails when training GAT on ogbl-collab, Cluster-GCN consistently falls behind on PPI, and GraphSAINT-RW's performance drops on the ogbl-collab when using SAGE-Mean and GAT backbones. We think the robust performance of VQ-GNN is its unique value among the many other scalable solutions. VQ-GNN framework is robust because it provides bounded approximations of "full-graph" training (Theorem 2 and Corollary 3), while most of the other scalable algorithms do not enjoy such a theoretical guarantee. VQ-GNN is also universal to various backbone models, including but not limited to GCN, SAGE-Mean, and GAT shown here; see Appendix G for more experiments on GNNs that utilize multi-hop neighborhoods and global context, e.g., graph transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Summary of our framework: strengths, weaknesses, future directions, and broader impacts. This paper introduced the proposed VQ-GNN framework, which can scale most state-of-the-art GNNs to large graphs through a principled and fundamentally different approach compared with samplingbased methods. We have shown both theoretically and experimentally that our approach is efficient in memory usage, training and inference time, and convergence speed. VQ-GNN can be universally applied to most GNN models and different graph learning tasks and can equivalently scale-up GNNs utilizing many-hops-away or global context for each layer. However, the performance of VQ-GNN relies on the quality of approximation provided by VQ. In practice, for VQ to work adequately in GNN, a set of techniques are necessary. Because of the limited time, we did not heuristically explore all possible techniques or optimize the VQ design. Given that our preliminary design of VQ in GNN already achieved competitive performance compared with the state-of-the-art sampling baselines, we hypothesize that further optimization of VQ design could improve performance. We hope our work opens up promising new avenues of research for scaling up GNNs, which also has the potential to be applied to other data domains wherever the size of a single sample is large, e.g., long time-series or videos. Considering broader impacts, we view our work mainly as a methodological and theoretical contribution, which paves the way for more resource-efficient graph representation learning. We envision our methodological innovations can enable more scalable ways to do large-network analysis for social good. However, progress in graph embedding learning might also trigger other hostile social network analyses, e.g., extracting fine-grained user interactions for social tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Generalized Graph Convolution Framework</head><p>In this section, we present more results and discussions regarding the common generalized graph convolution framework in Section 2.</p><p>Summary of GNNs re-formulated into the common graph convolution framework. As stated in Section 2, most GNNs can be interpreted as performing message passing on node features, followed by feature transformation and an activation function (Eq. <ref type="figure">(6)</ref>), which is known as the common "generalized graph convolution" framework. We list more GNN models that fall into this framework in the following <ref type="table" target="#tab_6">Table 5</ref>. </p><formula xml:id="formula_26">? ? ? ? ? ? ? C (1) = I n , C (2) = 2L/? max ? I n , C (s) = 2C (2) C (s?1) ? C (s?2)</formula><p>and h (s)</p><formula xml:id="formula_27">? (s) = ? (s) GDC 3 [18] Diffusion Fixed 1 C = S Graph Transformers 4 [15-17] Self-Attention Learnable # of heads ? ? ? ? ? C (s) i,j = 1 and h (s) (W (l,s) Q ,W (l,s) K ) (X (l) i,: , X (l) j,: ) = exp 1 ? dk,l (X (l) i,: W (l,s) Q )(X (l) j,: W (l,s) K ) T 1</formula><p>The weight matrices of the two convolution supports are the same, W (l,1) = W (l,2) . <ref type="bibr" target="#b1">2</ref> Where normalized Laplacian L = I n ? D ?1/2 AD ?1/2 and ? max is its largest eigenvalue, which can be approximated as 2 for a large graph. <ref type="bibr" target="#b2">3</ref> Where S is the diffusion matrix S = ? k=0 ? k T k , for example, decaying weights ? k = e ?t t k k! and transition matrix T = D ?1/2 A D ?1/2 . 4 Need row-wise normalization. Only describes the global self-attention layer, where W Receptive fields. The model in the top part of <ref type="table" target="#tab_6">Table 5</ref> (GIN) and the models in <ref type="table" target="#tab_0">Table 1</ref> in Section 2 (GCN, SAGE-Mean, and GAT) follow direct-neighbor message passing, and their single-layer receptive fields, defined as a set of nodes R 1 i whose features {X</p><formula xml:id="formula_28">(l) j,: | j ? R i } determines X (l+1) i,:</formula><p>, are exactly R 1 i = {i} ? N i , where N i is the set of direct neighbors of node i. The models in the bottom part of <ref type="table" target="#tab_6">Table 5</ref> (ChebNet, GDC, Transformer) can utilize many-hops-away or gloabl context each layer, and their single-layer receptive field</p><formula xml:id="formula_29">R 1 i ? {i} ? N i .</formula><p>GNNs that cannot be defined as graph convolution. Some GNNs, including Gated Graph Neural Networks <ref type="bibr" target="#b30">[31]</ref> and ARMA Spectral Convolution Networks <ref type="bibr" target="#b13">[14]</ref> cannot be re-formulated into this common graph convolution framework because they rely on either Recurrent Neural Networks (RNNs) or some iterative processes, which are out of the paradigm of message passing.</p><p>Further row-wise normalization of a learnable convolution matrix. Sometimes a learnable convolution matrix many be further row-wise normalized as C</p><formula xml:id="formula_30">(l,s) i,j ? C (l,s) i,j / j C (l,s)</formula><p>i,j . This is required for self-attention-based GNNs, including GAT and Graph Transformers. This normalization process will not affect the single-layer receptive fields of the two models and can be handled by a small modification to the VQ-GNN algorithm; see Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of Theoretical Results</head><p>This section provides the formal proofs of the theoretical results in the paper.</p><p>Proof of Theorem 1. The proof is based on the sparse distributional Johnson-Lindenstrauss (JL) lemma in <ref type="bibr" target="#b26">[27]</ref>, which is stated as the following Lemma 1 for completeness. Lemma 1. For any integer n &gt; 0, and any 0 &lt; ?, ? &lt; 1/2, there exists a probability distribution of R ? R n?k where R has only an O(?)-fraction of non-zero entries, such that for any x ? R d ,</p><formula xml:id="formula_31">Pr (1 ? ?) x 2 ? R T x 2 ? (1 + ?) x 2 &gt; 1 ? ? ,<label>(10)</label></formula><p>with k = ?(? ?2 log(1/?)) and thus ? = O(e ?? 2 k ).</p><p>Proof of Theorem 1: From the given lemma, consider the inner product x T y between any two vectors x, y ? R n . Using the union-bound, one can derive that,</p><formula xml:id="formula_32">Pr |x T RR T y ? x T y| ? ?|x T y| &gt; 1 ? 2? .</formula><p>Now, let x be any row vector of C and y be any column vector of X, i.e., x = C b,: and y = X :,a for any b ? {1, . . . , n} and a ? {1, . . . , f }, then we get,</p><formula xml:id="formula_33">Pr |C b,: RR T X :,a ? C b,: X :,a | ? ?|C b,: X :,a | &gt; 1 ? 2? .</formula><p>Therefore, by the union bound again, we have,</p><p>Pr CRR T X :,a ? CX :,a 2 ? ? CX :,a 2</p><formula xml:id="formula_34">? 1 ? n b=1</formula><p>Pr |C b,: RR T X :,a ? C b,: X :,a | &gt; ?|C b,: X :,a | &gt; 1 ? 2n? .</p><p>Now we denote ? = 2n? . The required ? = O(1/n) can be achieved when k = ?(log(n)/? 2 ). Thus we finally obtain,</p><p>Pr CRR T X :,a ? CX :,a 2 &lt; ? CX :,a 2 &gt; 1 ? ?, with k = ?(log(n)/? 2 ) and ? = O(1/n), which concludes the proof.</p><p>Proof of Theorem 2. The proof is a direct application of the VQ relative-error bound and the Lipschitz continuity properties.</p><p>Proof of Theorem 2: If we denote the learnable convolution matrix calculated using the approximated features (i.e., feature codewords) by C (l) , then we have,</p><formula xml:id="formula_35">X (l+1) B ? X (l+1) B F ? Lip(?) C (l) R diag ?1 (R T 1 n )R T X (l) W (l) ? C (l) X (l) W (l) F ? Lip(?) C (l) R diag ?1 (R T 1 n )R T X (l) ? C (l) X (l) F W (l) F , where C (l) R diag ?1 (R T 1 n )R T X (l) ? C (l) X (l) F ? C (l) ? C (l) F R diag ?1 (R T 1 n )R T X (l) F + C (l) F X (l) ? R diag ?1 (R T 1 n )R T X (l) F .<label>(11)</label></formula><p>The relative error (l) of VQ of the l-th layer is defined as,</p><formula xml:id="formula_36">X (l) ? R diag ?1 (R T 1 n )R T X (l) F ? (l) X (l) F .</formula><p>Thus the second term on the right-hand side of Eq. (11) satisfies,</p><formula xml:id="formula_37">C (l) F X (l) ? R diag ?1 (R T 1 n )R T X (l) F ? (l) ? C (l) F X (l) F</formula><p>For the first term on the right-hand side of Eq. (11) satisfies, first note that,</p><formula xml:id="formula_38">R diag ?1 (R T 1 n )R T F = ? k</formula><p>is a constant where k is the number of codewords.</p><p>And C (l) is different to C (l) , because the convolution matrix is learnable. We have</p><formula xml:id="formula_39">C (l) i,j = C i,j h ? (l) (X i,: , X j,: ) and C (l) i,j = C i,j h ? (l) ( X (l) i,: , X (l) j,: ), where X (l) = diag ?1 (R T 1 n )R T X (l) .</formula><p>Therefore, using the Lipschitz constant of h ? (l) , because for any i ? {1, . . . , n},</p><formula xml:id="formula_40">X i,: ? X (l) i,: 2 ? X (l) ? R diag ?1 (R T 1 n )R T X (l) F ? (l) X (l) F ,</formula><p>we have for any i, j ? {1, . . . , n},</p><formula xml:id="formula_41">|C (l) i,j ? C (l) i,j | ? 2|C i,j | ? Lip(h ? (l) ) (l) X (l) F</formula><p>Summing up for all (i, j) ? {1, . . . , n} 2 , we have, for the first term on the right-hand side of Eq. <ref type="formula" target="#formula_1">(11)</ref>,</p><formula xml:id="formula_42">C (l) ? C (l) F ? 2 C F ? Lip(h ? (l) ) (l) X (l) F</formula><p>Combining these two inequalities, we finally have,</p><formula xml:id="formula_43">X (l+1) B ? X (l+1) B F ? (l) ? (1 + O(Lip(h ? (l) )))Lip(?) C (l) F X (l) F W (l) F . which concludes the proof.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Corollary 3</head><p>The proof is similar to the proof of Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Corollary 3:</head><p>This time, we use the message passing equation for the back-propagation process, Eq. (3), restated as follows,</p><formula xml:id="formula_44">? X (l) = s C (l,s) T ? X (l+1) ? ? ?1 X (l+1) W (l,s) T .</formula><p>Note that,</p><formula xml:id="formula_45">? X (l+1) ? ? ?1 X (l+1) F ? ? max ? X (l+1) F ,</formula><p>The rest of the proof simply follows the proof of Theorem 2, we similarly obtain,</p><formula xml:id="formula_46">? X (l) B ? ? X (l) B F ? (l) ? (1 + O(Lip(h ? (l) ))? max C (l) F ? X (l+1) F W (l) F ,</formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Theoretical Discussions</head><p>Upper-bound the estimation error of learnable parameters' gradients. Given that the gradients of each node in each layer is approximated with bounded error (Corollary 3), it is not hard to see that the gradients of learnable parameters W (l) and ? (l) are also estimated with bounded errors. Firstly, the gradients of W (l) , ? W (l) , can be calculated from ? X (l+1) before going into the approximated back-propagation process. Secondly, the gradients of ? (l) , ? ? (l) has bounded estimation error as long as for any (i, j) ? {1, . . . , n} 2 that |C</p><formula xml:id="formula_47">(l) i,j ? C (l)</formula><p>i,j | is bounded, as we have shown in Appendix B.</p><p>Upper-bound the size of VQ codebook. In order to effectively upper-bound the size of the VQ codebook, i.e., the number of reference vectors in VQ and the number of clusters in k-means, we generally need some extra mild assumptions on the distributions of the (hidden) features and gradients. For instance, if we assume the distributions of each feature and gradient in the l-th layer are sub-Gaussian, i.e., they have strong tail decay dominated by the tails of a Gaussian. Then, we can show the relative error of VQ = O(k ?1/f ), where f is the dimensionality of features and gradients.</p><p>In addition to this, if we use product VQ (see Appendix E) to utilize multiple VQ process, each working on a subset of f prod f features and gradients, then we can bound the size of codebook as k = O( ?fprod ).</p><p>Justification of choosing VQ as the dimensionality reduction method. We choose VQ as the method of dimensionality reduction to scale up GNNs because of the following reasons:</p><p>? Categorical constraint of VQ: We choose VQ mainly because of its categorical constraint, i.e., the rows of projection matrix R are unit vectors, shown in Eq. (5) of the manuscript. Intuitively, this means each node feature vector corresponds to exactly one codeword vector at a time, and thus we can replace its feature vector with the corresponding codeword. This is what we mean by "node-identity-preserving" in Sections 3 and 4. ? Compared with PCA: PCA is not suitable when we want to compress the feature table of n nodes into a compact codebook when the number of nodes n is much larger than the number of features per node. Moreover, PCA is shown to have the same objective function as VQ under some settings but without the categorical constraint <ref type="bibr" target="#b31">[32]</ref>. In PCA, each node feature is represented using the set of principal components (i.e., eigenvectors of the covariance matrix). Thus, we must use the complete set of principal components to recover each node feature vector before passing it to GNNs. Moreover, it is much harder to develop an online PCA algorithm to be used along with the training of GNNs. ? Compared with Fisher's LDA: Fisher's LDA, compared with PCA, is supervised and takes class labels into consideration. However, node classification or link prediction on a large graph is a semi-supervised learning problem, and we do not have access to all the node labels during training. We do not know how to compress the test nodes' features under the transductive learning setting, and thus LDA is not a choice. ? Compared with randomized projection and locality-sensitive hashing: VQ is a better choice than randomized projection and locality-sensitive hashing because it is deterministic. Using VQ, we do not have to deal with the extra burden introduced by stochasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Discussions of Related Work</head><p>In this section, we continue the discussions of related work in Section 5 and review some other scalable or efficient methods for GNNs and beyond.</p><p>Other efficient methods for graph representation learning: MLP-based sample models and mixed-precision approaches. There exist some other MLP-based models, e.g., SGC <ref type="bibr" target="#b32">[33]</ref>, which requires only a one-time message passing during the pre-computation stage with O(Lmf ) time. However, SGC, PPRGo <ref type="bibr" target="#b33">[34]</ref> and SIGN <ref type="bibr" target="#b34">[35]</ref> over-simplify the GNN model and limit the expressive power. Despite the fact that they are fast, their performance is not generally comparable with other GNNs. Degree-Quant <ref type="bibr" target="#b35">[36]</ref> and SGQuant <ref type="bibr" target="#b36">[37]</ref> applies mixed-precision techniques specifically designed for GNNs to improve efficiency. Although Degree-Quant can reduce the runtime memory usage from 4x to 8x (SGQquant achieves 4.25-31.9x reduction), they did not provide a solution to effectively mini-batch the GNN training. We note that Degree-Quant and SGQuant do not provide means to mini-batch a large graph and are still "full-graph" training.</p><p>Outside graph learning: VQ in neural network and efficient self-attention. The general idea of using Vector Quantization (VQ) in a neural network is initially proposed for Variational Auto-Encoders (VAEs) in VQ-VAE <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>, and is later generalized to other generative modeling <ref type="bibr" target="#b38">[39]</ref>, contrastive learning <ref type="bibr" target="#b39">[40]</ref>, and self-supervised learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Our work learns form their success and is one of the first attempts of applying VQ to large-scale semi-supervised learning. Our work also shares similarities with the recent advances of efficient self-attention techniques to speed up Transformer models. Linformer <ref type="bibr" target="#b42">[43]</ref> linearizes the time and memory complexities of self-attention by projecting the inputs to a compact representation and approximating the self-attention scores by a low-rank matrix. Hamburger <ref type="bibr" target="#b43">[44]</ref> generally discussed the update rule and back-propagation problem of VQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Algorithm Details</head><p>The complete pseudo-code. Here, we present the complete pseudo-code of our VQ-GNN algorithm as Algorithm 1. Three important components of VQ-GNN, namely approximated forward message-passing, approximated backward message-passing, and VQ Update, are highlighted in Algorithm 1 and in <ref type="figure">Fig. 3</ref> in Section 4.  Dealing with the row-wise normalized learnable convolutions. As mentioned in Section 2 and Appendix A, some self-attention based GNNs including GAT and Graph Transformers require further row-wise normalization of the convolution matrix as C (l,s)</p><formula xml:id="formula_48">3 E[V ] ? E[V ] ? ? + E[V ] ? (1 ? ?) (EMA update of smoothed mean) 4 Var[V ] ? Var[V ] ? ? + Var[V ] ? (1 ? ?) (EMA update of smoothed variance) 5 R B ? FINDNEAREST(V , V ) (update assignment, (R B ) i,v = 1 if V v,: is closest toV i,: ) 6 ? ? ? ? ? + R T B 1 b ? (1 ? ?) (momentum update of cluster sizes) 7 ? ? ? ? ? + R T BV ? (1 ? ?) (</formula><formula xml:id="formula_49">i,j ? C (l,s) i,j / j C (l,s)</formula><p>i,j . However, such a procedure is not characterized by the approximated message passing design in Section 4. Some special treatment is required to normalize the message weights passed to each target node. Actually, this normalization process can be realized by the following three steps:</p><p>1. Padding an extra dimension of ones to the messages, X (l) ? X (l) 1 n 2. Message passing using the unnormalized convolution matrix 3. Normalization by dividing the last dimension, X i,f +1 for any i = 1, . . . , n. In this regard, we decouple the normalization of message weights with the actual message passing process. At the cost of an extra dimension of features (and thus gradients), we can VQ the message passing process again as we did in Section 4. In practice, we found this trick works well, and the experiment results on GAT and Graph Transformer in Section 6 and Appendix G are obtained using this setup.</p><p>Regularizing the Lipschitz constants of learnable convolutions. Since our error bounds on the approximated features and gradients (Theorem 2 and Corollary 3) rely on the Lipschitz constant of learnable convolutions, and the "decoupled row-wise normalization" trick discussed above requires some means to control the unnormalized message weights, we have to Lipschitz regularize some learnable convolution GNNs including GAT and Graph Transformers. We follow the approach described in <ref type="bibr" target="#b46">[47]</ref> to control the Lipschitz constant of GAT and Graph Transformer without affecting their expressive power. Please see <ref type="bibr" target="#b46">[47]</ref> for details.</p><p>Non-linearities, dropout, normalization. In experiments, we found our algorithm, VQ-GNN, is compatible with any non-linearities, dropout, and additional batch-or layer-normalization layers.</p><p>Another implementation of VQ-GNN. Following recent parallel work, GNNAutoScale <ref type="bibr" target="#b47">[48]</ref>, it is also possible to implement the VQ-GNN similarly. We can reconstruct the node features of the out-of-mini-batch neighbors for a mini-batch using the learned codewords, and then perform forward-pass message passing between the with-in-mini-batch nodes and the reconstructed nodes. This implementation is more straightforward but may suffer from larger memory overhead when the underlying graph is dense, e.g., on the Reddit benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Implementation Details</head><p>Hardware specs. Experiments are conducted on hardware with Nvidia GeForce RTX 2080 Ti (11GB GPU), Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz, and 32GB of RAM.</p><p>Dataset statistics. Detailed statistics of all datasets used in the experiments are summarized in Table 6. We note from <ref type="table" target="#tab_9">Table 6</ref> that:</p><p>? PPI is a node classification benchmark under the inductive learning setting, i.e., neither attributes nor connections of test nodes are present during training. ? PPI benchmark comes with multiple labels per node, and the evaluation metric is F1 score instead of accuracy. ? Flickr and Reddit have 500 and 602 features per node, respectively. The increased dimensionality of input node features may be challenging for VQ-GNN because VQ has to compress higherdimensional vectors. Moreover, Reddit's average node degree is 49.8. More memory is required for mini-batches of the same size because more messages are passed in a layer of GNN.</p><p>Hyper-parameter setups of VQ-GNN. To simplify the settings, we fix the hidden dimension of GNNs to 128 and layer number to 3 across the experiments. We set the size of the VQ codewords to 1024, and its size as small as 256 should also work well. We choose RMSprop (alpha=0.99) as the optimizer, and the learning rate is fixed at 3e-3. To mitigate the error induced by VQ in the high-dimensional space, we split feature vectors into small pieces. In practice, we find that when the dimension of each piece is 4, our algorithm generally works well. When the split dimension is 4 we have 32 separate branches each layer to do the VQ. These branches are independent and can be paralleled. At the end of each layer, separated feature vectors are concatenated together to restore the original hidden dimension, and the restored feature is input to the next layer. Batch norm is used for stable training. We do not use drop out for either our method or the baselines.</p><p>Hyper-parameter setups of other scalable methods. For baseline models, we follow the practice of OGB. The optimizer is Adam, and the learning rate is fixed at 1e-3. For a fair comparison with respect to memory consumption, on the ogbn-arxiv dataset we set hyper-parameters as below: SAGE-NS with the batch size 85K, per-layer sampling sizes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">5]</ref>; ClusterGCN batch size 80, number of partitions 40; GraphSAINT-RW batch size 40K, walk-length 3, number of steps 2. We use the hyper-parameter setting for experiments in <ref type="table" target="#tab_4">Table 3</ref> (the fixed node setting) and <ref type="figure" target="#fig_2">Figure 4</ref>. The parameter setting ensures to traverse over all the nodes in the graph in one epoch of training. For fixed message setting in 3, we alter the batch size of SAGE-NS to 35K, ClusterGCN to 60, GraphSAINT-RW to 60K. Here the batch size for ClusterGCN is small because each batch item means a cluster group of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Ablation Studies and More Experiments</head><p>Experiments on the Flickr benchmark We conduct another set of performance comparison experiments on the Flickr node classification benchmark in <ref type="table" target="#tab_10">Table 7</ref>, whose information and statistics are listed in <ref type="table" target="#tab_9">Table 6</ref>. As in Section 6, we can draw a similar conclusion that VQ-GNN shows more robust performance than the three scalable baselines. Ablation studies We also conduct several ablation experiments on the ogbn-arxiv benchmark with GCN backbone. Please note that if not mentioned otherwise, the hyper-parameter setups will follow the corresponding model and are described in Appendix F.</p><p>? Performance vs. the number of layers: We vary the number of layers of GCN backbone from one to five, and the performance of VQ-GNN on ogbn-arxiv is shown in the following table. We see that a two-layer GCN is sufficient to achieve good performance on ogbn-arxiv while using only one layer harms the performance. Stacking more layers will not bring any performance gain.</p><p># of Layers 1 2 3 4 5</p><p>Accuracy 0.6599 0.7006 0.7055 0.7080 0.7037</p><p>? Performance vs. codebook size: We vary the codebook size from 64 to 4096 and fix the minibatch size at 40K. Performance is shown as follows, where we can see the performance is not sensitive to the codebook size. Setting the codebook size to 64 is enough to achieve relatively good performance on ogbn-arxiv. This may indicate that the node feature distribution of ogbn-arxiv is sparse, which is reasonable as the 128-dimensional node features are obtained by averaging the embedding of words in the titles and abstracts of arXiv papers <ref type="bibr">[5]</ref>.</p><p>Codebook Size 64 256 1024 4096</p><p>Accuracy 0.6950 0.7011 0.7030 0.7049</p><p>? Performance vs. mini-batch size: We vary the mini-batch size from 10K to 80K, and the performance is shown as follows. We see that smaller mini-batch sizes can slightly lower the overall performance. When the mini-batch size is smaller, more messages come from out-ofmini-batch nodes (see <ref type="figure">Fig. 1</ref>), and they are approximated by the VQ codewords. This increased number of approximated messages can harm the performance. However, generally speaking, the performance is not sensitive to the mini-batch size as long as it is not small.</p><p>Mini-batch Size 10K 20K 40K 60K 80K Accuracy 0.6843 0.6920 0.7011 0.7055 0.7061</p><p>? Performance vs. mini-batch sampling strategy: We compare three different sampling strategies:</p><p>(1) randomly sampling nodes, (2) randomly sampling edges, and (3) sampling using random walks as in GraphSAINT-RW. As shown in the following table, we did not observe an obvious difference in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-batch Sampling Techniques Sampling Nodes Sampling Edges Sampling using Random Walks</head><p>Accuracy 0.7020 0.7034 0.7023</p><p>Performance of VQ-GNN with Graph Transformers <ref type="bibr" target="#b14">[15]</ref>. Our algorithm enables graph transformer architectures to compute global attention on large graphs. For each layer, we input hidden features into VQ-GNN, Graph Transformer, and Linear modules separately and sum the output features of each module together. In this way, the holistic model can not only leverage global attention but can also absorb local information. The transformer module is adapted from <ref type="bibr" target="#b14">[15]</ref>. We show the performance in <ref type="table" target="#tab_11">Table 8</ref>. We find that the removal of the batch norm will mitigate the problem of overfitting, so our model does not involve batch norm. Benchmark ogbn-arxiv (Acc.?std.)</p><p>Global Attention + GAT <ref type="bibr" target="#b14">[15]</ref> .7108 ? .0055</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) as defined above and C (l,s) out = C (l,s) out R. Here, C (l,s) out is the remaining part of the convolution matrix after removing the intra-mini-batch messages, thus (C (l,s) out ) :,j = (C (l,s) B ) :,j 1{j ? i b } for any j ? {1, . . . , n}, and C (l,s) is the sketch of C (l,s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Convergence curves (validation accuracy vs. training time). Mini-batch size and learning rate are kept the same. Tested for GCN and SAGE-Mean on the ogbn-arxiv benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>R f l ,d k,l are weight matrices which compute the queries and keys vectors. In contrast to GAT, all entries of C (l,s) i,j are non-zero. Different design of Graph Transformers [15-17] use graph adjacency information in different ways, and is not characterized here, see the original papers for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of GNNs re-formulated as generalized graph convolution.</figDesc><table><row><cell>Model Name</cell><cell>Design Idea</cell><cell cols="2">Conv. Matrix Type # of Conv. Convolution Matrix</cell></row><row><cell>GCN 1 [1]</cell><cell>Spatial Conv.</cell><cell>Fixed</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>update it for each iteration, which takes an extra O(Lkf ) memory and O(Lnkf ) training time per epoch, where L and f are the numbers of layers and (hidden) features of the GNN respectively. We can effectively preserve all messages related to a mini-batch while randomly sampling nodes from the graph. The number of intra-mini-batch messages is O(b 2 d/n) when the nodes are sampled randomly. Thus we only need to pass O(b 2 d/n + bk) messages per iteration and O(bd + nk) per epoch. In practice, when combined with techniques including product VQ and implicit whitening (see Appendix E), we can further improve the stability and performance of VQ-GNN. These theoretical and experimental analyses justify the efficiency of the proposed VQ-GNN framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Memory and time complexities of sampling-based methods and our approach; see Section 5 for details.</figDesc><table><row><cell>Scalable Method</cell><cell>Memory Usage</cell><cell>Pre-computation Time</cell><cell>Training Time</cell><cell>Inference Time</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Peak memory usage. Evaluated when fixing the number of gradient-descended nodes or the number of messages passed per mini-batch to be the same. Tested for GCN and SAGE-Mean on the ogbn-arxiv benchmark.</figDesc><table><row><cell>Fixed</cell><cell cols="2">85K nodes per batch</cell><cell cols="2">1.5M messages passed per batch</cell></row><row><cell>GNN Model</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GCN</cell><cell>SAGE-Mean</cell></row><row><cell>NS-SAGE</cell><cell>-</cell><cell>1140.3 MB</cell><cell>-</cell><cell>953.7 MB</cell></row><row><cell>Cluster-GCN</cell><cell>501.5 MB</cell><cell>514.1 MB</cell><cell>757.4 MB</cell><cell>769.3 MB</cell></row><row><cell cols="2">GraphSAINT-RW 526.5 MB</cell><cell>519.2 MB</cell><cell>661.6 MB</cell><cell>650.4 MB</cell></row><row><cell cols="2">VQ-GNN (Ours) 758.0 MB</cell><cell>801.8 MB</cell><cell>485.5 MB</cell><cell>508.5 MB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between sampling-based baselines and our approach, VQ-GNN. .9615 ? .0089 .9426 ? .0043 Cluster-GCN .6805 ? .0074 .6976 ? .0049 .6960 ? .0062 .9264 ? .0034 .9456 ? .0061 .9380 ? .0055 GraphSAINT-RW .7079 ? .0057 .6987 ? .0039 .7117 ? .0032 .9225 ? .0057 .9581 ? .0074 .9431 ? .0067 VQ-GNN (Ours) .7055 ? .0033 .7028 ? .0047 .7043 ? .0034 .9399 ? .0021 .9449 ? .0024 .9438 ? .0059 .0039 .9358 ? .0046 .9722 ? .0035 .4475 ? .0107 .4810 ? .0081 .4048 ? .0125 .0066 .8810 ? .0091 .9051 ? .0077 .4068 ? .0096 .3486 ? .0216 .3905 ? .0152 GraphSAINT-RW .9110 ? .0057 .9382 ? .0074 .9612 ? .0042 .4368 ? .0169 .3359 ? .0128 .3489 ? .0114 VQ-GNN (Ours) .9549 ? .0058 .9578 ? .0019 .9737 ? .0033 .4316 ? .0134 .4673 ? .0164 .4102 ? .0099</figDesc><table><row><cell>Task</cell><cell cols="3">Node Classification (Transductive)</cell><cell cols="3">Node Classification (Transductive)</cell></row><row><cell>Benchmark</cell><cell></cell><cell>ogbn-arxiv (Acc.?std.)</cell><cell></cell><cell></cell><cell>Reddit (Acc.?std.)</cell><cell></cell></row><row><cell>GNN Model</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GAT</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GAT</cell></row><row><cell>"Full-Graph"</cell><cell cols="3">.7029 ? .0036 .6982 ? .0038 .7097 ? .0035</cell><cell>OOM 2</cell><cell>OOM 2</cell><cell>OOM 2</cell></row><row><cell>NS-SAGE.</cell><cell>NA 1</cell><cell cols="2">.7094 ? .0060 .7123 ? .0044</cell><cell>NA 1</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="3">Node Classification (Inductive)</cell><cell cols="3">Link Prediction (Transductive)</cell></row><row><cell>Benchmark</cell><cell></cell><cell>PPI (F1-score 3 ?std.)</cell><cell></cell><cell cols="2">ogbl-collab (Hits@50?std.)</cell><cell></cell></row><row><cell>GNN Model</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GAT</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GAT</cell></row><row><cell cols="2">"Full-Graph" .9173 ? NS-SAGE. NA 1</cell><cell cols="2">.9121 ? .0033 .9407 ? .0025</cell><cell>NA 1</cell><cell cols="2">.4776 ? .0041 .3499 ? .0142</cell></row><row><cell>Cluster-GCN</cell><cell>.8852 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1 NS-SAGE sampling method is not compatible with the GCN backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Summary of more GNNs re-formulated as generalized graph convolution.</figDesc><table><row><cell>Model Name</cell><cell>Design Idea</cell><cell>Conv. Matrix Type</cell><cell># of Conv.</cell><cell>Convolution Matrix</cell></row><row><cell>GIN 1 [4]</cell><cell>WL-Test</cell><cell>Fixed + Learnable</cell><cell>2</cell><cell>C (1) = A C (2) = I n and h (2) (l) = 1 + (l)</cell></row><row><cell>ChebNet 2 [12]</cell><cell>Spectral Conv.</cell><cell>Learnable</cell><cell>order of poly.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Information and statistics of the benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell>ogbn-arxiv</cell><cell>Reddit</cell><cell>PPI</cell><cell cols="2">ogbl-collab Flickr</cell></row><row><cell>Task</cell><cell>node</cell><cell>node</cell><cell>node</cell><cell>link</cell><cell>node</cell></row><row><cell>Setting</cell><cell cols="5">transductive transductive inductive transductive transductive</cell></row><row><cell>Label</cell><cell>single</cell><cell>single</cell><cell>multiple</cell><cell>single</cell><cell>single</cell></row><row><cell>Metric</cell><cell>accuracy</cell><cell>accuracy</cell><cell cols="2">F1-score hits@50</cell><cell>accuracy</cell></row><row><cell># of Nodes</cell><cell>169,343</cell><cell>232,965</cell><cell>56,944</cell><cell>235,868</cell><cell>89,250</cell></row><row><cell># of Edges</cell><cell>1,166,243</cell><cell>11,606,919</cell><cell>793,632</cell><cell>1,285,465</cell><cell>449,878</cell></row><row><cell cols="2"># of Features 128</cell><cell>602</cell><cell>50</cell><cell>128</cell><cell>500</cell></row><row><cell># of Classes</cell><cell>40</cell><cell>41</cell><cell>121</cell><cell>(2)</cell><cell>7</cell></row><row><cell>Label Rate</cell><cell>54.00%</cell><cell>65.86%</cell><cell>78.86%</cell><cell>92.00%</cell><cell>50.00%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison between sampling-based baselines and our approach, VQ-GNN, on the Flickr benchmark. .5209 ? .0053 .5177 ? .0041 .5156 ? .0067NS-SAGE.NA .5165 ? .0077 .5282 ? .0052 Cluster-GCN .4976 ? .0078 .4996 ? .0045 .4907 ? .0107 GraphSAINT-RW .5239 ? .0071 .5040 ? .0046 .5163 ? .0062 VQ-GNN (Ours) .5315 ? .0031 .5323 ? .0083 .5288 ? .0054</figDesc><table><row><cell>Task</cell><cell cols="3">Node Classification (Transductive)</cell></row><row><cell>Benchmark</cell><cell></cell><cell>Flickr (Acc.?std.)</cell><cell></cell></row><row><cell>GNN Model</cell><cell>GCN</cell><cell>SAGE-Mean</cell><cell>GAT</cell></row><row><cell>"Full-Graph"</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performance of VQ-GNN with Graph Transformer Backbone on the ogbn-arxiv benchmark.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We call the neighbor sampling method in<ref type="bibr" target="#b1">[2]</ref> NS-SAGE and the GNN model in the same paper SAGE-Mean to avoid ambiguity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Goldstein, Kong, and Chen were supported by the Office of Naval Research, AFOSR MURI program, the DARPA Young Faculty Award, and the National Science Foundation Division of Mathematical Sciences. Additional support was provided by Capital One Bank and JP Morgan Chase. Huang and Ding were supported by a startup fund from the Department of Computer Science of the University of Maryland, National Science Foundation IIS-1850220 CRII Award 030742-00001, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD), Air Force Material Command, and Adobe, Capital One and JP Morgan faculty fellowships. Li and Dickerson were supported in part by NSF CAREER Award IIS-1846237, NSF D-ISN Award #2039862, NSF Award CCF-1852352, NIH R01 Award NLM-013039-01, NIST MSE Award #20126334, DARPA GARD #HR00112020007, DoD WHS Award #HQ003420F0035, ARPA-E Award #4334192 and a Google Faculty Research Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , <ref type="bibr">[No]</ref> , or [N/A] . You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><p>? Did you include the license to the code and datasets? <ref type="bibr">[</ref> Algorithm 1 VQ-GNN: our proposed universal framework to scale most state-of-the-art Graph Neural Networks to large graphs using Vector Quantization. For ease of presentation, we assume the GNN has only one fixed convolution matrix.</p><p>Require: Input node features X, ground-truth labels Y Require: GNN's convolution matrix C (fixed convolution) or C and h(?, ?) (learnable convolution)</p><p>Randomly initialize GNN learnable parameters W (l) and ? (l) and codewords</p><p>Initialize codeword assignment R (l) according to the initial codewords Update learnable parameters W (l) using the estimated gradients ? W (l) Product VQ and VQ-update rule. As described in Section 4, we basically follow the exponential moving average (EMA) update rule for codewords as proposed in <ref type="bibr" target="#b28">[29]</ref>. In addition, we propose two further improvements:</p><p>1. Product VQ: we divide the 2f -dimensional features concatenated with gradients into several f prod -dimensional blocks. And apply VQ to each of the blocks independently in parallel. 2. Implicit whitening: we whitening transform the input features and gradients before using them for VQ update, exponentially smooth the mini-batch mean and variance of inputs, and inversely transform the learned codewords using the smoothed mean and variance.</p><p>In practice, the product VQ technique is generally required for VQ-GNN to achieve competitive performance across benchmarks and using different GNN backbones. Whitening and Lipschitz regularization are tricks that are helpful in some cases (for example, Lipschitz regularization is only helpful when training GATs). The three techniques are not introduced by us and are already used by some existing work related to VQ but outside of the graph learning community. For example, product VQ is used in <ref type="bibr" target="#b44">[45]</ref>, and whitening is used in <ref type="bibr" target="#b45">[46]</ref>.</p><p>The complete pseudo-code of VQ-update is Algorithm 2. It is important to note that in the experiments, we observed that implicit whitening helps stabilize VQ and makes it more robust across different GNN models and graph datasets. However, we observed that the smoothing of mini-batch mean and variance of gradients (which is used by the approximated backward message passing) is not</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07323</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omripuny</forename><surname>Heliben-Hamu Yaronlipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<title level="m">Global attention improves graph networks generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
		<title level="m">What can neural networks reason about? In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renton</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparser johnson-lindenstrauss transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelani</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neuhoff</surname></persName>
		</author>
		<title level="m">Quantization. IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2325" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Degree-quant: Quantization-aware training for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas D</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05000</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sgquant: Squeezing the last bit on graph neural networks with specialized quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1044" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is attention better than matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning product codebooks using vector-quantized autoencoders for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Flierl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07543</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Lipschitz normalization for self-attention layers with application to graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04886</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05609</idno>
		<title level="m">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2021. 3. If you ran experiments..</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?</title>
		<imprint/>
	</monogr>
	<note>Yes</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?</title>
		<imprint/>
	</monogr>
	<note>Yes</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">code, data, models) or curating/releasing new assets... (a) If your work uses existing assets</title>
		<imprint/>
	</monogr>
	<note>If you are using existing assets (e.g.. did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you&apos;re using/curating? [N/A</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">This practical incompatibility is solved by using RMSprop instead of Adam Algorithm 2 VQ-Update: our proposed algorithm to update VQ codewords and assignment using exponential moving average (EMA) estimates of codewords with implicit whitening of inputs. Require: Input mini-batch vectors V ? R b?fprod as a part of the 2f -dim feature and gradients Require: Exponential decay rate ? for momentum estimates of codewords Require: Exponential decay rate ? for momentum estimates of mini-batch mean and variance Require: Codewords V ? R</title>
	</analytic>
	<monogr>
		<title level="m">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable</title>
		<meeting><address><addrLine>Adam</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] compatible with some optimization algorithms which consider the cumulative history of gradients. k?fprod before update Require: Smoothed mean E[V ] and variance Var[V ] before update</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
