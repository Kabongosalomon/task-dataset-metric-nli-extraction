<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera Distortion-aware 3D Human Pose Estimation in Video with Optimization-based Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyel</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yooshin</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemyung</forename><surname>Yu</surname></persName>
							<email>jaemyung@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Camera Distortion-aware 3D Human Pose Estimation in Video with Optimization-based Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing 3D human pose estimation algorithms trained on distortion-free datasets suffer performance drop when applied to new scenarios with a specific camera distortion. In this paper, we propose a simple yet effective model for 3D human pose estimation in video that can quickly adapt to any distortion environment by utilizing MAML, a representative optimization-based meta-learning algorithm. We consider a sequence of 2D keypoints in a particular distortion as a single task of MAML. However, due to the absence of a large-scale dataset in a distorted environment, we propose an efficient method to generate synthetic distorted data from undistorted 2D keypoints. For the evaluation, we assume two practical testing situations depending on whether a motion capture sensor is available or not. In particular, we propose Inference Stage Optimization using bone-length symmetry and consistency. Extensive evaluation shows that our proposed method successfully adapts to various degrees of distortion in the testing phase and outperforms the existing state-of-the-art approaches. The proposed method is useful in practice because it does not require camera calibration and additional computations in a testing set-up. Code is available at https://github. com/hanbyel0105/CamDistHumanPose3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation is a task that localizes 3D human body joint from an RGB input. As a fundamental task in computer vision, it is applied to many downstream applications, e.g., action recognition <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref>, human body reconstruction <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b8">9]</ref>, and human-computer interaction <ref type="bibr" target="#b5">[6]</ref>. Particularly, 3D pose estimation for monocular video, which predicts 3D joint in inputs from a singlecamera, has attracted a lot of academic interest recently <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref> because of the simplicity of the hardware setting in use and its advantage of being able to leverage temporal information to resolve inherent depth ambiguity.</p><p>Recently, many state-of-the-art studies adopted two-   <ref type="table">Table 1</ref>: Performance drop in environments with distortion of a network trained with a distortion-free dataset. Distortion 1 and Distortion 2 are the cases of barrel distortion and pincushion distortion, with tangential distortion, respectively. stage architecture to achieve higher performance. In this architecture, 2D keypoints are first extracted from the off-theshelf 2D keypoint detector <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">28]</ref>, and 3D keypoints are inferred using the predicted 2D keypoints sequence as input. These approaches simplify the 3D pose estimation problem to solve depth ambiguity from 2D joint sequences. This allows the study <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">36]</ref> of algorithms explicitly using information such as skeleton kinematics and motion of human, which showed plausible results.</p><p>Despite significant advances in 2D-keypoint-based 3D pose estimation, there still remain certain limitations. That is existing 3D human pose estimation algorithms trained on distortion-free datasets show severe performance drop when applied to new scenarios with a specific camera distortion, as shown in <ref type="figure">Figure 1</ref> and <ref type="table">Table 1</ref>. Previously, preprocessed images were used when inferring 3D joints from distorted inputs. However, it is important to make models that can adapt themselves to arbitrary distortion in the testing phase, as algorithms that are needed in preprocessing, such as camera calibration, are sometimes difficult to apply and they also introduce certain errors of their own. This is substantially important issue in the wild use of the algorithms; however, cross-scenario research on camera distortion has been out of scope due to the absence of a dataset with various degrees of camera distortion.</p><p>To overcome this limitation, in this work, we propose a simple yet effective model for 3D human pose estimation in video that can quickly adapt to any distortion environment by utilizing model-agnostic meta-learning (MAML) <ref type="bibr" target="#b6">[7]</ref>, a representative optimization-based meta-learning algorithm. We focus on training a distorted-2D-keypoints-conditioned 3D pose estimator to be able to quickly adapt to camera distortion, because we found that 2D keypoint detector is good at finding distorted 2D keypoints consistent with distorted images. Therefore, we consider a sequence of 2D keypoints in a particular distortion as a single task of MAML. However, due to the absence of a large-scale dataset with a distorted environment, we propose an efficient method to generate synthetic distorted data from undistorted 2D keypoints. Note that, the goal of training phase is not to just increase the performance at a particular distortion, but to train a network sensitive to distortion, allowing the network to adapt quickly to arbitrary distortion in the testing phase. For the testing phase, the trained network is first adapted to a specific camera distortion environment by fine-tuning or Inference Stage Optimization, which as proposed in recent work <ref type="bibr" target="#b34">[34]</ref> in the following two scenarios.</p><p>For evaluation, we assume two practical situations in which the proposed method will be used and confirm that our algorithm is useful for each case. First, Scenario 1 is a situation in which a user can collect data using motion capture sensors in front of a testing environment, as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (b). In this case, data with the same distortion as the testing environment can be obtained, but it would be in relatively small amounts compared with a large-scale dataset (e.g., Human3.6M <ref type="bibr" target="#b9">[10]</ref>) collected in the laboratory environment. Therefore, it is important to transfer knowledge trained with a large-scale dataset as much as possible. To validate the usefulness of the proposed method, we construct a small-scale dataset with the same distortion as the test environment, and evaluate whether the network can adapt well through naive fine-tuning. Second, Scenario 2 is when the user is unable to obtain data in a testing environment, as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (c). In this case, the network should be adapted to specific distortions using only test videos. In a recent study <ref type="bibr" target="#b34">[34]</ref>, the authors proposed the concept named Inference Stage Optimization (ISO) to adapt network using only test data before testing. We also utilize ISO in this case. To this end, we propose a novel ISO method based on skeleton symmetry and consistency. This might be a weak constraint, but we confirm that our network is fully adaptable even with these constraints because it has been sensitively trained on distortion.</p><p>In summary, our overall contribution is four-fold:</p><p>? To the best of our knowledge, our method, which utilized optimization-based meta-learning, is the first algorithm that can adapt to arbitrary camera distortion at the testing phase. ? We propose an efficient method to generate synthetic distorted data from undistorted 2D keypoints, enabling cross-scenario research on camera distortion, which has been out-of-scope due to the absence of datasets with distortion. ? We validate the effectiveness of the proposed method for each case, assuming two practical testing environments. In particular, we propose the ISO method using bone-length symmetry and consistency. ? Our proposed method is useful in practical applications because it does not require calibration for the testing camera and additional computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Human Pose Estimation</head><p>Since the success of 2D human pose estimation, 3D human pose estimation has been widely studied. Martinez et al. <ref type="bibr" target="#b16">[17]</ref> successfully predicted 3D poses from 2D joint locations using simple and lightweight networks. It showed better results than previous studies that involved training with raw image pixels. To make better use of 2D keypoints, GCN and attention mechanism were applied to learn the global relationship between joints <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b15">16]</ref>. In contrast, Pavllo et al. <ref type="bibr" target="#b20">[21]</ref> predicted 3D poses using video to overcome inherent ambiguity that multiple 3D poses can be mapped to the same 2D pose. Furthermore, prior knowledge about human body structure was explicitly utilized to give constraints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. Despite substantial progress in this field, the performance severely drops when camera distortion occurs due to the changes in camera parameters in test environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-scenario Pose Estimation</head><p>Deep learning models have substantially improved in recent years. However, due to the limitation of supervised learning on datasets that lack diversity, even state-of-the-art algorithms show poor results in-the-wild. To be robust on a domain gap between training and inference, many studies have been conducted <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34]</ref>. A recent study <ref type="bibr" target="#b34">[34]</ref> proposed the domain (e.g., varying poses, camera viewpoints, body size, and appearances) robust 3D pose estimation algorithm that adapts to the target domain using selfsupervised learning schemes named Inference Stage Optimization (ISO) using cycle consistency among 2D and 3D spaces. In this paper, we focus on the domain gap of camera distortion caused by the different camera settings at the testing phase, which has been out-of-scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Optimization-based Meta-Learning</head><p>There are three common categories in meta-learning. The first category is the metric-based approach <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">29]</ref>, which learns a good metric that expresses the relationship between inputs in task space and applies it well to new samples. The second category is a model-based approach <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> that controls the structure of a target model through another model called meta-learner. The last category is an optimization-based approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> that looks for sensitive initial parameters for tasks and quickly adapts to new tasks with only a few samples. In this work, we utilize MAML <ref type="bibr" target="#b6">[7]</ref>, which belongs to the optimization-based approach so that the network can adapt quickly to arbitrary camera distortion in the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>In this section, we introduce background knowledge on camera distortion and framework of the MAML algorithm.</p><p>Camera Distortion. There are two kinds of camera distortion. The first is radial distortion, which is caused by the refractive index of the convex lens. Radial distortion is determined by the distance from the center of the image, and it is usually expressed in parameters k 1 , k 2 , and k 3 . The value of k 1 determines the main form of the distortion. A negative k 1 and a positive k 1 result in barrel distortion and pincushion distortion respectively, as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b) and (c). The second is tangential distortion, which is caused by the misalignment of the camera lens and the image sensor (e.g., CCD and CMOS) during manufacturing of the camera. This can be approximated by parameters p 1 and p 2 . The p 1 and p 2 cause ladder shape distortion mainly in x-axis and yaxis respectively, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>  Model-Agnostic Meta-Learning. The stage of metalearning consists of meta-training and meta-testing. We consider a model represented by a function g ? with parameters ?, that outputs y with input as x. The objective of meta-training is to find initial transferable weights that can be adapt to new tasks. For meta-training, a batch of tasks T i is sampled from task distribution p(T ). The model is first optimized through the task-specific loss L Ti using training samples within a task (task-level training), and meta-optimization is performed using test samples (tasklevel testing). In meta-testing, the model adapts to a new task T new using only a few samples. In this study, we use MAML <ref type="bibr" target="#b6">[7]</ref>, in which input x and output y are distorted 2D keypoint trajectory and 3D joints respectively. Various distortion parameters construct task distribution, and each task corresponds to a 3D pose estimation from the distorted 2D keypoint trajectory with a particular distortion parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>The overall framework of our method is shown in <ref type="figure">Figure</ref> 3. In this section, we first propose a method for generating synthetic distorted tasks. Then, we describe two phases that constitute our method: training phase and adaptation before testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Distorted Task Generation</head><p>We found that 2D keypoint detector is good at finding distorted 2D keypoints consistent with distorted images, as shown in the top row of <ref type="figure">Figure 1</ref> because it is based on the texture of images. Thus, in the training phase, our goal is to train a 3D pose estimator conditioned on distorted 2D detection to be able to quickly adapt to various distortions by applying meta-learning. Meta-learning in our case requires tasks under varying degrees of distortion. In this section, we describe how to efficiently generate distorted tasks from undistorted videos.</p><p>Given a video clip with frame length of T , first 2D keypoints are obtained by a pretrained 2D keypoint detector (e.g., Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>). Letp t ? R J?2 denotes predicted 2D coordinates of J keypoints of the human in the frame andP = {p t } T t=1 denotes the set of joints for a video clip. Specifically,p t = {[? t,j ,b t,j ]} J j=1 where? t,j andb t,j denote x and y coordinates of jth joint at frame t, respectively. To generate synthetic distorted tasks, we apply the camera distortion model <ref type="bibr" target="#b31">[31]</ref> directly to predicted 2D keypoints. We omit subscript t and j for simplicity. As shown in <ref type="figure" target="#fig_6">Figure 4 (a)</ref>, the process of generating the task with particular distortion parameters (i.e., k 1 , k 2 , k 3 , p 1 , p 2 ) is divided into three steps. The first is obtaining normalized 2D keypoints (denoted as [? n ,b n ]) and distance between the point and image center (denoted as r). As camera distortion models should be applied on a normalized image plane, we first normalize 2D keypoints with focal length (denoted as f = [f x , f y ]) and optical center (denoted as c = [c x , c y ]) using the following equations:</p><formula xml:id="formula_0">an =? ? cx fx ,bn =b ? cy fy , r = ? 2 n +b 2 n .<label>(1)</label></formula><p>Then, we apply distortion to the normalized 2D keypoints using the following equations:</p><formula xml:id="formula_1">a n,d =?n(dr + dt) + p1r 2 ,b n,d =bn(dr + dt) + p2r 2 ,<label>(2)</label></formula><p>where intermediate variable d r and d t are obtained by dr = 1 + k1r 2 + k2r 4 + k3r 6 and dt = 2p1?n + 2p2bn respectively. Finally, distorted 2D keypoints (denoted as [? d ,b d ]) are obtained by unnormalization using following equations:</p><formula xml:id="formula_2">a d =? n,d fx + cx,b d =b n,d fy + cy.<label>(3)</label></formula><p>We apply this process to all joints J and frames T to obtain a distorted 2D trajectory (denoted asP dist ) reflecting a specific distortion. Then, we consider a pair of the distorted 2D trajectory and ground-truth 3D joints (denoted as s = [x, y, z] ? R J?3 ) as a single task T of MAML. This method is highly efficient because it does not apply distortion in image domain, and thus, we can generate numerous distortions in the training phase, as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (a). Furthermore, it can reflect the jittered outputs of the 2D keypoint detector caused by inherent ambiguity (e.g., occlusion), because it generates distorted joints from predicted 2D keypoints. Synthetic tasks can also be generated from ground-truth 3D joints, as shown in <ref type="figure" target="#fig_6">Figure 4</ref> (b). In this case, normalized 2D keypoints are obtained from the ground-truth 3D joints through projection. However, as shown in <ref type="table" target="#tab_5">Table 4</ref>, this method is less effective because it could not reflect the noisy output of the 2D keypoint detector, resulting in a domain gap during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Phase</head><p>In the training phase, we will perform meta-learning using synthetic distorted tasks from undistorted videos. Our goal in the training phase is to find sensitive initial transferable weights to camera distortion by utilizing optimizationbased meta-learning. Our algorithm mostly follows the framework of MAML, but to achieve better performance, there are two modifications: stratified sampling and random distortion pretraining.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref> (a), given predicted undistorted 2D trajectory, we generate a batch of distorted 2D trajectories (denoted as {P dist,i } N i=1 , where N represents the number of tasks in meta-batch) with sampled distortion parameters. Then, we construct each task T i by pairing a distorted 2D trajectoryP dist,i and ground-truth 3D joints s.   Specifically, parameters related to radial distortion are sampled by k 1 , k 2 , k 3 ? U[?? 1 , ? 1 ] and tangential distortion parameters are sampled by p 1 , p 2 ? U[?? 2 , ? 2 ] where ? 1 and ? 2 denote the maximum value of each distribution. We basically use the sampling method to both task-level training and task-level testing. However, for the task-level training, we adopt stratified sampling for sampling parameter k 1 , which determines the main form of distortion. In this case, a k 1 of ith sample in the meta-batch is sampled as follows:</p><formula xml:id="formula_3">k 1,i ? ?? 1 + 2 ? ? 1 ? U i ? 1 N , i N .<label>(4)</label></formula><p>By sampling the distortion parameter k 1 from evenly spaced bins, the meta-batch can consist of tasks with varying degrees of distortion. This enhances the adaptability of our network as shown in <ref type="table" target="#tab_4">Table 3</ref>. We denote the distribution of tasks generated using stratified sampling as p strat (T ), and using only uniform distribution as p rand (T ).</p><p>Finally, we consider a 3D pose estimator model represented by a parameterized function g ? with parameters ?. We perform only one gradient descent update when the parameters ? is adapted to a new task T i . Thus, the newly adapted parameters ? i are obtained by</p><formula xml:id="formula_4">? i = ? ? ?? ? L Ti (g ? ),<label>(5)</label></formula><p>where ? is the learning rate for task-level training.</p><p>The parameters ? of model are optimized by maximizing the performance of g ? i with respect to ? across tasks sampled for task-level testing. Specifically, the meta-objective is expressed as follows:</p><formula xml:id="formula_5">arg min ? Ti?p(T ) L Ti (g ? i ) = arg min ? Ti?p(T ) L Ti (g ???? ? L T i (g ? ) ).<label>(6)</label></formula><p>Finally, we perform meta-optimization by using the Eq. 6. For the stochastic gradient descent, model parameters ? are updated as follows:</p><formula xml:id="formula_6">? ? ? ? ?? ? Ti?p(T ) L Ti (g ? i ),<label>(7)</label></formula><p>where ? is the learning rate for meta-optimization. We use a loss function, MPJPE, which is the L2 distance between ground-truth 3D joints and predicted ones as a task-level objective in the entire process of meta-optimization. Additionally, we pretrain the network before training meta-learner through random distortion pretraining that regresses 3D joints from randomly distorted 2D keypoint trajectories. This allows the network to learn feature representation under various distortions and consequently enables stable MAML training. However, while random distortion pretraining helps in the stability of MAML, the pretraining without meta-learning shows poor results, as shown in <ref type="figure" target="#fig_10">Figure 7</ref>, when the network is adapted to the specific distortion, because it is not a transferable initial weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptation before Testing</head><p>When the trained model that can quickly adapt to arbitrary distortions is used, it must, first, be adapted to the specific distortion of the testing environment. This is similar to meta-testing in the MAML framework. We assume two practical situations, Scenario 1 and Scenario 2, and propose an adaptation method for each case. Scenario 1 is a situation in which a user can collect data using motion capture sensors in front of a testing environment. In this case, data with the same distortion as the testing environment can be obtained. Thus, we adopt naive fine-tuning using the MPJPE loss function to adapt the network to the specific distortion, as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (b). The collected data would be in relatively small amounts than the large-scale dataset (e.g., Human3.6M). Therefore, we will check whether the network can adapt well with small amounts of collected data. Detailed settings are provided in Section 5.1. Scenario 2 is a situation when the user is unable to obtain data in a testing environment. In this case, the network should be adapted to the specific distortion using only test videos. As shown in <ref type="figure" target="#fig_3">Figure 3</ref> (c), we adopt Inference Stage Optimization (ISO) <ref type="bibr" target="#b34">[34]</ref>, which performs self-supervised training using the test data before testing. Usually, the inferred 3D joints are orthogonally projected to 2D plane and compared with the predicted 2D keypoints to perform ISO. However, if there is distortion in the video, this method cannot be used. Therefore, we propose the novel ISO method which utilizes bone-length symmetry and bone-length consistency that allows self-supervision within the inferred 3D joint itself (details in Appendix A.2). The former constrains the length of a person's left and right bones to be equal, whereas the latter constrains each bone to be equal in length between consecutive frames within a video. The constraints based on bone-length have been used for regularization in fully-supervised training, but have never been used for ISO. Also, these methods might be a weak constraint, but our network is fully adaptable even with these constraints because it has been sensitively trained on distortion via MAML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithm</head><p>Algorithm 1 shows the entire process of Section 4.2. As shown in lines 2-8, random distortion pretraining is performed before meta-learning. Subsequently, meta-learning is performed, as shown in lines 9-17. Lines 11-14 and lines 15-16 present task-level training and meta-optimization with task-level testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation</head><p>Human3.6M <ref type="bibr" target="#b9">[10]</ref> is a large-scale dataset containing 3.6 million video frames and corresponding 2D and 3D human keypoint labels. We construct a cross-scenario on distortion to validate the effectiveness of the proposed method. For training, we use five subjects (S1, S5, S6, S7, S8) with undistorted videos, as in previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref>, since our method can generate synthetic distorted tasks from undistorted 2D keypoints. For testing, only one subject (S11) is used. Due to the absence of test videos with distortion, we generate four different kinds of distorted videos (denoted as d 1 , d 2 , d 3 , and d 4 , details in Appendix A.1) from undistorted videos of S11 by using Blender 1 software, as shown in <ref type="figure" target="#fig_8">Figure 5</ref>. We evaluate the proposed method in each kind of distortion. For Scenario 1 in adaptation before the testing phase, collected small-scale dataset with the same distortion as the test data is required. Therefore, we adopt only 1% of S9 and apply the same distortion as S11 to it.</p><p>Evaluation Metrics. We use three evaluation protocols following previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">34]</ref>. The first is mean per joint position error (MPJPE) in millimeters, the L2 distance between the predicted 3D joints and groundtruth joints. The second is P-MPJPE. This is similar to MPJPE, but calculates the error between the joints after alignment using Procrustes Analysis. The last one is percentage of correct 3D joints with a threshold as 50% of the head segment length (PCKh@0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>For 3D pose estimator, the proposed method is not about network architecture but training methods. Therefore, we adopt the state-of-the-art model for 3D human pose estimation in video proposed in the previous work <ref type="bibr" target="#b20">[21]</ref> as our base model. It is fully convolutional and based on dilated Calculate loss by MPJPE: L Tstrat,i (g ? ) <ref type="bibr" target="#b12">13</ref> Compute updated parameters:</p><formula xml:id="formula_7">? i = ? ? ?? ? L Tstrat,i (g ? ) 14 end 15</formula><p>Update ? with respect to average test loss: temporal convolutions with residual blocks. For 2D keypoint detector, we use Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> with a ResNet-101-FPN <ref type="bibr" target="#b12">[13]</ref> backbone as off-the-shelf 2D keypoint detector. We fine-tune the COCO <ref type="bibr" target="#b13">[14]</ref> pretrained model on 2D keypoints of Human3.6M. Similar to previous work <ref type="bibr" target="#b20">[21]</ref>, the 2D keypoint format of COCO differs from Human3.6M, and hence, we reinitialize the last layer of the keypoint network of the detector and carry out fine-tuning, after which the 2D keypoint detector is frozen in the entire training process for the 3D pose estimator because it is robust to camera distortion. We use Adam <ref type="bibr" target="#b10">[11]</ref> optimizer, with batch size 1024. During the training phase, we set the required ? 1 and ? 2 to 5 and 0.5 respectively, to sample the distortion parameters. The learning rate ? for task-level training is set to 0.1 and the ? for meta-optimization is set to 0.001. We use 5 for the number of samples in meta-batch. The learning rate decay is set to 0.95 and network is trained with 60 epochs.   In both Scenario 1 and Scenario 2 of adaptation before testing, learning rate is set to 0.6 and epochs for adaptation is set to 100. Note that, during the adaptation process, we train the model for 100 epochs, however since it is done on a small-scale dataset, the overall training time required for the adaptation is within a few minutes.</p><formula xml:id="formula_8">16 ? ? ? ? ?? ? T rand,i ?p rand (T ) L T rand,i (g ? i ) 17 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment Results</head><p>In this section, we validate effectiveness of the proposed method. We evaluate performance on four different kinds of distortion in all experiments and report the average performance of d 1 and d 2 which applied heavy distortion and the average performance of d 3 and d 4 which applied moderate distortion. All reported values are performance after adaptation to the specific distortion. For Scenario 1, the trained network is adapted by fine-tuning on 1% of S9, which went through the same distortion as S11. For Scenario 2, the network is adapted by ISO on 0.1% of test videos (S11). Comparison with State-of-the-Art. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of existing 3D pose estimation algorithms and our method. The baseline models do not take cross-scenario about distortion into account. However, for fair evaluation, we evaluate the performance in Scenario 1 after fine-tuning on the small-scale dataset, just like our method. For Scenario 2, we did not apply ISO to baseline models because they show poor performance when ISO is applied.</p><p>The proposed method outperforms other methods re-gardless of the kinds of distortions and scenarios. Specifically, compared to our base model <ref type="bibr" target="#b20">[21]</ref>, the proposed method shows -14.64mm, -10.35mm, and +17.7% average performance improvement in Scenario 1 for each metric (i.e., MPJPE, P-MPJPE, and PCKh@0.5) and -30.45mm, -15.55mm, and +18.75% average performance improvement in Scenario 2. Especially in Scenario 2, our method rather shows better performance than Scenario 1 under moderate distortion. This demonstrates that our bone-length-based ISO method is effective and also that the trained model has transferable initial weights. <ref type="figure" target="#fig_9">Figure 6</ref> shows qualitative results of the estimated 3D pose from distorted videos. Unlike others, our method successfully adapts to the distortion that the test video has, and consequently, we can see that estimated 3D joints from the distorted video by using the proposed method is almost the same with ground-truth joints. Results predicted from videos with more diverse poses and distortion can be seen in the supplementary material (Appendix A.4). Ablation Studies. We first look at the contribution of each of the proposed method. We evaluate the performance changes, adding each proposed method with Pavllo et al. <ref type="bibr" target="#b20">[21]</ref> as our base model. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we can notice that each method provides a positive contribution under all metrics. In particular, the significant improvement comes from utilizing MAML using synthetic distorted tasks and learning rich feature representation on distortion through MPJPE(?) P-MPJPE(?) PCKh@0.5(?) base model <ref type="bibr" target="#b20">[21]</ref> 84.   random distortion pretraining. <ref type="table" target="#tab_5">Table 4</ref> shows the performance when applying each of the two methods that generate synthetic distorted 2D keypoints based on frame length of 27. Predicted 2D keypoints denotes the case in which synthetic distorted 2D keypoints are generated using noisy results inferred from the 2D keypoint detector, as shown in <ref type="figure" target="#fig_6">Figure 4</ref> (a) and Ground-truth 3D keypoints denotes the case in which synthetic keypoints are generated using the ground-truth 3D keypoints as shown in <ref type="figure" target="#fig_6">Figure 4 (b)</ref>. We can notice that the former method shows better performance under all metrics and scenarios since there is less domain gap between training and testing. <ref type="table" target="#tab_7">Table 5</ref> reports the performance and complexity of the model (i.e., parameters and FLOPs) with respect to different input frame lengths. Our method uses the same model structure as Pavllo et al. <ref type="bibr" target="#b20">[21]</ref> because our work is about the learning method rather than the structure of the model. When the input frame length is 3, the proposed method shows comparable performance to the base model, even though the capacity is one-fifty of the base model. Furthermore, our method with an input frame length of 27 outperforms the base model of the same size significantly. Moreover, our method has the same floating-point operations (FLOPs) for inference as the base model <ref type="bibr" target="#b20">[21]</ref>, thus no additional computational cost is required compared to the base model when testing after adaptation to the test environment.</p><p>Performance Changes during Adaptation. We also validate the ability of the model, trained in the training phase, to adapt well to the specific camera distortion. In this experiment, we observe the performance changes of the model with and without MAML during the adaptation process based on frame length 27. As shown in <ref type="figure" target="#fig_10">Figure 7</ref>, in the case of a model using MAML, we can notice that it adapts well regardless of the degrees of distortion and scenarios.   Also, the mean and standard deviation of MPJPE are 6.5mm (10%) and 2.3mm (25%) lower than those of w/o MAML (at epoch 0), respectively. In contrast, in the case of the model not using MAML, the model is not stably adapted, and its performance is rather significantly degraded. Specifically, it performs well at epoch 0 with the effect of the proposed random distortion pretraining, however since it is not a transferable initial weight, it is highly degraded when the adaptation process starts. This demonstrates the superior potential of MAML to adapt to various distortion environments. Note that, as mentioned in Section 5.2, training time required for the adaptation process is within a few minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced a model for 3D human pose estimation that can adapt quickly to arbitrary camera distortion. Our model finds initial transferable weights that are sensitive to distortion through meta-learning. For this, we overcome the limitations of the absence of publically available distorted data by generating synthetic distorted tasks from undistorted data. Furthermore, we propose a novel ISO method based on bone-length that can adapt the model to the test environment without 3D joint labels. Our method is expected to be very useful in practice because once trained, it can adapt to any distortion without camera calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary material A.1. About Distortion Parameters</head><p>Our work targets two main types of in-the-wild situations. The first is the distortion that occurs in cameras for special purposes, such as fisheye and wide-angle cameras (e.g., insta360), and the second is the distortion that occurs in low-cost cameras such as surveillance cameras. We defined the former and latter as "heavy" and "moderate" (equivalent to "light") distortion, respectively. Since there is no common benchmark and public dataset with such level of distortions, we randomly selected two sets of parameters (i.e., k 1 , k 2 , k 3 , p 1 , p 2 ) that well reflect real-world situations at each distortion, and we synthesized videos and used them for evaluation. The d 1 , d 2 , d 3 , and d 4 have distortion parameters of (?4.142, ?4.956, ?0.062, -0.488, -0.712), (?2.071, ?2.478, ?0.031, -0.010, -0.014), respectively. The original distortion present in H3.6M is (-0.207, 0.248, -0.003, -0.001, -0.001), which is almost identical to no distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Bone-Length based ISO</head><p>Given a video clip with frame length of T , predicted 3D jointsS = {s t } T t=1 ? R T ?J?3 wheres t ? R J?3 represents the predicted 3D joints at frame t can be obtained. Then, we can get the predicted bone-lengths (denoted as l = {l t,j } T t=1 ? R T ?(J?1) wherel t,j denotes the predicted length of jth bone at frame t) from the predicted 3D joints by calculating the distance between adjacent joints. Finally, we can calculated the bone-length symmetry loss as follows:</p><p>L symmetry = T t=1 (j l ,jr)?P l t,j l ?l t,jr ,</p><p>where P contains all the pair of bones that are symmetrical to the left and right (denoted as j l and j r , respectively). Also, the bone-length consistency loss is obtained by:</p><formula xml:id="formula_10">L consistency = T ?1 t=1 J?1 j=1 l t+1,j ?l t,j .<label>(9)</label></formula><p>Thus, our final objective for the Inference Stage Optimization in Scenario 2 is as follows:</p><formula xml:id="formula_11">L ISO = L symmetry + L consistency .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Quantitative Results</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we provided the average performance on each of the heavy distortion and moderate distortion. <ref type="table">Table 6</ref> shows the performance at each distortion (i.e., d 1 , d 2 , d 3 , and d 4 ). In addition, <ref type="table">Table 7</ref> shows the reconstruction accuracy (PCKh@0.5) for each action. The reported accuracy here are the average value for all kinds of distortions. We can notice that our method outperforms other methods regardless of the kinds of distortions and actions.  A.4. Qualitative Results <ref type="figure" target="#fig_12">Figure 8</ref> shows qualitative results from videos with more diverse poses and distortions. We can notice that our method adapts better to the distorted environments than our base model <ref type="bibr" target="#b20">[21]</ref>, showing more similar results to the ground-truth 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Performance in Undistorted Environments</head><p>Since our model is trained to be sensitive to all kinds of distortions, it performs well even in undistorted environments. Our method shows an MPJPE of 50.6mm in the test environment with no distortion. This is 2.1mm higher than the base model <ref type="bibr" target="#b20">[21]</ref>, but it is a reasonable trade-off because it has great advantages in other situations with distortions. <ref type="table">Table 7</ref>: Comparison with other state-of-the-art models on Human3.6M. The top two rows <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36]</ref> are based on a singleframe and others <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>, including our method, are based on video with a frame length of 27. The reported performance is the average value for all kinds of distortions. Higher is better, best in bold, second-best underlined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 :</head><label>21</label><figDesc>3D reconstruction for videos with varying degrees of distortion using a network trained with a distortion-free dataset. Top: input video frames with 2D pose overlay. Bottom: 3D reconstruction. 3D reconstruction of (a) is predicted from undistorted video, and (b) and (c) are predicted from video with different degrees of distortion, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d)  and (e). Both distortions are common in commercial cameras, and radial distortion is particularly severe in wide-angle cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Tangential x (e) Tangential y Types of camera distortion. (b) and (c) represent radial distortion, and (d) and (e) represent tangential distortion. Radial distortion and tangential distortion can occur simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?Figure 3 :</head><label>3</label><figDesc>Fine-tuning using small-scale dataset with same distortion as test environment ? Transfer knowledge trained with large-scale dataset as much as possibleSelf-supervised Loss ? Self-supervised training on the test video (Inference Stage Optimization) ? Bone-length symmetry &amp; bone-length consistency lossPredicted distorted 2D trajectory Overall framework of our methods. (a) We train a 2D-keypoint-conditioned 3D pose estimator that can quickly adapt to any distortions using only an undistorted large-scale dataset. Before the trained network can be used in practice, it must be adapted to a certain distortion. (b) and (c) represent adaptation method for Scenario 1 and Scenario 2, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Generating distorted 2D keypoints from predicted ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Generating distorted 2D keypoints from 3D ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Methods to generate distorted 2D keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 6 not done do 10 Sample</head><label>610</label><figDesc>https://www.blender.org/ Algorithm 1: Training Phase Input: D: a large-scale 3D human pose dataset Input: ?, ?: learning rate hyperparameters Output: Model parameters ? 1 Randomly initialize ? 2 while not done do 3 Sample batch of tasks T rand,i ? p rand (T ) 4 for all T rand,i do 5 Calculate loss by MPJPE: L T rand,i (g ? ) Compute updated parameters: ? = ? ? ?? ? L T rand,i (g ? ) batch of tasks T strat,i ? p strat (T ) 11 for all T strat,i do 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Videos rendered with different kinds of distortion. B, P, and T represent barrel, pincushion, and tangential distortion respectively. For (a) and (b) heavy distortion is applied, and moderate distortion is applied to (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on heavily distorted videos of Human3.6M. The five columns from the leftmost are the result under the Scenario 1 setting, while the rest columns are the result under the Scenario2 setting. Top row: 3D reconstruction results on d 1 . Bottom row: 3D reconstruction results on d 2 . More results can be seen in Appendix A.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Performance changes during adaptation to the specific distortion. S1 and S2 denote Scenario 1 and Scenario 2, respectively. A solid line w/ MAML denotes our final model trained using all the elements proposed in Section 4.2, and a dashed line w/o MAML denotes a model trained only with random distortion pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Qualitative results under the Scenario 1 setting. GT Pavllo et al. Ours Undistorted video Distorted video (b) Qualitative results under the Scenario 2 setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on heavily distorted videos of Human3.6M under the Scenario 1 and Scenario 2 setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of average performance on (heavy) / (moderate) with other state-of-the-art models. The top two rows<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36]</ref> are based on a single-frame and others<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>, including our method, are based on a video with a frame length of 27. Best in bold, second-best underlined. More results can be seen in the supplementary material (Appendix A.3).</figDesc><table><row><cell>Undistorted video</cell><cell>GT</cell><cell>Distorted video</cell><cell>Pavllo et al.</cell><cell>Ours</cell><cell>Undistorted video</cell><cell>GT</cell><cell>Distorted video</cell><cell>Pavllo et al.</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness of each proposed method based on input frame length of 9 under Scenario 1 setting. Each value denotes performance on (distortion d 1 ) / (distortion d 2 ).</figDesc><table><row><cell>Method</cell><cell>MPJPE(?)</cell><cell cols="2">P-MPJPE(?) PCKh@0.5(?)</cell></row><row><cell cols="2">Predicted 2D keypoints 62.0 / 53.6</cell><cell>46.4 / 40.6</cell><cell>78.4 / 83.3</cell></row><row><cell>Ground-truth 3D joints</cell><cell>64.7 / 56.1</cell><cell>48.2 / 42.0</cell><cell>77.0 / 82.0</cell></row><row><cell cols="2">Predicted 2D keypoints 66.1 / 51.6</cell><cell>47.8 / 39.2</cell><cell>76.3 / 85.7</cell></row><row><cell>Ground-truth 3D joints</cell><cell>71.3 / 55.6</cell><cell>51.9 / 42.6</cell><cell>72.8 / 83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison of average performance on (heavy)</cell></row><row><cell>/ (moderate) between the methods generating synthetic 2D</cell></row><row><cell>keypoints. Top rows: Scenario 1. Bottom rows: Scenario 2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance and computational complexity of various models under Scenario 1. The reported performance is the average value for all kinds of distortions.</figDesc><table><row><cell></cell><cell>130</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>120</cell><cell></cell><cell>80</cell><cell></cell><cell>80</cell></row><row><cell></cell><cell>110</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPJPE (mm)</cell><cell>80 90 100</cell><cell>P-MPJPE (mm)</cell><cell>60 70</cell><cell>PCKh@0.5 (%)</cell><cell>60 70</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell>50</cell><cell></cell><cell>50</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell>40</cell><cell></cell><cell>40</cell></row><row><cell></cell><cell cols="2">0 1 10 100 epoch</cell><cell cols="2">0 1 10 100 epoch</cell><cell>0 1 10 100 epoch</cell></row><row><cell cols="2">S1 Heavy w/ MAML S1 Heavy w/o MAML</cell><cell cols="2">S1 Moderate w/ MAML S1 Moderate w/o MAML</cell><cell>S2 Heavy w/ MAML S2 Heavy w/o MAML</cell><cell>S2 Moderate w/ MAML S2 Moderate w/o MAML</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 3</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3988" to="3996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Introduction to Cyberpsychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grinne</forename><surname>Kirwan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Routledge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human motion generation via cross-space constrained sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="757" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Sen-ching Cheung, and Vijayan Asari</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">zero-shot&quot; super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">U V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<editor>I. Guyon,</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metatransfer learning for zero-shot super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3516" to="3525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new calibration model of camera lens distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanhuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="607" to="615" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structureconstrained motion sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1799" to="1812" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">Method MPJPE(?) P-MPJPE(?) PCKh@0.5(?) MPJPE(?) P-MPJPE(?) PCKh@0.5(?)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">Method MPJPE(?) P-MPJPE(?) PCKh@0.5(?) MPJPE(?)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-Mpjpe</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>?) PCKh@0.5(?)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Comparison with other state-of-the-art models on Human3.6M. The top two rows</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>21, 4, 16. including our method. Best in bold, second-best underlined</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Eat Greet Phone Photo Pose Purch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Disc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Eat Greet Phone Photo Pose Purch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Disc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
