<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi Receptive Field Network for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country>China Beijing</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelu</forename><surname>Deng</surname></persName>
							<email>zelu.deng@stu.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country>China Beijing</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country>China Beijing</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
							<email>luozhenbo@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country>China Beijing</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi Receptive Field Network for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is one of the key tasks in computer vision, which is to assign a category label to each pixel in an image. Despite significant progress achieved recently, most existing methods still suffer from two challenging issues: 1) the size of objects and stuff in an image can be very diverse, demanding for incorporating multi-scale features into the fully convolutional networks (FCNs); 2) the pixels close to or at the boundaries of object/stuff are hard to classify due to the intrinsic weakness of convolutional networks. To address the first issue, we propose a new Multi-Receptive Field Module (MRFM), explicitly taking multi-scale features into account. For the second issue, we design an edge-aware loss which is effective in distinguishing the boundaries of object/stuff. With these two designs, our Multi Receptive Field Network achieves new state-of-the-art results on two widely-used semantic segmentation benchmark datasets. Specifically, we achieve a mean IoU of 83.0% on the Cityscapes dataset and 88.4%</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of semantic segmentation is one of the key technologies in visual understanding, which is widely used in object parsing, scene parsing, human body parsing and automatic driving, etc. The task is to predict each pixel in the image into a prescribed set of categories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>, which is a dense per-pixel prediction task. In recent years, compared with systems relying on hand-crafted features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>, semantic segmentation methods based on deep FCN <ref type="bibr" target="#b24">[25]</ref> have made tremendous progress, and these methods have achieved very impressive results on semantic segmentation benchmarks. Despite the success, FCNs still have two limitations.</p><p>First, the sizes of objects/stuff in images can be very diverse, making accurate prediction very challenging. In order to solve this problem, it is necessary to capture multiscale features into FCNs. It is well known that multi-scale (a) Image (b) Ground Truth <ref type="figure">Figure 1</ref>. Illustration of challenging scenes in the Cityscpaes dataset <ref type="bibr" target="#b8">[9]</ref>. Various scales and indistinguishable boundaries of objects/stuff makes it challenging to accurately parse each pixel.</p><p>features benefit many computer vision tasks such as image classification, object detection and semantic segmentation. A straightforward approach is to resize the input image to different scales and input each copy to a shareparameter network. At some stage of the network, convolutional features are fused by concatenation or summation <ref type="bibr" target="#b21">[22]</ref>. The second approach is to use skip connections to fuse features from different layers as introduced in ResNet <ref type="bibr" target="#b14">[15]</ref>. DeeplabV3 <ref type="bibr" target="#b5">[6]</ref> proposes the ASPP module, which applies several parallel atrous convolutions to obtain multiscale receptive fields. The PSPNet <ref type="bibr" target="#b40">[41]</ref> method proposes a PSP module, which applies multiple pooling with different sizes to obtain multi-scale features. These two methods are widely used and represent the current state-of-the-art methods. However, previous works haven't focused on how to acquire multi-scale receptive field in the backbone. Here we propose a new multi-receptive field module (MRFM). Unlike previous works, we introduce the multi-receptive field module by re-designing the backbone work. Meanwhile, considering the trade-off between speed and performance, we also propose a light-weight version of MRFM, which does not introduce computation overhead for inference. The second problem is that it is difficult to distinguish the boundaries of objects/stuff. Recently, some works pay extra attention to the classification of pixels at edges, achieving improved results. DFN <ref type="bibr" target="#b36">[37]</ref> combines the edge detection task with the semantic segmentation task. By observing the fact that edge pixels tend to be incorrectly classified, we design a new edge aware loss. The main idea is that during the training, a pixel close to or on the edge is assigned a weight which incurs a weighted penalty for the classifier. Specially, our approach achieves state-of-the-art performance on the Cityscapes dataset <ref type="bibr" target="#b8">[9]</ref> and the Pascal VOC2012 dataset <ref type="bibr" target="#b10">[11]</ref>. Our main contributions are summarized as follows.</p><p>? We propose a multi-receptive field module (MRFM), which is crucial for improving prediction performance. Meanwhile, considering the trade-off between performance and inference efficiency, we design a lightweight MRFM.</p><p>? We design an edge-aware loss to separate the boundary more accurately. The loss function does not need additional annotated data, but can be computed only on existing semantic segmentation data.</p><p>? The proposed network achieves new state-of-the-art performance on the Cityscapes and Pascal VOC2012 dataset. In particular, we achieve an mIoU score of 83.0% on Cityscapes, and an mIoU score of 88.4% on the Pascal VOC2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>Models based on Fully Convolution Networks <ref type="bibr" target="#b24">[25]</ref> have achieved high performance on several segmentation benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. Here we review some works most relevant to ours, focusing on the issues of receptive fields and improving edge pixel classification.</p><p>Receptive Field: It is well known that the size of receptive fields is critically important for high-level computer vision tasks. In dense prediction tasks like semantic segmentation, depth estimation, it is crucial for each prediction pixel to have a receptive field that is sufficiently large such that context information can be considered for making a correct prediction. To enlarge the receptive field, the feature map's output stride is set to 8, or 16 of the input size in semantic segmentation. On the other hand, as one has to upsample the output to the original input size, a large output stride may degrade the accuracy of per-pixel prediction. That is one of the reasons why multi-scale receptive field, corresponding to multi-scale features, can be beneficial for per-pixel prediction. Another interpretation is that, to make the network robust to a certain degree of invariance to translation, FCNs employ convolutions with stride and/or pooling for this purpose. However, this translation invariance is very harmful to accurately predict the labels of pixels at the edge as a shift of several pixels may result in the same feature for pixel classification. This is dilemma. To use features from different levels of layers can partially alleviate this problem, which is essentially the same as using multiple different scales of features. In order to aggregate multi-scale context, the work of <ref type="bibr" target="#b37">[38]</ref> employs a series of atrous convolution with increasing rates. Following <ref type="bibr" target="#b37">[38]</ref>, deeplabv2 <ref type="bibr" target="#b4">[5]</ref> proposes a astrous spatial pyramid pooling (ASPP) which applies four parallel atrous convolution with different rates at the bottom of ResNet <ref type="bibr" target="#b14">[15]</ref>. With the evolution of Deeplab architectures, ASPP forms several parallel atrous convolutions and an global average pooling structure, and all of operations contain batch normalization. ASPP has shown its sucess in extracting features of different scales. Besides, DenseASPP <ref type="bibr" target="#b34">[35]</ref> proposes an improved ASPP which is inspired by DenseNet <ref type="bibr" target="#b16">[17]</ref>. RefinNet <ref type="bibr" target="#b20">[21]</ref> proposes an chained residual pooling which captures background context from a large image region. PSPNet <ref type="bibr" target="#b40">[41]</ref> proposes a Spatial Pyramid Pooling (PSP) to extract feature maps with different sizes of pooling, then concatenates all the feature maps after upsampling. Notably, all of these modules are placed at the end of the backbone. We design a different approach that achieves multi-receptive field by interleaving in the backbone.</p><p>Objects/stuff Edge: In the traditional image segmentation task, many methods attempt to use edge or low-level information to improve performance. For the semantic segmentation method based on deep learning, the use of edges can be divided into two categories. The first one is using an Encoder-Decoder architecture that refines the final prediction with low-level features. Low-level features make the results sharper. For examples, SegNet <ref type="bibr" target="#b0">[1]</ref> utilizes the saved pool indices to recover the reduced spatial information. Unet <ref type="bibr" target="#b25">[26]</ref> uses different spatial information by skip connection. MSCI <ref type="bibr" target="#b19">[20]</ref> aggregates features form different scales with two LSTM chains. The other is multi-task that shares parameters for both semantic segmentation and edge detection, such as DFN <ref type="bibr" target="#b36">[37]</ref>. The work of <ref type="bibr" target="#b43">[44]</ref> shows that an auxiliary task (the edge agreement head) leads to faster training of the mask segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section, we introduce our proposed Multi-Receptive Field module in detail first. Then, we present the Edge Aware Loss. Finally, we describe the complete network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Receptive Field Module</head><p>Most of the methods capture multi-receptive field by modifying the structure at the end of the network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref>. However, previous works have not pay much attention on how to design a backbone to capture multi-receptive field. Therefore, we propose the multi receptive field module (MRFM) by redesigning the backbone, illustrated in the <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Multi-Receptive Field Module (MRFM): Generally, the backbone has a basic module, such as bottleneck in ResNet <ref type="bibr" target="#b14">[15]</ref>, the basic module in Xception <ref type="bibr" target="#b7">[8]</ref>. Take Xception for example. The basic module is composed of three depthwise convolution, one skip connection or skip connection with 1 ? 1 convolution. It's easy to understand that the basic module only captures single scale receptive field. Therefore, it is difficult to segment multi-scale objects accurately. So, we propose a multi-receptive field module (MRFM) that can simply replace any basic module, illustrated in the <ref type="figure" target="#fig_0">Figure 2</ref>. MRFM is composed of two paths with different receptive field. One path of MRFM is the same as the basic module, which called standard path. The other path, called atrous path, has the same structure as the basic module, but the convolution method is replaced by atrous convolution. In order to adaptively choose which receptive field is suitable, the output of each path is added by weights. In particular, two weights are normalized. To sum up, MRFM is explained by Equation 1, where f 1 ( * ) denotes the basic module in the standard path, g k ( * ) denotes the basic module replaced by atrous convolution in the atrous path (k denotes dilated rate), w 1 is weights of the standard path, and w 2 is weights of the atrous path.</p><formula xml:id="formula_0">y = w 1 ? f 1 (x) + w 2 ? g k (x) (1) Conv (rate=1) Conv (rate=k) Fusion</formula><p>Standard path Atrous path </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Receptive Field Module Lite(MRFM-lite):</head><p>In order to achieve a trade-off between speed and performance, we explore whether it can capture multi-scale receptive fields without increasing parameters. The two paths of MRFM are exactly the same except for the dilated rate. Now that two paths are identical except for the dilated rate, then we share all the parameters of each path. This method can effectively reduce the number of parameters, but it can still capture the multi-receptive field. Therefore, the model obtained through this structure training will have better per-formance. The number of parameters decrease, but the computation doesn't decrease during inference. In order to reduce the computation during inference, thanks to sharing parameters, we could remove atrous path easily, then we fine tune parameters for the better performance. In addition, when we fine tune the parameters, we use a smaller learning rate and fix the batch normalization. So, we call all of these processes Multi-Receptive Field Module Lite (MRFM-lite), illustrated in the <ref type="figure" target="#fig_1">Figure 3</ref>. In simple terms, the first step is to train the network with the shared parameters dual paths, and the second step is to remove atrous path and fine tune the parameters. MRFM-lite achieves a trade-off between speed and performance. In summary, we propose two versions of MRFM. Standard MRFM could replace any basic module to capture multi-receptive field in the backbone. Although introducing parameters, MRFM performs better. MRFM-lite does not increase parameters and computation, but it still can capture multi-receptive field. MRFM and MRFM-lite both can be applied to mainstream backbone, such as ResNet, Xception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Edge Aware Loss</head><p>By observing the results in the <ref type="figure" target="#fig_2">Figure 4</ref>, we find that pixels near the edge of an object often tend to be incorrectly classified. So, we propose a new edge aware loss(EAL).</p><p>The main idea is that during the training, a pixel close to or on the edge is assigned a weight which incurs a weighted penalty for the classifier. As shown in the <ref type="figure" target="#fig_3">Figure 5</ref>, the closer the pixel to the edge, the greater the weight. Im-  portantly, EAL doesn't need introduce additional annotated samples. Here is the calculation process for EAL: First, edge is detected on annotated ground truth with Sobel <ref type="bibr" target="#b9">[10]</ref>, and the edge map is got. Second, to get the weight associated with the pixel's distance to the edge, we apply a k ? k convolution, which is filled by 1, on the edge map. Third, to prevent the weight from getting too large, set a threshold.</p><p>The entire calculation process can be expressed in the following <ref type="bibr">Equation 2</ref>, where x denotes the ground truth, C k ( * ) donates the k ? k convolution filled by 1, E( * ) donates the edge detection, T hreshold donates max value of the weight map. Finally, we get a weight map which incurs a weighted penalty for classified. This weight map multiplies the output that compute by cross entropy, as shown in <ref type="bibr">Equation 3</ref>, where y denotes the outputs of the network, w donates the weight map.</p><formula xml:id="formula_1">w = min{C k (E(x)), T hreshold} (2) L = Sof tmaxLoss(y; w)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network Architecture</head><p>With the multi-receptive field module, we propose a Multi-Receptive Field Network architecture based on Xception modified by <ref type="bibr" target="#b6">[7]</ref> as illustrated in <ref type="figure" target="#fig_4">Figure 6</ref> . Here we think of <ref type="bibr" target="#b6">[7]</ref> as a powerful backbone. Given an input image, we use Xception model with the dilated network strategy. Meanwhile, replace the basic module in middle flow and exit flow's block1 with MRFM. In addition, the final feature map spatial size is 1/16 of the input image. Then ASPP and Decoder, which are same as <ref type="bibr" target="#b6">[7]</ref>, are also employed. At last, we employ EAL to compute loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our approach on two public and widely used semantic image segmentation datasets, Cityscapes <ref type="bibr" target="#b8">[9]</ref> and Pascal VOC2012 <ref type="bibr" target="#b10">[11]</ref>. We first report the implementation details. Then we perform a series of ablation experiments on Cityscapes, and analyze the results in detail. Finally, we report our results on Pascal VOC2012 compared with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Our proposed network is based on the Xception modified by <ref type="bibr" target="#b6">[7]</ref> pretrained on ImageNet <ref type="bibr" target="#b18">[19]</ref>. In order to better demonstrate the improvement of the network performance of the module, our work is based on FCN.</p><p>Training: Following previous work <ref type="bibr" target="#b4">[5]</ref>, we use the "poly" learning rate policy where the base learning rate is multiplied by (1? iter itermax ) power . The initial learning rate is set to 0.1, and power is set to 0.9. We train the network using mini-bath stochastic gradient descent(SGD) <ref type="bibr" target="#b18">[19]</ref>. Momentum and weight decay coefficients are set to 0.9 and 0.0004 respectively. The size of mini-batch is set to 12.</p><p>We employ crop size of 513 during Cityscapes and Pascal VOC2012 dataset.</p><p>Data augmentation:</p><p>Following <ref type="bibr" target="#b5">[6]</ref>, we use mean subtraction, apply data augmentation by randomly scaling the input images with 7 scales {0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.0} and randomly left-right flipping during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cityscapes Dataset</head><p>Dataset and Evaluation Metrics: Cityscapes is a large and famous city street scene semantic segmentation dataset <ref type="bibr" target="#b8">[9]</ref>. 19 classes of which 30 classes of this dataset are considered for training and evaluation. Each image has a high resolution 2048 ? 1024 pixels. Cityscapes has 5,000 images taken from 50 different cities which are all fine annotation. In these 5,000 images, there are 2,979 images for training, 500 images for validation and 1,525 images for testing. There are also 19,998 images with coarse annotation. Cityscapes In the same image, cityscapes have many scale objects, such as road, sky, vehicles, persons and so on. In the same category, persons, cars, sky and so on also have a lot of scales. So cityscapes is a dataset that is extremely sensitive to the scale. In this challenging dataset, our method outperforms previous methods again. For evaluation, mean class-wise intersection over union (mIoU) is used.</p><p>Ablation Study for MRFM in Xception: In order to evaluate the improvement of MRFM and MRFM-lite, we carry out some experiments. As listed in the <ref type="table">Table 1</ref>, without more parameters, MRFM-lite-2 significantly improves the segmentation performance over the Xception41baseline model by 2.7%. MRFM-lite is trained in two stages, and we also provide the results of two stages. Compared with baseline, MRFM-lite-2(DP) get a 2.2% promotion. Compared with MRFM-lite-2(DP), MRFM-lite-2(OP), which is fine tuned with one path, get a 0.5% promotion. Particularly, when we fine tune MRFM-lite, we set the learning rate to 0.01 and fix batch normalization. We  find it is important to fix batch normalization. It is worth noting that compared with Xception41, MRFM-lite has the same structure, the same speed and higher performance. In order to fairly compare the performance of MRFM under the similar model capacity, we also give the baseline performance of Xception65 and Xception71. With the increase of model capacity, the performance of the model has been improved, but MRFM is still better than Xcpetion71, which has the largest model capacity. Specially, with MRFM-4, the network is the best, which increases 3.4% that is compared with Xception41, and increases 1.9% that is compared with Xception71! In addition, compared with ASPP that is famous method to capture multi-scale receptive field, MRFM-lite is slightly better than ASPP without introducing more parameters and time, and MRFM also has a great improvement over ASPP.   ever, in the standard MRFM, MRFM-4 is better than MRFM-2, but MRFM-8 is very terrible. As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, with the increase of the dilated rate, the information of different receptive field becomes more different. So, if the receptive field of each path is much different, it is harmful. In addition, the convolution parameters of the two paths in MRFM-lite are more relevant than those of the two paths in MRFM. Therefore, the difference of dilated rate that is in the each path of MRFM can be greater. It is precisely because the receptive fields are not mutually constrained that MRFM is 0.7% higher than MRFM-lite. Comparing the performance differences of different parts using MRFM. MRFM in exit flow and MRFM in middle flow both bring about improvement than baseline. MRFM in both middle flow and exit flow is better than MRFM in one flow.</p><p>At last, all of experiments show that we use MRFM when we need higher accuracy and MRFM-lite when we need trade-off speed and accuracy. Both versions of MRFM can improve the basic performance of the model.</p><p>Ablation Study for MRFM in ResNet: In order to demonstrate the general of MRFM, we also product some experiments on ResNet50. As illustrated in the <ref type="table">Table 2</ref>, all of settings are better than baseline. MRFM-4 has increased 4.8% over the baseline network. Compared with FCN, MRFM-lite-2 has improved 3.5% without any more parameters. Compared with MRFM-lite-2(DP), MRFMlite-2(OP) has improved 1.8% . These experiments show MRFM and MRFM-lite could extend to other backbone easily. In addition, compared with ASPP, MRFM and MRFM-lite are also standout. Ablation Study for Receptive Field: As illustrated in the <ref type="table">Table 3</ref>, some experiments are organized to represent the effects of different receptive field methods. In-ceptionV4 <ref type="bibr" target="#b30">[31]</ref> uses a lot of pooling operations, and the operation of pooling will result in the loss of details. So the result of InceptionV4 <ref type="bibr" target="#b30">[31]</ref> is very poor. ResNet50-DCN <ref type="bibr" target="#b41">[42]</ref> applies deformable convolution in the stage2, stage3 and stage4. DCN <ref type="bibr" target="#b41">[42]</ref> effectively improved performance by 2.6%. However, MRFM-lite is still 0.9% better than ResNet50-DCN, and MRFM is still 2.2% better than ResNet50-DCN. We think that DCN is an implicit way to learn multi-scale information, but MRFM and MRFMlite are an explicit way to learn multi-scale information, so MRFM and MRFM-lite perform better.  <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref> 73.0 Lovas Loss <ref type="bibr" target="#b1">[2]</ref> 72.8 EAL 74.1  <ref type="table">Table 5</ref>. In order to demonstrate the influence of the different hyperparameters, experiments are conduct. Except that backbone is replaced with Xception41, the rest of baseline is the same as deeplabv3+. Ablation Study for Edge Aware Loss: The introduced EAL helps to detect object contour while not affecting learning process in the main master. In particular, as we have mentioned before, EAL doesn't require additional annotations. In the <ref type="table" target="#tab_3">Table 4</ref>, compared with baseline, EAL brings 1.6% improvement. DFN <ref type="bibr" target="#b36">[37]</ref> proposed the multi task methods, including edge segmentation task and semantic segmentation task. So EAL is also compared with edge branch method. When adding edge branch, the model shares the backbone, and each task has its own two convolution operations. As illustrated in the <ref type="table" target="#tab_3">Table 4</ref>, the result is almost similar with Cross Entropy. Because different tasks have its own unique parameters, It's hard for edge branch to influence the main task. In addition, edges can be considered as difficult samples, so we also compare OHEM <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>. Compared with OHEM <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, EAL brings 1.1% improvement. Lovas loss solves the gap between loss function and mIoU in semantic segmentation. To better illustrate the effectiveness of EAL, we also compared the difference with Lovas loss. Compared with Lovas loss, EAL brings 1.3% improvement. In the <ref type="figure" target="#fig_8">Figure 9</ref>, with EAL, edges can be clearly seen. When using EAL, two hyperparameters are added. One is k, which determines the longest distance of the edge, and the other is m, which determines the maximum weight. In the <ref type="table">Table 5</ref>, it shows that EAL can improve steadily under different hyperparameters.</p><p>Framework Details: In order to show the effect of each module in our framework and show the continuous improvement with our methods, we conducted some experiments, illustrated in the <ref type="table">Table 6</ref>. Based on FCN (Xcep-tion41) and deeplabv2 (Xception41+ASPP) <ref type="bibr" target="#b4">[5]</ref>, MRFM, MRFM-lite and EAL could bring improvement. Based on powerful deeplabv3+ (Xception65) <ref type="bibr" target="#b6">[7]</ref>, our MRFM could bring 1.4%. It is noteworthy that MRFM has continued to improve with ASPP. Although MRFM has acquired multiscale information in backbone, as shown in the <ref type="table">Table 6</ref>, ASPP can still obtain multi-scale information. Since ASPP is located behind backbone and is closer to output, it is meaningful to obtain multi-scale in any part of the network. We can also see that MRFM does improve the performance of backbone, so MRFM does have the prospect of improving segmentation baseline. In addition, we speculate that replacing ASPP with better modules will lead to better results, such as replacing DPC <ref type="bibr" target="#b3">[4]</ref>, DRN <ref type="bibr" target="#b42">[43]</ref>, etc. The highest performance MRFM can be improved by 0.5% with EAL and get a mIoU of 80.2% on the validation. At last, we get the best performance model, which contains deeplabv3+, MRFM-4 and EAL.</p><p>Ablation Study for different scale testing: In order to show more clearly that MRFM performs better than singlescale model in different scales, we organized a group of experiments. Image sent to the network are scaled to form different scales. Illustrated in the Tabel 7, the results of each scale are more better than the results of single-scale model. Moreover, by calculating the standard deviation, it is found that the results fluctuate less in each scale, so it can be proved that MRFM is robust for multi-scale.   <ref type="table">Table 9</ref>. Per-class results on PASCAL VOC2012 testing set. Methods are all pre-trained on MS-COCO. The best entry in each columns is marked in gray color.(Note: the methods that use the public dataset are included.) <ref type="figure">Figure 10</ref>. Visualization results on Cityscapes test set.</p><p>use the best structure in the <ref type="table">Table 6</ref>, which consists of DeeplabV3+(Xception65), MRFM and EAL. In particular, we use only fine-coarse data training networks, and submit our results to the official evaluation server. Results are shown in <ref type="table">Table 8</ref>. The result shows that our method is better than previous method in some categories, such as building, pole, traffic light, terrain, person. Because these classes have diverse scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pascal VOC2012 Dataset</head><p>Pascal VOC2012: The Pascal VOC2012 is well-known object segmentation dataset that includes 20 object cate- <ref type="figure">Figure 11</ref>. Visualization results on Pascal VOC2012 test set. gories and one background categories. There are 1,464 images for training, 1,449 images for evaluation and 1,456 images for testing. According to the common convention, we augment the training set by additional annotated VOC images provided in <ref type="bibr" target="#b13">[14]</ref> and the MS COCO dataset <ref type="bibr" target="#b22">[23]</ref>.</p><p>We follow the same strategy to train our model on the Pascal VOC2012 dataset. The model used in Pascal VOC dataset is the same as the model used in cityscapes dataset. <ref type="table">Table 9</ref> indicates the experiment results on Pascal VOC2012 testing set. Our proposed network achieves 88.4% mIoU, outperforming other previous state-of-the-art methods. Compared with <ref type="bibr" target="#b6">[7]</ref>, it increases 0.6%. The results show that our method is better than the previous method for the variable scales object, such as aeroplane, bird, cow, horse, person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have presented a Multi-Receptive Field Network for semantic segmentation. In order to capture multi-receptive field, the Multi-Receptive Field Module is proposed as a general module. Furthermore, MRFM-lite could achieve the trade-off between performance and speed. You can use different versions of the MRFM according to your needs. We have also provided an Edge Aware Loss which is effective in distinguishing the boundaries of object/stuff. At last, We hope our method could help all tasks of semantic segmentation improve the overall baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The structure of standard Multi-Receptive Field module. Each Conv represents a basic unit in backbone, such as bottleneck in ResNet. Each path has different receptive field. The output of each path is added by the weight. Each color represents a path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The process of Multi-Receptive Field Module Lite. Each Conv represents a basic unit in backbone, such as bottleneck in ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>It is difficult to distinguish the boundaries of objects/stuff. Pixels located near the edge of Road and Pole are often incorrectly predicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The weight map of edge aware loss. (a) shows the edge detection result on the ground truth. (b) shows the weights map that edge aware loss generates. The closer the edge is, the brighter it will be.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Overview of Multi Receptive Field Network Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visual improvements on Cityscapes. Based on FCN, MRFM and MRFM-lite produces more accurate and detailed results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>is Xception41-based FCN in which output stride is 16. MRFM-(*) present dilated rate in the atrous path. DP present dual path of MRFM-lite in training stage. OP present MRFM-lite is fine tuned with one path. MRFM(Exit flow) is represented as replacing the exit flow module with MRFM module. MRFM(Middle flow) is represented as replacing the middle flow module with MRFM module. To simplify the presentation, MRFM without special instructions replaces the basic module in middle flow and exit flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Different receptive field. The same color represents the same parameters. The top row shows MRFM-lite. The bottom row shows standard MRFM. Each row shows different MRFM's receptive field. As shown in each row, receptive field becomes large different by increasing dilation rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Visual improvements with EAL. (a) shows RGB image; (b) shows feature map with EAL; (c) shows feature map without EAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Different loss functions brings different improvement. To prove EAL's more efficient, some experiments are conduct.</figDesc><table><row><cell>Method</cell><cell cols="2">k m mIoU(%)</cell></row><row><cell>Baseline</cell><cell></cell><cell>78.1</cell></row><row><cell>EAL</cell><cell>5 3</cell><cell>78.6</cell></row><row><cell>EAL</cell><cell>7 3</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 6. Ablation Study for the each module.</figDesc><table><row><cell>Baseline</cell><cell>ASPP MRFM-lite-2 MRFM-4 EAL mIoU</cell></row><row><cell>Xception41</cell><cell>72.5</cell></row><row><cell>Xception41</cell><cell>75.2</cell></row><row><cell>Xception41</cell><cell>75.9</cell></row><row><cell>Xception41</cell><cell>75.0</cell></row><row><cell>Xception41</cell><cell>76.2</cell></row><row><cell>Xception41</cell><cell>76.3</cell></row><row><cell>Xception41</cell><cell>76.7</cell></row><row><cell>Xception41</cell><cell>77.7</cell></row><row><cell>Xception65+Decoder</cell><cell>78.4</cell></row><row><cell>Xception65+Decoder</cell><cell>79.7</cell></row><row><cell>Xception65+Decoder</cell><cell>80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Each column represents how many times the input image is the original image. The results show that MRFM is superior to single-scale model in different scales. The baseline is same as Deeplabv3+. We further compare our method with the state-of-the-art methods on the Cityscapes test set. For maximum performance, we Methods road swalk build. wall fence pole tlight sign veg terrain sky person rider car truck bus train mbike bike IoU Results on Cityscapes testing set. Methods are trained using both fine and coarse data. The best entry in each columns is marked in gray color.(Note: the methods that only use cityscapes dataset are included.)</figDesc><table><row><cell>DUC [33]</cell><cell>98.5</cell><cell>85.9</cell><cell>93.2</cell><cell cols="4">57.7 61.1 67.2 73.7 78.0 93.4</cell><cell>72.3</cell><cell>95.4</cell><cell>85.9</cell><cell>70.5</cell><cell>95.9</cell><cell>76.1 90.6 83.7</cell><cell>67.4</cell><cell>75.7 80.1</cell></row><row><cell>DFN [37]</cell><cell>98.6</cell><cell>85.9</cell><cell>93.2</cell><cell cols="4">59.6 61.0 66.6 73.2 78.2 93.5</cell><cell>71.6</cell><cell>95.5</cell><cell>86.5</cell><cell>70.5</cell><cell>96.1</cell><cell>77.1 89.9 84.7</cell><cell>68.2</cell><cell>76.5 80.3</cell></row><row><cell>ResNet38 [34]</cell><cell>98.7</cell><cell>86.9</cell><cell>93.3</cell><cell cols="4">60.4 62.9 67.6 75.0 78.7 93.7</cell><cell>73.7</cell><cell>95.5</cell><cell>86.8</cell><cell>71.1</cell><cell>96.1</cell><cell>75.2 87.6 81.9</cell><cell>69.8</cell><cell>76.7 80.6</cell></row><row><cell>PSPNet [41]</cell><cell>98.7</cell><cell>86.9</cell><cell>93.5</cell><cell cols="4">58.4 63.7 67.7 76.1 80.5 93.6</cell><cell>72.2</cell><cell>95.3</cell><cell>86.8</cell><cell>71.9</cell><cell>96.2</cell><cell>77.7 91.5 83.6</cell><cell>70.8</cell><cell>77.5 81.2</cell></row><row><cell>Deeplabv3 [6]</cell><cell>98.6</cell><cell>86.2</cell><cell>93.5</cell><cell cols="4">55.2 63.2 70.0 77.1 81.3 93.8</cell><cell>72.3</cell><cell>95.9</cell><cell>87.6</cell><cell>73.4</cell><cell>96.3</cell><cell>75.1 90.4 85.1</cell><cell>72.1</cell><cell>78.3 81.3</cell></row><row><cell>AdapNet++ [32]</cell><cell>98.6</cell><cell>86.2</cell><cell>93.3</cell><cell cols="4">57.8 62.0 67.3 75.0 79.6 93.6</cell><cell>72.3</cell><cell>95.3</cell><cell>86.4</cell><cell>72.2</cell><cell>96.2</cell><cell>81.5 92.4 88.0</cell><cell>71.2</cell><cell>76.6 81.3</cell></row><row><cell>Mapillary [27]</cell><cell>98.4</cell><cell>85.0</cell><cell>93.6</cell><cell cols="4">61.7 63.9 67.7 77.4 80.8 93.7</cell><cell>71.9</cell><cell>95.6</cell><cell>86.7</cell><cell>72.8</cell><cell>95.7</cell><cell>79.9 93.1 89.7</cell><cell>72.6</cell><cell>78.2 82.0</cell></row><row><cell>Deeplabv3+ [7]</cell><cell>98.7</cell><cell>87.0</cell><cell>93.9</cell><cell cols="4">59.5 63.7 71.4 78.2 82.2 94.0</cell><cell>73.0</cell><cell>95.8</cell><cell>88.0</cell><cell>73.3</cell><cell>96.4</cell><cell>78.0 90.9 83.9</cell><cell>73.8</cell><cell>78.9 82.1</cell></row><row><cell cols="2">AutoDeeplab [24] 98.8</cell><cell>87.6</cell><cell>93.8</cell><cell cols="4">61.4 64.4 71.2 77.6 80.9 94.1</cell><cell>72.7</cell><cell>96.0</cell><cell>87.8</cell><cell>72.8</cell><cell>96.5</cell><cell>78.2 90.9 88.4</cell><cell>69.0</cell><cell>77.6 82.1</cell></row><row><cell>DPC [4]</cell><cell>98.7</cell><cell>87.1</cell><cell>93.8</cell><cell cols="4">57.7 63.5 71.0 78.0 82.1 94.0</cell><cell>73.3</cell><cell>95.4</cell><cell>88.2</cell><cell cols="2">74.5 96.5</cell><cell>81.2 93.3 89.0</cell><cell>74.1</cell><cell>79.0 82.7</cell></row><row><cell>DRN [43]</cell><cell>98.8</cell><cell>87.7</cell><cell>94.0</cell><cell cols="4">65.1 64.2 70.1 77.4 81.6 93.9</cell><cell>73.5</cell><cell>95.8</cell><cell>88.0</cell><cell>74.9</cell><cell>96.5</cell><cell>80.8 92.1 88.5</cell><cell>72.1</cell><cell>78.8 82.8</cell></row><row><cell>Ours</cell><cell>98.8</cell><cell>88.0</cell><cell>94.2</cell><cell cols="4">63.8 64.7 72.3 78.3 81.8 94.2</cell><cell>73.9</cell><cell>95.7</cell><cell>88.3</cell><cell>74.6</cell><cell>96.4</cell><cell>79.5 92.2 88.1</cell><cell>72.8</cell><cell>78.6 83.0</cell></row><row><cell>Method</cell><cell cols="5">aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="7">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>RefineNet [21]</cell><cell cols="3">95.0 73.2 93.5 78.1</cell><cell>84.8</cell><cell cols="6">95.6 89.8 94.1 43.7 92.0 77.2 90.8 93.4</cell><cell>88.6</cell><cell>88.1</cell><cell>70.1</cell><cell>92.9</cell><cell>64.3 87.7</cell><cell>78.8</cell><cell>84.2</cell></row><row><cell>ResNet38 [34]</cell><cell cols="3">96.2 75.2 95.4 74.4</cell><cell>81.7</cell><cell cols="6">93.7 89.9 92.5 48.2 92.0 79.9 90.1 95.5</cell><cell>91.8</cell><cell>91.2</cell><cell>73.0</cell><cell>90.5</cell><cell>65.4 88.7</cell><cell>80.6</cell><cell>84.9</cell></row><row><cell>PSPNet [41]</cell><cell cols="3">95.8 72.7 95.0 78.9</cell><cell>84.4</cell><cell cols="6">94.7 92.0 95.7 43.1 91.0 80.3 91.3 96.3</cell><cell>92.3</cell><cell>90.1</cell><cell>71.5</cell><cell>94.4</cell><cell>66.9 88.8</cell><cell>82.0</cell><cell>85.4</cell></row><row><cell>Deeplabv3 [6]</cell><cell cols="3">96.4 76.6 92.7 77.8</cell><cell>87.6</cell><cell cols="6">96.7 90.2 95.4 47.5 93.4 76.3 91.4 97.2</cell><cell>91.0</cell><cell>92.1</cell><cell>71.3</cell><cell>90.9</cell><cell>68.9 90.8</cell><cell>79.3</cell><cell>85.7</cell></row><row><cell>EncNet [39]</cell><cell cols="3">95.3 76.9 94.2 80.2</cell><cell>85.3</cell><cell cols="6">96.5 90.8 96.3 47.9 93.9 80.0 92.4 96.6</cell><cell>90.5</cell><cell>91.5</cell><cell>70.9</cell><cell>93.6</cell><cell>66.5 87.7</cell><cell>80.8</cell><cell>85.9</cell></row><row><cell>DFN [37]</cell><cell cols="3">96.4 78.6 95.5 79.1</cell><cell>86.4</cell><cell cols="6">97.1 91.4 95.0 47.7 92.9 77.2 91.0 96.7</cell><cell>92.2</cell><cell>91.7</cell><cell>76.5</cell><cell>93.1</cell><cell>64.4 88.3</cell><cell>81.2</cell><cell>86.2</cell></row><row><cell>SDN [12]</cell><cell cols="3">96.9 78.6 96.0 79.6</cell><cell>84.1</cell><cell cols="6">97.1 91.9 96.6 48.5 94.3 78.9 93.6 95.5</cell><cell>92.1</cell><cell>91.1</cell><cell>75.0</cell><cell>93.8</cell><cell>64.8 89.0</cell><cell>84.6</cell><cell>86.6</cell></row><row><cell cols="4">Deeplabv3+ [7] 97.0 77.1 97.1 79.3</cell><cell>89.3</cell><cell cols="6">97.4 93.2 96.6 56.9 95.0 79.2 93.1 97.0</cell><cell>94.0</cell><cell>92.8</cell><cell>71.3</cell><cell>92.9</cell><cell>72.4 91.0</cell><cell>84.9</cell><cell>87.8</cell></row><row><cell>ExFuse [40]</cell><cell cols="3">96.8 80.3 97.0 82.5</cell><cell>87.8</cell><cell cols="6">96.3 92.6 96.4 53.3 94.3 78.4 94.1 94.9</cell><cell>91.6</cell><cell>92.3</cell><cell>81.7</cell><cell>94.8</cell><cell>70.3 90.1</cell><cell>83.8</cell><cell>87.9</cell></row><row><cell>MSCI [20]</cell><cell cols="3">96.8 76.8 97.0 80.6</cell><cell>89.3</cell><cell cols="6">97.4 93.8 97.1 56.7 94.3 78.3 93.5 97.1</cell><cell>94.0</cell><cell>92.8</cell><cell>72.3</cell><cell>92.6</cell><cell>73.6 90.8 85.4</cell><cell>88.0</cell></row><row><cell>Ours</cell><cell cols="3">97.1 78.6 97.1 80.6</cell><cell>89.7</cell><cell cols="6">97.3 93.6 96.7 59.0 95.4 81.1 93.2 97.5</cell><cell>94.2</cell><cell>92.9</cell><cell>72.3</cell><cell>93.1</cell><cell>74.2 91.0</cell><cell>85.0</cell><cell>88.4</cell></row></table><note>Comparing with State-of-the-art:</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cocostuff: Thing and stuff classes in context. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1612.03716</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04184</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611v3</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pattern classification and scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2012 results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel?</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira-Perpi??n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>pages II-II</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selfsupervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09337</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03821</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dense relation network: Learning consistent and context-aware representation for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3698" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien N</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siems</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07069</idno>
		<title level="m">Faster training of mask r-cnn by focusing on instance boundaries</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
