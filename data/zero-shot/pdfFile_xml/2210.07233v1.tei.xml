<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRADOS, BUENAPOSADA, BAUMELA: SHAPE PRESERVING FACIAL LANDMARKS Shape Preserving Facial Landmarks with Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Prados-Torreblanca</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETSII</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Departamento de Inteligencia Artificial. Universidad Polit?cnica de Madrid</orgName>
								<address>
									<addrLine>Boadilla del Monte</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
							<email>josemiguel.buenaposada@urjc.es</email>
							<affiliation key="aff1">
								<orgName type="institution">ETSII</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
							<email>lbaumela@fi.upm.es</email>
							<affiliation key="aff2">
								<orgName type="department">Departamento de Inteligencia Artificial. Universidad Polit?cnica de Madrid</orgName>
								<address>
									<addrLine>Boadilla del Monte</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Universidad</forename><surname>Rey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>M?stoles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">.prados@upm.es</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PRADOS, BUENAPOSADA, BAUMELA: SHAPE PRESERVING FACIAL LANDMARKS Shape Preserving Facial Landmarks with Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top-performing landmark estimation algorithms are based on exploiting the excellent ability of large convolutional neural networks (CNNs) to represent local appearance. However, it is well known that they can only learn weak spatial relationships. To address this problem, we propose a model based on the combination of a CNN with a cascade of Graph Attention Network regressors. To this end, we introduce an encoding that jointly represents the appearance and location of facial landmarks and an attention mechanism to weigh the information according to its reliability. This is combined with a multi-task approach to initialize the location of graph nodes and a coarse-to-fine landmark description scheme. Our experiments confirm that the proposed model learns a global representation of the structure of the face, achieving top performance in popular benchmarks on head pose and landmark estimation. The improvement provided by our model is most significant in situations involving large changes in the local appearance of landmarks. The code is publicly available at https://github.com/andresprados/SPIGA</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Landmarks (or keypoints) are a widely used representation to address high-level vision tasks such as image retrieval <ref type="bibr">[18]</ref>, facial expression recognition <ref type="bibr">[23]</ref>, face reenactment <ref type="bibr" target="#b34">[35]</ref>, etc. The performance of computer vision algorithms on the final task depends, to a great extent, on the accuracy and robustness of this intermediate representation. Thus, although many algorithms with excellent performance have recently emerged, research is still very intense in this area.</p><p>Top facial landmark estimation methods may be broadly grouped into coordinate and heatmap regression approaches. Coordinate regression approaches directly estimate the landmark position by projecting the representation estimated by a CNN encoder onto a set of 2D coordinates <ref type="bibr">[6,</ref><ref type="bibr">7,</ref><ref type="bibr">12,</ref><ref type="bibr">17,</ref><ref type="bibr">24]</ref>. They are the most efficient since they only require an encoder architecture to compute the facial representation. The heatmap regression approach is based on appending multiple encoder-decoder modules to estimate a 2D data structure modeling the landmark position likelihood, the heatmap <ref type="bibr">[8,</ref><ref type="bibr">9,</ref><ref type="bibr">10,</ref><ref type="bibr">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. The landmark coordinates are typically estimated at the maximum of each heatmap. This architecture provides an increase in accuracy at the expense of a considerable boost in computational and memory requirements. A fundamental limitation of both approaches is their degradation when there is ambiguity or noise contaminating the local landmark appearance. This typically happens at the presence of occlusions, heavy make-up, blur and extreme illuminations or poses. This is because of the known fact that CNNs cannot learn simple spatial relationships <ref type="bibr">[21]</ref> and, in the case of facial landmarks, are unable to learn a global representation of the face structure. However, a human face is a highly structured object with a prominent landmark configuration. Therefore, an effective way of representing the local appearance of each landmark and its geometric relationship to the other landmarks is needed.</p><p>This problem has been partially addressed in the literature with a local attention module combining landmarks with facial boundaries <ref type="bibr">[9,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b30">31]</ref>. This is a solution that learns short-distance geometrical relationships. An alternative solution combines the advantages of a CNN description with traditional Ensemble of Regression Trees (ERT) <ref type="bibr">[25,</ref><ref type="bibr">26]</ref>. Although this solution is able to learn long-distance geometrical dependencies, it is not fully satisfactory because of the limited learning capabilities of ERTs and the impossibility of end-to-end training. Other approaches use a Graph Convolutional Network (GCN) to learn the facial geometrical structure <ref type="bibr">[16,</ref><ref type="bibr">17]</ref>. This is achieved by combining the landmark local description, extracted from the CNN representation, with geometrical information represented by the relative landmark locations. However, poor initialization and the lack of an advanced attention mechanism reduce the performance of these models. More recent approaches use transformers <ref type="bibr">[15,</ref><ref type="bibr" target="#b31">32]</ref> in a cascade shape regressor, obtaining very good results due to the built-in attention mechanisms.</p><p>In this paper, we present the SPIGA (Shape Preserving wIth GAts) model for the estimation of human face landmarks. We follow the traditional regressor cascade approach <ref type="bibr">[2]</ref> and present an algorithm that combines a multi-stage heatmap backbone with a cascade of Graph Attention Network (GAT) regressors <ref type="bibr">[28]</ref>. The backbone provides a top-performing facial appearance representation. The cascaded GAT regressor is endowed with a positional encoding and attention mechanism that learn the geometrical relationship among landmarks. Another element of our proposal that improves the convergence of the GAT cascade is a coarse-to-fine feature extraction procedure and a good initialization. To do this, we train our backbone with a multi-task approach that also estimates the head pose, using its projection to establish the initial landmark locations. We evaluate the performance of our proposal in 300W, COFW-68, MERL-RAV and WFLW datasets. It achieves top performance on both head pose and face landmarks estimation. The improvement is most significant in situations involving large appearance changes, such as occlusions, heavy make-up, blur and extreme illuminations. We make the following contributions: 1) A GAT cascade with an attention mechanism to weigh the information provided by each landmark according to its reliability; 2) A positional encoding to jointly represent relative landmark locations and local appearance; 3) A multi-task approach to initialize the location of graph nodes; 4) A coarse-to-fine landmark description scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shape Regressor Model</head><p>We propose a coarse-to-fine cascade of landmark regressors <ref type="bibr">[2,</ref><ref type="bibr">4]</ref> that iteratively refines the landmarks coordinates while preserving the face shape. Our approach involves three critical components: 1) the initialization, 2) the features used for regression, and 3) the regressors that estimate the face shape deformation at each step of the cascade.</p><p>In our proposal, we use a multi-task CNN backbone to provide both, the initialization and the local appearance representation. We set the initial shape of the face, x 0 ? R L?2 , by projecting L landmarks from a generic 3D rigid face mesh oriented using the head pose backbone prediction. At each cascade step t, a GAT-based <ref type="bibr">[28]</ref> regressor computes a displacement vector, ?x t , to update the landmarks location, x t = x t?1 + ?x t . After K steps, the final face shape is x K = x 0 + ? K t=1 ?x t . We denote the 2D location of l-th landmark at step t as x l t ? R 2 . In <ref type="figure" target="#fig_0">Fig. 1</ref> we show the regressor with a two-step cascade configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Initialization by Head Pose Estimation</head><p>Our multi-task backbone, termed Multi Task Network (MTN), is a cascade of M encoderdecoder Hourglass (HG) modules. Each HG module in MTN is composed of a shared encoder with two task branches: 1) a 3D head pose estimation branch and 2) a landmark estimation decoder to the end of which we attach the next HG module. Defining and balancing the depth of the three components is a critical factor to boost the head pose estimation accuracy. We supervise the h-th module pose head by comparing its estimation, p ? R 6 , with the ground truth,p, using the L2 loss, L h p (p,p) = ||p ? p|| 2 . Our annotations for pose, p, are obtained from the ground truth landmarks using a rigid head model (see <ref type="figure" target="#fig_0">Fig. 1</ref>). In the landmarks task we optimize a coordinate smooth L1 loss (L coord ) enhanced by a local attention mechanism (L att ) on the heatmaps, like <ref type="bibr">[10,</ref><ref type="bibr" target="#b29">30]</ref>. The final landmark loss is defined</p><formula xml:id="formula_0">as L lnd = ? M h=1 2 h?1 (? c L h coord + ? att L h att ),</formula><p>where ? 's are scalars empirically optimized. For further details, please see the supplementary material.</p><p>To obtain a top-performing head pose estimation model (see <ref type="table" target="#tab_0">Table 1</ref>) we pre-train the network only with the landmark task, L lnd , and fine-tune with both tasks, landmarks and pose, like <ref type="bibr">[27]</ref>. For multi-task fine-tuning we use the loss L mt = L lnd + ? p ? M h=1 2 h?1 L h p , where ? p is a hyperparameter. Although we use intermediate supervision at every HG module, the prediction of p to estimate x 0 , as well as the visual features, are extracted from the last module. Let X ? R L?3 be the 3D coordinates on the 3D head model that correspond to the L 2D landmarks. If the pose estimated by the backbone is given by p, then the initial shape, x 0 , is computed by projecting the 3D model, x 0 = ?(X; p), where ?(?) is the 3D?2D projection function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Geometric and Visual Feature Extraction</head><p>For each step in the cascaded regressor, the input features are a combination of local appearance at each landmark (i.e. visual features) and global representation of the facial structure (i.e. geometric features). How visual and positional information is extracted and combined has a direct impact on the performance of the regressor (see <ref type="table" target="#tab_7">Table 5</ref>).</p><p>Let F be the output feature map of the last stacked HG module in the MTN. We extract local appearance information from a square window, W t , of size w t ? w t , centered at each landmark location, x l t?1 , in F. We use a fixed affine transform with a grid generator and sampler <ref type="bibr">[11]</ref> to crop and re-sample W t at a fixed size, regardless of w t . Then, using convolutional layers, we extract the visual features, v l t , corresponding to the l-th landmark at step t. We iteratively reduce w t at each step t, in a coarse-to-fine approach.</p><p>Positional information is crucial to maintain the shape of the face when local appearance alone is not sufficient (e.g. in presence of occlusions, blur, make-up, etc.). Relative distances between landmarks provide enhanced geometrical features compared to their absolute locations since they explicitly represent the facial shape. This relative positional information can be defined from displacement vectors between landmarks <ref type="bibr">[16]</ref>. Let q l t = {x l t?1 ? x i t?1 } i =l ? R 2?(L?1) be the displacement vector corresponding to l-th landmark in the t-th step. In contrast to <ref type="bibr">[16]</ref>, we learn a high dimensional embedding from q l t using a Multi layer Perceptron (MLP), r l t = ? t (q l t ), that facilitates the aggregation of the visual local appearance and the facial shape information. In the experiments, we show that this way of encoding relative positional information in r improves the shape-preserving ability of the network (see section <ref type="bibr">3.4</ref>).</p><p>Let f l t be the feature vector used to compute ?x l t . At each step t of the cascade (see <ref type="figure" target="#fig_1">Fig. 2</ref>), and for each landmark l, we add the visual features extracted from the backbone network, v l t , with the relative positional features, r l t , computed from the current shape, x t?1 , to produce the encoded features, f l t = v l t + r l t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cascade Shape Regressor Using GATs</head><p>The step regressor architecture <ref type="figure" target="#fig_1">(Fig. 2)</ref> is composed of stacked GAT layers inspired by the ones in the Attentional Graph Neural Net <ref type="bibr">[22]</ref>. We consider the facial shape as a single densely connected graph where nodes are the landmark locations, x t . To weigh the shared information across nodes, we compute a dynamic adjacency matrix per GAT layer s, A s t . We learn these matrices as an attention from a given landmark to every other in the graph.</p><p>The input to the first GAT layer at step t are the encoded features,</p><formula xml:id="formula_1">{f i t } L i=1 . Let f i,s?1 t</formula><p>be the features of the i-th landmark produced by the (s-1)-th GAT layer, that are also the input to s-th layer (f i,0 t ? f i t ). From now on, we drop the step-index t to simplify the notation. The updated feature vector after the s-th layer is defined as</p><formula xml:id="formula_2">f i,s = f i,s?1 + MLP([f i,s?1 ||m i,s ]) where [?||?]</formula><p>is the concatenation operator, m i,s is the information aggregated, or message, of the nodes neighboring i. Focusing on the message generation procedure, a query vector h i,s q , is assigned to landmark i and key h j,s k , and value vectors h j,s v , to every other landmark j. The attention weight of landmark i to landmark j is the SoftMax over the key-query similarities ? i j = SoftMax j (h i,s q ? h j,s k ), being ? i j the elements of the adjacency matrix A s t and the transmitted message m i,s the weighted average of the value vectors:</p><formula xml:id="formula_3">m i,s = ? i = j ? i j h j,s v , where h i,s q = W s 1 f i,s + b s 1 , h j,s k = W s 2 f j,s + b s 2 and h j,s v = W s 3 f j,s + b s 3 . Matrices W i and bias vectors b i are learned.</formula><p>Finally, the last GAT layer output f i,4 t is processed by a decoder, an MLP, to obtain the corresponding displacement, ?x i t . We constraint the values in ?x i t , applying an ArcTan activation and scaling the result, to be in the interval [?w t /2, w t /2]. In practice, this constraint makes the single-step regressor search problem simpler, boosting training convergence. Given a trained MTN backbone, we train the cascade with the</p><formula xml:id="formula_4">L CR = ? K t=1 L1 smooth [x? (x t?1 + ?x t )]</formula><p>loss, wherex are the ground truth landmark coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To train and evaluate our method, we conduct different experiments in four complementary datasets which have been acquired in-the-wild and bear different levels of difficulty:</p><p>300W <ref type="bibr">[20]</ref> provides 68 manually annotated landmarks. We employ the 300W private extension, which uses 3837 images as training set and adds 600 test images divided into indoor and outdoor subgroups.</p><p>COFW-68 is a re-annotated version of COFW <ref type="bibr">[1]</ref> with 68 landmarks. It is conceived for testing landmark detectors with occlusions in a cross-dataset approach. The testing set in COFW-68 is made of 507 images. The annotations include the landmark positions and the visibility labels for the same 68 points as in 300W.</p><p>WFLW <ref type="bibr" target="#b30">[31]</ref> is composed of challenging in-the-wild images and provides 98 manually annotated landmarks. The dataset has 7500 training and 2500 testing faces. It is divided into 6 subgroups: pose, expression, illumination, make-up, occlusion and blur.</p><p>MERL-RAV <ref type="bibr">[13]</ref> is a re-annotated version of 19,000 AFLW images with 68 landmarks, like 300W. It provides 15,449 training and 3,865 test faces divided into 3 orientation subsets: frontal, half-profile and profile. This recent dataset includes externally occluded visibility and self-occluded labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>In order to quantify the head pose estimation error, we use the Mean Absolute Error (MAE)</p><formula xml:id="formula_5">metric, MAE = 1 N N ? i=1 |p i ? p i |,</formula><p>where N is the number of testing images,p i is the ground truth and p i represents a single predicted pose parameter. Focusing on the landmark estimation task, Normalized Mean Error (NME) is the standard metric, NME = 100</p><formula xml:id="formula_6">N N ? i=1 L ? l=1 ||x i l ?x i l || 2 d i</formula><p>. Wherex i l and x i l denote, respectively, the groundtruth and predicted coordinates of the i-th landmark and d i is a normalization value which varies depending on the dataset: inter-ocular (int-ocul), distance between outer eye corners; inter-pupils, distance between pupil/eye centers; and box, computed as the geometric mean of the landmarks ground truth bounding box (d = ? w bbox * h bbox ). We also use Failure Rate (FR) and Area Under the Curve (AUC). FR evaluates the robustness of algorithms in terms of NME, indicating the percentage of images with an NME above a given threshold. AUC is calculated by computing the area under the Cumulative Error Distribution (CED) curve from 0 to the FR threshold. We introduce the Normalized mean Percentile Error 90 (NPE 90 ) which represents the NME for the image at the 90% of the dataset, sorted by NME. This metric is particularly convenient for small data subsets where the FR is not representative.</p><p>In all our tables results ranked first, second and third are shown respectively in blue, green and red colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Pose Estimation Results</head><p>First, we evaluate the MTN performance in 3D pose estimation. In <ref type="table" target="#tab_0">Table 1</ref>, we compare our pose estimation in 300W and WFLW with previous works in the literature. Our model shows a significant improvement. We reduce the mean MAE of the previous top performer, <ref type="bibr">MNN [27]</ref>, by 17% and 27% respectively in 300W and WFLW. The main reason behind this improvement is a better network architecture, stacked HGs vs. a single encoder-decoder in <ref type="bibr">[27]</ref> and the use of an attention mechanism. Having such a precise head pose estimation is a critical factor in our proposal, since the cascade shape regressor initialization relies on this prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Landmark Detection Results</head><p>WFLW is the most popular benchmark to evaluate the performance of facial landmark detection. Recent methods that adopt this dataset use the bounding boxes provided by HRnet <ref type="bibr" target="#b28">[29]</ref>, that were obtained from the ground truth landmark annotations. By doing so, they achieve better performance (see <ref type="table" target="#tab_10">Table 2</ref>, AWing results improve from 4.36 to 4.21 NME). In <ref type="table" target="#tab_10">Table 2</ref>, we clearly distinguish the bounding boxes used in the evaluation. Another important aspect to perform a fair comparison is the use of additional training data. In our discussion we do not consider methods that train with images or annotations other than those provided by WFLW.  <ref type="table" target="#tab_10">Table 2</ref>: Evaluation of landmark detection on WFLW.</p><p>In <ref type="table" target="#tab_10">Table 2</ref>, we show that our model outperforms current state-of-the-art (SOTA) in most of the WFLW subsets, as well as in the full set metrics. When it is compared with other GraphNets-based methods, our approach is 4% and 32% better in terms of NME and FR than SLD <ref type="bibr">[16]</ref>, and 7% and 23% better than SDFL <ref type="bibr">[17]</ref>. These results show that our relative positional encoding and the per layer graph attention mechanism have a strong impact on the performance of GraphNets. Further, our proposal is also more accurate than recent approaches based on transformers, when these models are trained only with WFLW data, DTLD-s <ref type="bibr">[15]</ref> and SPLT <ref type="bibr" target="#b31">[32]</ref>, both with 4.14 NME in the full set. If we analyze the performance on some of the subsets, our method is 35%, 25%, 23% and 39% better than the previous SOTA, ADNet <ref type="bibr">[10]</ref>, in the illumination, make-up, occlusion and blur subsets. This proves the importance of learning a global representation of the facial structure, that CNNs alone do not provide. Additionally, the low FR across the different subsets and better AUC values reaffirm that our model achieves a balanced trade-off between robustness and precision, taking advantage of the complementary benefits from the CNN and GAT architectures.</p><p>On the other hand, results of subsets where our approach is not competitive also bear some relevant insights. First, further research is needed in the expression subset, where our performance is not as good as the rest. This is due to the fact that the 3D facial model used to initialize the cascade is rigid (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Second, seemingly, in the pose subset, we are not the top performers. However, as we can see in <ref type="figure" target="#fig_2">Fig. 3</ref>, faces with extreme poses are not well annotated and self-occlusions are not marked. So, the evaluation on this subset of WFLW is   questionable. MERL-RAV is one of the newest datasets, created to evaluate 2D facial alignment inthe-wild. It improves landmark annotations at half-profile and profile images by labeling the self-occlusion of landmarks. Hence, this dataset allows to correctly measure the performance of landmark detectors on samples with extreme poses. As we can see in <ref type="table" target="#tab_3">Table 3</ref>, in terms of NME box , our model is 6% better than LUVLI's <ref type="bibr">[13]</ref> baseline, performing the best in all pose subsets.</p><p>Finally, to verify the generalization and performance against occlusions, we conduct a cross-dataset experiment training with the 300W public split and testing with COFW-68 and 300W private. Results are summarized in <ref type="table" target="#tab_5">Table 4</ref>. They prove the importance of the graph attention mechanism, which dynamically weighs landmark relationships according to the local image appearance and relative position, versus a learned static relationship approach, such as SLD <ref type="bibr">[16]</ref>, (NME int?ocul of 3.93 vs 4.22 in COFW-68). Further, SPIGA trained on the 300W public dataset beats LUVLI <ref type="bibr">[13]</ref> (NME box of 2.52 vs 2.75 in COFW-68) with a backbone that has half the number of HG modules. It also obtains comparable results to a recent transformer-based method trained from scratch, DTLD-s <ref type="bibr">[15]</ref>. It is marginally better than DTLD-s in 300W private and worse in COFW-68. These results prove that a general architecture using GATs can complement and enhance CNN-based models, reaching better results in situations where ambiguity or noise is contaminating the local landmark appearance, where preserving structural landmarks consistency contributes to the final solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We conduct our ablation study on WFLW to understand how SPIGA components impact specific subset metrics. <ref type="table" target="#tab_7">Table 5</ref> shows that the addition of the cascade shape regressor outperforms the bare MTN backbone (using SoftArgMax). Our new relative positional encoding is better than stacking the vector q l t with the visual features, and much better than NME box (%)(?) AUC 7 box (%)(?) NME int?ocul (%)(?) 300W priv. COFW-68 300W priv. COFW-68 COFW-68 HRNetV2-W18 <ref type="bibr">[</ref>   using no positional information. The estimation of an attention per layer with the GAT improves with respect to use of a common attention matrix (GCN). An extended view of the effect of the learned adjacency matrix is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Occlusion images show how the attention mechanism relies on visible landmarks regardless of the layer. The regressor "looks" at distant and unoccluded landmarks at the first GAT layer and then at closer ones in the last layers. The contribution of the proposed coarse-to-fine scheme w.r.t. a constant size window (w = 8) or a single pixel window (w = 1) is also clear in <ref type="table" target="#tab_7">Table 5</ref>. The improvement provided by SPIGA can be seen across all metrics. However, it is more prominent with the hard cases, as demonstrated by the results for the subsets Makeup, Occlusion, and Blur, and the NPE 90 of the full set. In each row of <ref type="table">Table 6</ref>, we display respectively the performance of three SPIGA models configured with one, two and three steps cascade. In each column, we show the NME obtained at each step. The final NME is reduced gradually as we increase the number of steps. Further, shorter cascades tend to have a better NME at the first step (4.17 vs 4.22). However, given also the larger FR they achieve (2.60 vs 2.44), we can conclude that longer cascades focus their first steps on improving their robustness.   <ref type="table">Table 6</ref>: SPIGA results for cascades with different number of steps, shown in (). In <ref type="figure" target="#fig_4">Fig. 5</ref> we show the initialization and the landmark locations estimated at each step of the regressor cascade. When the face displays a neutral expression (top row), the initialization is reasonably good and the model converges to a solution within one regression step. Since SPIGA initializes landmarks with a 3D model featuring a neutral expression, when the face displays any other configuration, the initialization is much worse (lower row). However, even in this situation, the model is able to estimate the correct landmark locations in three regression steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented SPIGA, a face landmark regressor that combines a CNN with a cascade of Graph Attention Networks (GATs). The CNN provides the local appearance representation. The GAT regressor is endowed with a positional encoding and attention mechanism that learn the geometrical relationship among landmarks and encourage the model to produce plausible face shapes. It establishes a new SOTA in the WLFW, COFW-68 and MERL-RAV datasets. In our experimentation we verify that the positional encoding is the component that contributes most to the final result and the first steps of the cascade focus on improving the robustness. In addition, at each step, the regressor "looks" at distant and reliable landmarks in the first GAT layer and progressively focuses its attention on closer landmarks in the following ones. These insights from our ablation analysis confirm that SPIGA is learning a global representation and explains why its improvement is most significative in challenging situations involving occlusions, heavy make-up, blur and extreme illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Implementation Details</head><p>In this section, we present a complete overview of SPIGA's implementation. Including an extended study of the CNN multi-stage backbone configuration used to provide the initialization of the 2D landmark location and the visual feature representation (F) for our GAT regressor (see <ref type="figure" target="#fig_0">Fig. 1</ref>). <ref type="figure" target="#fig_0">Figure 1</ref>: SPIGA workflow. Given as inputs an image and the facial 3D model, the CNN (MTN) infers the pose parameters, p, and the visual feature representation, F. Iteratively, the cascaded GAT regressor refines the initial 2D landmark projection provided by the 3D model, combining visual and structural information.</p><p>During training, we perform random data augmentation to the input images using the following transformations: rotation ?45 ? , scaling 60 ? 15% of the bounding box size, translation 5% of the bounding box size, horizontal flip 50%, blur 50%, HSV color jittering and synthetic rectangular occlusions. Input face images are finally cropped and resized to 256 ? 256 pixels. Similarly, 64x64 output heatmaps are generated following Awing <ref type="bibr">[21]</ref> recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CNN Multitask Backbone</head><p>Our backbone (MTN) consists of a cascade of M = 4 Hourglass stages (HG) with an Attention Module, similar to the one used by <ref type="bibr">[6]</ref>. First, a residual encoder reduces the size of the input image from 256?256 to 64?64 pixels before entering the HG cascade. Each HG reduces the spatial extent of the feature maps to a resolution of 8?8 at the bottleneck. Following <ref type="bibr">[19]</ref>, we add an encoder to each HG bottleneck to extract a 3D pose estimation head, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We first pre-train the backbone in the landmark detection task (without the pose encoders) using the Adam optimizer during 450 epochs with an initial learning rate of 10 ?3 and a step decay of 0.1 at epoch 380. During training, the batch size is set to 24 and the Automatic Mixed Precision (AMP) from Pytorch is used. <ref type="figure" target="#fig_0">In Equation 1</ref>, we show the loss function computed for the landmark detection task. We aggregate the losses of each HG module, represented by index h, doubling the loss weight of a module compared to the previous one.</p><formula xml:id="formula_7">L lnd = M ? h=1 2 h?1 (? c L h coord + ? att (L h points + L h edges )),<label>(1)</label></formula><p>Where ? coord and ? att are empirically set to 4 and 50, respectively. L coord is a smooth L1 function computed between the annotated and predicted landmarks coordinates. L points and L edges are Awing losses <ref type="bibr">[21]</ref> applied to the point and edges heatmaps, respectively.</p><p>Once the model has been pre-trained with landmarks, it is fine-tuned with both tasks, pose and landmarks. Sharing the same hyperparameter configuration as in the previous pretraining stage during 150 epochs, with a step decay from 10 ?3 to 10 ?4 at epoch 100. In Equation 2, we show the final loss, where ? p is empirically set to 1 and L pose is the L2 loss computed for the pose estimation. Once the model is trained, we freeze the backbone to train the GAT regressor.</p><formula xml:id="formula_8">L total = L lnd + M ? h=1 2 h?1 (? p L h pose )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Cascaded Regressor Based on GATs</head><p>The full cascaded regressor is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> and the architecture of a single-step regressor is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Similar to previous training configurations, the full shape regressor uses the Adam optimizer, setting an initial learning rate of 10 ?4 with a step decay of 0.1 at epoch 100.  The detailed extraction of visual and geometric features can be visualized in <ref type="figure" target="#fig_4">Fig. 5</ref>. Including the encoding and combination applied to get the input features of the regressor.</p><p>Let F be the last feature map of the last stacked HG module in the MTN. We first look at a square window, W t , of size w t ? w t and centered at each landmark location, x l t?1 in F. We use a fixed affine transform with the grid generator and sampler of the Spatial Transformer Networks <ref type="bibr">[7]</ref> to have a differentiable crop operation of W t . The crop operation re-samples W t to a fixed size 7 ? 7 ? 256 tensor, regardless of the dimension of the w t ? w t window. Then, using a convolution with a 7 ? 7 kernel, a 1 ? 1 ? 256 feature map is extracted. Finally, with a 1 ? 1 convolution, we compute the 512 channels of the visual features vector, v l t , corresponding to l-th landmark at step t. For each landmark l, we combine the visual features extracted from the backbone network, v l t , and the relative positional features, r l t , computed from x t?1 (i.e. the current shape) into the encoded features, f l t = v l t + r l t . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Extended Experimentation</head><p>In this section, we report an extended study of our proposal by adding new results on 300W (public and private) and WFLW datasets. In all our tables, results ranked first, second and third are shown respectively in blue, green and red colors.</p><p>300W public. In <ref type="table" target="#tab_0">Table 1</ref>, we present the comparison of state-of-the-art (SOTA) results in the 300W public. In this dataset, our approach achieves results comparable to the top performers in the literature: ADNet <ref type="bibr">[6]</ref> and SLD <ref type="bibr">[13]</ref>. Since most images in this data set are fully visible semi-frontal faces, CNN-based methods already have a highly accurate performance (e.g. Wing). Our method is better than the other two methods using Graph Neural Networks (GraphNets), SDFL <ref type="bibr">[14]</ref> and SLD <ref type="bibr">[13]</ref>, although results are comparable with SLD <ref type="bibr">[13]</ref> (NME int?ocul of 2.99 vs 3.04). ADNet <ref type="bibr">[6]</ref>, using a stacked encoder-decoder model is the SOTA and our method obtains a comparable result (NME int?ocul of 2.93 vs 2.99).</p><p>300W private. <ref type="table" target="#tab_10">Table 2</ref> shows an extended SOTA comparison in terms of NME int?ocul on 300W private dataset.</p><p>WFLW. In <ref type="table" target="#tab_7">Table 5</ref> we present an extended SOTA comparison on WLFW.</p><p>Method NME int?ocul (%)(?) NME int?pupil (%)(?) Common Challeng. Full Common Challeng. Full mnv2 <ref type="bibr">[4]</ref> 3.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extended Ablation study</head><p>In this section we show more examples of the learned adjacency matrix per GAT module in the first cascade step (i.e. the attention of each landmark to others within the face graph). In <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref> we show the estimated landmark locations (green dots) by SPIGA. On top of landmarks locations, we show as edges the attention estimated in the first cascade regressor step for two landmarks: one from the eye pupil (see <ref type="figure">Fig. 6</ref>) and one from the jaw (see <ref type="figure">Fig. 7</ref>). From left to right, we show the attention estimated in GAT 1 to 4. When we have no occlusions (see the first row in <ref type="figure">Fig. 6</ref>) to estimate the pupil features, GAT 1 looks mainly at the other eye landmarks. Then, GATs progressively pay more attention to closer landmarks and also to the other pupil. To compute the pupil displacement, GAT 4 only attends the landmarks over the same eye. Interestingly, when we have the other eye occluded (see second and third rows in <ref type="figure">Fig. 6</ref>) GAT 1 does not pay only attention to the other eye landmarks, but it looks mainly to landmarks over the nose. Finally, when we have heavy occlusions (see the last row in <ref type="figure">Fig. 6</ref>), the attention is given first to not occluded parts (i.e. nose and the other eye in GAT 1) and to landmarks over the same eye in GAT 4.  <ref type="table" target="#tab_3">Table 3</ref>: Extended evaluation of landmark detection on WFLW. <ref type="figure">Figure 6</ref>: Attention from left eye pupil to other landmarks shown as edges. From left to right, attention at GAT layer 1, GAT layer 2, GAT layer 3 and GAT layer 4. The greener the higher is the attention. We only show edges with an attention over a threshold for clarity. <ref type="figure">Figure 7</ref>: Attention from a landmark over the jaw to other landmarks shown as edges. From left to right, attention at GAT layer 1, GAT layer 2, GAT layer 3 and GAT layer 4. The greener the higher is the attention. We only show edges with an attention over a threshold for clarity. Now we study the estimated attention of a jaw landmark (see <ref type="figure">Fig. 7</ref>). Without occlusions (first row in <ref type="figure">Fig. 7)</ref>, the jaw landmark is paying attention to the mouth and other distant jaw landmarks in GAT 1. Progressively, the attention is concentrated on closer jaw landmarks. When we have heavy occlusions, the attention is given first to non-occluded landmarks in GAT 1. This allows the first graph convolution to compute features that use non-occluded landmarks. Then, the other GATs can use closer landmarks given that the starting features were free of occlusions.</p><p>We can conclude that the estimated attention allows us to extract occlusion-free features in the first GAT module. Then, the next GAT modules can use features from closer landmarks given the initial ones are correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Challenging examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Regressor architecture with a two-step cascade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Appearance and shape feature extraction for the t-th step regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>WFLW results on expressions (first 2 cols.) and pose examples (last 4 cols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Left pupil attention mechanism at first and last layer, respectively, of the first regressor step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Estimated landmark locations: from 2D projection of the rigid 3D model (left) to the final result after the 3 regressor steps (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>CNN Multitask backbone (MTL) architecture used during the fine-tuning with landmarks and pose estimation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>SPIGA cascaded regressor with the 3 steps used in the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>SPIGA step regressor with the 4 GATs layers used in the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>SPIGA extraction of visual and geometric features. Including the encoding and combination applied to get the input features of the regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>WFLW Challenging examples. In blue we show the ground truth and in green the landmark locations estimated by SPIGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ours) 1.41 1.70 0.77 1.29 1.78 1.86 0.93 1.52 3.23 2.24 1.71 2.39 Head pose MAE, in degrees, for 300W public, WFLW and MERL-RAV datasets.</figDesc><table><row><cell>Method</cell><cell cols="5">300W Angular error ( ? )(?) yaw pitch roll mean yaw pitch roll mean yaw pitch roll mean WFLW MERL-RAV Angular error ( ? )(?) Angular error ( ? )(?)</cell></row><row><cell>Yang [34] JFA [33] ASMNet [5] MNN [27] SPIGA (</cell><cell>4.2 2.5 1.62 1.80 1.24 1.55 2.97 2.93 2.21 2.70 5.1 2.4 3.9 ----3.0 2.6 2.7 -------1.56 ---2.08</cell><cell>----</cell><cell>----</cell><cell>----</cell><cell>----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Shown in blue the ground truth and in green estimated landmarks.</figDesc><table><row><cell>NME box (%)(?) All Frontal Half-Prof. Profile 1.99 1.89 2.50 1.92 1.61 1.74 1.79 1.25 SPIGA (Ours) 1.51 Method DU-Net LUVLI [13] 1.62 1.68 1.19</cell><cell>AUC 7 box (%)(?) Frontal Half-Prof. Profile 71.80 73.25 All 64.78 72.79 77.08 75.33 74.69 82.10 78.47 76.96 75.64 83.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evaluation of landmark detection on MERL-RAV.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Landmark detection results on 300W private and COFW-68. In (?) we show the number of HG modules.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>To NME NPE 90 NME NPE 90 NME NPE 90 NME NPE 90</figDesc><table><row><cell>Changed from SPIGA model: Changed From ? Shape model SPIGA ? MTN backbone 4.13 6.93 4.06 7.43 5.10 8.58 4.81 7.70 Full Make-up Occlusion Blur Positional encoding SPIGA ? w/o pos. encod. 4.17 7.07 4.01 6.71 5.03 8.33 4.72 7.52 SPIGA ? stacking 4.09 6.87 3.83 6.47 4.97 8.15 4.68 7.37 Attention GAT ? GCN 4.08 6.79 3.84 6.54 4.98 8.05 4.68 7.37 Coarse-to-Fine w = 16, 8, 4 ? w = 1, 1, 1 4.12 6.95 3.88 6.76 4.99 8.19 4.71 7.44 w = 16, 8, 4 ? w = 8, 8, 8 4.08 6.84 3.82 6.53 4.98 8.13 4.67 7.43 -Best SPIGA model 4.06 6.76 3.81 6.32 4.95 8.09 4.65 7.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell>NME int?ocul</cell><cell>Step 1 AUC 10</cell><cell>FR 10</cell><cell>NME int?ocul</cell><cell>Step 2 AUC 10</cell><cell>FR 10</cell><cell>NME int?ocul</cell><cell>Step 3 AUC 10</cell><cell>FR 10</cell></row><row><cell></cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell><cell>(?)</cell></row><row><cell>SPIGA(1) SPIGA(2) SPIGA(3)</cell><cell>4.17 4.17 4.22</cell><cell>59.53 59.55 59.10</cell><cell>2.60 2.44 2.44</cell><cell>-4.07 4.08</cell><cell>-60.45 60.41</cell><cell>-2.20 2.12</cell><cell>--4.06</cell><cell>--60.56</cell><cell>--2.08</cell></row></table><note>Contribution of the SPIGA components to the NME int?ocul (?) and NPE 90 (?) in WFLW.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 :</head><label>1</label><figDesc>Comparison against state-of-the-art on 300W public dataset. FR 8 NME inter?ocul AUC 8 FR 8 NME inter?ocul AUC 8 FR 8</figDesc><table><row><cell>Method</cell><cell cols="2">Indoor NME inter?ocul AUC 8 (?) (?)</cell><cell>(?)</cell><cell>(?)</cell><cell>Outdoor (?)</cell><cell>(?)</cell><cell>(?)</cell><cell>Full</cell><cell>(?)</cell><cell>(?)</cell></row><row><cell>DAN [8] SHN [26] DCFE [17] 3DDE [18] SPIGA (Ours)</cell><cell>-4.10 3.96 3.74 3.43</cell><cell cols="2">--52.28 2.33 --53.93 2.00 57.35 1.00</cell><cell>-4.00 3.81 3.71 3.43</cell><cell cols="2">--52.56 1.33 --53.95 2.66 57.17 0.33</cell><cell>4.30 4.05 3.88 3.73 3.43</cell><cell cols="3">47.00 2.67 --52.42 1.83 53.94 2.33 57.27 0.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2 :</head><label>2</label><figDesc>Results on 300W private test set. Face alignment methods are exclusively trained on 300W public dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2210.07233v1 [cs.CV] 13 Oct 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Use RetinaFace detections.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Result comes from a personal communication with authors of<ref type="bibr" target="#b36">[37]</ref>, 2.09 mistakenly in the paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The following funding is gratefully acknowledged. Andr?s Prados was funded by the Comunidad de Madrid, Ayudantes de Investigaci?n grant PEJ-2019-AI/TIC-15032. Jos? M. Buenaposada is funded by the Comunidad de Madrid project RoboCity2030-DIH-CM (S2018 /NMT-4331).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material. Shape Preserving Facial Landmarks with Graph Attention Networks</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decafa: Deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6892" to="6900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asmnet: A lightweight deep neural network for face alignment and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Ali Pourramezan Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>CVF/IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rectified wing loss for efficient and robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2126" to="2145" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Propagationnet: Propagate points to curve to learn structure information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiehe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiubao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adnet: Leveraging error-bias towards normal direction in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3080" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2034" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8233" to="8243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisting quantization error in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1521" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards accurate facial landmark detection via cascaded transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zidong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Seon-Min Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4176" to="4185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured landmark detection via topology-adapting deep graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Tung</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Fu</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-coherent deep feature learning for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beier</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5313" to="5326" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Keypoint-aligned embeddings for image retrieval and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Moskvyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="676" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep spatial-temporal feature fusion for facial expression recognition in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for endto-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Epameinondas Antonakos, and Stefanos Zafeiriou</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="609" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face alignment using a 3D deeply-initialized ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">102846</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2874" to="2881" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse local patch transformer for robust face alignment and landmarks inherent relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint head pose estimation and face alignment framework using global and local CNN features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face alignment assisted by head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatice</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="130" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Freenet: Multi-identity face reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving robustness of facial landmark detection by defending against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jide</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songmin</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11751" to="11760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaorong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinzheng</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="11112" to="11121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decafa: Deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6892" to="6900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asmnet: A lightweight deep neural network for face alignment and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Ali Pourramezan Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>CVF/IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adnet: Leveraging error-bias towards normal direction in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3080" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2034" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8233" to="8243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisting quantization error in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1521" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards accurate facial landmark detection via cascaded transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zidong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Seon-Min Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4176" to="4185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structured landmark detection via topology-adapting deep graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Tung</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Fu</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structure-coherent deep feature learning for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beier</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5313" to="5326" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3691" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="609" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face alignment using a 3D deeply-initialized ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">102846</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2874" to="2881" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fake it till you make it: Face analysis in the wild using synthetic data alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dziadzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Estellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sparse local patch transformer for robust face alignment and landmarks inherent relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving robustness of facial landmark detection by defending against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jide</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songmin</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11751" to="11760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaorong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinzheng</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="11112" to="11121" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
