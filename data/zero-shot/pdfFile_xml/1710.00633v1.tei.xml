<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP CONVOLUTIONAL NEURAL NETWORKS FOR INTERPRETABLE ANALYSIS OF EEG SLEEP STAGE SCORING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017. SEPT. 25-28, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Vilamala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristoffer</forename><forename type="middle">H</forename><surname>Madsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Danish Research Centre for Magnetic Resonance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP CONVOLUTIONAL NEURAL NETWORKS FOR INTERPRETABLE ANALYSIS OF EEG SLEEP STAGE SCORING</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING</title>
						<meeting> <address><addrLine>TOKYO, JAPAN</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2017">2017. SEPT. 25-28, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Networks</term>
					<term>Transfer Learning</term>
					<term>Sleep Stage Scoring</term>
					<term>Multitaper Spectral Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sleep studies are important for diagnosing sleep disorders such as insomnia, narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw polisomnography signals, which is a tedious visual task requiring the workload of highly trained professionals. Consequently, research efforts to purse for an automatic stage scoring based on machine learning techniques have been carried out over the last years. In this work, we resort to multitaper spectral analysis to create visually interpretable images of sleep patterns from EEG signals as inputs to a deep convolutional network trained to solve visual recognition tasks. As a working example of transfer learning, a system able to accurately classify sleep stages in new unseen patients is presented. Evaluations in a widely-used publicly available dataset favourably compare to state-of-the-art results, while providing a framework for visual interpretation of outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Studies of sleep assist doctors to diagnose sleep disorders and provide the baseline for appropriate follow up. Clinical sleep study design is based on polysomnography (PSG) in which several biological signals are acquired while the patient is asleep, including electroencephalography (EEG) for monitoring brain activity, electrooculogram (EOG) for eye movements and electromyogram (EMG) to measure muscle tone. The measurements are used to classify sleep score, i.e., classify the sleep stages the patient goes through during the study and assess the existence of any dysfunction. With the increasing accessibility of EEG signals (e.g., using permanent implanted electrodes or semi-permanent measures through sensors placed, e.g., in the ear <ref type="bibr" target="#b0">[1]</ref>) there is a growing interest in sleep quantification based on EEG alone. The EEG signal This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 659860. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. often presents significant bursts of rhythmic components, In particular the frequency band 'alpha' <ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref> Hz) is prominent. The American Academy of Sleep Medicine (AASM) recommends segmentation of sleep in five stages <ref type="bibr" target="#b1">[2]</ref>:</p><p>? W (wakefulness): alpha <ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref> Hz) rhythm is present, high-amplitude muscle contractions and movement artefacts on EMG and eye blinking on EOG, which can also be appreciated in low frequency EEG (0.5-2 Hz). ? N1 (Non-REM 1): alpha <ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref> Hz) rhythm is attenuated and replaced by mixed frequency theta signal (4-7 Hz), decrease in muscle tone and slow eye movements. ? N2 (Non-REM 2): presents K-complexes (negative peak followed by a positive complex and a final negative voltage) in the &lt;1.5 Hz range and sleep spindles (burst of oscillatory waves) at sigma <ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> band. ? N3 (Non-REM 3): slow wave activity exists (0.5-3 Hz), eye movements are unusual and EMG tone is low. ? R (REM): existence of rapid eye movements clearly visible in EOG, relatively low-amplitude and mixedfrequency activity in EEG, and presents the lowest muscle tone on EMG.</p><p>Manually scoring sleep stages is a tedious task requiring sleep experts to visually inspect PSG data recorded during the whole sleep study. Thus, there has been considerable effort over the past years to develop machine learning (ML) methods for automatic sleep scoring. For a recent survey on the available literature refer to <ref type="bibr" target="#b2">[3]</ref>. Recent research based on fully-connected artificial neural networks (ANN) with a variety of hand-crafted features extracted from the EEG signal includes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. More recently, recurrent neural network variants have been used to capture long-term dependencies or stage transition rules <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Here our aim is to discuss and test the hypothesis that sleep stage classification can be assisted by transfer learning (e.g., support the classifier training process by use of related data). We will take advantage of the fact that sleep scoring is a visual classification problem, hence, the strong progress in developing artificial visual perception using convolutive neural networks (CNNs, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>). In particular, timefrequency spectrogram images are created from windowed EEG signals and fed to a CNN pre-trained on a visual object recognition task, allowing the use of this powerful model for sleep stage classification in EEG data, which would otherwise lead to overfitting due to limited availability of data.</p><p>Despite the quite extensive research efforts in sleep scoring, there exist several challenges related to evaluation and comparison: one is the large variety of datasets (most outside the public domain) used to evaluate the methods; another frequent obstacle is the design of experiments and evaluation procedures, specifically proper cross-validation procedures. Some use a single train/test split, where no cross-validation is performed; others completely lack an independent test set, hence have no unbiased performance measure. Finally, others neglect the important dependency structures in the data when cross-validating, e.g., by completely random sampling. Random sampling ignores the strong dependence between data from the same subject or even obtained on the same night.</p><p>In this paper we have selected two recent results for comparison of performance. They both have been cross-validated in a proper sense and provide relevant information for reproducibility. In <ref type="bibr" target="#b11">[12]</ref>, authors conduct a time-frequency analysis of the EEG signal to extract relevant features to feed an ensemble of stacked sparse autoencoders. In a later investigation <ref type="bibr" target="#b12">[13]</ref>, authors construct an end-to-end ANN by combining a CNN architecture using raw EEG signals with a 2D stack of frequency-specific activity over time. Both studies obtain state-of-the-art results.</p><p>ANNs have often been criticized to work as black boxes: input data are fed into the one end and output values are obtained at the other end. Understanding the complex internal transformations that ANNs perform to the data is crucial for both domain practitioners, who need to comprehend the ANNs functioning in order to rely on them, and ML engineers, who require a deep knowledge of the system for correcting errors and improving the models. Therefore, different efforts have been made to provide a set of visualisation techniques to aid on this matter <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Our contributions in this paper are the following: we provide a framework for the automatic analysis of single sensor EEG sleep stage scoring, which is both highly accurate and visually interpretable by first using multitaper spectral estimation to generate colour image spectrograms, containing natural image-like features (blobs, vertices, etc.), emphasizing sleep patterns for the experts to analyse; secondly, given the natural image look of our inputs, we employ and refine the super-accurate models trained on natural images, which have been proven to excel on natural image visualisation, to classify the different sleep stages; and finally, we make use of sensitivity analysis to map the most influential features in our network back to the input space, providing highly interpretable images about the network's functioning. In particular, we employ a highly used publicly available database to classify the 5 sleep stages mentioned before with an average accuracy of 86% on an independent test set, but more im-portantly, we report the time-frequencies of interest for our network to decide on each sleep stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TRANSFER LEARNING WITH CONVOLUTIONAL NEURAL NETWORKS</head><p>CNNs were originally introduced in the late 80's <ref type="bibr" target="#b7">[8]</ref> as biologically inspired models to perform image recognition. However, a major breakthrough in the field occurred in 2012, when the deep CNN AlexNet <ref type="bibr" target="#b8">[9]</ref>, won the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) by a big margin to all other competing models. Since then, CNNs have become the workhorse for visualisation tasks and ILSVRC the preferred benchmark to test their performance. AlexNet consisted of five convolutional layers, max-pooling layers and three fully-connected layers. In 2013, ZFNet <ref type="bibr" target="#b14">[15]</ref> won the competition with a similar architecture to AlexNet but better optimised hyperparameters; authors also provided an approach to visualise the working of the CNN. GoogLeNet <ref type="bibr" target="#b15">[16]</ref> won the competition in 2014 with a CNN composed of 22 layers; many of which contained an inception module with several convolutions and pooling in parallel, as well as replacing fully-connected layers by average-pooling. Nevertheless, a highly successful model this year was the VGGNet <ref type="bibr" target="#b9">[10]</ref>, thanks to its simplicity: it is a 16-layer network composed exclusively of 3 ? 3 convolutions, 2 ? 2 max-pooling and 3 fully-connected layers. In 2015, the challenge was won by ResNet <ref type="bibr" target="#b10">[11]</ref>, a 152-layer CNN that uses residual blocks in which convolutions and activations do not compute a transformation function of the feature maps but rather the deviation from the identity function.</p><p>One big challenge when training such flexible models is to avoid overfitting: all previously mentioned CNNs use large amounts of training data and apply data augmentation and dropout techniques to address this issue. Still, there exist many domains in which acquiring input data is very costly (e.g., medical sciences) and training a deep neural network de novo is simply not feasible.</p><p>Therefore transfer learning is of interest. In the case of CNNs trained on natural images there seems to be a consistent behaviour in which lower layers of the network hierarchy learn general features similar to Gabor filters and colour blobs, while higher layers capture more domain specific representations. This observation has been studied in <ref type="bibr" target="#b16">[17]</ref>, where the layers' generality-specificity trade-offs have been quantitatively evaluated. Authors also analysed the decrease of transferability performance as the target task differs from the original one. In a different study <ref type="bibr" target="#b17">[18]</ref>, authors use the features learned in an object recognition task to extract features from a varying set of visual problems and datasets, obtaining superior results than task-specific state-of-the-art systems. Finally, <ref type="bibr" target="#b18">[19]</ref> used transfer learning in CNNs to extract features from unrelated tasks presenting insufficient data, an approach allowing the use of these flexible models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image creation</head><p>EEG data is represented as time-frequency images. Spectral estimation is based on Fourier analysis, which assumes a series of properties on the data such as infinite signals, continuity, periodicity and stationarity. However, none of these assumptions are typically fulfilled in EEG data, therefore in practise the spectrogram becomes a highly biased estimator. A common strategy to reduce the spectrogram's bias is by convolving the raw signal with a window function (called taper) before performing spectral estimation. Another important drawback of the spectrogram is that it produces estimates with high variance across all frequencies, which are even increased with the use of tapers. A method called multitaper spectral estimation <ref type="bibr" target="#b19">[20]</ref> can be used to reduce both bias and variance by applying multiple taper functions to the raw signal and averaging their results. This technique has recently been proven to be highly successful in analysing neurophysiological dynamics of sleep using EEG <ref type="bibr" target="#b20">[21]</ref>. There exists a set of hyperparameters that highly influence the results of the multitaper spectral estimation: they are the window size ? in seconds, the window stepsize ? in seconds, the minimum frequency resolution that can be resolved f in Hz, the timehalf-bandwidth product, usually defined as W = ?f /2 and the number of tapers used, often set according to the heuristic L = 2W ? 1, where x is a function that rounds x down to the closest integer <ref type="bibr" target="#b20">[21]</ref>. After estimating the multitaper spectrogram, we convert its values to a logarithmic scale x = log(x) + 1 and split it into equally-length bins of size s, called epochs. Then, we convert each epoched spectrogram to an RGB colour matrix (i.e., an image) by applying our preferred colourmap (i.e., a lookup table that translates real values in the [0, 1] interval to RGB colours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>The CNN of choice to analyse the images created according to the previous block is the VGGNet <ref type="bibr" target="#b9">[10]</ref>, due to its simplicity and flexibility. It is composed of 16 weighted layers: ccm 64 ccm 128 cccm 256 cccm 512 cccm 512 fcr 4096 fcr 4096 fcs 1000 , where c means a 3 ? 3 convolutional filter of stride 1 using a ReLU activation function, m stands for 2 ? 2 maxpooling layer with a stride of 2, fcr and fcs correspond to fully-connected layers with ReLU and soft-max activations, respectively; sub-indexed values represent the number of channels in each block. Transfer learning is employed by using weight values in all convolutional layers that have been previously trained on ILSVRC-2014 data provided by the authors in http://www.robots.ox.ac.uk/?vgg/ research/very_deep. Fully-connected layers are initialised from scratch using Xavier's initialisation <ref type="bibr" target="#b21">[22]</ref> and trained using dropout. The number of final outputs is set according to the task we are tackling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network visualisation</head><p>In the current study, we resort to sensitivity analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> as a visualisation tool to better understand the decisions made by our network. More precisely, let D = {x n , t n } N n=1 be a dataset of P -dimensional input vectors x (i.e., spectral images in our work) and corresponding class labels t ? {1, . . . , C}, the built ANN acts as a function approximator, such thatt = f (x). We can estimate the relative importance that our network places to every input feature j (i.e., RGB colour channel in a pixel, in our context) to discriminate among the existing classes as:</p><formula xml:id="formula_0">s (j) = 1 N N n=1 ??(f (x), t) ?x (j) x=xn<label>(1)</label></formula><p>where ? is the loss function of choice and |x| is the absolute value of x. Sensitivity maps are created by disposing? <ref type="bibr">(j)</ref> in the corresponding RGB colour matrix forming an image.</p><p>The fact that most of the current frameworks supplying ANN building capabilities are provided with automatic differentiation procedures reduces the calculation of sensitivity maps to a simple function call.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EMPIRICAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We use EEG sleep recordings from the Sleep-EDF Database <ref type="bibr" target="#b25">[26]</ref> in the PhysioNet repository <ref type="bibr" target="#b26">[27]</ref>. In particular, a subset of data from a study of age effects on sleep in healthy subjects, containing two whole-night EEG recordings (approximately 20 hours) from Fpz-Cz and Pz-Oz channels sampled at 100 Hz and corresponding hypnograms (expert annotations of sleep stages) from 20 subjects (10 males and 10 females) between 25-34 years old (second night of subject 13 was not provided). Sleeping time was retrieved from each recording as the interval between annotated lights off and lights on times or from 15 minutes before/after the first/last scored sleep epoch, if these annotations were not provided. Class labels were obtained from the hypnograms at every 30 s. Images were created for Fpz-Cz sensor as explained in Section 3.1, setting ? = 3.0 s., f = 2 Hz, W = 3 and L = 5 tapers, with the purpose to capture the sleeping dynamics at the microevent time scale while maintaining a somewhat fine resolution <ref type="bibr" target="#b20">[21]</ref>. The window stepsize was set to ? = 0.67 s. in order to match the final image resolution (prefixed to 224 ? 224 pixels by the pre-trained VGGNet). Bin size was set to s = 150 s., corresponding to the current epoch plus the two previous and two posterior epochs, as it has been shown to improve overall accuracy by better classifying N1-N2, N1-R and N2-R transition stages <ref type="bibr" target="#b11">[12]</ref>. Spectrogram log values  <ref type="table" target="#tab_2">W  3529  579  97  46  258 78  13  2  1  6  93  78  85  86  N1  458 1219  353  29  703 17  44  13  1 25  85  44  58  68  N2  346 1215 13118 1676 1222  2  7  75  10  7  91  75  82  84  N3  80  31  461 5003  16  1  1  8  89  0  97  89  93  93  R  219  781  470  6 6235  3  10  6  0 81  89  81  85  86   VGG-FT   W  3505  671  52  39  242 78  15  1  1  5  96  78  86  87  N1  301 1553  334  19  555 11  56  12  1 20  89  56  69  75  N2  192  985 13884 1411 1105  1  6  79  8  6  92  79  85  86  N3  73  24  462 5015  17  1  0  8  90  0  97  90  93  94  R  82  563  378  14 6674  1  7  5  0 87  92  87  89  89   Table 1</ref>. Raw confusion matrix, normalised confusion matrix and per-class metrics for both VGG-FE and VGG-FT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-FE</head><p>were thresholded to the [0, 1] interval before applying the 'Jet' colourmap to generate the images. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we used a leave-one-subject-out sampling schema to partition the data into train and test datasets, further splitting the training dataset by randomly selecting 4 subjects for validation and the remaining 15 for actual training. The mentioned works address the skewed performance favouring the most represented classes during training in such imbalanced datasets using different approaches: first study employs class-balanced random sampling with an ensemble of classifiers, while the second one uses a different classbalanced batch at each stochastic gradient descent (SGD) epoch. We virtually use both strategies by randomly balancing all classes in each SGD epoch and obtaining ensemble behaviour thanks to the dropout layers <ref type="bibr" target="#b27">[28]</ref>.</p><p>CNNs were trained by optimising the categorical crossentropy between predicted values and class labels using adam <ref type="bibr" target="#b28">[29]</ref> SGD on mini batches of 250 training examples with a learning rate of 10 ?5 , and decay rate of first and second moments set to 0.9 and 0.999, respectively. The validation set was employed to choose the hyper-parameters and its loss as a stopping criterion to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We evaluated our method in two different scenarios: first, we used the VGGNet as a feature extractor (VGG-FE), where all convolutional layers were kept fixed and only the last 3 fullyconnected layers of the network were trained from scratch to the specific EEG sleep scoring problem. In the second scenario, all weights in the network were updated, obtaining a fine-tuned network (VGG-FT). Preliminary results including a randomly initialised network consistently showed suboptimal classification accuracy, being it more pronounced the smaller the training set was. Convergence time for this network was on average 3 times slower than VGG-FT.</p><p>We report the aggregated confusion matrix for both scenarios over all test sets <ref type="table">(Table 1</ref>, left), out of which we compute all different scoring performance metrics. To do so, we first class-balance the confusion matrix ( <ref type="table">Table 1</ref>, middle) and split the results into 5 binary one-vs-all classification, which are again renormalised in order to calculate the per-class precision, sensitivity, F1-score and accuracy (Table 1, right). Comparison with existing literature is presented in <ref type="table" target="#tab_2">Table 2</ref>, where the 95% confidence interval for each measure is computed using bootstrapping. In each of the 1000 bootstrap iterations, 20 confusion matrices out of the 20 subjects are sampled (with replacement) and their values aggregated. Then, each evaluation metric is calculated. The 1000 bootstrap samples per evaluation metric are ordered and the values at 26th, 500th and 975th positions are reported.</p><p>As shown in <ref type="table">Table 1</ref>, top block, the most correctly sleep stage classified by VGG-FE is N3 with 89% of epochs properly assigned. It is followed by R (81%), W (78%) and N2 (75%). The most difficult stage to classify is N1, with 44% of epochs correctly assigned. The highest misclassification rate for this stage is for assigning epochs to R stage (25%), followed by W (17%) and finally N2 (13%); a behaviour that is consistent with the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. These results seem to indicate that, while N1-N2 transitions have been acceptably captured by our method, N1-R clearly misses some important information. If we turn our attention towards per-class metrics ( <ref type="table">Table 1</ref>, top-right), we can observe that their precisions range between 89 ? 97%, except for the N1, which reaches a not inconsiderable 85%. However, sensitivities are scoring significantly lower, their values lying in the 75 ? 89% interval, with the exception of N1, which presents a score of 44%, highly impacting the accuracy and F1-score for this class. <ref type="table">Table 1</ref>, bottom block, shows the results obtained by VGG-FT. The improvement of this network with respect to VGG-FE is homogeneous across classes, the most prominent one being the increase of sensitivity for N1 class by 12%. This is achieved by reducing the misclassification between N1-W by 6%, N1-R by 5% and N1-N2 by 1%. It is followed by an increase of sensitivity in R (6%) and N2 (4%), and precision in N1 (4%), W (3%) and R (3%).</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we compare the performance of our method to the existing literature. As can be seen, both VGG-FE and VGG-FT exhibit state-of-the-art results, VGG-FT aligning with the best model to date <ref type="bibr" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study</head><p>Precision Sensitivity F1-score Accuracy <ref type="bibr" target="#b11">[12]</ref> 92  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualisation</head><p>To better understand the internal representation of the network, we selected our best performing subject (i.e., subject 7) and calculated the sensitivity maps of each class independently. <ref type="figure" target="#fig_0">Fig. 1</ref>, top row, depicts the most accurately classified spectrogram of each sleep stage, as characteristic examples of inputs to the classification system. Notice that the current epoch for classification is at the centre of the spectrogram (time between 0 and 30 s.), the two previous epochs span between [?60, 0] s. and the two posterior epochs are placed within the [30, 90] s. interval. <ref type="figure" target="#fig_0">Fig. 1, bottom row,</ref> shows the per-class sensitivity map, each generated according to Eq. 1 using only instances of subject 7 for this specific class, summing over the RGB colour channels, followed by 0-1 normalisation and converted back to RGB image using the 'Jet' colour map. Now, we analyse them according to the sleep stage definitions in Section 1: <ref type="figure" target="#fig_0">Fig. 1</ref>-a presents high sensitivity in the alpha band (8-12 Hz), characteristic of W stage, mainly centred at the current epoch and the immediately following one, maybe in order to be able to identify a transition stage. Sensitivity to low frequency activity corresponding to eye blinks is also evident. N1 stage ( <ref type="figure" target="#fig_0">Fig. 1-b</ref>) seems to show slightly decreased sensitivity in the alpha band as compared to W and rather higher sensitivity to theta (4-7 Hz) in the preceding epochs. Interestingly, <ref type="figure" target="#fig_0">Fig. 1</ref>-c exhibits increased sensitivity around the upper sigma band <ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> Hz) for the current epoch and low frequencies (&lt;1.5 Hz) across the whole image, which might correspond to the network identifying spindles and K-complexes, respectively, which are characteristic features of the N2 stage. Slow wave activity (0.5-3 Hz) seems to be present in the N3 sensitivity map ( <ref type="figure" target="#fig_0">Fig. 1-d)</ref>, with highest impact in the current and succeeding epochs. Finally, wide power band spanning from approximately 0.5 to 9 Hz is present in <ref type="figure" target="#fig_0">Fig. 1</ref>-e, probably accounting for the mixedfrequency signal distinctive of R stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have demonstrated that classification of sleep stages can be effectively framed as a visual task by first creating natural colour like images using multitaper spectral estimation and then applying recent achievements in the object recognition field to obtain state-of-the-art classification accuracy. Moreover, this approach greatly enhances the interaction with the domain expert by providing interpretable patterns to make sense of as well as a framework based on sensitivity analysis to easily inspect the network's reasoning. We think that the tools presented here can transcend EEG sleep scoring and be applied to other tasks within EEG analysis or, more generally, to other biological domains (e.g., EMG) where time-frequency signals are recorded. Further improvement of the method includes better hyperparameter optimisation when generating the spectral images. A thorough study of the obtained VGGNet layers might also be of interest to gain a deeper understanding of the internal structure of the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sensitivity analysis for subject 7. Top row shows characteristic sleep stage spectra; bottom row presents per-class sensitivity maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to existing literature. Mean values in bold within the corresponding 95% confidence interval.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EEG recorded from the ear: Characterizing the ear-EEG method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Mikkelsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">438</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Berry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>American Academy of Sleep Medicine</publisher>
		</imprint>
	</monogr>
	<note>Version 2.4.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sleep stage classification using EEG signal analysis: A comprehensive survey and new investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A I</forename><surname>Aboalayon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sleep scoring using artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronzhina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep Medicine Reviews</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic sleep stage classification based on EEG signals by using neural networks and wavelet packet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1151" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic sleep stage recurrent neural classifier using energy features of EEG signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-L</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepSleepNet: a model for automatic sleep stage scoring based on raw single-channel EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<idno>1703.04046</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic sleep stage scoring using time-frequency analysis and stacked sparse autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1587" to="1597" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic sleep stage scoring with single-channel EEG using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<idno>abs/1610.01683</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1312.6034</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>abs/1411.1792</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<idno>abs/1403.6382</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectrum estimation and harmonic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1055" to="1096" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sleep neurophysiological dynamics through the lens of multitaper spectral analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Prerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="92" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conf. on Artificial Intelligence and Statistics</title>
		<meeting>the Intl. Conf. on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualization of nonlinear kernel models in neuroimaging by sensitivity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neu-roImage</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1120" to="1131" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for feedforward artificial neural networks with differentiable activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hashem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for minimization of input data dimension for feedforward neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems</title>
		<meeting>the IEEE International Symposium on Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="447" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Physiobank, physiotoolkit, and physionet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="220" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
