<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Network In Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-23">23 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
							<email>xiangsx@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Aws</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runjie</forename><surname>Usa</surname></persName>
							<email>runjie@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Shanghai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><forename type="middle">Jiahang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Paul</forename><surname>Wipf</surname></persName>
							<email>daviwipf@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<country>Peking University China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Network In Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-23">23 Nov 2021</date>
						</imprint>
					</monogr>
					<note>AWS Shanghai AI Lab China AWS Shanghai AI Lab China</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computer systems organization ? Embedded systems</term>
					<term>Re- dundancy</term>
					<term>Robotics</term>
					<term>? Networks ? Network reliability KEYWORDS Graph Neural Network, Link Prediction, Node Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have shown success in learning from graph structured data containing node/edge feature information, with application to social networks, recommendation, fraud detection and knowledge graph reasoning. In this regard, various strategies have been proposed in the past to improve the expressiveness of GNNs. For example, one straightforward option is to simply increase the parameter size by either expanding the hidden dimension or increasing the number of GNN layers. However, wider hidden layers can easily lead to overfitting, and incrementally adding more GNN layers can potentially result in over-smoothing. In this paper, we present a model-agnostic methodology, namely Network In Graph Neural Network (NGNN ), that allows arbitrary GNN models to increase their model capacity by making the model deeper. However, instead of adding or widening GNN layers, NGNN deepens a GNN model by inserting non-linear feedforward neural network layer(s) within each GNN layer. Although, some works mentioned that adding MLPs within GNN layers could benefit the performance, they did not systematically analyze the reason for the improvement, nor evaluate with numerous GNNs on large-scale graph datasets. In this paper, we demonstrate that NGNN can keep the model stable against either node feature or graph structure perturbations through an analysis of it as applied to a GraphSage base GNN on ogbn-products data. Furthermore, we take a wideranging evaluation of NGNN on both node classification and link prediction tasks and show that NGNN works reliably across diverse GNN architectures. For instance, it improves the test accuracy of GraphSage on the ogbn-products by 1.6% and improves the hits@100 score of SEAL on ogbl-ppa by 7.08% and the hits@20 score of GraphSage+Edge-Attr on ogbl-ppi by 6.22%. And at the time of this submission, it achieved two first places on the OGB link prediction leaderboard.</p><p>(a) The test accuracy of GraphSage with different settings.</p><p>(b) The model parameter sizes of GraphSage with different settings. <ref type="figure">Figure 1</ref>: The test accuracy of GraphSage on ogbn-products with different number of GNN layers (from 2 to 4) and different hidden dimension sizes (from 128 to 1024) and the corresponding model parameter sizes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) capture local graph structure and feature information in a trainable fashion to derive powerful node representations. They have shown promising success on multiple graph-based machine learning tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> and are widely adopted by various web applications including social network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref>, recommendation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref>, fraud detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>, etc. Various strategies have been proposed to improve the expressiveness of GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>One natural candidate for improving the performance of a GNN is to increase its parameter size by either expanding the hidden dimension or the number of GNN layers. However, this can result in a large computational cost with only a modest performance gain. As a representative example, <ref type="figure">Figure 1</ref> displays the performance of GraphSage <ref type="bibr" target="#b7">[8]</ref> under different settings on the ogbn-products dataset and the corresponding model parameter sizes. From these results, it can be seen that either increasing the hidden dimension or increasing the number of GNN layers increases the model parameter size exponentially, but brings little performance improvement in tersm of test accuracy. For example, in order to improve the accuracy of a 3-layer GraphSage model by 1%, we need to add 2.3? more parameters (by increasing the hidden dimension from 256 to 512). Furthermore, with a larger hidden dimension a model is more likely to overfit the training data. On the other head, stacking multiple GNN layers may oversmooth the features of nodes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. As shown in <ref type="figure">Figure 1a</ref>, GrageSage reaches its peak performance with only 3 GNN layers and a hidden dimension of 512.</p><p>Inspired by the Network-in-Network architecture <ref type="bibr" target="#b19">[20]</ref>, we present Network-in-Graph Neural-Network (NGNN ), a model agnostic methodology that allows arbitrary GNN models to increase their model capacity by making the model deeper. However, instead of adding more GNN layers, NGNN deepens a GNN model by inserting nonlinear feedforward neural network layer(s) within each GNN layer. This leads to a much smaller memory footprint than recent alternative deep GNN architectures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and can be applied to all kinds of GNN models with various training methods including fullgraph training, neighbor sampling <ref type="bibr" target="#b7">[8]</ref>, cluster-based sampling <ref type="bibr" target="#b4">[5]</ref> and local subgraph sampling <ref type="bibr" target="#b39">[40]</ref>. Thus, it can easily scale to large graphs. Moreover, analysis of NGNN in conjunction with Graph-Sage on perturbed ogbn-products showed that NGNN is a cheap yet effective way to keep the model stable against either node feature or graph structure perturbations.</p><p>In this work, we applied NGNN to GCN <ref type="bibr" target="#b13">[14]</ref>, GraphSage <ref type="bibr" target="#b7">[8]</ref>, GAT <ref type="bibr" target="#b28">[29]</ref> and AGDN <ref type="bibr" target="#b27">[28]</ref> and SEAL <ref type="bibr" target="#b39">[40]</ref>. We also combine the proposed technique with different mini-batch training methods including neighbor sampling, graph clustering and local subgraph sampling. We conducted comprehensive experiments on several large-scale graph datasets for both node classification and link prediction leading to the following conclusions (which hold as of the time of this submission):</p><p>? NGNN improves the performance of GraphSage and GAT and their variants on node classification datasets including ogbn-products, ogbn-arxiv, ogbn-proteins and reddit. It improves the test accuracy by 1.6% on the ogbn-products datasets for GraphSage. Furthermore, NGNN with AGDN+BoT+self-KD+C&amp;S <ref type="bibr" target="#b12">[13]</ref> achieves the forth place on the ogbn-arxiv leaderboard <ref type="bibr" target="#b0">1</ref> and NGNN with GAT+BoT <ref type="bibr" target="#b31">[32]</ref> achieves second place on the ogbn-proteins leaderboard with many fewer model parameters. ? NGNN improves the performance of SEAL, GCN and Graph-Sage and their variants on link prediction datasets including ogbl-collab, ogbl-ppa and ogbl-ppi. For example, it increases the test hits@100 score by 7.08% on the ogbl-ppa dataset for SEAL, which outperforms all the state-of-the-art approaches on the ogbl-ppa leaderboard 2 by a substantial margin. Furthermore, NGNN achieves an improvement of the test hits@20 score by 6.22% on the ogbl-ppi dataset for GraphSage+EdgeAttr, which also takes the first place on the ogbl-ppi learderboard. ? NGNN improves the performance of GraphSage and GAT under different training methods including full-graph training, neighbor sampling, graph clustering, and subgraph sampling. ? NGNN is a more effective way of improving the model performance than expanding the hidden dimension. It takes less parameter size and less training time to get better performance than simply doubling the hidden dimension.</p><p>In summary, we present NGNN , a method that deepens a GNN model without adding extra GNN message-passing layers. We show that NGNN significantly improves the performance of vanilla GNNs on various datasets for both node classification and link prediction. We demonstrate the generality of NGNN by applying them to various GNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep models have been widely studied in various domains including computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, natural language processing <ref type="bibr" target="#b1">[2]</ref>, and speech recognition <ref type="bibr" target="#b42">[43]</ref>. VGG <ref type="bibr" target="#b26">[27]</ref> investigates the effect of the convolutional neural network depth on its accuracy in the largescale image recognition setting. It demonstrates the depth of representations is essential to the model performance. But when the depth grows, the accuracy will not always grow. Resnet <ref type="bibr" target="#b8">[9]</ref> eases the difficulties on training the deep model by introducing residual connections between input and output layers. DenseNet <ref type="bibr" target="#b11">[12]</ref> takes this idea a step further by adding connections across layers. GPT-3 <ref type="bibr" target="#b1">[2]</ref> presents an autoregressive language model with 96 layers that achieves SOTA performance on various NLP tasks. Even so, while deep neural networks have achieved great success in various domains, the use of deep models in graph representation leaning is less well-established.</p><p>Most recent works <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> attempt to train deep GNN models with a large number of parameters and achieved SOTA performance. For example, DeepGCN <ref type="bibr" target="#b17">[18]</ref> adapts the concept of residual connections, dense connections, and dilated convolutions <ref type="bibr" target="#b36">[37]</ref> to training very deep GCNs. However DeepGCN and its successor DeeperGCN <ref type="bibr" target="#b18">[19]</ref> have large memory footprints during model training which can be subject to current hardware limitations. RevGNN <ref type="bibr" target="#b16">[17]</ref> explored grouped reversible graph connections to train a deep GNN and has a much smaller memory footprint. However, RevGNN can only work with full-graph training and cluster-based mini-batch training, which makes it difficult to work with other methods designed for large scale graphs such as neighbor sampling <ref type="bibr" target="#b7">[8]</ref> and layer-wise sampling <ref type="bibr" target="#b3">[4]</ref>. In contrast, NGNN deepens a GNN model by inserting non-linear feedforward layer(s) within each GNN layer. It can be applied to all kinds of GNN models with various training methods including full-graph training, neighbor sampling <ref type="bibr" target="#b7">[8]</ref>, layer-wise sampling <ref type="bibr" target="#b3">[4]</ref> and cluster-based sampling <ref type="bibr" target="#b4">[5]</ref>.</p><p>Xu et al. <ref type="bibr" target="#b33">[34]</ref> used Multilayer Perceptrons (MLPs) to learn the injective functions of the Graph Isomophism Network (GIN) model and showed its effectiveness on graph classification tasks. But they did not show whether adding an MLP within GNN layers works effectively across wide-ranging node classification and link prediction tasks. Additionally, You et al. <ref type="bibr" target="#b35">[36]</ref> mentioned that adding  MLPs within GNN layer could benefit the performance. However, they did not systematically analyze the reason for the performance improvement introduced by extra non-linear layers, nor evaluate with numerous SOTA GNN architectures on large-scale graph datasets for both node classification and link prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BUILDING NETWORK IN GRAPH NEURAL NETWORK MODELS 3.1 Preliminaries</head><p>A graph is composed of nodes and edges G = (V, E) , where V = ( 1 , . . . , ) is the set of nodes and E ? V ? V is the set of edges. Furthermore, A ? {0, 1} ? denotes the corresponding adjacency matrix of G. Let X ? R ? be the node feature space such that X = ( 1 , . . . , ) where represents the node feature of . Formally, the ( + 1)-th layer of a GNN is defined as: 3 <ref type="bibr" target="#b2">3</ref> We omit edge features for simplicity. </p><formula xml:id="formula_0">? ( +1) = ( (G, ? )),<label>(1)</label></formula><p>where the function (G, ? ) is determined by learnable parameters and (?) is an optional activation function. Additionally, ? represents the embeddings of the nodes in the -th layer, and ? = when = 1. With an -layer GNN, the node embeddings in the last layer ? are used by downstream tasks like node classification and link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic NGNN Design</head><p>Inspired by the network-in-network architecture <ref type="bibr" target="#b19">[20]</ref>, we deepen a GNN model by inserting non-linear feedforward neural network layer(s) within each GNN layer. The ( + 1)-th layer in NGNN is thus constructed as:</p><formula xml:id="formula_1">? ( +1)</formula><p>= ( ( (G, ? ))).</p><p>(2) The calculation of is defined layer-wise as:</p><formula xml:id="formula_2">1 = ( (G, ? ) 1 ) . . . = ( ?1 )<label>(3)</label></formula><p>where 1 , . . . , are learnable weight matrices, (?) is an activation function, and is the number of in-GNN non-linear feedforward neural network layers. The first in-GNN layer takes the output of as input and performs the non-linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>In this section, we demonstrate that a NGNN architecture can better handle both noisy node features and noisy graph structures relative to its vanilla GNN counterpart.</p><formula xml:id="formula_3">R 1.</formula><p>GNNs work well when the input features consist of distinguishable true features and noise. But when the true features are mixed with noise, GNNs can struggle to filter out the noise, especially as the noise level increases.</p><p>GNNs follow a neural message passing scheme <ref type="bibr" target="#b6">[7]</ref> to aggregate information from neighbors of a target node. In doing so, they can perform noise filtering and learn from the resulting signal when the noise is in some way distinguishable from true features, such as when the latter are mostly low-frequency <ref type="bibr" target="#b21">[22]</ref>. However, when the noise level becomes too large and is mixed with true features, it cannot easily be reduced by GNNs <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates this scenario. Here we randomly added Gaussian noise N = (0, ) to node features in ogbn-products data, where is the standard deviation ranging from 0.1 to 5.0. We adopt two different methods for adding noise: 1) X = [X|N ] as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>, where | is a concatenation operation, and 2) X = [X + N ] as shown in <ref type="figure" target="#fig_1">Figure 2b</ref>. We trained GraphSage models (using the DGL <ref type="bibr" target="#b30">[31]</ref> implementation) under five different settings: 1) baseline Graph-Sage with default 3-layer structure and hidden dimension of 256, denoted as GraphSage; 2) GraphSage with the hidden dimension increased to 512, denoted as GraphSage-512; 3) 4-layer GraphSage, denoted as GraphSage-4layer; 4) GraphSage with one additional non-linear layer in each GNN layer, denoted as NGNN-GraphSage-1 and 5) GraphSage with two additional non-linear layers in each GNN layer, denoted as NGNN-GraphSage-2. In all cases we used ReLU as the activation function. As shown in <ref type="figure" target="#fig_1">Figure 2a</ref>, GraphSage performs well when the noise is highly distinguishable from the true features. But the performance starts dropping when the noise is mixed with the true features and decays faster when becomes larger than 1.0 as shown in <ref type="figure" target="#fig_1">Figure 2b</ref>.</p><p>The same scenario happens with the gfNN model <ref type="bibr" target="#b21">[22]</ref>, which is formed by transforming input node features via muliplications of the adjacency matrix followed by application of a single MLP block. This relatively simple model was shown to be more noise tolerant than GCN and SGC <ref type="bibr" target="#b32">[33]</ref>; however, the performance of gfNN turns out to be much lower than the baseline GraphSage model in our experiments, so we do not present results here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><p>2. NGNN is a cheap yet effective way to form a GNN architecture that is stable against node feature perturbations.</p><p>One potential way to improve the denoising capability of a GNN model is to increase the parameter count via a larger hidden dimension. As shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, GraphSage-512 does perform better than the baseline GraphSage. But it is also more expensive as its parameter size (675,887) is 3.27? larger than that of baseline Graph-Sage (206,895). And it is still not as effective as either NGNN model, both of which use considerably fewer parameters (see below) and yet have more stable performance as the noise level increases.</p><p>An alternative strategy for increasing the model parameter count is to add more GNN layers. As shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, by adding one more GNN layer, GraphSage-4layer does outperform baseline GraphSage when is smaller than 4.0. However, as a deeper GNN potentially aggregates more noisy information from its hop neighbors <ref type="bibr" target="#b38">[39]</ref>, the performance of GraphSage-4layer drops below baseline GraphSage when is 5.0.</p><p>In contrast to the above two methods, NGNN-GraphSage achieves much better performance as shown in <ref type="figure" target="#fig_1">Figure 2b</ref> with fewer parameters (272,687 for NGNN-GraphSage-1 and 338,479 for NGNN-GraphSage-2) than GraphSage-512 and without introducing new GNN layers. It can help maintain model performance when is We now show that by applying NGNN to a GNN, it can better deal with graph structure data perturbation. For this purpose, we randomly added edges to the original graph of ogbn-products, where is the ratio of newly added noise edges to the existing edges. For example = 0.01 means we randomly added 618.6K edges. <ref type="bibr" target="#b3">4</ref> We trained 3-layer GraphSage models with a hidden dimension of 256 under three different settings: 1) vanilla Graph-Sage, denoted as GraphSage; 2) GraphSage with one additional non-linear layer in each GNN layer, denoted as NGNN -GraphSage-1 and 3) GraphSage with two additional non-linear layers in each GNN layer, denoted as NGNN -GraphSage-2. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results. It can be seen that, NGNN can help preserve the model performance when is smaller than 0.01 and ease the trend of performance downgrade after is larger than 0.01 comparing to vanilla GraphSage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We next provide experimental evidence to show that NGNN works will with various GNN architectures for both node classification and link prediction tasks in Sections 4.2 and 4.3. We also show that NGNN works with different training methods in Section 4.4. Finally, we discuss the impact of different NGNN settings in Sections 4.5 and 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head><p>Datasets. We conducted experiments on seven datasets, including ogbn-products, ogbn-arxiv and ogbn-proteins from ogbn <ref type="bibr" target="#b9">[10]</ref> and reddit 5 for node classification, and ogbl-collab, ogbl-ppa and ogbl-citaiton2 from ogbl <ref type="bibr" target="#b9">[10]</ref> for link prediction. The detailed statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We evaluated the effectiveness of NGNN by applying it to various GNN models including GCN <ref type="bibr" target="#b13">[14]</ref>, Graphsage <ref type="bibr" target="#b7">[8]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b28">[29]</ref>, Adaptive Graph Diffusion Networks (AGDN) <ref type="bibr" target="#b27">[28]</ref>, and SEAL <ref type="bibr" target="#b41">[42]</ref> and their variants. <ref type="table">Table 2</ref> presents <ref type="table">Table 2</ref>: Baseline GNN models used in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Model Description</head><p>Node classification task GraphSage Vanilla GraphSage with neighbor sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphSage-Cluster</head><p>Vanilla GraphSage with cluster based sampling <ref type="bibr" target="#b4">[5]</ref>. GAT-FLAG GAT with FLAG <ref type="bibr" target="#b14">[15]</ref> enhancement. <ref type="bibr">GAT+BoT</ref> GAT with bag of tricks <ref type="bibr" target="#b31">[32]</ref>. AGDN+BoT AGDN with bag of tricks. AGDN+BoT+self-KD+C&amp;S AGDN with bag of tricks, knowledge distillation and correct&amp;smooth <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link prediction task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEAL-DGCNN</head><p>Vanilla SEAL using DGCNN <ref type="bibr" target="#b40">[41]</ref> as the backbone GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN-full</head><p>Vanilla GCN with full graph training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphSage-full</head><p>Vanilla Sage with full graph training. GraphSage+EdgeAttr</p><p>GraphSage with edge attribute.</p><p>GraphSage+EdgeAttr comes from https://github.com/lustoo/OGB_link_prediction Any score difference between vanilla GNN and NGNNGNN that is greater than 0.5% is highlighted with boldface. all the baseline models. We directly followed the implementation and configuration of each baseline model from the OGB <ref type="bibr" target="#b9">[10]</ref> leaderboard and added non-linear layer(s) into each GNN layer for NGNN . <ref type="table" target="#tab_0">Table 11</ref> presents the detail configuration of each model. All models were trained on a single V100 GPU with 32GB memory. We report average performance over 10 runs for all models except SEAL related models. As training SEAL models is very expensive, we took 5 runs instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node classification</head><p>Firstly, we analyzed how NGNN improves the performance of GNN models on node classification tasks. <ref type="table" target="#tab_1">Table 3</ref> presents the overall results. It can be seen that NGNN-based models outperform their baseline models in most of the cases. Notably, NGNN tends to performs well with GraphSage. It improves the test accuracy of Graph-Sage on ogbn-products and ogbn-arxiv by 1.61 and 0.62 respectively. It also improves the ROC-AUC score of GraphSage on ogbnproteins by 0.63. But as the baseline performance of reddit dataset is quite high, not surprisingly, the overall improvement of NGNN is not significant. We further analysis the performance of NGNN combined with bag of tricks <ref type="bibr" target="#b31">[32]</ref> on ogbn-arxiv and ogbn-proteins in <ref type="table" target="#tab_2">Table 4</ref>. It can The evaluation metrics used in ogb learderboard are hit@50 for ogbl-collab, hit@100 for ogbl-ppa and hit@20 for ogbl-ddi. Any hit score difference between vanilla GNN and NGNN GNN that is greater than 1% is highlighted with boldface. The evaluation metrics used for ogbl-ddi when profiling GCN-full and GraphSage-full are hit@20, hit@50 and hit@100.</p><p>be seen that NGNN-based models outperform their vanilla counterparts. NGNN with AGDN+BoT+self-KD+C&amp;S even achieves the first place over all the methods with no extension to the input data on the ogbn-arxiv leaderboard as of the time of this submission (The forth place on the entire ogbn-arxiv leaderboard). NGNN with GAT+BoT also achieves the second place on the ogbn-proteins leaderboard with 5.83 times fewer parameters compared with the current leading method RevGNN-Wide. <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link prediction</head><p>Secondly, we analyzed how NGNN improves the performance of GNN models on link prediction tasks. <ref type="table" target="#tab_3">Table 5</ref> presents the results on the ogbl-collab, ogbl-ppa and ogbl-ppi datasets. As shown in the tables, the performance improvement of NGNN over SEAL models is significant. NGNN improves the hit@20, hit@50 and hit@100 of SEAL-DGCNN by 4.72%, 4.67% and 7.08% respectively on ogblppa. NGNN with SEAL-DGCNN achieves the first place on the ogbn-ppa leaderboard with an improvement of hit@100 by 5.82% over the current leading method MLP+CN&amp;RA&amp;AA 7 . Furthermore, NGNN with GraphSage+EdgeAttr achieves the first place on the ogbl-ddi leaderboard with an improvement of hit@20 by 5.47% over the current leading method vanilla GraphSage+EdgeAttr. As GraphSage+EdgeAttr only provided the performance on ogbl-ppi, we do not compare its performance on other datasets. NGNN also works with GCN and GraphSage on link prediction tasks. As shown in the tables, It improves the performance of GCN and GraphSage in all cases. In particular, it improves the hit@20, hit@50 and hit@100 of GCN by 1.64%, 4.21% and 6.57% respectively on ogbl-ppa. <ref type="bibr" target="#b5">6</ref> NGNN -GAT+Bot has 11,740,552 parameters while RevGNN-Wide has 68,471,608 parameters. 7 https://github.com/lustoo/OGB_link_prediction    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">NGNN with Different Training Methods</head><p>Finally, we presents the effectiveness of using NGNN with different training methods including full-graph training, neighbor sampling and cluster-based sampling. <ref type="table" target="#tab_4">Table 6</ref> presents the results. It can be seen that NGNN improves the performance of GraphSage and GAT with all kinds of training methods on ogbn-products. It is worth mentioning that NGNN also works with local subgraph sampling method proposed by SEAL <ref type="bibr" target="#b39">[40]</ref> as shown in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of Multiple NGNN Layers</head><p>We studied the effectiveness of adding multiple non-linear layers to GNN layers on ogbn-products using GraphSage and GAT. Table 7 presents the results. The baseline model is a three-layer GNN model. We applied 1, 2 or 4 non-linear layers to each hidden GNN layer denoted as NGNN-1layer, NGNN-2layer and NGNN-4layer respectively. The GAT models use eight attention heads and all heads share the same NGNN layer(s). <ref type="table" target="#tab_5">Table 7</ref> presents the result. As shown in the table, NGNN-2layer always performed best with different hidden sizes in most of the cases. This reveals that adding non-linear layers can be effective, but the effect may vanish significantly when we continuously add more layers. The reason is straightforward, given that adding more non-linear layers can eventually cause overfitting.</p><p>We also observe that deeper models can achieve better performance with many fewer trainable parameters than wider models. <ref type="table" target="#tab_6">Table 8</ref> presents the model parameter size of each model. As shown in the table, the parameter size of GraphSage with NGNN-2layer and a hidden size of 256 is 338,479 which is 2? smaller than the parameter size of vanilla GraphSage with a hidden-size of 512, i.e., 675,887. And its performance is much better than vanilla Graph-Sage with a hidden size of 512. Furthermore, we also observe that adding NGNNlayers only slightly increase the model training time. <ref type="table" target="#tab_7">Table 9</ref> presents the single training epoch time of GraphSage under different configurations. As shown in the table, the epoch time of GraphSage with NGNN-2layer and a hidden size of 256 is only 3.1% longer than that of vanilla GraphSage with the same hidden size. However the corresponding parameter size is 1.63? larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effectiveness of Applying NGNN to Different GNN Layers</head><p>Finally, we studied the effectiveness of applying NGNN to only the input GNN layer (NGNN-input), only the hidden GNN layers (NGNN-hidden), only the output GNN layer (NGNN-output) and all the GNN layers on the ogbn-products dataset using GraphSage and GAT. The baseline model is a three-layer GNN model. The hidden dimension size is 256 and 128 for GraphSage and GAT respectively. <ref type="table" target="#tab_0">Table 10</ref> presents the results. As the table shows, only applying NGNN to the output GNN layer brings little or no benefit. While applying NGNN to hidden and input GNN layers can improve the model performance, especially applying NGNN to hidden layers. It demonstrates that the benefit of NGNN mainly comes from adding additional non-linear layers into the input and hidden GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We present NGNN, a model agnostic methodology that allows arbitrary GNN models to increase their model capacity by inserting non-linear feedforward neural network layer(s) inside GNN layers. Moreover, unlike existing deep GNN approaches, NGNN does not have large memory overhead and can work with various training methods including neighbor sampling, graph clustering and local subgraph sampling. Empirically, we demonstrate that NGNN can work with various GNN models on both node classification and link prediction tasks and achieve state-of-the-art results. Future work includes evaluating NGNN on more GNN models and investigating whether NGNN can work on broader graph-related prediction tasks. We also plan to explore methodologies to make a single GNN layer deeper in the future. <ref type="table" target="#tab_0">Table 11</ref>: Model settings of NGNN models. The column of NGNN position presents where we put the non-linear layers. hidden-only means only applying NGNN to the hidden GNN layers, input-only means only applying NGNN to the input layer, all-layer means applying NGNN to all the GNN layers.. The column of NGNN setting presents how we organize each NGNN layer. For example, 1-relu+1-sigmoid means NGNN contains one feedforward neural network with ReLU as its activation function followed by another feedforward neural network with Sigmoid as its activation function and 2-relu means NGNN contains two feedforward neural network layers with ReLU as the activation function of each layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Gaussian noise is concatenated with node features.(b) Gaussian noise is added to node features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The test accuracy (%) of GraphSage, GraphSage with the hidden dimension of 512 (GraphSage-512), 4layer GraphSage (GraphSage-4layer), GraphSage with one additional non-linear layer in each GNN layer (NGNN-GraphSage-1) and GraphSage with two additional nonlinear layers in each GNN layer (NGNN-GraphSage-2) on ogbn-product with randomly added Gaussian noise in node features. By default, a GraphSage model has three GNN layers with a hidden dimension of 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The model performance of GraphSage, GraphSage with one additional non-linear layer in each GNN layer (NGNN-GraphSage-1) and GraphSage with two additional non-linear layers in each GNN layer (NGNN-GraphSage-2) on ogbn-product with randomly added noise edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell>Datasets</cell><cell># Nodes</cell><cell># Edges</cell></row><row><cell cols="3">Node Classification</cell></row><row><cell cols="3">ogbn-products 2,449,029 61,859,140</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell></row><row><cell>ogbn-proteins</cell><cell>132,524</cell><cell>39,561,252</cell></row><row><cell>reddit</cell><cell cols="2">232,965 114,615,892</cell></row><row><cell cols="2">Link Prediction</cell><cell></cell></row><row><cell>ogbl-collab</cell><cell>235,868</cell><cell>1,285,465</cell></row><row><cell>ogbl-ppa</cell><cell>576,289</cell><cell>30,326,273</cell></row><row><cell>ogbl-ddi</cell><cell>4,267</cell><cell>1,334,889</cell></row><row><cell cols="3">smaller than 1.0 and slow the downward trend when is larger</cell></row><row><cell cols="3">than 1.0 compared to the other three counterparts.</cell></row></table><note>R 3. NGNN with GNNs can also keep the model stable against graph structure perturbation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="3">ogbn-products ogbn-arxiv ogbn-proteins</cell><cell>reddit</cell></row><row><cell>Eval Metric</cell><cell></cell><cell>Accuracy(%)</cell><cell cols="3">Accuracy(%) ROC-AUC(%) Accuracy(%)</cell></row><row><cell>GraphSage</cell><cell>Vanilla NGNN</cell><cell>78.27?0.45 79.88?0.34</cell><cell>71.15?1.66 71.77?1.18</cell><cell>75.67?1.72 76.30?0.96</cell><cell>96.19?0.08 96.21?0.04</cell></row><row><cell>GraphSage-Cluster</cell><cell>Vanilla NGNN</cell><cell>78.72?0.63 78.91?0.59</cell><cell>56.57?1.56 56.76?1.08</cell><cell>67.45?1.21 68.12?0.96</cell><cell>95.27?0.09 95.34?0.09</cell></row><row><cell>GAT-NS</cell><cell>Vanilla NGNN</cell><cell>79.23?0.16 79.67?0.09</cell><cell>72.10?1.12 71.88?1.10</cell><cell>81.76?0.17 81.91?0.21</cell><cell>96.12?0.02 96.45?0.05</cell></row><row><cell>GAT-FLAG</cell><cell>Vanilla NGNN</cell><cell>80.75?0.14 80.99?0.09</cell><cell>71.56?1.11 71.74?1.10</cell><cell>81.81?0.15 81.84?0.11</cell><cell>95.27?0.02 95.68?0.03</cell></row></table><note>Performance of NGNN on ogbn-products, ogbn-arxiv, ogbn-proteins and reddit.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance (as measured by classification accuracy and ROC-AUC for ogbn-arxiv and ogbnproteins, respectively) of NGNN combined with bag of tricks on ogbn-arxiv and ogbn-proteins.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell></cell><cell>Accuracy(%)</cell></row><row><cell>ogbn-arxiv ogbn-arxiv</cell><cell>AGDN+BoT</cell><cell cols="2">Vanilla 74.03?0.15 NGNN 74.25?0.17</cell></row><row><cell>ogbn-arxiv</cell><cell cols="3">AGDN+BoT+ Vanilla 74.28?0.13</cell></row><row><cell>ogbn-arxiv</cell><cell cols="2">self-KD+C&amp;S NGNN</cell><cell>74.34?0.14</cell></row><row><cell>ogbn-proteins ogbn-proteins</cell><cell>GAT+BoT</cell><cell cols="2">Vanilla 87.73?0.18 NGNN 88.09?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance of NGNN on ogbl-collab, ogbl-ppa and ogbl-ppi. We use the hit@20, hit@50 and hit@100 as the evaluation metrics.</figDesc><table><row><cell></cell><cell>Metric (%)</cell><cell cols="2">ogbl-collab Vanilla GNN NGNN</cell><cell cols="2">ogbl-ppa Vanilla GNN NGNN</cell><cell cols="2">ogbl-ddi Vanilla GNN</cell><cell>NGNN</cell></row><row><cell>SEAL-</cell><cell>hit@20</cell><cell>45.76?0.72</cell><cell>46.19?0.58</cell><cell>16.10?1.85</cell><cell>20.82?1.76</cell><cell>30.75?2.12</cell><cell cols="2">31.93?3.00</cell></row><row><cell>DGCNN</cell><cell>hit@50</cell><cell>54.70?0.49</cell><cell>54.82?0.20</cell><cell>32.58?1.42</cell><cell>37.25?0.98</cell><cell>43.99?1.11</cell><cell cols="2">42.39?3.23</cell></row><row><cell></cell><cell>hit@100</cell><cell>60.13?0.32</cell><cell>60.70?0.18</cell><cell>49.36?1.24</cell><cell>56.44?0.99</cell><cell>51.25?1.60</cell><cell cols="2">49.63?3.65</cell></row><row><cell></cell><cell>hit@10</cell><cell>35.94?1.60</cell><cell>36.69?0.82</cell><cell>4.00?1.46</cell><cell>5.64?0.93</cell><cell cols="3">47.82 ? 5.90 48.22 ? 7.00</cell></row><row><cell>GCN-full</cell><cell>hit@50</cell><cell>49.52?0.70</cell><cell>51.83?0.50</cell><cell>14.23?1.81</cell><cell>18.44?1.88</cell><cell>79.56?3.83</cell><cell cols="2">82.56?4.03</cell></row><row><cell></cell><cell>hit@100</cell><cell>55.74?0.44</cell><cell cols="3">57.41?0.22 20.21? 1.92 26.78?0.92</cell><cell>87.58?1.33</cell><cell cols="2">89.48?1.68</cell></row><row><cell>GraphSage-</cell><cell>hit@10</cell><cell>32.59?3.56</cell><cell>36.83?2.56</cell><cell>3.68?1.02</cell><cell>3.52?1.24</cell><cell>54.27?9.86</cell><cell cols="2">60.75?4.94</cell></row><row><cell>full</cell><cell>hit@50</cell><cell>51.66?0.35</cell><cell>52.62?1.04</cell><cell>15.02?1.69</cell><cell>15.55?1.92</cell><cell>82.18?4.00</cell><cell cols="2">84.58?1.89</cell></row><row><cell></cell><cell>hit@100</cell><cell>56.91?0.72</cell><cell>57.96?0.56</cell><cell>23.56?1.58</cell><cell>24.45?2.34</cell><cell>91.94?0.64</cell><cell cols="2">92.58?0.88</cell></row><row><cell>GraphSage+</cell><cell>hit@20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.06?4.81</cell><cell cols="2">93.28?1.61</cell></row><row><cell>EdgeAttr</cell><cell>hit@50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.98?0.42</cell><cell cols="2">98.39?0.21</cell></row><row><cell></cell><cell>hit@100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.98?0.16</cell><cell cols="2">99.21?0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) of GraphSage and GAT with and without NGNN trained with different training methods on ogbn-products.</figDesc><table><row><cell>Sampling Methods</cell><cell>full-graph</cell><cell cols="2">neighbor cluster-based sampling sampling</cell></row><row><cell>GraphSage</cell><cell>78.27</cell><cell>78.70</cell><cell>78.72</cell></row><row><cell>GraphSage-NGNN</cell><cell>79.88</cell><cell>79.11</cell><cell>78.91</cell></row><row><cell>GAT</cell><cell>80.75</cell><cell>79.23</cell><cell>71.41</cell></row><row><cell>GAT-NGNN</cell><cell>80.99</cell><cell>79.67</cell><cell>76.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy (%) of GraphSage and GAT with different number of non-linear layers added into GNN layers on ogbn-products.</figDesc><table><row><cell>Model</cell><cell cols="2">GraphSage</cell><cell></cell></row><row><cell>Hidden-size</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell>baseline</cell><cell cols="3">77.44 78.27 79.37</cell></row><row><cell cols="4">NGNN -1layer 77.39 79.53 79.12</cell></row><row><cell cols="4">NGNN -2layer 78.79 79.88 79.94</cell></row><row><cell cols="4">NGNN -4layer 78.79 79.52 79.88</cell></row><row><cell>Model</cell><cell></cell><cell>GAT</cell><cell></cell></row><row><cell>Hidden-size</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>baseline</cell><cell cols="3">68.41 79.23 75.26</cell></row><row><cell cols="4">NGNN -1layer 69.72 79.67 77.53</cell></row><row><cell cols="4">NGNN -2layer 69.86 78.26 78.76</cell></row><row><cell cols="4">NGNN -4layer 69.41 78.23 78.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>The parameter size of each model inTable 7.</figDesc><table><row><cell>Model</cell><cell></cell><cell>GraphSage</cell><cell></cell></row><row><cell>Hidden-size</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell>baseline</cell><cell>70,703</cell><cell>206,895</cell><cell>675,887</cell></row><row><cell>NGNN -1layer</cell><cell>87,215</cell><cell>272,687</cell><cell>938,543</cell></row><row><cell cols="2">NGNN -2layer 103,727</cell><cell cols="2">338,479 1,201,199</cell></row><row><cell cols="2">NGNN -4layer 136,751</cell><cell cols="2">470,063 1,726,511</cell></row><row><cell>Model</cell><cell></cell><cell>GAT</cell><cell></cell></row><row><cell>Hidden-size</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>baseline</cell><cell cols="3">510,056 1,543,272 5,182,568</cell></row><row><cell cols="4">NGNN -1layer 514,152 1,559,656 5,248,104</cell></row><row><cell cols="4">NGNN -2layer 518,248 1,576,040 5,313,640</cell></row><row><cell cols="4">NGNN -4layer 526,440 1,608,808 5,444,712</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>The single training epoch time of GraphSage with different number of non-linear layers added into GNN layers on ogbn-products. 71?0.41 20.71?0.74 25.40?0.35 NGNN -1layer 19.57?1.04 20.98?0.47 29.07?0.69 NGNN -2layer 19.25?0.87 21.36?0.48 30.01?0.13 NGNN -4layer 19.79?0.72 24.41?0.38 32.33?0.19</figDesc><table><row><cell>Model</cell><cell cols="2">GraphSage (secs)</cell><cell></cell></row><row><cell>Hidden-size</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell>baseline</cell><cell>18.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Test accuracy (%) of GraphSage and GAT when applying NGNN on different GNN layers on ogbn-product.</figDesc><table><row><cell></cell><cell>GraphSage GAT</cell></row><row><cell>baseline</cell><cell>78.27 79.23</cell></row><row><cell>NGNN -all</cell><cell>79.88 79.49</cell></row><row><cell>NGNN -input</cell><cell>79.81 78.87</cell></row><row><cell>NGNN -hidden</cell><cell>79.91 79.68</cell></row><row><cell>NGNN -output</cell><cell>78.60 78.45</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://ogb.stanford.edu/docs/leader_nodeprop/ 2 https://ogb.stanford.edu/docs/leader_linkprop/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The graph of ogbn-product has 61,859,140 edges. 5 http://snap.stanford.edu/graphsage/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open graph benchmark: Datasets for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Residual or gate? towards deeper graph neural networks for inductive graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08035</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<title level="m">Flag: Adversarial data augmentation for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spam review detection with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runshi</forename><surname>Zhou Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2703" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07476</idno>
		<title level="m">Training Graph Neural Networks with 1000 Layers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alleviating the inconsistency problem of applying graph neural network to fraud detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1569" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10637</idno>
		<title level="m">Temporal graph networks for deep learning on dynamic graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive graph diffusion networks with hop-wise attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15024</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fdgars: Fraudster detection via graph convolutional networks in online app review system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of The 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Node Classification with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13355</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-propagation Graph Neural Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01380</idno>
		<title level="m">Deep Graph Neural Networks with Shallow Subgraph Samplers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">This figure &quot;sample-franklin.png&quot; is available in &quot;png</title>
		<ptr target="http://arxiv.org/ps/2111.11638v1" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
