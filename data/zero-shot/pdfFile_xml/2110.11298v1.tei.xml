<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video and Text Matching with Conditioned Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameen</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netapp</forename><forename type="middle">Tamir</forename><surname>Hazan</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video and Text Matching with Conditioned Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for matching a text sentence from a given corpus to a given video clip and vice versa. Traditionally video and text matching is done by learning a shared embedding space and the encoding of one modality is independent of the other. In this work, we encode the dataset data in a way that takes into account the query's relevant information. The power of the method is demonstrated to arise from pooling the interaction data between words and frames. Since the encoding of the video clip depends on the sentence compared to it, the representation needs to be recomputed for each potential match. To this end, we propose an efficient shallow neural network. Its training employs a hierarchical triplet loss that is extendable to paragraph/video matching. The method is simple, provides explainability, and achieves state-of-the-art results for both sentence-clip and video-text by a sizable margin across five different datasets: Activi-tyNet, DiDeMo, YouCook2, MSR-VTT, and LSMDC. We also show that our conditioned representation can be transferred to video-guided machine translation, where we improved the current results on VATEX. Source code is available at https://github.com/AmeenAli/VideoMatch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The process of matching data from two domains is often seen as a sequential process of comparison between two static representations, obtained by preprocessing these domains. However, it is possible that a more effective way is to consider recent models for cognitive search, in which the content of the query signal from the first domain changes our perception of the data from the second domain.</p><p>In this work, we consider hierarchical representations of two domains, in which repeated interaction between the two domains modifies the representations of each data source. In particular, we consider the problem of matching multiclip video to multi-sentence text. The interaction-based representations are a form of attention. As seen in <ref type="figure">Fig. 1</ref>, when matching the sentence "fry onions until golden then add carrots and fry for 5 mins" to a cooking video segment, fry onions until golden then add carrots and fry for 5 mins Joint conditioned-embedding space fry onions until golden then add carrots and fry for 5 mins Joint embedding space ? ! ? ! <ref type="figure">Figure 1</ref>. Current approaches use a joint embedding space for efficient video and text retrieval via a dot-product (illustrated in the top row). However, retrieving the relevant video to a given text query requires a concise representation that consists of only the relevant frames. For this reason, we propose a new conditioned embedding that encodes all the videos conditioned on a given query. On the bottom row, we highlight with red color the relevant words given the video clip. We also mark with a red border the relevant frames given the text query. Note the query is "fry onions ... add carrots ...", and the selected relevant frames show onion and carrots, while the non-relevant frame shows meat. the frames showing onions, or carrots are highlighted in the video representation. Focusing on the video and text domains enables us to benefit from the extensive literature on image and text embedding, as well as from the existence of multiple existing benchmarks.</p><p>We represent the video frames with a pre-trained CNN and the words using the GloVe <ref type="bibr" target="#b29">[30]</ref> representation, both of which are standard. Then, a temporal context is added to the video by applying a unidirectional GRU <ref type="bibr" target="#b5">[6]</ref>. This is done at the clip level for video and the sentence level for the text. Given a video clip and a possibly-matching sentence, we compute an affinity score between every frame (clip) and word (sentence) by projecting the domain-specific representations to a shared vector space, in which the cosine similarity is employed.</p><p>The individual similarities are aggregated to define per frame (per word) weights, which are used to obtain one weighted-sum representation of the video clip (sentence). The video clips and sentences are then each processed by another pair of GRUs to create another layer of representation, whose initial state depends on information collected from the entire video and text paragraph. Max pooling is then employed to obtain a single vector representing each video (all clips) and each paragraph (all sentences). These representations are projected to a joint vector space, in which they are compared.</p><p>Since the encoding of the video clip depends on the sentence it was compared to, the representation needs to be recomputed for each potential match. While this does not affect the asymptotic complexity of the search process, it does require applying the encoding mechanism N 2 times, instead of 2N times, where N is the number of videos. However, in practice, we had no challenge running on the existing benchmarks, using a shortlist obtained using static embeddings.</p><p>The new method is able to outperform by a sizable margin the state of the art results for the cross-modality text/video retrieval on a diverse set of video to text matching benchmarks, including ActivityNet <ref type="bibr" target="#b16">[17]</ref>, DiDeMo <ref type="bibr" target="#b12">[13]</ref>, YouCook2 <ref type="bibr" target="#b49">[50]</ref> , Vatex <ref type="bibr" target="#b37">[38]</ref> , MSR-VTT <ref type="bibr" target="#b41">[42]</ref> and LSMDC <ref type="bibr" target="#b30">[31]</ref>. The robustness of our method is further demonstrated by its application to the task of video-guided based translation on Vatex, surpassing the recent baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the task of video/text retrieval, given a set of video descriptions, we rank them according to video relevancy. Early works clustered videos and captions by considering metadata, video tags, and video features(i.e., SIFT, BoW) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref>. Afterward, a model that finetunes video caption generation model was proposed <ref type="bibr" target="#b16">[17]</ref>. However, a dedicated matching model significantly improved the results <ref type="bibr" target="#b47">[48]</ref>. We follow the same strategy but improve in two aspects: (i) we condition the video and paragraph embeddings, and (ii) we use a weighted interaction score, instead of a dot product. The video-paragraph retrieval relies on dense clip annotations, which are not always available. Therefore, instead, recent methods focus on the sentence-video retrieval variant. Liu et al. <ref type="bibr" target="#b20">[21]</ref> utilize different experts such as speech, audio, motion, OCR, appearance, and face detection. Gabeur et al. <ref type="bibr" target="#b9">[10]</ref>, improved this approach by leveraging transformers networks. Our approach is much more straightforward and considers only the video and the text. Lei et al. <ref type="bibr" target="#b17">[18]</ref> suggested the use of just a few sparsely sampled clips for matching. Patrick et al. <ref type="bibr" target="#b28">[29]</ref> propose incorporating a generator and a contrastive loss to consider semantically-related negative samples better. Different from all the methods mentioned above, we are the first to study the power of conditioned video/clip embedding, which results in a significant performance improvement. Recently, the HowTo100M dataset has been used as a basis for training large-scale approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b22">23]</ref>. Notably, out approach remains competitive even when compared to pre-trained approaches, despite the significant increase in data pre-training methods use.</p><p>In temporal grounding, where the task is to localize a segment in the video that semantically corresponds to the given sentence. Most of the existing approaches to this problem use dense supervision of event locations for training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. These annotations are not always available and recent methods, such as Mithun et al. focus on weakly-supervised temporal grounding by leveraging text-to-frame attention scores <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. The most relevant to our approach, Yuan et al. <ref type="bibr" target="#b46">[47]</ref> uses co-attention for video localization. However, our co-attention is simpler as it works in a parallel manner, while Yuan et al. infers attention in an alternating manner, i.e., the attention is computed multiple times sequentially.</p><p>Hierarchical encoding deals with long text (i.e., paragraph) or a long video with multiple clips. Li et al. <ref type="bibr" target="#b18">[19]</ref> propose encoding a paragraph with a hierarchical autoencoder. Following, a hierarchical RNN was proposed <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45]</ref>. Niu et al. <ref type="bibr" target="#b26">[27]</ref> leverage the hierarchy of a sentence parse tree to match a sentence with an image. Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed using hierarchical graph encoding on the local and global level of the data. Similar to our approach, Zhang et al. <ref type="bibr" target="#b47">[48]</ref> embed video and paragraph in a two-level hierarchical manner. The first level embeds the clips and sentences, followed by a second level that embeds the video and paragraph. However, the embeddings are independent. In this work, we demonstrate that conditioning the clip embedding on the matched sentence, and vice versa significantly improves the performance.</p><p>Multimodal Attention has been a prominent tool for modeling the interaction between the two domains, since it models interactions in a way that selects important frames and words. We follow the same practice and use efficient interaction-based attention as a tool for conditioning. In early work, image attention was proposed for improved image captioning <ref type="bibr" target="#b42">[43]</ref>. This idea was later extended to visual question answering <ref type="bibr" target="#b40">[41]</ref>. Other approaches improved the vectorfusion operator for better interaction modeling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. Some approaches imitate a multi-step reasoning with stacked attention modules sequentially <ref type="bibr" target="#b43">[44]</ref>. Lu et al. <ref type="bibr" target="#b21">[22]</ref> proposed a co-attention module that attends to visual and textual modalities jointly. A more advanced approach for co-attention used a bilinear module that efficiently generates attention for every pair <ref type="bibr" target="#b14">[15]</ref>. Next, a general factor-based framework that allows joint attention to any number of modalities was proposed <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>. In recent work, the factor graph attention is used to allow interaction between video frames or list of images <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Learning a similarity measure between text-to-video allows an efficient video retrieval of a video given text. In the following, we propose to learn from training data a similarity score between a video v ? V , and a paragraph p ? P . The training dataset S = {(v 1 , p 1 ), . . . , (v n , p n )} consists of n video samples and matching paragraphs. Each video sample v i ? V is a sequence of clips v i = {c i 1 , . . . , c i ni } and each clip j is composed of frames c i j = {f ij 1 , . . . , f ij nij }. The matching paragraph of a video/paragraph pair (v i , p i ) consists of a sequence of sentences p i = {s i 1 , . . . , s i ni }, while for any j = 1, ..., n i the sentence s i j corresponds to the clip c i j . The sentence itself is composed of words s i j = {w ij 1 , . . . , w ij mij } where m ij is the number of words in the j-th sentence s i j of paragraph p i . We note that although the j-th clip and the j-th sentence, of the a matching video/paragraph pair of any index i, are matched, their words and frames do not match, and the number of frames in a clip n ij often differ from the number of words m ij in its corresponding sentence.</p><p>Our training procedure is learning the similarity of paired video and paragraph (v i , p i ) ? S by embedding both v i and p i in a joint space R dvp . Each such training pair should lie close together in the joint space, while embedding a paragraph and a video that do not match should lie far apart. The embedding of the video and paragraph are dependent on each other. To search for the matching paragraph of a given video, we consider the pairwise Euclidean distances between the embedding of the video given each possible paragraph and the embedding of each paragraph given the video. The same process, in the other direction, is used for searching for the matching video given a paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint Clip and Sentence Encoding</head><p>The dataset S = {(v i , p i )} n i=1 contains paired videos and corresponding paragraphs that are describing the videos. These are embedded in a bottom-up manner that matches the hierarchical structure of the data. In the video pathway, the frames f ij k of the clip c i j = {f ij 1 , . . . , f ij nij } are encoded using a pre-trained CNN, followed by a GRU-type RNN GRU c which captures the temporal context of the n ij frames of video clip c i j :</p><formula xml:id="formula_0">(f ij 1 , . . . ,f ij nij ) = GRU c CNN(f ij 1 ), . . . , CNN(f ij nij ) . (1) Similarly, the m ij words w ij k of the sentence s i j = {w ij 1 , .</formula><p>. . , w ij mij } are encoded by GloVe <ref type="bibr" target="#b29">[30]</ref> followed by an RNN</p><formula xml:id="formula_1">(w ij 1 , . . . ,w ij mij ) = GRU s GloVe(w ij 1 ), . . . , GloVe(w ij mij ) ,<label>(2)</label></formula><p>where GRU s is the sentence RNN whose sequence is composed of words. Both GRUs have a single layer of GRU cells and an output of size d e .</p><p>Up until this stage, the video and text pathways have been independent. Next, we apply our conditioning mechanism in order to highlight individual frames that are related to the words of the sentence and vice versa as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. For this purpose, we define the following matrix of interaction scores, which is given as a normalized dot product:</p><formula xml:id="formula_2">I ii ? jj ? kk ? = ? ? Af ij k Af ij k ? ? ? ? ? Bw i ? j ? k ? Bw i ? j ? k ? ? ? ,<label>(3)</label></formula><p>where I ? R nij ?m i ? j ? and A ? R de?de , B ? R de?de are learned matrices that re-embed the frames and the words to their joint space R de . The indices of the video/clip/frame i, j, k are distinguished from those of the paragraph/sentence/word i ? , j ? , k ? . We define the marginal interaction potential over frames and words to be</p><formula xml:id="formula_3">? c (k|c i j , s i ? j ? ) = k ? I ii ? jj ? kk ? , ? s (k ? |c i j , s i ? j ? ) = k I ii ? jj ? kk ? .</formula><p>The potentials are then transformed, using softmax, to a conditional probability distribution over frames and words given their clip and sentence.</p><formula xml:id="formula_4">? c (k|c i j , s i ? j ? ) ? exp(? c (k|c i j , s i ? j ? )), ? s (k ? |c i j , s i ? j ? ) ? exp(? s (k ? |c i j , s i ? j ? )).<label>(4)</label></formula><p>With these marginal probabilities, we entangle the clips and the sentences by reducing their representation, namely, summarize the clip representation by combining its sentencerelated frames and the sentence representation by its cliprelated words:</p><formula xml:id="formula_5">c ii ? jj ? = mij k=1 ? c (k|c i j , s i ? j ? )f ij k , s i ? i j ? j = n i ? j ? k ? =1 ? s (k ? |c i j , s i ? j ? )w i ? j ? k ? .<label>(5)</label></formula><p>Since these representations are derived from the joint interactions space of clips and sentences, the resulting embeddings of c ii ? jj ? and s ii ? jj ? are conditioned on each other. The next section describes an additional encoding hierarchy that enables video and paragraph matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Video and Paragraph Encoding</head><p>We find it easy to extend out conditioned representation to the matching of a video and a paragraph. Given interaction-conditioned representation. I.e., video's clips</p><formula xml:id="formula_6">{c ii ? 11 , . . . , c ii ? n ii ? n i ? i } and paragraph's sentences {s i ? i 11 , . . . , s i ? i n i ? i n ii ? }.</formula><p>We encode them with a pair of GRUs where n ii ? = max(n i ? , n i )</p><formula xml:id="formula_7">(? ii ? 1 , . . . ,? ii ? n ii ? ) = GRU v (c ii ? 11 , . . . , c ii ? n ii ? ) , (? i ? i 1 , . . . ,? i ? i n ii ? ) = GRU p (s i ? i 11 , . . . , s i ? i n i ? i n ii ? ) .<label>(6)</label></formula><p>In case the number of clips and the number of sentences are not equal, we pad with a zero vector the missing representations.  of the paragraph p i ? and the sentence s i ? j ? , are processed in the context of each other. A similarity matrix I is computed based on a dot-product. We calculate interaction potential scores ?c and ?s by marginalization. We then produce the conditioned embeddings c ii ? jj ? and s i ? i j ? j based on the interaction potential score.</p><p>Notably, to this point, each clip and sentence interact locally. In other words, objects in a clip only interact with words in the corresponding sentence and vice versa. However, we find it beneficial to also condition the video and paragraph in a global context, i.e., words/objects interact with non-corresponding sentence/clip. To this end, we sample n f frames from the entire video, along with the words of the entire paragraph. We then apply the same process as in Eq. 3 -Eq. 5, treating the sampled frames and words as a single clip and a single sentence. The only difference is that we employ a different set of matrices, instead of A and B (Eq. 3), which we denote by A 0 ? R de?de and B 0 ? R de?de . We use the interaction-conditioned global video embeddings, denoted as v 0</p><p>ii ? as the RNN GRU v 's initial state. Similarly, we denote the global paragraph embedding p 0</p><p>ii ? , which is used to initialize the RNN GRU p .</p><p>Finally, to co-dependently embed the entire video v i and the entire paragraph p i , max-pooling is applied across all video clips and paragraph sentences, respectively:</p><formula xml:id="formula_8">v ii ? = MaxPool ? ii ? 1 , . . . ,? ii ? n ii ? , p i ? i = MaxPool ? i ? i 1 , . . . ,? i ? i n ii ? .<label>(7)</label></formula><p>MaxPool MaxPool <ref type="figure">Figure 3</ref>. Our second hierarchy for the embedding and then matching a video and a paragraph based on the interaction-conditioned representations of the video's clips and the paragraph's sentences. Note that we employ this hierarchy only in the case of paragraph and video matching.</p><p>The architecture of the processing at the video and paragraph level is summarized in <ref type="figure">Fig. 3</ref>.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The loss employed is based on the triplet loss of the video and paragraph embeddings together with an auxiliary loss that also takes the form of a triplet loss. Recall that for matching videos and paragraphs (v i , p i ), the j-th clip c i j matches the j-th sentence s i j . For this purpose, we learn another pair of matrices U and V to define a match score:</p><formula xml:id="formula_9">M(x, y) = U x ?U x? ? V y ?V y? .<label>(8)</label></formula><p>We note that for the video/paragraph case, U and V are only used during training and do not play a role in matching in inference time. In total, we learn three normalized dotproduct cross-domain interactions. In Eq. 3 we learn A and B at the frame and word level. Similarly, we learn A 0 and B 0 for matching frames sampled from entire-video and words sampled from the entire-paragraph. Last, we learn U and V at the clip and sentence level.</p><p>In this metric learning task, positive pairs are obtained by co-embedding corresponding video and paragraphs (index i) using the corresponding clip/sentence (index j). Negative pairs are obtained using either unmatching clips and sentences, introducing a second index j ? ? = j, or using entirely different video and paragraph, denoted by the index i ? ? = i. The mini-batches are constructed by considering b pairs of matching videos and paragraphs. Then, all non-matching pairs in the mini-batch are considered as negative samples.  <ref type="bibr" target="#b30">[31]</ref>, and MSR-VTT <ref type="bibr" target="#b41">[42]</ref>. Shown is the recall at a certain number of retrievals and MdR=median rank. We gray out models that used large-scale pre-training on HowTo100M <ref type="bibr" target="#b23">[24]</ref>. We mark with ? models that used appearance, scene, motion, face, audio, OCR, ASR features from 11 different models. ? indicates models that used appearance and motion. (*) denotes models use 2-second clips instead of the default 1-second clips.</p><p>The loss is given by:</p><formula xml:id="formula_10">L = b i=1 i ? ? =i ReLU(?v ii ? p ii ? ? ?v ii ? ? p i ? i ? + ? 1 ) + b i=1 ni j=1 j ? ? =j ReLU(M(c ii jj , s ii jj ) ? M(c ii jj ? , s ii j ? j ) + ? 2 ) + b i=1 i ? ? =i ni j=1 n i ? j ? =1 ReLU(M(c ii jj , s ii jj )?M(c ii ? jj ? , s ii ? j ? j )+? 2 ),<label>(9)</label></formula><p>where ? 1 and ? 2 are two margin parameters. The loss has three parts. The first compares the representation of entire video i and considers the matching paragraph, and an unmatched paragraph obtained as the matching paragraph of another video i ? . Note that this term only applies in the case of video/paragraph matching. The second term is at the level of clips and sentences. It considers matching clips and sentences (matching video and paragraph, both with index i, and same clip and sentence, both with sentence j) and unmatching sentences of the same paragraph (same i different j ? ? = j). The third term is similar, only the unmatched sentences are taken from an unmatched paragraph (different i ? ? = i and all possible j ? ). Note, we pad with zeros the  <ref type="table">Table 2</ref>. Video/Paragraph retrieval on ActivityNet <ref type="bibr" target="#b16">[17]</ref>, DiDeMo <ref type="bibr" target="#b12">[13]</ref> , YouCook2 <ref type="bibr" target="#b49">[50]</ref> and MSR-VTT <ref type="bibr" target="#b41">[42]</ref>. Shown is the recall at a certain number of retrievals and MdR=median rank. long video/paragraph to have an equal length as the shorter video/paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Test Time Similarity Computation</head><p>To evaluate our video retrieval capabilities, we compute a similarity score S between every clip/sentence or video/paragraph pair. Given a clip c i , we use the trained matching operator M (see <ref type="formula" target="#formula_9">(8)</ref>), i.e.,</p><formula xml:id="formula_11">S(c i j , s i ? j ? ) = M c ii ? jj ? , s i ? i j ? j .<label>(10)</label></formula><p>Note, for video/sentence matching, all clips are concatenated into one clip. In the case of video paragraph retrieval, given a paragraph p i ? , we calculate the similarity score based on a dot product between their dependent representation as follows:</p><formula xml:id="formula_12">S(v i , p i ? ) = exp(??v ii ? ? p i ? i ?).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our video and text matching method on a variety of datasets containing video clips with corresponding descriptions. To demonstrate the versatility of our model, we perform a comprehensive set of experiments in a diverse set of domains, such as movies, personal collections, and commercials. In addition to the matching task, we also explore weakly supervised localization in video and even video-guided machine translation. Datasets: We employ five different datasets, each having a different distribution of videos and a different descriptive style. For example, Activi-tyNet focuses on actions ("The girl dances around the room while the camera captures her movements.") while YouCook descriptions capture the order of very specific actions ("combine lemon juice sumac garlic salt and oil in a bowl"). The datasets we employ are listed next. ActivityNet Dense Caption <ref type="bibr" target="#b16">[17]</ref> is a human activities video dataset, where each video has multiple events. The events are separated into clips, and each clip has a corresponding description. There are 849 video hours with 100,000 total sentences, giving rise to 10,009 videos with matching paragraphs for the training set, and 4,917/4,885(val1/val2) for the validation set. We follow previous works and report our results on val1. Results on val2 are available in the supplementary. Distinct Describable Moments (DiDeMo) <ref type="bibr" target="#b12">[13]</ref> is collected from diverse personal videos and include everyday indoor scenes, nature scenes, events etc. The dataset consists of over 10,000 unedited videos. Each video comes with several actions described in natural language. The data is split into 8,395 training videos, 1,065 validation videos and 1,004 test videos. We report our results on the test-set. YouCook2 [50] contains 2,000 long untrimmed videos capturing the preparation of 89 cooking recipes, where each distinct recipe has 22 videos on average. The videos are provided with temporal boundaries and description sentences in imperative English. The dataset is split into a train set of size 1,500 and a test set with 500 videos. Vatex <ref type="bibr" target="#b37">[38]</ref> contains 41,250 videos with 825,000 multilingual (English and Chinese) captions and includes over 600 fine-grained human activities. MSR-VTT <ref type="bibr" target="#b41">[42]</ref> contains a total of 10K videos, each having 20 text descriptions. This dataset's standard split comprises 6,576 videos for training, 497 videos for validating, and 2,990 videos for testing. LSMDC <ref type="bibr" target="#b30">[31]</ref> contains 118,081 short videos extracted from 202 movies, each described by a text caption extracted from the movie or the audio description. The test set is composed of 1000 videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video/Sentence Retrieval</head><p>In this task, we match a single sentence and a video without clips. In datasets with clips, we treat each video as a single clip. Similarly, we concatenate all the captioning sentences into a single caption. In Tab. 1, we compare our method the state-of-the-art baselines. The baselines include the method of Liu et al. <ref type="bibr" target="#b20">[21]</ref>, who suggested using many video-expert networks, such as speech, audio, motion, OCR, appearance, and face detection. Another recent baseline is the Multimodal Transformer (MMT) <ref type="bibr" target="#b9">[10]</ref>, which encodes all the expert networks' representations jointly with selfattention. Our method, which considers only the appearance and text embeddings and does not use external knowledge, is much more straightforward, and also outperforms all those methods that did not use conditioned embeddings.</p><p>The most recent work clipBERT <ref type="bibr" target="#b17">[18]</ref> study different frame sampling strategies. The approach we propose outperforms clipBERT on both ActivityNet and MSR-VTT by 4%, despite clipBERT's use of motion features. Notably, our technique outperforms pre-trained versions except for MMT, which also relies on many video-experts networks.</p><p>We also compare the efficiency of our architecture in terms of number of parameters. In total, our model has only 40.2mil parameters, while clipBERT, a Transformer-based network, has 199.3mil parameters. Additionally, MMT that  <ref type="table">Table 3</ref>. An ablations study on ActivityNet.</p><p>considers various experts networks has 133.3mil parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video/Paragraph Retrieval</head><p>Our next task is to match videos with descriptive paragraphs. Compared to the video/sentence retrieval, this variant uses clips segmentation and corresponding sentences. Each corresponding clip and sentence are conditioned separately. Note that we also employ global conditioning (i.e., the global video and paragraph embeddings). The second hierarchy is used to handle the conditioned clips and sentences representations (see Sec. 3.2). Tab. 2 reports the results of our method in comparison to the results collected from the literature as well as HSE results we ran, ourselves, on YouCook2 , and MSRT-VTT. HSE <ref type="bibr" target="#b47">[48]</ref> also uses hierarchy encoding. Our loss is considerably simpler than that of HSE. Moreover, our approach leverage conditioned embeddings. Our method obtains an improvement in R@1 over HSE result in both matching directions of 14% in ActivityNet, 9.6% in DiDeMo, 7.5% in YouCook2 and 5.3% in MSR-VTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation study</head><p>In Tab. 3 we assess several design choices evaluated on video and paragraph retrieval setup: 1) We examine the importance of different architecture components. In 'w/o 2nd H', we used only one hierarchy by treating the output of our global embedding as the conditioned sentence and video representations (i.e., v 0</p><p>ii ? , p 0 ii ? ). While the performance dropped significantly, this score is impressive because the model does not take into account the events' temporal locations (i.e., clips). In 'w/o M match', we evaluate our model with a normalized dot-product instead of our trainable matching module M . Last, in 'w/o-global', we do not use a global video and paragraph embeddings to initialize GRU v and GRU p . In this case, the performance drops by 1% on R@1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall, we show that all the integral components in our</head><p>There is one person in front of the woman riding a camel.</p><p>A woman is working out on an elliptical machine. People are doing tricks on a skateboard.</p><p>Children ride on a camel guided by a trainer in front in a parking lot area.</p><p>Two women are standing outside the bathtub spraying the dog with a hose.</p><p>Then she dries the dog off using a towel and blow dries the dog and combs it's hair. Low-relevance frames High-relevance frames <ref type="figure">Figure 4</ref>. Illustration of queries, relevancy of frames ?c and words ?s derived from Eq. (4). We show results on ActivityNet (1st, 2nd, 5th rows), YouCook2 (3rd row), DiDeMo (6th row), MSR-VTT (4th row). Each row displays two queries and a clip from the retrieved video.</p><p>In the clip, we show two of the most relevant frames to the query (marked with red borders) and two of the least relevant frames (marked with blue borders). We also illustrated relevant words to the clip using color maps. Note, our approach also serves the additional benefit of describing the specific frames the model uses for retrieval.</p><p>architecture contribute to the performance. 2) We assess our conditioning approach. We treat it as a form of co-attention, i.e., picking the relevant frames and words. In 'w/o-atten', we show the importance of attention, omitting the attention leads to a significant drop in performance (47.4% vs. 58.2%).</p><p>In 'w/o text attn', we infer the clip's attention as usual, but instead of text attention, we performed a max-pooling across the sentence representation. As a result, the performance drops by 5.3% on R@1. Similarly, in 'w/o frame attn', we removed the clips' attention, which leads to a significant performance drop of 7% on R@1. In 'w/o global attn', we use max polling for the global vector instead of attention, which leads to a drop of 2% on R@1. In 'alternative attn', we replace our attention with that of <ref type="bibr" target="#b46">[47]</ref>, which leads to a 2.6% drop on R@1. Additionally, our version runs 36% faster. 3) Removing the auxiliary clip and sentence matching drops the performance by 7.2% on R@1. 4) We change our main embedding parameter d h . It is evident that our method is stable to this parameter in terms of Recall@k metric and it outperforms state of the art (Tab. 2) for multiple values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative analysis</head><p>In <ref type="figure">Fig. 4</ref> we use the interaction scores (see Eq. (4)) to identify the relevant frames and words selected by conditioning. Our approach simplifies explanations since finding the frames responsible for retrieving a specific long video can be challenging. In the first step, we highlight the words' relevance through a heatmap. We find that most of the relevant words are grounded nouns. For example, in the first row, the relevant words in the query are 'children,' 'camel,' 'trainer,' 'parking.' These are useful words for visual matching. Next, we randomly selected a clip from the retrieved video and displayed the top two frames that most closely match the query, as indicated by the red border.  <ref type="figure">Figure 5</ref>. Comparison of different shortlist sizes. The purpose of the shortlist set is to allow the user to reduce inference latency. Hence, we use a static retrieval strategy to construct a candidate set. Notably, the performance improves with increasing set size, but the inference latency increases (see Sec. 4.3). We find the size of 50, which we also used in our experiments, to yield a good trade-off between latency and performance.</p><p>from the MSR-VTT dataset. The top 2 frames in the left examples are related to 'girl' as highlighted in the query attention, and for the right example, we can see that the top 2 frames contain 'group of girls.' In the 3rd row, we show two queries from Youcook2 and find our approach to pick the relevant frames showing the query's ingredients (e.g., a sandwich, margarine, white bread). Lastly, in the 6th row, we show queries from DiDeMo. The attention behaves well by selecting relevant frames, e.g. a shark, and a bird.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference time and shortlist</head><p>For each inference call, our approach requires recomputing the representations. This computational burden can be alleviated through the use of a static method (e.g., HSE <ref type="bibr" target="#b47">[48]</ref>) to retrieve a shortlist of relevant candidates. In <ref type="figure">Fig. 5</ref>, we examine different shortlist sizes and find that a shortlist of 50 already allows most of the performance gain. We measured the inference time on ActivityNet in the paragraph/video task. The inference latency for a single inference call is ?0.4s for a shortlist of 5, ?1.1s for a shortlist of 10, ?1.8s for a shortlist of 50, and ?3.2s for a shortlist of 100. The overall inference time of ActivityNet test set for a shortlist of 50 is ?7 minutes, in comparison to ?50 minutes without a shortlist and ?50s using the HSE method as static approach. Note, inference of advanced static approaches also takes a longer time compared to HSE. For example, MSE <ref type="bibr" target="#b9">[10]</ref> inference takes about four minutes over the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Weakly Supervised Retrieval</head><p>In many scenarios, the supervision of the events' temporal location is not available. The weakly supervised setup is similar to the Video/Paragraph retrieval task, excluding the events segmentation. In Tab. 4, following <ref type="bibr" target="#b47">[48]</ref>, we report results obtained when breaking the paragraph into sentences, and replacing the ground-truth events in the Activi-tyNet dataset with equal-length non-overlapping temporal  segments. For the textual input, in case there are fewer segments than sentences, we truncate the paragraph. If there are more segments than sentences, we repeat the last sentence. The absence of ground-truth events and truncating the paragraph harms the performance (24% drop on R@1). Still, our model outperforms the previous baseline by 5.0-6.5% on R@1 depending on the number of segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Video-guided Machine Translation</head><p>Finally, following <ref type="bibr" target="#b37">[38]</ref>, we consider the task of Videoguided Machine Translation (VMT), in which a sentence in a source language is translated to a target language, using both the source-text and a matching video. Similar to the video/paragraph retrieval task, our solution starts with training a retrieval model to embed the video and text closer together. Since the videos are not segmented into clips, our model only utilizes the first hierarchy. We then fine-tune the video and sentence representations for the VMT, by using an LSTM decoder, followed by MLE loss on words. Like <ref type="bibr" target="#b37">[38]</ref>, our LSTM has 1024 cells and a beam search with a width of 5 was used for evaluation. As is evident from the results, shown in Tab. 5, our method outperforms the method of <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>For retrieval, it is convenient to embed the data in each domain statically, which can be preprocessed and stored for future use. Nevertheless, our method highlights the importance of interactions or the cross-domain context when embedding the data of the two domains that are being matched. Meanwhile, if complexity is an issue for a specific application, one can consider using a shortlist that is obtained using static embeddings. In the future, we believe that combining conditioned embeddings and large-scale pretraining can lead to a significant performance gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a flute in the large church.Another woman plays a flute in the large church.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An overview of our first hierarchy for clip and sentence embedding computation. The frames f ij k of the video vi and the clip c i j and the words w i ? j ? k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>shark comes close to the camera for first time camera turns to flying bird in the sky spread margarine on two slices of white bread She shows a panini sandwich on the screen.A judge talks to a little girl on the voice a group of girls mixing paint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The bottom two frames, are marked by blue borders. The bottom frames are usually irrelevant to the query. For instance, in the 2nd row, on the left side, the query is "... spraying the dog," but the bottom frames do not depict any dog. We also find the frames to be uninformative. In the first row, on the left, the woman's hand hides most of the details. In addition, clips often have frames with production logos, e.g., the bottom frames in 5th-row videos.We also show a complicated case of two similar queries. Notably, both queries in the first row deal with camels; in the second row, both queries deal with dogs; in the third row, both videos deal with bread sandwiches. Despite being similar, our model identified subtle details and retrieved the correct video. The 4th row shows samples</figDesc><table><row><cell></cell><cell>100</cell><cell>96.6</cell><cell>96.6</cell><cell>96.6</cell><cell>96.6</cell><cell>97.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R@1</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R@5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R@50</cell></row><row><cell>Score</cell><cell>60</cell><cell>49.5</cell><cell>49.5</cell><cell>53.5</cell><cell>58.8</cell><cell>59.1</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>20.5</cell><cell>21.9</cell><cell>23.0</cell><cell>25.2</cell><cell>25.4</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shortlist size</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>HSE 18.0% 45.5% 16.5% 44.9% Ours 21.9% 49.3% 21.5% 49.1% 2 HSE 20.0% 48.9% 18.4% 47.6% Ours 24.8% 55.6% 24.3% 54.6% 3 HSE 20.0% 48.6% 18.2% 47.9% Ours 24.9% 55.8% 24.7% 55.1% 4 HSE 20.5% 49.3% 18.7% 48.1% Ours 24.9% 55.9% 24.8% 55.3%</figDesc><table><row><cell></cell><cell cols="2">Text2Vid</cell><cell cols="2">Vid2Text</cell></row><row><cell>N Alg.</cell><cell>R@1</cell><cell>R@5</cell><cell>R@1</cell><cell>R@5</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 4. Video/Paragraph retrival using N fixed-length segments on</cell></row><row><cell cols="3">the text-truncated ActivityNet.</cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell>en?zh</cell><cell>zh?en</cell><cell></cell></row><row><cell cols="4">Vatex [38] 29.12% 26.42%</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">32.34% 28.11%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Video-guided translation between English (en) and Chinese (zh). BLEU-4 scores.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research, innovation programme (grant ERC CoG 725974), and Grant No 2019783 from the United States-Israel Binational Science Foundation (BSF).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video2text: Learning to annotate video content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards coherent visual storytelling with ordered image attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Braude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02180</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic proposal for activity localization in videos via sentence query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches. SSST-8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised dense event captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wslln: Weakly supervised natural language localization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multi-modal clustering method for web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCTCS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<title level="m">Hadamard product for low-rank bilinear pooling. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICMR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised video moment retrieval from text queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hierarchical multimodal lstm for dense visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Niket Tandon, and Bernt Schiele. A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">High-order attention models for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple baseline for audio-visual scene-aware dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Factor graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Latent graph co-attention network for weaklysupervised video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, highquality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multimodal fusion for video search reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
