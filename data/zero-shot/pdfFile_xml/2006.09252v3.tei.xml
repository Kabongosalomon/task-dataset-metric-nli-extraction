<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Twitter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Twitter</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often intimately related to downstream tasks. To this end, we propose "Graph Substructure Networks" (GSN), a topologically-aware message passing scheme based on substructure encoding. We theoretically analyse the expressive power of our architecture, showing that it is strictly more expressive than the WL test, and provide sufficient conditions for universality. Importantly, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We perform an extensive experimental evaluation on graph classification and regression tasks and obtain state-of-the-art results in diverse realworld settings including molecular graphs and social networks. The code is publicly available at https: //github.com/gbouritsas/graph-substructure-networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of graph representation learning has undergone a rapid growth in the past few years. In particular, Graph Neural Networks (GNNs), a family of neural architectures designed for irregularly structured data, have been successfully applied to problems ranging from social networks and recommender systems <ref type="bibr" target="#b0">[1]</ref> to bioinformatics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, chemistry <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and physics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, to name a few. Most GNN architectures are based on message passing <ref type="bibr" target="#b4">[5]</ref>, where the representation of each node is iteratively updated by aggregating information from its neighbours.</p><p>A crucial difference from traditional neural networks operating on grid-structured data is the absence of canonical ordering of the nodes in a graph. To address this, the aggregation function is constructed to be invariant to neighbourhood permutations and, as a consequence, to graph isomorphism. This kind of symmetry is not always desirable and thus different inductive biases that disambiguate the neighbours have been proposed. For instance, in geometric graphs, such as 3D molecular graphs and meshes, directional biases are usually employed in order to model the positional information of the nodes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>; for proteins, ordering information is used to disambiguate amino-acids at different positions in the sequence <ref type="bibr" target="#b13">[14]</ref>; in multi-relational knowledge graphs, a different aggregation is performed for each relation type <ref type="bibr" target="#b14">[15]</ref>.</p><p>The structure of the graph itself does not usually explicitly take part in the aggregation function. In fact, most models rely on multiple message passing steps as a means for each node to discover the global structure of the graph. However, since message-passing GNNs are at most as powerful as the Weisfeiler Leman test (WL) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, they are limited in their abilities to adequately exploit the graph structure, e.g. by counting substructures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. This uncovers a crucial limitation of GNNs, as substructures have been widely recognised as important in the study of complex networks. For example, in molecular chemistry, functional groups and rings are related to a plethora of chemical properties, while cliques are related to protein complexes in Protein-Protein Interaction networks and community structure in social networks, respectively <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, three major questions arise when designing GNN architectures: (a) How to go beyond isotropic, i.e., locally symmetric, aggregation functions ? (b) How to ensure that GNNs are aware of the structural chatacteristics of the graph? (c) How to achieve the above two without sacrificing invariance to isomorphism and hence the ability of GNNs to generalise?</p><p>In this work we attempt to simultaneously provide an answer to the above. We propose to break local symmetries by introducing structural information in the aggregation function, hence addressing (a) and (b). In particular, the contribution of each neighbour (message) is transformed differently depending on its structural relationship with the central node. This relationship is expressed by counting the appearance of certain substructures. Since substructure counts are vertex invariants, i.e. they are invariant to vertex permutations, it is easy to see that the resulting GNN will be invariant to isomorphism, hence also addressing (c). Moreover, by choosing the substructures, one can provide the model with different inductive biases, based on the graph distribution at hand.</p><p>We characterise the expressivity of our message-passing scheme, coined as Graph Substructure Network (GSN), showing that GSN is strictly more expressive than traditional GNNs for the vast majority of substructures, while retaining the locality of message passing, as opposed to higher-order methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref> that follow the WL hierarchy (see Section 2). In the limit, our model can yield a unique representation for every isomorphism class and is thus universal. We provide an extensive experimental evaluation on hard instances of graph isomorphism testing (strongly regular graphs), as well as on real-world networks from the social and biological domains, including the recently introduced large-scale benchmarks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. We observe that when choosing the structural inductive biases based on domain-specific knowledge, GSN achieves state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let G = (V G , E G ) be a graph with vertex set V G and edge set E G , directed or undirected. A subgraph G S = (V G S , E G S ) of G is any graph with V G S ? V G , E G S ? E G . When E G S includes all the edges of G with endpoints in V G S , i.e., E G S = E G ? V G S ? V G S , the subgraph is said to be induced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Isomorphism &amp; Automoprhism</head><p>Two graphs G, H are isomorphic (denoted H G), if there exists an adjacency-preserving bijective mapping (isomorphism) f : V G ? V H , i.e., (v, u) ? E G iff (f (v), f (u)) ? E H . Given some small graph H, the subgraph isomorphism problem amounts to finding a subgraph G S of G such that G S H. An automorphism of H is an isomorphism that maps H onto itself. The set of all the unique automorphisms forms the automorphism group of the graph, denoted as Aut(H), which contains all the possible symmetries of the graph.</p><p>The automorphism group yields a partition of the vertices into disjoint subsets of V H called orbits. Intuitively, this concept allows us to group the vertices based on their structural roles, e.g. the endpoint vertices of a path, or all the vertices of a cycle (see <ref type="figure" target="#fig_0">Figure 1</ref>). Formally, the orbit of a vertex v ? V H is the set of vertices to which it can be mapped via an automorphism: Orb(v) = {u ? V H | ?g ? Aut(H) s.t. g(u) = v}, and the set of all orbits H \ Aut(H) = {Orb(v) | v ? V H } is usually called the quotient of the automorphism when it acts on the graph H. We are interested in the unique elements of this set that we will denote as {O V H,1 , O V H,2 , . . . , O V H,d H }, where d H is the cardinality of the quotient.</p><p>Analogously, we define edge structural roles via edge automorphisms, i.e., bijective mappings from the edge set onto itself, that preserve edge adjacency (two edges are adjacent if they share a common endpoint). In particular, every vertex automorphism g induces an edge automorphism by mapping each edge (u, v) to (g(u), g(v)). <ref type="bibr" target="#b0">1</ref> In the same way as before, we construct the edge automorphism group, from which we deduce the partition of the edge set in edge orbits</p><formula xml:id="formula_0">{O E H,1 , O E H,2 , . . . , O E H,d H }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weisfeiler-Leman tests</head><p>The Weisfeiler-Leman graph-isomorphism test <ref type="bibr" target="#b27">[28]</ref>, also known as naive vertex refinement, 1-WL, or just WL), is a fast heuristic to decide if two graphs are isomorphic or not. The WL test proceeds as follows: every vertex v is initially assigned a colour c 0 v that is later iteratively refined by aggregating neighbouring information:</p><formula xml:id="formula_1">c t+1 v = HASH c t v , c t u u?Nv ,<label>(1)</label></formula><p>where ? denotes a multiset (a set that allows element repetitions) and N (v) is the neighbourhood of v. The WL algorithm terminates when the colours stop changing, and outputs a histogram of colours. Two graphs with different histograms are non-isomorphic; if the histograms are identical, the graphs are possibly, but not necessarily, isomorphic. Note that the neighbour aggregation in the WL test is a form of message passing, and GNNs are the learnable analogue.</p><p>A series of works on improving GNN expressivity mimic the higher-order generalisations of WL, known as k-WL and k-Folklore WL (WL hierarchy) and operate on k-tuples of nodes (see Appendix B.1). The (k + 1)-FWL is strictly stronger than k-FWL, k-FWL is as strong as (k + 1)-WL and 2-FWL is strictly stronger than the simple 1-WL test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Substructure Networks</head><p>Graphs consist of nodes (or edges) with repeated structural roles. Thus, it is natural for a neural network to treat them in a similar manner, akin to weight sharing between local patches in CNNs for images <ref type="bibr" target="#b28">[29]</ref> or positional encodings in language models for sequential data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Nevertheless, GNNs are usually unaware of the nodes' different structural roles, since all nodes are treated equally when performing local operations. Despite the initial intuition that the neural network would be able to discover these roles by constructing deeper architectures, it has been shown that GNNs are ill-suited for this purpose and are blind to the existence of structural properties, e.g. triangles or larger cycles <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To this end, we propose to explicitly encode structural roles as part of message passing, in order to capture richer topological properties. Our method draws inspiration from <ref type="bibr" target="#b32">[33]</ref>, where it was shown that GNNs become universal when the nodes in the graph are uniquely identified, i.e when they are equipped with different features. However, it is not clear how to choose these identifiers in a permutation equivariant way. Structural roles, when treated as identifiers, although not necessarily unique, are not only permutation equivariant, but also more amenable to generalisation due to their repetition across different graphs. Thus, they can constitute a trade-off between uniqueness and generalisation.  : Node (left) and edge (right) induced subgraph counting for a 3-cycle and a 3-path. Counts are reported for the blue node on the left and for the blue edge on the right. Different colors depict orbits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural features</head><formula xml:id="formula_2">x V H,i (v) = G S H v ? V G S , f (v) ? O V H,i } .<label>(2)</label></formula><p>Note that there exist |Aut(H)| different functions f that can map a subgraph G S to H, but any of those can be used to determine the orbit mapping of each node v. By combining the counts from different substructures in H and different orbits, we obtain the feature vector</p><formula xml:id="formula_3">x V v = [x V H1 (v), . . . , x V H K (v)] ? N D?1 of dimension D = Hi?H d Hi .</formula><p>Similarly, we can define edge structural features x E H,i (u, v) by counting occurrences of edge automorphism orbits:</p><formula xml:id="formula_4">x E H,i (u, v) = G S H (u, v) ? E G S , (f (u), f (v)) ? O E H,i ,<label>(3)</label></formula><p>and the combined edge features</p><formula xml:id="formula_5">x E u,v = [x E H1 (u, v), . . . , x E H K (u, v)</formula><p>]. An example of vertex and edge structural features is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure-aware message passing</head><p>The key building block of our architecture is the graph substructure layer, defined in a general manner as a Message Passing Neural Network (MPNN) <ref type="bibr" target="#b4">[5]</ref>, where now the messages from the neighbouring nodes also contain the structural information. In particular, each node v updates its state h t v by combining its previous state with the aggregated messages:</p><formula xml:id="formula_6">h t+1 v = UP t+1 h t v , m t+1 v (4) m t+1 v = ? ? ? ? ? ? ? ? ? ? ? M t+1 (h t v , h t u , x V v , x V u , e u,v ) u?N (v) (GSN-v) or M t+1 (h t v , h t u , x E u,v , e u,v ) u?N (v) (GSN-e),<label>(5)</label></formula><p>where UP t+1 is an arbitrary function approximator (e.g. a MLP), M t+1 is the neighborhood aggregation function, i.e. an arbitrary function on multisets (e.g., of the form u?N (v) MLP(?)) and e u,v are the edge features. The two variants, named GSN-v and GSN-e, correspond to vertex-or edge-counts, respectively, which are analogous to absolute and relative positional encodings in language models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>It is important to note here that contrary to identifier-based GNNs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> that obtain universality at the expense of permutation equivariance (since the identifiers are arbitrarily chosen with the sole requirement of being unique), GSNs retain this property, hence they are by construction ivariant to isomorphism. This stems from the fact that the process generating our structural identifiers (i.e. subgraph isomorphism) is permutation equivariant itself (proof provided in the Appendix A.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How powerful are GSNs?</head><p>We now turn to the expressive power of GSNs in comparison to MPNNs and the WL tests, a key tool for the theoretical analysis of the expressivity of graph neural networks so far. Since GSN is a generalisation of MPNNs, it is easy to see that it is at least as powerful. Importantly, GSNs have the capacity to learn functions that traditional MPNNs cannot learn. The following observation derives directly from the analysis of the counting abilities of the 1-WL test <ref type="bibr" target="#b17">[18]</ref> and its extension to MPNNs <ref type="bibr" target="#b18">[19]</ref> (for proofs, see Appendices A.2-A.4).</p><p>Theorem 3.1. GSN is strictly more powerful than MPNN and the 1-WL test when one of the following holds:</p><p>? H is any graph except for star graphs of any size, and structural features are inferred by subgraph matching, i.e. we count all subgraphs</p><formula xml:id="formula_7">G S ? = H for which it holds that E G S ? E G . Or,</formula><p>? H is any graph except for single edges and single nodes, and structural features are inferred by induced subgraph matching, i.e. we count all subgraphs G S ? = H for which it holds that</p><formula xml:id="formula_8">E G S = E G ? V G S ? V G S .</formula><p>Proof. It is easy to see that GSN model class contains MPNNs, and is thus at least as expressive. We can also show that GSN is at least as expressive as the 1-WL test by repurposing the proof of Theorem 3 in <ref type="bibr" target="#b15">[16]</ref> (see Appendix A.2)</p><p>Given the first part of the proposition, in order to show that GSNs are strictly more expressive than the 1-WL test, it suffices to show that GSN can distinguish a pair of graphs that 1-WL deems isomorphic. <ref type="bibr" target="#b17">[18]</ref> showed that 1-WL, and consequently MPNNs, can count only forests of stars. Thus, if the subgraphs are required to be connected, then they can only be star graphs of any size (note that this contains single nodes and single edges). In addition, <ref type="bibr" target="#b18">[19]</ref> showed that 1-WL, and consequently MPNNs, cannot count any connected induced subgraph with 3 or more nodes, i.e. any connected subgraph apart from single nodes and single edges.</p><p>If H is a substructure that 1-WL cannot learn to count, i.e. the ones mentioned above, then there is at least one pair of graphs with different number of counts of H, that 1-WL deems isomorphic. Thus, by assigning counting features to the nodes/edges of the two graphs based on appearances of H, a GSN can obtain different representations for G 1 and G 2 by summing up the features. Hence, G 1 , G 2 are deemed non-isomorphic. An example is depicted in <ref type="figure">Figure 2</ref> (left), where the two non-isomorphic graphs are distinguishable by GSN via e.g. cycle counting, but not by 1-WL.</p><p>Universality. A natural question that emerges is what are the sufficient conditions under which GSN can solve graph isomorphism. This would entail that GSN is a universal approximator of functions defined on graphs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref>. To address this, we can examine whether there exists a specific substructure collection that can completely characterise each graph. As of today, we are not aware of any results in graph theory that can guarantee the reconstruction of a graph from a smaller collection of its subgraphs. However, the Reconstruction Conjecture <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, states that a graph with size n ? 3 can be reconstructed from its vertex-deleted subgraphs (proven for n ? 11 <ref type="bibr" target="#b41">[42]</ref>). Consequently, (proof in the Appendix A.3):  GSN-v vs GSN-e. We can also examine the expressive power of the two proposed variants. A crucial observation that we make is that for each graph H in the collection, the vertex structural identifiers can be reconstructed by the corresponding edge identifiers. Thus, we can show that for every GSN-v there exists a GSN-e that can simulate the behaviour of the former (proof provided in the Appendix A.4).</p><p>Theorem 3.3. For a given subgraph collection H, let C V the set of functions that can be expressed by a GSN-v with arbitrary depth and with, and C E the set of functions that can be expressed by a GSN-e with the same properties. Then, it holds that C E ? C V , or in other words GSN-e is at least as expressive as GSN-v.</p><p>Comparison with higher-order WL tests. Finally, the expressive power of GSN can be compared to higher-order versions of the WL test. In particular, for each k-th order Folklore WL test in the hierarchy, it is known that there exists a family of graphs that will make the test fail. These are known in the literature as k-isoregular graphs <ref type="bibr" target="#b42">[43]</ref>, and the most well-known example is the Strongly Regular (SR) graph family, for k = 2 (more details can be found in Appendix B).</p><p>Hence, if we can find a substructure collection that allows GSN to distinguish certain pairs from these families, then this guarantees that the corresponding k-FWL test is no stronger than GSN. In this work, we identify such counterexamples for the 2-FWL test. Formally:</p><formula xml:id="formula_9">Proposition 3.4.</formula><p>There exist substructure families with O(1) size, i.e., independent of the size of the graph n, such that 2-FWL is no stronger than GSN.</p><p>We provide numerous counterexamples that prove this claim. <ref type="figure">Figure 2</ref> (right) provides a typical pair of SR graphs that can be distinguished with a 4-clique, while in section 5.1 this is extended to a large-scale study, where other constant size substructures (paths, cycles and cliques) can achieve similar results.</p><p>Although it is not clear if there exists a certain substructure collection that results in GSNs that align with the WL hierarchy, we stress that this is not a necessary condition in order to design more powerful GNNs. In particular, despite the increase in expressivity, k-WL tests are not only more computationally involved, but they also process the graph in a non-local fashion. However, locality is presumed to be a strong inductive bias of GNNs and key to their excellent performance in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">How to choose the substructures?</head><p>Expressivity. The Reconstruction Conjecture provides a sufficient, albeit impractical condition for universality. This motivates us to analyse the constant size case k = O(1) for practical scenarios, similar to the argument put forward for hard instances of graph isomorphism (Proposition 3.4).</p><p>In particular, one can count only the most discriminative subgraphs, i.e. the ones that can achieve the maximum possible vertex disambiguation, similarly to identifier-based approaches. Whenever these subgraph counts can provide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type="bibr" target="#b32">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the number of subgraphs needed for unique, or near-unique identification, are far fewer than those dictated by Corollary 3.2. This is consistent with our experimental findings, where we observed that certain small substructures such as paths and trees, significantly improve vertex disambiguation, compared to the initial vertex features (see <ref type="figure" target="#fig_5">Figure 5</ref> (left) and <ref type="table" target="#tab_3">Table 2</ref> in the appendix). As expected this allows for better fitting of the training data, which validates our claim that GNN expressivity improves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalisation.</head><p>However, none of the above claims can guarantee good generalisation in unseen data. For example, in <ref type="figure" target="#fig_5">Figure 5</ref>, we observe that the test set performance does not follow the same trend with train performance when choosing substructures with strong vertex disambiguation. Aiming at better generalisation, it is desirable to make use of substructures for which there is prior knowledge of their importance in certain network distributions and have been observed to be intimately related to various properties. For example, small substructures (graphlets) have been extensively analysed in protein-protein interaction networks <ref type="bibr" target="#b43">[44]</ref>, triangles and cliques characterise the structure of ego-nets and social networks in general <ref type="bibr" target="#b19">[20]</ref>, simple cycles (rings) are central in molecular distributions, directed and temporal motifs have been shown to explain the working mechanisms of gene regulatory networks, biological neural networks, transportation networks and food webs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>In <ref type="figure" target="#fig_5">Figure 5</ref> (right), we showcase the importance of these inductive biases: a cycle-based GSN predicting molecular properties achieves smaller generalisation gap compared to a traditional MPNN, while at the same time generalising better with less training data. Choosing the best substructure collection is still an open problem that does not admit a straightforward solution due to its combinatorial nature. Alternatively, various heuristics can be used, e.g., motif frequencies or feature selection strategies. Answering this question is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity</head><p>The complexity of GSN comprises two parts: precomputation (substructure counting) and training/testing. The key appealing property is that training and inference are linear w.r.t the number of edges, O(|E|), as opposed to higher-order methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> with O(n k ), and <ref type="bibr" target="#b47">[48]</ref> with O(n 2 ) training complexity and relational pooling <ref type="bibr" target="#b48">[49]</ref> with O(n!) training complexity in absence of approximations.</p><p>The worst-case complexity of subgraph isomorphism of fixed size k is O(n k ), by examining all the possible k-tuples in the graph. However, for specific types of subgraphs, such as paths and cycles, the problem can be solved even faster (see e.g. <ref type="bibr" target="#b49">[50]</ref>). Approximate counting algorithms are also widely used, especially for counting frequent network motifs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, and can provide a considerable speed-up. Furthermore, recent neural approaches <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> provide fast approximate counting.</p><p>In our experiments, we performed exact counting using the common isomorphism algorithm VF2 <ref type="bibr" target="#b56">[57]</ref>. Although its worst case complexity is O(n k ), it scales better in practice, for instance when the candidate subgraph is infrequently matched or when the graphs are sparse, and is also trivially parallelisable. In <ref type="figure" target="#fig_2">Figure 3</ref>, we show a quantitative analysis of the empirical runtime of the counting algorithm against the worst case, for three different graph distributions: molecules, protein contact maps, social networks. It is easy to see that when the graphs are sparse (for the first two cases) and the number of matches is small, the algorithm is significantly faster than the worst case, while it scales better with the size of the graph n. Even, in the case of social networks, where several examples are near-complete graphs, both the runtime and the growth w.r.t both n and k are better than the worst case. Overall, the preprocessing computational burden in most of the cases remains negligible for relatively small and sparse graphs, as it is the case of molecules.</p><p>4 Related Work 4.1 Expressive power of GNNs WL hierarchy. The seminal results in the theoretical analysis of the expressivity of GNNs <ref type="bibr" target="#b15">[16]</ref> and k-GNNs <ref type="bibr" target="#b16">[17]</ref> established that traditional message passing-based GNNs are at most as powerful as the 1-WL test. <ref type="bibr" target="#b38">[39]</ref> showed that graph isomorphism is equivalent to universal invariant function approximation. Higher-order Invariant Graph Networks (IGNs) have been studied in a series of works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>, establishing connections with the WL hierarchy, similarly to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b59">60]</ref>. The main drawbacks of these methods are the training and inference time complexity and memory requirements of O(n k ) and the super-exponential number of parameters (for linear IGNs) making them impractical, as well as their non-local nature making them more prone to overfitting. Finally, <ref type="bibr" target="#b60">[61]</ref> also analysed the expressive power of MPNNs and other more powerful variants and provided generalisation bounds.</p><p>Unique identifiers. From a different perspective, <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b32">[33]</ref> showed the connections between GNNs and distributed local algorithms <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref> and suggested more powerful alternatives based on either local orderings or unique global identifiers (in the form of random features in <ref type="bibr" target="#b35">[36]</ref>) that make GNNs universal. Similarly, <ref type="bibr" target="#b36">[37]</ref> propose to use random colorings in order to uniquely identify the nodes. However, these methods lack a principled permutation equivariant way to choose orderings/identifiers. To date this is an open problem in graph theory called graph canonisation and it is at least as hard as solving graph isomorphism itself. A possible workaround is proposed in <ref type="bibr" target="#b48">[49]</ref>, where the authors take into account all possible vertex permutations. However, obviously this quickly becomes intractable (O(n!)) even when considering small-sized graphs.</p><p>More expressive permutation equivariant GNNs. Concurrently with our work, other more expressive GNNs have been proposed using equivariant message passing. In <ref type="bibr" target="#b65">[66]</ref>, the authors propose to linearly transform each message with a different kernel based on the local isomorphism class of the corresponding edge (similar to our definition of structural roles). However, as also noted by the authors, taking into account all possible local isomorphism classes leads to insufficient weight sharing and hence to overfitting. In contrast, in GSN, usually the substructure collection is small (?5-10 graphs) and the substructures are repetitive in the graph distribution, and as a result generalisation improves. Vignac et al. <ref type="bibr" target="#b47">[48]</ref> propose a message passing scheme where matrices of order equal to the size of the graph are propagated instead of vectors. This can be perceived as a practical unique identification scheme, but the neural network complexity becomes quadratic in the number of nodes. Finally, <ref type="bibr" target="#b66">[67]</ref> and <ref type="bibr" target="#b67">[68]</ref> enhance the aggregation function with distance encodings (a strategy more relevant for vertex-level tasks) and graph eigenvectors respectively as alternative symmetry breaking mechanisms. In the experimental section, GSN is compared against these methods in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantifying expressivity.</head><p>Solely quantifying the expressive power of GNNs in terms of their ability to distinguish non-isomorphic graphs does not provide the necessary granularity: even the 1-WL test can distinguish almost all (in the probabilistic sense) non-isomorphic graphs <ref type="bibr" target="#b68">[69]</ref>. As a result, there have been several efforts to analyse the power of k-WL tests in comparison to other graph invariants <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b72">72]</ref>, while recently <ref type="bibr" target="#b18">[19]</ref> approached GNN expressivity by studying their ability to count substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Substructures in Complex Networks.</head><p>The idea of analysing complex networks based on small-scale topological characteristics dates back to the 1970's and the notion of triad census for directed graphs <ref type="bibr" target="#b73">[73]</ref>. The seminal paper of <ref type="bibr" target="#b44">[45]</ref> coined the term network motifs as over-represented subgraph patterns that were shown to characterise certain functional properties of complex networks in systems biology. The closely related concept of graphlets <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b76">76]</ref>, different from motifs in being induced subgraphs, has been used to analyse the distribution of real-world networks and as a topological signature for network similarity. Our work is similar in spirit with the graphlet degree vector (GDV) <ref type="bibr" target="#b74">[74]</ref>, a node-wise descriptor based on graphlet counting.</p><p>Substructures have been also used in the context of ML. In particular, subgraph patterns have been used to define Graph Kernels (GKs) <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b81">81]</ref>, with the most prominent being the graphlet kernel <ref type="bibr" target="#b78">[78]</ref>. Motif-based node embeddings <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b83">83]</ref> and diffusion operators <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b86">86</ref>] that employ adjacency matrices weighted according to motif occurrences, have recently been proposed for graph representation learning. Our formulation provides a unifying framework for these methods and it is the first to analyse their expressive power. Finally, GNNs that operate in larger induced neighbourhoods <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b88">88]</ref> or higher-order paths <ref type="bibr" target="#b89">[89]</ref> have prohibitive complexity since the size of these neighbourhoods typically grows exponentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>In the following section we evaluate GSN in comparison to the state-of-the-art in a variety of datasets from different domains. We are interested in practical scenarios where the collection of subgraphs, as well as their size, are kept small. Depending on the dataset domain we experimented with typical substructure families (cycles, paths and cliques) and maximum substructure size k (note that for each setting, our substructure collection consists of all the substructures of the family with size ? k). We also experimented with both graphlets and motifs and observed similar performance in most cases. To showcase that structural features can be used as an off-the-shelf strategy to boost GNN performance, we usually choose a base message passing architecture and minimally modify it into a GSN. Unless otherwise stated, the base architecture is a generalpurpose MPNN with MLPs used in the message and update functions. Additional implementation details can be found in the Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Graph Isomorphism test</head><p>We tested the ability of GSNs to decide if two graphs are non-isomorphic on a collection of Strongly Regular graphs of size up to 35 nodes, attempting to disambiguate pairs with the same number of nodes (for different sizes the problem becomes trivial). As we are only interested in the bias of the architecture itself, we use GSN with random weights to compute graph representations. Two graphs are deemed isomorphic if the Euclidean distance of their representations is smaller than a predefined threshold . <ref type="figure" target="#fig_4">Figure 4</ref> shows the failure percentage of our isomorphism test when using different graphlet substructures (cycles, paths, and cliques) of varying size k. Interestingly, the number of failure cases of GSN decreases rapidly as we increase k; cycles and paths of maximum length k = 6 are enough to tell apart all the graphs in the dataset. Note that the performance of cliques saturates, possibly because the largest clique in our dataset has 5 nodes. Observe also the discrepancy between GSN-v and GSN-e. In particular, vertex-wise counts do not manage to distinguish all graphs, although missing only a few instances, which is in accordance with Theorem 3.3. Finally, 1-WL <ref type="bibr" target="#b15">[16]</ref> and 2-FWL <ref type="bibr" target="#b23">[24]</ref> equivalent models demonstrate 100% failure, as expected from theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TUD Graph Classification Benchmarks</head><p>We evaluate GSN on datasets from the classical TUD benchmarks. We use seven datasets from the domains of bioinformatics and computational social science and compare against various GNNs and Graph Kernels. The base architecture that we used is GIN <ref type="bibr" target="#b15">[16]</ref>. We follow the same evaluation protocol of <ref type="bibr" target="#b15">[16]</ref>, performing 10-fold cross-validation and then reporting the performance at the epoch with the best average accuracy across the 10 folds. <ref type="table" target="#tab_2">Table 1</ref> lists all the methods evaluated with the split of <ref type="bibr" target="#b90">[90]</ref>. We select our model by tuning architecture and optimisation hyperparameters and substructure related parameters, that is: (i) k, (ii) motifs against graphlets. Following domain evidence we choose the following substructure families: cycles for molecules, cliques for social networks. Best performing substructures both for GSN-e and GSN-v are reported. As can be seen, our model obtains state-of-the-art performance in most of the datasets, with a considerable margin against the main GNN baselines in some cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ZINC Molecular graphs</head><p>We evaluate GSN on the task of regressing the "penalized water-octanol partition coefficient -logP" (see <ref type="bibr" target="#b98">[98,</ref><ref type="bibr" target="#b99">99,</ref><ref type="bibr" target="#b100">100]</ref> for details) of molecules from the ZINC database <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b24">25]</ref>. We use structural features obtained with k-cycle counting and report the result of the best performing substructure w.r.t. the validation set.</p><p>As dictated by the evaluation protocol of <ref type="bibr" target="#b24">[25]</ref>, the total number of parameters of the model is approximately 100K, which is achieved by selecting an appropriate network width. <ref type="bibr" target="#b1">2</ref> The data split is obtained from <ref type="bibr" target="#b24">[25]</ref> and the evaluation metric is the Mean Absolute Error (MAE). We compare against a variety of baselines, ranging from traditional message passing NNs to recent more expressive architectures <ref type="bibr" target="#b102">[102,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b103">103]</ref> and a molecular-specific one which is based on the junction tree molecular decomposition <ref type="bibr" target="#b104">[104]</ref>. Wherever possible, GCN <ref type="bibr" target="#b107">[107]</ref> 0.469?0.002 -GIN <ref type="bibr" target="#b15">[16]</ref> 0.408?0.008 -GraphSage <ref type="bibr" target="#b108">[108]</ref> 0.410?0.005 -GAT <ref type="bibr" target="#b109">[109]</ref> 0.463?0.002 -MoNet <ref type="bibr" target="#b9">[10]</ref> 0.407?0.007 -GatedGCN <ref type="bibr" target="#b110">[110]</ref> 0.422?0.006 0.363?0.009 MPNN 0.254?0.014 0.209?0.018 MPNN-r 0.322?0.026 0.279?0.023 PNA <ref type="bibr" target="#b102">[102]</ref> 0.320?0.032 0.188?0.004 DGN <ref type="bibr" target="#b67">[68]</ref> 0.219?0.010 0.168?0.003 GNNML <ref type="bibr" target="#b103">[103]</ref> 0.161?0.006 -HIMP <ref type="bibr" target="#b104">[104]</ref> -0.151?0.006 SMP <ref type="bibr" target="#b47">[48]</ref> 0.219? 0.138? GSN 0.140?0.006 0.115?0.012  <ref type="bibr" target="#b15">[16]</ref> 0.7707 ? 0.0149 0.8479 ? 0.0068 DeeperGCN <ref type="bibr" target="#b111">[111]</ref> 0.7858 ? 0.0117 0.8427 ? 0.0063 HIMP <ref type="bibr" target="#b104">[104]</ref> 0.7880 ? 0.0082 -GCN+GraphNorm <ref type="bibr" target="#b97">[97]</ref> 0.7883 ? 0.0100 0.7904 ? 0.0115 PNA <ref type="bibr" target="#b102">[102]</ref> 0.7905 ? 0.0132 0.8519 ? 0.0099 PHC-GNN <ref type="bibr" target="#b112">[112]</ref> 0.7934 ? 0.0116 0.8217 ? 0.0089 DeeperGCN+FLAG <ref type="bibr" target="#b113">[113]</ref> 0.7942 ? 0.0120 0.8425 ? 0.0061 DGN + eigenvectors <ref type="bibr" target="#b67">[68]</ref> 0.7970 ? 0.0097 0.8470 ? 0.0047 P-WL <ref type="bibr" target="#b114">[114]</ref> 0.8039 ? 0.0040 0.8279 ? 0.0059 GSN (GIN+VN base) 0.7799?0.0100 0.8658?0.0084 GSN (DGN + substructures) 0.8039 ? 0.0090 0.8473 ? 0.0096 we compare two variants, one that does not take edge features into account and one that does. In both cases, GSN achieves state-of-the-art results outperforming all the baseline architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">OGB-MOLHIV</head><p>We use ogbg-molhiv from the Open Graph Benchmark -OGB - <ref type="bibr" target="#b25">[26]</ref> as a graph-level binary classification task, where the aim is to predict if a molecule inhibits HIV replication or not. We obtain baseline results from a plethora of different methods 3 , ranging from algorithms specific to molecular graphs to general purpose architectures. Following the same rationale as in the previous experiments, we choose a base architecture and modify it into a GSN variant by introducing structural features in the aggregation function (cycle counts, similar to other molecular datasets). Here we use the following two base architectures: (a) GIN-VN, a variation of GIN that allows for edge features and is extended with a virtual node, i.e. a node connected to every node in the graph. (b) Directional Graph Networks (DGN), a GNN that propagates messages in an anisotropic manner, based on a predefined graph vector field. Observe that the vector field is an alternative way to break local symmetries. The authors of DGN use vector fields defined by the eigenvectors of the graph, while in our case the vector field is defined by graph substructures. More information can be found in the supplementary material.</p><p>Using the evaluator provided by the authors, we report the ROC-AUC metric at the epoch with the best validation performance (substructures are also chosen based on the validation set). By examining the results in <ref type="table" target="#tab_4">Table 3</ref> the following observations can be made, (a) general purpose GNNs benefit from symmetry breaking mechanisms, either in the form of eigenvectors or in the form of substructures. (b) Cyclical substructures are a good inductive bias when learning on molecules (e.g. P-WL is a graph kernel based on topological features that contain information of graph cycles, similar to GSN). (c) Further evidence for that is provided by observing the performance of molecular fingerprints methods. In specific, a method based on the extended-connectivity fingerprints <ref type="bibr" target="#b105">[105]</ref>, which mainly focuses on the structure of the molecule reports 0.8060 ? 0.0010 test performance, while one that additionally uses the MACCS fingerprints <ref type="bibr" target="#b106">[106]</ref>, which mainly encode the presence of certain functional groups (i.e. both structure and attributes), reports 0.8232 ? 0.0047 test performance. These methods although not directly comparable to ours, currently achieve the best results in this dataset, thus this further motivates research on making GNNs structure-aware.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Comparison between substructure collections</head><p>In <ref type="figure" target="#fig_5">Figure 5</ref> (left), we compare the training and test error for different substructure families (cycles, paths and non-isomorphic trees -for each experiment we use all the substructures of size ? k in the family). Additionally, we measure the "uniqueness" of the identifiers each substructure yields as follows: for each graph G in the dataset, we measure the number of unique vertex features u G (input vertex features concatenated with vertex structural identifiers for GSN-v). Then, we sum them up over the entire training set and divide by the total number of nodes, yielding the disambiguation score ? = G u G G |V G | . The disambiguation scores for the different types of substructures are illustrated as horizontal bars in <ref type="figure" target="#fig_5">Figure 5</ref> (the exact values can be found in Appendix C.2, <ref type="table" target="#tab_3">Table 2</ref>).</p><p>A first thing to notice is that the training error is tightly related to the disambiguation score. As identifiers become more discriminative, the model gains expressive power. On the other hand, the test error is not guaranteed to decrease when the identifiers become more discriminative. For example, although cycles have smaller disambiguation scores, they manage to generalise much better than the other substructures, the performance of which is similar to the baseline architecture (MPNN with MLPs). This is also observed when comparing against <ref type="bibr" target="#b35">[36]</ref> (MPNN-r method in <ref type="table" target="#tab_3">Table 2)</ref>, where, akin to unique identifiers, random features are used to strengthen the expressivity of GNN architectures. This approach also fails to improve the baseline architecture in terms of the performance in the test set. This validates our intuition that unique identifiers can be hard to generalise when chosen in a non-permutation equivariant way and motivates once more the importance of choosing the identifiers not only based on their discriminative power, but also in a way that allows incorporating the appropriate inductive biases. Finally, we observe a substantial jump in performance when using GSN with cycles of size k ? 6. This is not surprising, as cyclical patterns of such sizes (e.g. aromatic rings) are very common in organic molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Generalisation</head><p>We repeat the experimental evaluation on ZINC using different fractions of the training set and compare the vanilla MPNN model against GSN. In <ref type="figure" target="#fig_5">Figure 5</ref> (right), we plot the training and test errors of both methods. Regarding the training error, GSN consistently performs better, following our theoretical analysis on its expressive power. More importantly, GSN manages to generalise much better even with a small fraction of the training dataset. Observe that GSN requires only 20% of the samples to achieve approximately the same test error that MPNN achieves when trained on the entire training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Structural Features &amp; Message Passing:</head><p>We perform an ablation study on the abilities of the structural features to predict the task at hand, when given as input to a graph-agnostic network. In particular, we compare our best performing GSN with a DeepSets model <ref type="bibr" target="#b115">[115]</ref> that treats the input features and the structural identifiers as a set. For fairness of evaluation the same hyperparameter search is performed for both models (see Appendix C.5. Interestingly, as we show in <ref type="table" target="#tab_6">Table 4</ref>, our baseline attains particularly strong performance across a variety of datasets and often outperforms other traditional message passing baselines. This demonstrates the benefits of these additional features and motivates their introduction in GNNs, which are unable to compute them. As expected, we observe that applying message passing on top of these features, brings performance improvements in the vast majority of the cases, sometimes considerably, as in the ZINC dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel way to design structure-aware graph neural networks. Motivated by the limitations of traditional GNNs to capture important topological properties of the graph, we formulate a message passing scheme enhanced with structural features that are extracted by subgraph isomorphism. We show both theoretically and empirically that our construction leads to improved expressive power and attains state-of-the-art performance in real-world scenarios. In future work, we will further explore the expressivity of GSNs as an alternative to the k-WL tests, as well as their generalisation capabilities. Another important direction is to infer prominent substructures directly from the data and explore the ability of graph neural networks to compose substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Deferred Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 GSN is permutation equivariant</head><p>Proof. Let A ? R n?n the adjacency matrix of the graph, H 0 ? R n?d V in the input vertex features, E ? R n?n?d E in the input edge features and E i ? R n?n the edge features at dimension i. Let S V (A) ? N n?d V , S E (A) ? N n?n?d E the functions generating the vertex and edge structural identifiers respectively.</p><p>It is obvious that subgraph isomorphism is invariant to the ordering of the vertices, i.e. we will always obtain the same matching between subgraphs G S and graphs H in the subgraph collection. Thus, each vertex v (edge (v, u)) in the graph will be assigned the same structural identifiers x V v (x E v,u ) regarless of the vertex ordering, and S V and S E are permutation equivariant, i.e., for any permutation matrix P ? {0, 1} n?n it holds that Overall, a GSN network is permutation equivariant as composition of permutation equivariant functions, or permutation invariant when composed with a permutation invariant layer at the end, i.e. the READOUT function.</p><formula xml:id="formula_10">S V (PAP T ) = PS V (A) S E (PAP T ) = P[S E 1 (A); . . . ; S E d E (A)]P T ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 3.1: GSN is at least as powerful as the 1-WL test</head><p>Proof. To show that GSN it is at least as expressive as the 1-WL test, we will repurpose the proof of Theorem 3 in <ref type="bibr" target="#b15">[16]</ref> and demand the injectivity of the update function (w.r.t. both the hidden state h t v and the message m t+1 v ), and the injectivity of the aggregation w.r.t. the multiset of the hidden states of the neighbours h t u u?N (v) . It suffices then to show that if injectivity is preserved then GSNs are at least as powerful as the 1-WL.</p><p>We will show the above statement for vertex-labelled graphs, since traditionally the 1-WL test does not take into account edge labels. <ref type="bibr" target="#b3">4</ref> We can rephrase the statement as follows: If GSN deems two graphs G 1 , G 2 as isomorphic, then also 1-WL deems them isomorphic. Given that the graph-level representation is extracted by a readout function that receives the multiset of the vertex colours in its input (i.e. the graphlevel representation is the vertex colour histogram at some iteration t), then it suffices to show that if for the two graphs the multiset of the vertex colours that GSN infers is the same, then also 1-WL will infer the same multiset for the two graphs.</p><p>Consider the case where the two multisets that GSN extracts are the same: i.e. h t v v?V G 1 = h t u u?V G 2 . Then both multisets contain the same distinct colour/hidden representations with the exact same multiplicity. Thus, it further suffices to show that if two vertices v, u (that may belong to the same or to different graphs) have the same GSN hidden representations h t v = h t u at any iteration t, then they will also have the same colours c t v = c t u , extracted by 1-WL. Intuitively, this means that GSN creates a partition of the vertices of each graph that is at least as fine-grained as the one created by 1-WL. We prove by induction (similarly to <ref type="bibr" target="#b15">[16]</ref>) that GSN model class contains a model where this holds (w.l.o.g. we show that for GSN-v; same proof applies to GSN-e).</p><p>For t = 0 the statement holds since the initial vertex features are the same for both GSN and 1-</p><formula xml:id="formula_11">WL, i.e. h 0 v = c 0 v , ?v ? V G1 ? V G2 . Suppose the statement holds for t ? 1, i.e. h t?1 v = h t?1 u ? c t?1 v = c t?1 u .</formula><p>Then we show that it also holds for t. Every vertex hidden representation at step t is updated as follows:</p><formula xml:id="formula_12">h t v = UP t h t?1 v , m t v .</formula><p>Assuming that the update function UP t is injective, we have the following: if h t v = h t u , then:</p><formula xml:id="formula_13">? h t?1 v = h t?1 u , which from the induction hypothesis implies that c t?1 v = c t?1 u . ? m t v = m t u ,</formula><p>where the message function is defined as in Eq. (5) of the main paper. Additionally here we require M t to be injective w.r.t. the multiset of the hidden representations of the neighbours: 5</p><formula xml:id="formula_14">m t v = m t u ? h t?1 w w?Nv = h t?1 z z?Nu</formula><p>From the induction hypothesis we know that h t?1</p><formula xml:id="formula_15">w = h t?1 z implies that c t?1 w = c t?1 z for any w ? N v , z ? N u , thus c t?1 w w?Nv = c t?1 z z?Nu .</formula><p>Concluding, given the update rule of 1-WL:</p><formula xml:id="formula_16">c t v = HASH c t?1 v , c t?1 w w?Nv , it holds that c t v = c t u .</formula><p>A.3 Proof of Corollary 3.2</p><p>Proof. In order to prove the universality of GSN, we will show that when the substructure collection contains all graphs of size n ? 1, then there exists a parametrisation of GSN that can infer the isomorphism classes of all vertex-deleted subgraphs of the graph G (the deck of G).</p><p>The reconstruction conjectures states that two graphs with at least three vertices are isomorphic if and only if they have the same deck. Thus, the deck is sufficient to distinguish all non-isomorphic graphs. The deck can be defined as follows:</p><p>Let H = {H 1 , H 2 , ? ? ? , H K } the set of all possible graphs of size n ? 1. The vertex-deleted subgraphs of G are by definition all the induced subgraphs of G with size n ? 1, which we denote as</p><formula xml:id="formula_17">G n?1 = {G S : induced subgraph of G with |V G S | = n ? 1}.</formula><p>Then, the deck D(G) can be defined as a vector of size |H|, where at the j-th dimension</p><formula xml:id="formula_18">D j (G) = G S ? G n?1 | G S H j = G S ?Gn?1 1[G S H j ],</formula><p>where 1[?] the indicator function. The structural feature x V Hj ,i (v) for each substructure H j and orbit i are computed as follows:</p><formula xml:id="formula_19">x V Hj ,i (v) = G S H j | v ? V G S , f (v) ? O V Hj ,i = G S ?Gn?1 1[v ? V G S ]1[G S H j ]1[f G S (v) ? O V Hj ,i ]</formula><p>where f G S (?) = ? if G S H j , otherwise it is the bijective mapping function. The deck can be inferred as follows:</p><formula xml:id="formula_20">v?V G d H j i=1 x V Hj ,i (v) = v?V G d H j i=1 G S ?Gn?1 1[v ? V G S ]1[G S H j ]1[f G S (v) ? O V Hj ,i ] = v?V G G S ?Gn?1 1[v ? V G S ]1[G S H j ] d H j i=1 1[f G S (v) ? O V Hj ,i ] = G S ?Gn?1 1[G S H j ] v?V G 1[v ? V G S ] = G S ?Gn?1 (n ? 1)1[G S H j ] = (n ? 1)D j (G)</formula><p>where we used that</p><formula xml:id="formula_21">d H j i=1 1[f G S (v) ? O V</formula><p>Hj ,i ] = 1, since each vertex can be mapped to a single orbit only. Thus, the deck can be inferred by a simple GSN-v parametrisation (a linear layer with depth equal to |H| that performs orbit-wise summation and division by the constant n ? 1 for each vertex separately, followed by a sum readout). Since GSN-v can be inferred by GSN-e (Theorem 3.3), then GSN-e is also universal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 3.3</head><p>Proof. Without loss of generality we will show Theorem 3.3 for the case of a single substructure H. In order to show that GSN-e can express GSN-v, we will first prove the following: the vertex identifier of a vertex v can be inferred by the edge identifiers of its incident edges.</p><p>To simplify notation we define the following: For an orbit Orb V (v), denote the orbit neighbourhood as the multiset of the orbits of the neighbours of v and the orbit degree as the degree of any vertex in Orb V (v) :</p><formula xml:id="formula_22">N Orb V (v) = Orb V (u) | u ? N (v) and deg(v) = deg(Orb V (v)) = |N Orb V (v) |.</formula><p>For brevity we will use the following notation: vertex orbits are indexed as follows</p><formula xml:id="formula_23">O V 1 , O V 2 , . . . O V d V and edge orbits O E 1,1 , O E 1,2 , . . . O E d V ,d V with O E i,j = O V i , O V j .</formula><p>Structural features are denoted accordingly: x V i (v) and x E ij (v, u) Let's assume that there exists only one matched subgraph G S H and the bijection between V G S and H is denoted as f . Then, for an abitrary vertex v and a vertex orbit O V i , one of the following holds:</p><formula xml:id="formula_24">? v ? V G S . Then x V i (v) = 0 and x E ij (v, u) = 0, ?u ? N (v),</formula><p>? v ? V G S and Orb(f (v)) = O V i . Then, x V i (v) = 0 and x E ij (v, u) = 0, ?u ? N (v). Note that here the directionality of the edge is important, otherwise we cannot determine the value of x E ij (v, u) unless we also know the orbit of f (u).</p><formula xml:id="formula_25">? v ? V G S and Orb(f (v)) = O V i . Then, x V i (v) = 1 and since f (v) has exactly deg(O V i ) neighbours in H, then v has exactly deg(O V i ) neighbours in G S with vertex orbits N O V i . In other words u?N (v) j:O V j ?N (O V i ) x E ij (u, v) = deg(O V i ) Thus, by induction, for m matched subgraphs G S H with v ? V G S and Orb(f (v)) = O V i , it holds that x V i (v) = m and u?N (v) j:O V j ?N (O V i ) x E ij (u, v) = m * deg(O V i )</formula><p>. Then it follows that:</p><formula xml:id="formula_26">x V i (v) = 1 deg(O V i ) u?N (v) j:O V j ?N (O V i ) x E ij (u, v)<label>(6)</label></formula><p>The rest of the proof is straightforward: we will assume a GSN-v using substructure counts of the graph H, with L layers and width w defined as in the main paper (Eq. (5)) Then, there exists a GSN-e with L + 1 layers, where the first layer has width d in V +d V and implements the following function:</p><formula xml:id="formula_27">UP t+1 h t v , m t+1 v = [h t v ; m t+1 v ], where: m t+1 v = M t+1 (h t v , h t u , x E v,u , e u,v ) u?N (v) = [ 1 deg(O V 1 ) u?N (v) j:O V j ?N (O V 1 ) x E 1j (u, v); . . . ; 1 deg(O V d V ) u?N (v) j:O V j ?N (O V d V ) x E d V ,j (u, v)] = x V v</formula><p>Note that since M is a universal multiset function approximator, then there exist parameters of M with which the above function can be computed. The next L layers of GSN-e can implement a traditional MPNN where now the input vertex features are [h t v ; x V v ] (which is exactly the formulation of GSN-v) and this concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison with higher-order Weisfeiler-Leman tests B.1 The WL hierarchy</head><p>Following the terminology introduced in <ref type="bibr" target="#b23">[24]</ref>, we describe the so-called Folklore WL family (k-FWL). Note that, in the majority of papers on GNN expressivity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref> another family of WL tests is discussed, under the terminology k-WL with expressive power equal to (k ? 1)-FWL. In contrast, in most graph theory papers on graph isomorphism <ref type="bibr" target="#b116">[116,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b17">18]</ref> the k-WL term is used to describe the algorithms referred to as k-FWL in GNN papers. Here, we follow the k-FWL convention to align with the work mostly related to ours.</p><p>The k-FWL operates on k-tuples of vertices v = (v 1 , v 2 , . . . , v k ) to which an initial colour c 0 v is assigned based on their isomorphism types (see section B.2), which can loosely be thought of as a generalisation of isomorphism that also preserves the ordering of the vertices in the tuple. Then, at each iteration the colour is refined as follows:</p><formula xml:id="formula_28">c t+1 v = HASH c t v , c t vu,1 , c t vu,2 , . . . , c t v u,k u?V ,<label>(7)</label></formula><p>where v u,j = (v 1 , v 2 , . . . , v j?1 , u, v j+1 , . . . , v k ).</p><p>The multiset c t vu,1 , c t vu,2 , . . . , c t v u,k u?V can be perceived as a form of generalised neighbourhood. Observe that all possible tuples in the graph store information necessary for the updates, thus each k-tuple receives information from the entire graph, contrary to the local nature of the 1-WL test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Why does 2-FWL fail on strongly regular graphs?</head><p>Proof. We first formally define what an isomorphism type is and what are the properties of the SR family:</p><formula xml:id="formula_29">Definition 1 (Isomorphism Types). Two k-tuples v a = {v a 1 , v a 2 , . . . , v a k }, v b = {v b 1 , v b 2 , .</formula><p>. . , v b k } will have the same isomorphism type iff:</p><formula xml:id="formula_30">? ? i, j ? {0, 1, . . . , k}, v a i = v a j ? v b i = v b j ? ? i, j ? {0, 1, . . . , k}, v a i ? v a j ? v b i ? v b j ,</formula><p>where ? means that the vertices are adjacent.</p><p>Note that this is a stronger condition than isomorphism, since the mapping between the vertices of the two tuples needs to preserve order. In case the graph is employed with edge and vertex features, these need to be preserved as well (see <ref type="bibr" target="#b18">[19]</ref>) for the extended case).</p><p>Definition 2 (Strongly regular graph). A SR(n,d,?,?)-graph is a regular graph with n vertices and degree d, where every two adjacent vertices have always ? mutual neighbours, while every two non-adjacent vertices have always ? mutual neighbours.</p><p>Now we can proceed to the details of the proof. For the 2-FWL test, when working with simple undirected graphs without self-loops, we have the following 2-tuple isomorphism types:</p><formula xml:id="formula_31">? v = {v 1 , v 1 }: vertex type. Mapped to the colour c 0 = c ? ? v = {v 1 , v 2 } and v 1 ? v 2 : non-edge type. Mapped to the colour c 0 = c ? ? v = {v 1 , v 2 } and v 1 ? v 2 : edge type. Mapped to the colour c 0 = c ? For each 2-tuple v = {v 1 , v 2 }, a generalised "neighbour" is the following tuple: (v u,1 , v u,2 ) = (u, v 2 ), (v 1 , u) ,</formula><p>where u is an arbitrary vertex in the graph. Now, let us consider a strongly regular graph SR(n,d,?,?). We have the following cases:</p><p>? generalised neighbour of a vertex type tuple: (v u,1 , v u,2 ) = (u, v 1 ), (v 1 , u) . The corresponding neighbour colour tuples are:</p><formula xml:id="formula_32">-(c ? , c ? ) if v 1 = u, -(c ? , c ? ) if v 1 ? u , -(c ? , c ? ) if v 1 ? u.</formula><p>The update of the 2-FWL is:</p><formula xml:id="formula_33">c 1 v = HASH c ? , (c ? , c ? ) 1 time , (c ? , c ? ) n ? 1 ? d times , (c ? , c ? ) d times</formula><p>same for all vertex type 2-tuples.</p><p>? generalised neighbour of a non-edge type tuple: <ref type="figure">u)</ref> . The corresponding neighbour colour tuples are:</p><formula xml:id="formula_34">(v u,1 , v u,2 ) = (u, v 2 ), (v 1 ,</formula><formula xml:id="formula_35">-(c ? , c ? ) if v 2 = u, -(c ? , c ? ) if v 1 = u, -(c ? , c ? ) if v 2 ? u and v 1 ? u, -(c ? , c ? ) if v 1 ? u and v 2 ? u, -(c ? , c ? ) if v 1 ? u and v 2 ? u, -(c ? , c ? ) if v 1 ? u and v 2 ? u.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Settings -Additional Details</head><p>In this section, we provide additional implementation details of our experiments. All experiments were performed on a server equipped with 8 Tesla V100 16 GB GPUs, except for the Collab dataset where a Tesla V100 GPU with 32 GB RAM was used due to larger memory requirements. Experimental tracking and hyperparameter optimisation were done via the Weights &amp; Biases platform (wandb) <ref type="bibr" target="#b117">[117]</ref>. Our implementation is based on native PyTorch sparse operations <ref type="bibr" target="#b118">[118]</ref> in order to ensure complete reproducibility of the results. PyTorch Geometric <ref type="bibr" target="#b119">[119]</ref> was used for additional operations (such as preprocessing and data loading).</p><p>In each one of the different experiments we aim to show that structural identifiers can be used off-theshelf and independently of the architecture. At the same time we aim to suppress the effect of other confounding factors in the model performance, thus wherever possible we build our model on top of a baseline architecture. For more details, please see the relevant subsections. Interestingly, we observed that in most of the cases it was sufficient to replace only the first layer of the baseline architecture with a GSN layer, in order to obtain a boost in performance.</p><p>Throughout the experimental evaluation the structural identifiers x V v and x E u,v are one-hot encoded, by taking into account the unique count values present in the dataset. Other more sophisticated methods can be used, e.g. transformation to continuous features via a normalisation scheme or binning. However, we found that the number of unique values in our datasets were usually relatively small (which is a good indication of recurrent structural roles) and thus such methods were not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Synthetic Experiment</head><p>For the Strongly Regular graphs dataset (available from http://users.cecs.anu.edu.au/~bdm/data/graphs. html) we use all the available families of graphs with size of at most 35 vertices:</p><p>Given the adversities that strongly regular graphs pose in graph isomorphism testing, it would be interesting to see how this method can perform in other categories of hard instances, such as the classical CFI counterexamples for k-WL proposed in <ref type="bibr" target="#b116">[116]</ref>, and explore further its expressive power and combinatorial properties. We leave this direction to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 TUD Graph Classification Benchmarks</head><p>For this family of experiments, due to the usually small size of the datasets, we choose a parameter-efficient architecture, in order to reduce the risk of overfitting. In particular, we follow the simple GIN architecture <ref type="bibr" target="#b15">[16]</ref> and we concatenate structural identifiers to vertex or edge features depending on the variant. Then for GSN-v, the hidden representation is updated as follows:</p><formula xml:id="formula_36">h t+1 v = UP t+1 [h t v ; x V v ] + u?Nv [h t u ; x V u ] ,<label>(8)</label></formula><p>and for GSN-e:</p><formula xml:id="formula_37">h t+1 v = UP t+1 [h t v ; x E v,v ] + u?Nv [h t u ; x E u,v ] ,<label>(9)</label></formula><p>where x E v,v is a dummy variable (also one-hot encoded) used to distinguish self-loops from edges. Empirically, we did not find training the parameter used in GIN to make a difference.</p><p>We implement an architecture similar to GIN <ref type="bibr" target="#b15">[16]</ref>, i.e. message passing layers: 4 , jumping knowledge from all the layers <ref type="bibr" target="#b120">[120]</ref> (including the input), transformation of each intermediate graph-level representation: linear layer, readout: sum for biological and mean for social networks. Vertex features are one-hot encodings of the categorical vertex labels. Similarly to the baseline, the hyperparameters search space is the following: batch size in {32, 128} (except for Collab where only 32 was searched due to GPU memory limits), dropout in {0,0.5}, network width in {16,32} for biological networks, 64 for social networks, learning rate in {0.01, 0.001}, decay rate in {0.5,0.9} and decay steps in {10,50} (number of epochs after which the learning rate is reduced by multiplying with the decay rate). For social networks, since they are not attributed graphs, we also experimented with using the degree as a vertex feature, but in most cases the structural identifiers were sufficient.</p><p>Model selection is done in two stages. First, we choose a substructure that we perceive as promising based on indications from the specific domain: triangles for social networks and Proteins, and 6-cycles (motifs) for molecules. Under this setting we tune model hyperparameters for a GSN-e model. Then, we extend our search to the parameters related to the substructure collection: i.e. the maximum size k and motifs vs graphlets. In all the molecular datasets we search cycles with k = 3, . . . , 12, except for NCI1, where we also consider larger sizes due to the presence of large rings in the dataset (macrocycles <ref type="bibr" target="#b121">[121]</ref>). For social networks, we searched cliques with k = 3, 4, 5. In <ref type="table" target="#tab_7">Table 5</ref> we report the hyperparameters chosen by our model selection procedure, including the best performing substructures.</p><p>The seven datasets 6 we chose are the intersection of the datasets used by the authors of our main baselines: the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b15">[16]</ref>, a simple, yet powerful GNN with expressive power equal to the 1-WL test, and the Provably Powerful Graph Network (PPGN) <ref type="bibr" target="#b23">[24]</ref>, a polynomial alternative to the Invariant Graph Network <ref type="bibr" target="#b21">[22]</ref>, that increases its expressive power to match the 2-FWL. We also compare our results to other GNNs as well as Graph Kernel approaches. Our main baseline from the GK family is the Graph Neural Tangent Kernel (GNTK) <ref type="bibr" target="#b94">[94]</ref>, which is a kernel obtained from a GNN of infinite width. This operates in the Neural Tangent Kernel regime <ref type="bibr" target="#b122">[122,</ref><ref type="bibr" target="#b123">123,</ref><ref type="bibr" target="#b124">124]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Graph Regression on ZINC</head><p>Experimental Details: The ZINC dataset includes 12k molecular graphs of which 10k form the training set and the remaining 2k are equally split between validation and test (splits obtained from https://github. com/graphdeeplearning/benchmarking-gnns). Molecule sizes range from 9 to 37 vertices/atoms. Vertex features encode the type of atoms and edge features the chemical bonds between them. Again, here vertex and edge features are one-hot encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our MPNN baseline model updates vertex representations as follows</head><formula xml:id="formula_38">: h t+1 (v) = MLP t+1 (h t v , m t+1 v ), m t+1 v = u?N (v) MLP t h t v , h t u , e u,v .</formula><p>Our instantiation of GSN is a simple extension where structural identifiers are also given as input to the message MLP.</p><p>Following the same rationale as before, the network configuration is minimally modified w.r.t. the baselines provided in <ref type="bibr" target="#b24">[25]</ref>, while here no hyperparameter tuning is done and we use the default ones provided by the authors. In particular, the parameters are the following: message passing layers: 4, transformation of the output of the last layer : MLP, readout: sum, batch size: 128, dropout: 0.0, network width: 128, learning rate: 0.001. The learning rate is reduced by 0.5 (decay rate) after 5 epochs (decay rate patience) without We modify this model, as follows: first the substructure counts are embedded into the same embedding space as the rest of the features. Then, for GSN-v, they are added to the corresponding vertex embeddings: h t v = h t v + W t V ? x V v , or for GSN-e, they are added to the edge embeddings? t v,u = e t v,u + W t E ? x E u,v .</p><p>DGN + substructures. We use the directional average operator as defined in <ref type="bibr" target="#b67">[68]</ref>:</p><formula xml:id="formula_39">m t+1 v = [ u?N (v) ? 1 v,u h t u ; . . . ; u?N (v) ? D v,u h t u ],<label>(12)</label></formula><p>where ? i v,u are weighting average coefficients. In our case, each orbit i induces a separate set of averaging coefficients. For example, for GSN-e ? v,u = |x E v,u | + u?N (v) |x E v,u | , where x E v,u denotes edge-wise substructure counts (the index of the orbit i was dropped to simplify notation). Similarly, for GSN-v,</p><formula xml:id="formula_40">? v,u = |x V v ?x V u | + u?N (v) |x V v ?x V u | .</formula><p>Subsequently, the vertex representation is updated as follows: h t+1 v = MLP t+1 (m t+1 v ). Observe that this model is simpler than the aforementioned, in terms of both its parameter count and its expressive power. Since the MOLHIV dataset poses a significant challenge w.r.t. generalisation (the data splits reflect different molecular distributions), architectures biased towards simpler solutions usually perform better, sincey the mitigate the risk of overfitting.</p><p>In both cases we use the the same hyperparameters as the ones provided by the authors, and only select the substructure related parameters based on the highest validation ROC-AUC (choosing the best scoring epoch as in <ref type="bibr" target="#b25">[26]</ref>). We search cycles with k = 3, . . . , 12, graphlets vs motifs, and GSN-v vs GSN-e. The chosen hyperparameters are: GSN-e, cycle graphlets of 6 vertices. We repeat the experiment 10 times with different seeds and report the mean and standard deviation of the train, validation and test ROC-AUC, again by choosing the best scoring epoch w.r.t the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Structural Features &amp; Message Passing</head><p>The baseline architecture treats the input vertex and edge features, along with the structural identifiers, as a set. In particular, we consider each graph as a set of independent edges (v, u) endowed with the features of the endpoint vertices h v , h u , the structural identifiers x V v , x V u and the edge features e(v, u), and we implement a DeepSets universal set function approximator <ref type="bibr" target="#b115">[115]</ref> to learn a prediction function:</p><formula xml:id="formula_41">f hv, hu, x V v , x V u , ev,u (v,u)?E G = ? (v,u)?E G ? hv, hux V v , x V u , ev,u ,<label>(13)</label></formula><p>with E G the edge set of the graph and ?, ? MLPs. This baseline is naturally extended to the case where we consider edge structural identifiers by replacing (x V v , x V u ) with x E v,u . For fairness of evaluation, we follow the exact same parameter tuning procedure as the one we followed for our GSN models for each benchmark, i.e. for the TUD datasets we first tune network and optimisation hyperaparameters (network width was set to be either equal to the ones we tuned for GSN, or such that the absolute number of learnable parameters was equal to those used by GSN; depth of the MLPs was set to 2) and subsequently we choose the substructure related parameters based on the evaluation protocol of <ref type="bibr" target="#b15">[16]</ref>. For ZINC and ogbg-molhiv we perform only substructure selection, based on the performance on the validation set. Using the same widths as in GSN leads to smaller baseline models w.r.t the absolute number of parameters, and we interestingly observed this to lead to particularly strong performance in some cases, especially Proteins and MUTAG, where our DeepSets implementation attains state-of-art results. This finding motivated us to explore 'smaller' GSNs (with either reduced layer width or a single message passing layer). These GSN variants exhibited a similar trend, i.e. to perform better than their 'larger' counterparts over these two datasets. We hypothesise this phenomenon to be mostly due to the small size of these datasets, which encourages overfitting when using architectures with larger capacity. In <ref type="table" target="#tab_6">Table 4</ref> in the main paper, we report the result for the best performing architectures, along with the number of learnable parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1: Node (left) and edge (right) induced subgraph counting for a 3-cycle and a 3-path. Counts are reported for the blue node on the left and for the blue edge on the right. Different colors depict orbits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Empirical (solid) vs worst case (dashed) runtime for different graph distributions (in seconds, log scale). For each distribution we count the best performing (and frequent) substructures of increasing sizes k. Computational complexity for real-life graphs is significantly better than the worst case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 3 . 2 .</head><label>32</label><figDesc>If the Reconstruction Conjecture holds and the substructure collection H contains all graphs of size k = n ? 1, then GSN can distinguish all non-isomorphic graphs of size n and is therefore universal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>SR graphs isomorphism test (log scale, smaller values are better). Different colours indicate different substructure sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(Left) Train (dashed) and test (solid) MAEs for Path-, Tree-and Cycle-GSN-EF as a function of the maximum substructure size k. Vertical bars indicate standard deviation; horizontal bars depict disambiguation scores ?. (Right) Train (dashed) and test (solid) MAEs for GSN-EF (blue) and MPNN-EF (red) as a function of the dataset fraction used for training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where the permutation is applied at each slice S E i (A) ? N n?n of the tensor S E (A). Let f (A, H, E) ? R n?dout a GSN layer. We will show that f is permutation equivariant. We need to show that if Y = f (A, H, E) the output of the GSN layer, then PY = f (PAP T , PH, P[E 1 ; . . . ; E d E ]P T ) for any permutation matrix P. It is easy to see that GSN-v can can be expressed as a traditional MPNN g(A, H, E) ? R n?dout (similar to Eq. (5) of the main paper) by replacing the vertex features H with the concatenation of the input vertex features and the vertex structural identifiers, i.e. Y = f (A, H, E) = g(A, [H; S V (A)], E). Thus, f (PAP T , PH, P[E 1 ; . . . ; E d E ]P T , ) = = g(PAP T , [PH; S V (PAP T )], P[E 1 ; . . . ; E d E ]P T ) = g(PAP T , P[H; S V (A)], P[E 1 ; . . . ; E d E ]P T ) = Pg(A, [H; S V (A)], E) = PY where in the last step we used the permutation equivariant property of MPNNs. Similarly, we can show that a GSN-e layer is permutation equivariant, since it can be expressed as a traditional MPNN layer by replacing the edge features with the concatenation of the original edge features and the edge structural identifiers f (A, H, E) = g(A, H, [E; S E (A)]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Structural roles are encoded into features by counting the appearance of certain substructures. Define a set of small (connected) graphs H = {H 1 , H 2 . . . H K }, for example cycles of fixed length or cliques. For each graph H ? H, we first find its isomorphic subgraphs in G denoted as G</figDesc><table /><note>S . For each node v ? V G S we infer its role w.r.t. H by obtaining the orbit of its mapping f (v) in H, Orb H (f (v)). By counting all the possible appearances of different orbits in v, we obtain the vertex structural feature x V H (v) of v, defined as follows. For all i ? {1, . . . , d H }:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Graph classification accuracy on TUD Dataset. First, Second, Third best methods are highlighted. For GSN, the best performing structure is shown. GIN+GraphNorm<ref type="bibr" target="#b97">[97]</ref> 91.6 ? 6.5 64.9 ? 7.5 77.4 ? 4.9</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>Proteins</cell><cell>NCI1</cell><cell>Collab</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell>RWK* [91]</cell><cell>79.2?2.1</cell><cell>55.9?0.3</cell><cell>59.6?0.1</cell><cell>&gt;3 days</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GK* (k=3) [78]</cell><cell>81.4?1.7</cell><cell>55.7?0.5</cell><cell>71.4?0.31</cell><cell>62.5?0.3</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PK* [92]</cell><cell>76.0?2.7</cell><cell>59.5?2.4</cell><cell>73.7?0.7</cell><cell>82.5?0.5</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WL kernel* [93]</cell><cell>90.4?5.7</cell><cell>59.9?4.3</cell><cell>75.0?3.1</cell><cell>86.0?1.8</cell><cell>78.9?1.9</cell><cell>73.8?3.9</cell><cell>50.9?3.8</cell></row><row><cell>GNTK* [94]</cell><cell>90.0?8.5</cell><cell>67.9?6.9</cell><cell>75.6?4.2</cell><cell>84.2?1.5</cell><cell>83.6?1.0</cell><cell>76.9?3.6</cell><cell>52.8?4.6</cell></row><row><cell>DCNN [95]</cell><cell>N/A</cell><cell>N/A</cell><cell>61.3?1.6</cell><cell>56.6?1.0</cell><cell>52.1?0.7</cell><cell>49.1?1.4</cell><cell>33.5?1.4</cell></row><row><cell>DGCNN [90]</cell><cell>85.8?1.8</cell><cell>58.6?2.5</cell><cell>75.5?0.9</cell><cell>74.4?0.5</cell><cell>73.8?0.5</cell><cell>70.0?0.9</cell><cell>47.8?0.9</cell></row><row><cell>IGN [22]</cell><cell>83.9?13.0</cell><cell>58.5?6.9</cell><cell>76.6?5.5</cell><cell>74.3?2.7</cell><cell>78.3?2.5</cell><cell>72.0?5.5</cell><cell>48.7?3.4</cell></row><row><cell>GIN [16]</cell><cell>89.4?5.6</cell><cell>64.6?7.0</cell><cell>76.2?2.8</cell><cell>82.7?1.7</cell><cell>80.2?1.9</cell><cell>75.1?5.1</cell><cell>52.3?2.8</cell></row><row><cell>PPGNs [24]</cell><cell>90.6?8.7</cell><cell>66.2?6.6</cell><cell>77.2?4.7</cell><cell>83.2?1.1</cell><cell>81.4?1.4</cell><cell>73.0?5.8</cell><cell>50.5?3.6</cell></row><row><cell>Natural GN [66]</cell><cell>89.4?1.60</cell><cell>66.8?1.79</cell><cell>71.7?1.04</cell><cell>82.7?1.35</cell><cell>N/A</cell><cell>74.8?2.01</cell><cell>51.3?1.50</cell></row><row><cell>WEGL [96]</cell><cell>N/A</cell><cell>67.5?7.7</cell><cell>76.5?4.2</cell><cell>N/A</cell><cell>80.6?2.0</cell><cell>75.4?5.0</cell><cell>52.3?2.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.7 ? 1.7</cell><cell>80.2 ? 1.0</cell><cell>76.0 ? 3.7</cell><cell>N/A</cell></row><row><cell>GSN-e</cell><cell>90.6?7.5</cell><cell>68.2?7.2</cell><cell>76.6?5.0</cell><cell>83.5? 2.3</cell><cell>85.5?1.2</cell><cell>77.8?3.3</cell><cell>54.3?3.3</cell></row><row><cell></cell><cell>6 (cycles)</cell><cell>6 (cycles)</cell><cell>4 (cliques)</cell><cell>15 (cycles)</cell><cell cols="2">3 (triangles) 5 (cliques)</cell><cell>5 (cliques)</cell></row><row><cell>GSN-v</cell><cell>92.2?7.5</cell><cell>67.4?5.7</cell><cell>74.59?5.0</cell><cell>83.5?2.0</cell><cell>82.7?1.5</cell><cell>76.8?2.0</cell><cell>52.6?3.6</cell></row><row><cell></cell><cell cols="2">12 (cycles) 10 (cycles)</cell><cell>4 (cliques)</cell><cell cols="4">3 (triangles) 3 (triangles) 4 (cliques) 3 (triangles)</cell></row></table><note>* Graph Kernel methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>MAE in ZINC</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>MAE (EF)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test and Validation ROC-AUC in OGB-MOLHIV.</figDesc><table><row><cell>Method</cell><cell>Test</cell><cell>Validation</cell></row><row><cell></cell><cell>ROC-AUC</cell><cell>ROC-AUC</cell></row><row><cell>GIN+VN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison between DeepSets and GSN with the same structural features</figDesc><table><row><cell>Dataset</cell><cell>DeepSets</cell><cell cols="2"># params GSN</cell><cell># params</cell></row><row><cell>MUTAG</cell><cell>93.3?6.9</cell><cell>3K</cell><cell>92.8?7.0</cell><cell>3K</cell></row><row><cell>PTC</cell><cell>66.4?6.7</cell><cell>2K</cell><cell>68.2?7.2</cell><cell>3K</cell></row><row><cell>Proteins</cell><cell>77.8?4.2</cell><cell>3K</cell><cell>77.8?5.6</cell><cell>3K</cell></row><row><cell>NCI1</cell><cell>80.3 ?2.4</cell><cell>10K</cell><cell>83.5? 2.0</cell><cell>10K</cell></row><row><cell>Collab</cell><cell>80.9 ?1.6</cell><cell>30K</cell><cell>85.5?1.2</cell><cell>52K</cell></row><row><cell>IMDB-B</cell><cell>77.1 ?3.7</cell><cell>51K</cell><cell>77.8?3.3</cell><cell>65K</cell></row><row><cell>IMDB-M</cell><cell>53.3 ?3.2</cell><cell>68K</cell><cell>54.3?3.3</cell><cell>66K</cell></row><row><cell>ZINC</cell><cell cols="2">0.288 ?0.003 366K</cell><cell cols="2">0.108 ?0.018 385K</cell></row><row><cell cols="2">ogbg-molhiv 77.34?1.46</cell><cell>3.4M</cell><cell>77.99?1.00</cell><cell>3.3M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Chosen hyperparameters for each of the two GSN variants for each dataset.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell cols="2">Proteins NCI1</cell><cell cols="3">Collab IMDB-B IMDB-M</cell></row><row><cell></cell><cell>batch size</cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell></cell><cell>width</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell></cell><cell>decay rate</cell><cell>0.9</cell><cell>0.5</cell><cell>0.5</cell><cell>0.9</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell></cell><cell>decay steps</cell><cell>50</cell><cell>50</cell><cell>10</cell><cell>10</cell><cell>50</cell><cell>10</cell><cell>10</cell></row><row><cell>GSN-e</cell><cell>dropout</cell><cell>0.5</cell><cell>0</cell><cell>0.5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>lr</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?3</cell></row><row><cell></cell><cell>degree</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell></cell><cell>substructure type</cell><cell cols="2">graphlets motifs</cell><cell>same</cell><cell cols="2">graphlets same</cell><cell>same</cell><cell>same</cell></row><row><cell></cell><cell>substrucure family</cell><cell>cycles</cell><cell>cycles</cell><cell>cliques</cell><cell>cycles</cell><cell>clique</cell><cell>clique</cell><cell>cliques</cell></row><row><cell></cell><cell>k</cell><cell>6</cell><cell>6</cell><cell>4</cell><cell>15</cell><cell>3</cell><cell>5</cell><cell>5</cell></row><row><cell></cell><cell>batch size</cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell></cell><cell>width</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell></cell><cell>decay rate</cell><cell>0.9</cell><cell>0.5</cell><cell>0.5</cell><cell>0.9</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell></cell><cell>decay steps</cell><cell>50</cell><cell>50</cell><cell>10</cell><cell>10</cell><cell>50</cell><cell>10</cell><cell>10</cell></row><row><cell>GSN-v</cell><cell>dropout</cell><cell>0.5</cell><cell>0</cell><cell>0.5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>lr</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?3</cell></row><row><cell></cell><cell>degree</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell></cell><cell>substructure type</cell><cell cols="3">graphlets graphlets same</cell><cell>same</cell><cell>same</cell><cell>same</cell><cell>same</cell></row><row><cell></cell><cell>substrucure family</cell><cell>cycles</cell><cell>cycles</cell><cell>cliques</cell><cell>cycles</cell><cell cols="2">cliques clique</cell><cell>cliques</cell></row><row><cell></cell><cell>k</cell><cell>12</cell><cell>10</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the edge automorphism group is larger than that of induced automorphisms, but strictly larger only for 3 trivial cases<ref type="bibr" target="#b26">[27]</ref>. However, induced automorphisms provide a more natural way to express edge structural roles.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A larger version of GSN using 500K parameters attains 0.101 ? 0.010 test MAE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">the most representative ones from the OGB public leaderboard: https://ogb.stanford.edu/docs/leader_graphprop/ #ogbg-molhiv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">if one considers a simple 1-WL extension that concatenates edge labels to neighbour colours, then the same proof applies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Lemma 5 from<ref type="bibr" target="#b15">[16]</ref> states that such a function always exists assuming that the elements of the multiset originate from a countable domain</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">more details on the description of the datasets and the corresponding tasks can be found at<ref type="bibr" target="#b15">[16]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This research was partially supported by the ERC Consolidator Grant No. 724228 -LEMAN (GB and MB). The work of GB is partially funded by a PhD scholarship from the Department of Computing, Imperial College London. SZ acknowledges support from the EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1) and a Google Faculty award. MB acknowledges support from Google Faculty awards and the Royal Society Wolfson Research Merit award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The update of the 2-FWL is:</p><p>? times same for all nonedge type 2-tuples.</p><p>? generalised neighbour of an edge type tuple:</p><p>The update of the 2-FWL is:</p><p>? times same for all edge type 2-tuples.</p><p>From the analysis above, it is clear that all 2-tuples in the graph of the same initial type are assigned the same colour in the 1st iteration of 2-FWL. In other words, the vertices cannot be further partitioned, so the algorithm terminates. Therefore, if two SR graphs have the same parameters n,d,?,? then 2-FWL will yield the same colour distribution and thus the graphs will be deemed isomorphic.</p><p>Disambiguation Scores: In <ref type="table">Table 6</ref>, we provide the disambiguation scores ? as defined in section 5.5.1 of the main paper for different types of substructures. These are computed based on vertex structural identifiers (GSN-v). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Graph Classification on ogbg-molhiv</head><p>The ogbg-molhiv dataset contains ? 41K graphs, with 25.5 vertices and 27.5 edges on average. As most molecular graphs, the average degree is small (2.2) and they exhibit a tree-like structure (average clustering coefficient 0.002). The average diameter is 12 (more details in <ref type="bibr" target="#b25">[26]</ref>). Below we describe how we extend the two base architectures:</p><p>GIN+VN. We follow the design choices of the authors of <ref type="bibr" target="#b25">[26]</ref> and extend their architectures to include structural identifiers. Initial vertex and edge features are multi-hot encodings passed through linear layers that project them in the same embedding space, i.e. h 0 v = W 0 h ? h in v , e t v,u = W t e ? e in u,v . The baseline model is a modification of GIN that allows for edge features: for each neighbour, the hidden representation is added to an embedding of its associated edge feature. Then the result is passed through a ReLU non-linearity which produces the neighbour's message. Formally, the aggregation is as follows:</p><p>In order to allow global information to be broadcasted to the vertices, a virtual node takes part in the message passing. The virtual node representation, denoted as G t , is initialised as a zero vector G 0 and then Message Passing becomes:h</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mm Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Richard C Gerkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10685</idno>
		<title level="m">Machine learning for scent: Learning generalizable perceptual representations of small molecules</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2693" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops, (ICCVW)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural 3d morphable models: Spiral convolutional networks for 3d shape representation learning and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Bokhnyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7213" to="7222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05425</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15794" to="15805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference (ESWC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: Subgraph counts and related graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikraman</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fuhlbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of Computation Theory (FCT)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11651</biblScope>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The strength of weak ties: A network theory revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granovetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sociological Theory</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Congruent graphs and the connectivity of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassler</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="168" />
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
		<ptr target="https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf" />
	</analytic>
	<monogr>
		<title level="j">NTI, Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
	<note>English translation is available at</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<title level="m">Random features strengthen graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A congruence theorem for trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="961" to="968" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A collection of mathematical problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>Interscience Publishers</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Small graphs are reconstructible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian J. Combinatorics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="126" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The weisfeiler-lehman method and graph isomorphism testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling interactome: scale-free or geometric?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Corneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurisica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3508" to="3515" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6295</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural message-passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A general purpose algorithm for counting simple cycles and simple paths of any length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2716" to="2737" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A faster algorithm for detecting network motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wernicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithms in Bioinformatics, 5th International Workshop, WABI</title>
		<editor>Rita Casadio and Gene Myers</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3692</biblScope>
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient detection of network motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wernicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="359" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FANMOD: a tool for fast network motif detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wernicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Rasche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1152" to="1153" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Frequent subgraph mining by walking in order embedding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshops</title>
		<imprint>
			<publisher>ICMLW</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyu</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arquimedes</forename><surname>Canedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03092</idno>
		<title level="m">Neural subgraph matching</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A (sub)graph isomorphism algorithm for matching large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1367" to="1372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7090" to="7099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">From graph low-rank global attention to 2-fwl approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go sparse: Towards scalable higherorder graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutzel</forename><surname>Petra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4083" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Locality in distributed graph algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">What can be computed locally?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08349</idno>
		<title level="m">Natural graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Distance encoding-design provably more powerful gnns for structural representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random graph isomorphism. SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">On the power of combinatorial and spectral invariants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page" from="2373" to="2380" />
		</imprint>
	</monogr>
	<note>Linear algebra and its applications</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On the combinatorial power of the weisfeiler-lehman algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithms and Complexity (CIAC)</title>
		<imprint>
			<biblScope unit="volume">10236</biblScope>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Lov?sz meets weisfeiler and leman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming (ICALP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>14. Schloss Dagstuhl -Leibniz-Zentrum f?r Informatik</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Local structure in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Uncovering biological network function via graphlet degree signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijana</forename><surname>Milenkovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Graphlet-based characterization of directed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anida</forename><surname>Sarajli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?l</forename><surname>Malod-Dognin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">35098</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>?mer Nebil Yaveroglu, and Nata?a Pr?ulj</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Cyclic pattern kernels for predictive graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fast neighborhood subgraph pairwise distance kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>De Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, (ICML)</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Graph homomorphism convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">motif2vec: Motif aware node representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Dareddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1052" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan A Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin Abbasi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yadkori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09303</idno>
		<title level="m">Hone: Higher-order network embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Motifnet: A motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="225" to="228" />
			<date type="published" when="2018" />
			<publisher>DSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Meta-gnn: metagraph neural network for semisupervised learning in attributed heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
		<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Michael Lingzhi Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05256</idno>
		<title level="m">A hierarchy of graph neural networks based on learnable local features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Near: Neighborhood edge aggregator for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolhyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeseong</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02746</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Neural message passing on high order paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<publisher>NeurIPSW</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory and Kernel Machines (COLT)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2777</biblScope>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangcheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Tie-Yan Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjam?n</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? Miguel Hern?ndez-Lobato</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2328" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">ZINC: A free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Breaking the limits of message passing graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Gin</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Reoptimization of mdl keys for use in drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nourse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, (ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<title level="m">Parameterized hypercomplex graph neural networks for graph classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases, 2020. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Surveying macrocyclic chemistry: from flexible crown ethers to rigid cyclophanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siva Krishna Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Fraser</forename><surname>Nalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoddart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Society Reviews</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2459" to="2478" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Hongler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8580" to="8589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Sr</surname></persName>
		</author>
		<title level="m">15 graphs ? SR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">227</biblScope>
		</imprint>
	</monogr>
	<note>41 graphs ? SR</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">We used a simple 2-layer architecture with width 64. The message aggregation was performed as in the general formulation of Eq. (5) of the main paper, where the update and the message functions are MLPs. The prediction is inferred by applying a sum readout function in the last layer and then passing the output through a MLP. Regarding the substructures, we use graphlet counting, as certain motifs (e.g. cycles of length up to 7) are known to be unable to distinguish strongly regular graphs</title>
	</analytic>
	<monogr>
		<title level="m">The total number of non-isomorphic pairs of the same size is ? 7 * 10 7</title>
		<imprint/>
	</monogr>
	<note>since they can be counted by the 2-FWL [71, 18</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Training is stopped when the learning rate reaches the minimum learning rate value of 10 ?5 . Validation and test metrics are inferred using the model at the last training epoch</title>
		<imprint/>
	</monogr>
	<note>improvement in the validation loss</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">We search cycles with k = 3, . . . , 10, graphlets vs motifs, and GSN-v vs GSN-e. The chosen hyperparameters for GSN are: GSN-e, cycle graphlets of 10 vertices and for GSN-EF : GSN-v, cycle motifs of 8 vertices. Once the model is chosen, we repeat the experiment 10 times with different seeds and report the mean and</title>
		<imprint/>
	</monogr>
	<note>We select our best performing substructure related parameters based on the performance in the validation set in the last epoch. standard deviation of the test MAE in the last epoch</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
