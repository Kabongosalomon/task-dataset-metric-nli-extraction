<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybridization of Capsule and LSTM Networks for unsupervised anomaly detection on multivariate data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayman</forename><surname>Elhalwagy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tatiana</forename><surname>Kalganova</surname></persName>
						</author>
						<title level="a" type="main">Hybridization of Capsule and LSTM Networks for unsupervised anomaly detection on multivariate data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly Detection</term>
					<term>Capsule</term>
					<term>LSTM</term>
					<term>Neural Networks</term>
					<term>Unsupervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have recently shown promise in the field of anomaly detection, providing a flexible and effective method of modelling systems in comparison to traditional statistical modelling and signal processing-based methods. However, there are a few well publicised issues Neural Networks (NN)s face such as generalisation ability, requiring large volumes of labelled data to be able to train effectively and understanding spatial context in data. This paper introduces a novel NN architecture which hybridises the Long-Short-Term-Memory (LSTM) and Capsule Networks into a single network in a branched input Autoencoder architecture for use on multivariate time series data. The proposed method uses an unsupervised learning technique to overcome the issues with finding large volumes of labelled training data. Experimental results show that without hyperparameter optimisation, using Capsules significantly reduces overfitting and improves the training efficiency. Additionally, results also show that the branched input models can learn multivariate data more consistently with or without Capsules in comparison to the non-branched input models. The proposed model architecture was also tested on an open-source benchmark, where it achieved state-of-the-art performance in outlier detection, and overall performs best over the metrics tested in comparison to current state-of-the art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ime Series data analysis is a prominent field of research due to the significant demand stemming from increasingly larger datasets being acquired in industrial and commercial environments. The automation of this analysis has been integral to the advancement of Industry 4.0 <ref type="bibr" target="#b0">[1]</ref>, which refers to the automation of industrial processes. One important use case for time series analysis is outlier detection, which is an important part of the function of intelligent systems in the context of fault diagnosis.</p><p>There are various traditional approaches to fault detection by way of hardware redundancy in literature <ref type="bibr" target="#b1">[2]</ref>. However, a rising need in industry for a lightweight and cost-effective solution to This paragraph of the first footnote will contain the date on which you submitted your paper for review, which is populated by IEEE. It is IEEE style to display support information, including sponsor and financial support acknowledgment, here and not in an acknowledgment section at the end of the article. For example, "This work was supported in part by the U.S. Department of Commerce under Grant BS123456." The name of the corresponding author appears after the financial information, e.g. (Corresponding author: M. Smith). Here you may also indicate if authors contributed equally or if there are co-first authors.</p><p>The next few paragraphs should contain the authors' current affiliations, including current address and e-mail. For example, First A. Author is with the National Institute of Standards and Technology, Boulder, CO 80305 USA (e-mail: author@ boulder.nist.gov).</p><p>Second B. Author, Jr., was with Rice University, Houston, TX 77005 USA. He is now with the Department of Physics, Colorado State University, Fort Collins, CO 80523 USA (e-mail: author@lamar.colostate.edu).</p><p>Third C. Author is with the Electrical Engineering Department, University of Colorado, Boulder, CO 80309 USA, on leave from the National Research Institute for Metals, Tsukuba 305-0047, Japan (e-mail: author@nrim.go.jp).</p><p>Mentions of supplemental materials and animal/human rights statements can be included here. Color versions of one or more of the figures in this article are available online at http://ieeexplore.ieee.org fault detection as a result of Industry 4.0 has encouraged the development of soft sensing systems, which use the existing sensors in a system to infer further information regarding the system. Recently, Neural Networks (NNs) have been identified as an effective tool in data analysis and fault detection due to their unique ability to be trained to identify numerical relationships in different forms of data <ref type="bibr" target="#b2">[3]</ref>. They provide an advantage over traditional signal processing and statistical techniques due to the level of complexity that they can model the data, as well as being generalizable to similar types of data <ref type="bibr" target="#b2">[3]</ref>. Furthermore, there is minimal data manipulation needed for the use of NNs, which can simplify the implementation of such systems.</p><p>For time series analysis, Recurrent Neural Network (RNN) based models such as the Long-Short Term Memory (LSTM) network <ref type="bibr" target="#b3">[4]</ref> are generally used due to their ability to identify dependencies in sequential data using their internal memory <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, however some recent works have utilised the Convolutional Neural Network (CNN), which has proved to be a powerful tool in image classification <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, in time series forecasting <ref type="bibr" target="#b8">[9]</ref> and outlier detection tasks <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Furthermore, the hybridisation of the aforementioned layers has also been shown to be a method of improving outlier detection performance <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, it is well documented that the CNN has a fundamental flaw in understanding spatial context in data; this is most prominently demonstrated in image classification tasks. The Capsule Network (CapsNet) <ref type="bibr" target="#b13">[14]</ref> was proposed by Hinton et al to address this flaw and has successfully shown state-ofthe-art performance in image classification tasks with different variants of the network <ref type="bibr" target="#b14">[15]</ref>. Additionally, some work has been done utilising the CapsNet for use on time series data in its raw format <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, but predominantly using image representations <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>; most approaches use an image representation of the data as the CapsNet has been proven to improve performance in this context. In this paper, we propose the hybridisation of the CapsNet and the LSTM Network in a branched Autoencoder architecture for use on raw time series T &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; data.</p><p>The contributions of this paper are summarised as follows: 1) A hybridisation of the LSTM layer and the Capsule layer is implemented in a novel branched input, merged output model architecture for use on raw multivariate time series data 2) The model is tested on a real-world dataset and benchmarked on another real-world dataset against the state-of-the-art anomaly detection methods in the field for a performance comparison</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND RELATED PUBLICATIONS</head><p>This section will first explore the advantages of soft sensing methods over traditional hardware redundancy techniques for fault detection, then outline the benefits of NN soft sensing methods over traditional system modelling and signal processing techniques. NN based fault detection methods will then be reviewed so that a justification for the proposed method can be made.</p><p>Fault detection systems have been researched and improved extensively over the last two decades due to the intense demand to automate industrial processes, also known as Industry 4.0 <ref type="bibr" target="#b20">[20]</ref>. There have been numerous approaches that aim to be effective in detecting different types of faults in different systems, due to the nature of the usage of the system or other reasons relating to the susceptibility of the system to certain faults. Some approaches for fault detection have involved using methods and techniques such as redundancy for sensors, sometimes paired with analytical redundancy methods <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>.</p><p>The common issue with these proposed solutions is that they involve the installation and maintenance of physical hardware to monitor the sensors or the system, which naturally means that they may require redundancy in more sensitive use cases: for example, that require the monitoring of life-threatening substances with very sensitive sensors. Furthermore, this guarantees an increase in the operating costs of these solutions due to increased energy usage and maintenance costs and provides another barrier to the goal of achieving automation. However more recently, soft sensing methods have been explored with the goal of using the information available from the sensors already implemented in the system to calculate an estimate of the quality of data being collected. This approach provides an economical and cost-effective alternative to physical systems by not needing to implement any additional physical hardware that could be expensive to buy or maintain whilst achieving robust fault detection scores that are comparable to and even better than physical systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Soft Sensing Methods for Fault Detection</head><p>Statistical analysis and signal processing are frequently used methods in the field of anomaly detection. As a method of soft sensing, they are able to overcome the drawbacks of physical hardware monitoring and provide a robust method of data inference. For instance in <ref type="bibr" target="#b23">[23]</ref> a dynamic model is proposed that is able to utilise the existing supervisory control and data acquisition (SCADA) system in wind turbines to dynamically model the relationship between the sensor readings by a parameter estimation process for the purpose of fault detection. A frequency domain analysis is used to determine damage sensitive indices which are then compared to the model sensor. The technique is tested on a 5-year wind turbine dataset where the system was able to detect faults as well as perform fault prognosis. Whilst the method is clearly effective in the specified use case, the flexibility of the method for other use cases comes into question as in-depth knowledge about the system and the relationships between the variables being analysed was utilised to be able to create the initial model. This issue is also mentioned in <ref type="bibr" target="#b24">[24]</ref> where the authors concluded from their survey of outlier detection techniques that modeldriven methods are heavily dependent on the understanding of the data being analysed. The lack of flexibility of such techniques is also mentioned, due to the heavy tailoring that must be made to the models for each dataset. This is a trend across many signal processing techniques including for motor condition monitoring where <ref type="bibr" target="#b25">[25]</ref> noted in their state-of-the art review of outlier detection techniques the lack of flexibility of data analysis techniques such as acoustic analysis and motor current signal analysis (MCSA) in detecting a wide range of faults that could occur within the system.</p><p>More recently, Machine Learning (ML) has been heavily utilised in literature for the modelling of such systems. As well as being a soft sensing technique, ML is able to provide a higher degree of flexibility in terms of application as well as being generally easier to implement than the aforementioned techniques. Various examples of literature can be found that utilise proposed NN models in multiple use cases and datasets <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b26">[26]</ref>. However, this is not to say that generalisation is still not an issue with NNs. The main issue found in literature with NNs is the importance of data volume and representation in being able to train NNs effectively. Most NN types such as the CNN and the LSTM require large quantities of data to effectively learn the shape and features of the data, and some methods even require the labelling of the data before training, known as supervised learning <ref type="bibr" target="#b11">[12]</ref>, which is very timeconsuming and costly as this is usually a manual process. Furthermore, with some types of data it is very difficult to distinguish faults and anomalies in raw sequential format.</p><p>To address these discussed issues with NNs, researchers have opted to combine signal processing techniques with NNs where applicable in order to utilise the advantages provided by the former with data representation and the latter in flexibility and ease of use. This approach has seen great success in motor fault detection <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, where in these cases frequency domain transformations were used to enhance the representation of the data for use with LSTM networks. Additionally, the use of various types of CapsNets in numerous cases was found to improve training and classification performance over smaller datasets <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. As well as hybridising signal processing and ML, many literatures also propose the hybridisation of NN types to take advantage of their advantages with different types of data; one popular hybridisation for TS data is RNNs and &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; CNNs <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LSTM based Autoencoder</head><p>The LSTM network <ref type="bibr" target="#b3">[4]</ref> has lately grown in popularity due to the overwhelming demand for time series analysis and forecasting in commercial environments, hence fuelling the demand for more research into the improvement of its performance. One recently proposed method <ref type="bibr" target="#b31">[31]</ref> explored the usage of the LSTM layer in an Autoencoder architecture. The authors correctly identified that a large number of the current machine learning methods that are used are unsuitable for use practically, as they usually require the use of labelled data which is impractical with time series data due to the large volumes being constantly produced. Furthermore, the authors go on to evaluate classical anomaly detection methods such as Support Vector Machines and Isolation forests as being flawed since they fail to account for the temporal aspect of the time series data and only take the current data into account. In addition to this, they demonstrate, with an initial experiment, that other methods such as signal decomposition are only effective when used with periodic data, so their use is limited in that aspect. However, the authors <ref type="bibr" target="#b31">[31]</ref> noted that the simplicity of this method is an advantage over the LSTM autoencoder approach that was being explored, but the necessity of manual parameter selection was considered a drawback.</p><p>The proposed method in <ref type="bibr" target="#b31">[31]</ref> uses the sliding window algorithm to feed the data into the LSTM Autoencoder, for which the number of layers and LSTM cells were optimised. The neural network was trained by fitting the output to the input signal, and the mean absolute error between the prediction and signal was used as the threshold for testing for anomalies. The system was tested on sound files from the DCASE dataset which were down sampled to 16000 samples per second, and the sliding window would take 1-second steps. Results <ref type="bibr" target="#b31">[31]</ref> show an 87% accuracy for anomaly detections, with the correct location identified 91.7% of the time.</p><p>Evaluating the approach used in this paper <ref type="bibr" target="#b31">[31]</ref>, it shows promise with the accuracy of detection and the wide application of its usage, but various drawbacks were identified: the authors selectively used data that was loud enough to be detected by the autoencoder and did not explore the sensitivity of detection. Furthermore, the authors assumed that the training data acquired was "clean" of any anomalies, which could be a reasonable assumption to make since the data was taken from an established dataset, but in a real-life use case, this may not be the case. However, since this is an unsupervised approach, it is expected that all initial errors will not be identified unless extensive data analysis is carried out before training the system, or if previous knowledge about the operation of the system being analysed is acquired. Furthermore, due to the sliding window approach, the location of the anomaly was a parameter that had to be measured, which could be overcome if a different approach to parsing the data was used. One main issue that the LSTM faces is its overfitting when used with gradient descent learning optimisation algorithms. Although very careful tweaking of hyperparameters can help to reduce this issue, this is often highly inefficient and time consuming and with complex datasets is sometimes unavoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Capsule Network application in time series data analysis</head><p>The CapsNet <ref type="bibr" target="#b13">[14]</ref> is a novel neural network developed to overcome issues faced with the spatial context of image classification that is encountered when using prevalent image classifications such as convolutional neural networks. It does this by "encapsulating" the entity being described in a vector format, where the length describes the probability of existence and the orientation of the vector describes the entity's characteristics, such as orientation and the special context that other traditional neural networks cannot capture. The original idea was conceptualized in <ref type="bibr" target="#b13">[14]</ref> by Hinton et al, but some literatures <ref type="bibr" target="#b32">[32]</ref> have built on this work by adjusting the architecture to work with Adaptive Gradient Descent optimisation algorithms for image classification. However, the effectiveness of the latter approach has not been fully explored for raw time series data at the time of writing. This architecture is mainly used for image classification as demonstrated by <ref type="bibr" target="#b33">[33]</ref> for brain tumour classification using MRI images and <ref type="bibr" target="#b34">[34]</ref> for "Hyperspectral Image Classification", but more recently its usage has been explored limitedly in a time series use case.</p><p>A few papers currently exist that utilise this neural network architecture for time series tasks <ref type="bibr" target="#b35">[35]</ref>. However, the approaches that most papers use is to transform the data into an image representation, which has already been identified as a powerful usage of this network. For example, one proposed approach <ref type="bibr" target="#b36">[36]</ref> aimed to utilise capsule networks to address an issue with the detection of short circuit faults in a power network transmission line. The authors state that the raw signals are difficult to analyse for this task and analyse methods for time series feature extraction in the literature review. The Fourier transform was identified as a prevalent method of frequency domain analysis, however, the authors identify that the former does not take the temporal context into account which, for the use case that this paper covers, is an essential factor. Therefore, another paper was outlined <ref type="bibr" target="#b37">[37]</ref> which overcame this issue using the discrete wavelet transform, which is able to provide information from both the frequency and time domain. The authors <ref type="bibr" target="#b36">[36]</ref> note that these signal processing techniques require a high level of expert knowledge in order to leverage properly to produce good results, and image representation techniques can extract more significant features more efficiently in comparison to signal processing methods. Using this information, the authors propose a deep learning approach that overcomes the issues that current machine learning methods have with poorer feature representation due to scalar values being used and max pooling inhibiting the information learned by the neural network.</p><p>A 3-phase voltage system is first simulated <ref type="bibr" target="#b36">[36]</ref> so that different fault models can be identified and simulated, and the signal processing can be applied and tested for robustness. The discrete wavelet transform that was identified as a superior method to the Fourier analysis is used in combination with a high pass and low pass filter in order to filter the signal noise, and a polar representation of the signal is acquired which is then represented as a Gramian Angular Field (GAF), proposed in &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; <ref type="bibr" target="#b38">[38]</ref>, which is a method of time series image representation. The authors reasoning for this is a more feature-rich representation of the data in comparison to other image representations, due to the preservation of the temporal context as well as the other points identified in their literature review.</p><p>The proposed approach in <ref type="bibr" target="#b36">[36]</ref> utilises the capsule network to overcome the discussed issues in the literature review. Furthermore, the architecture proposed uses convolutional layers that accept 6 inputs corresponding to the number of signals in the 3-phase voltage system transformed into pictures. The convolutional operation then outputs to a self-attention layer, which the author <ref type="bibr" target="#b36">[36]</ref> claims produces promising results by focusing on the more relevant areas in the GAF image. A Rectified Linear Unit (Relu) activation is then used before 2 capsule layers which output the classification results. The neural network uses a novel technique referred to as weight sharing, which connects the neurons in a different configuration to a normal fully connected network so that the neurons in the previous layer share the weights so that the same number of weights as the neurons in the next layer is used when connecting to the next layer.</p><p>The authors test the proposed model architecture's anomaly detection performance on the 11 types of short circuit faults identified <ref type="bibr" target="#b36">[36]</ref>, where an overall classification accuracy of 99.81% is achieved. However, they go on to state that the classification accuracy is not detailed enough to provide a conclusion as to whether the system is robust. They also go on to test the effect of current transformer saturation on the classification ability of the network, as well as voltage and current inversions. The results achieved for the stated cases are 99.4% and 97% respectively. The proposed network was also validated on real-world data, with an accuracy of 92% attained.</p><p>This paper <ref type="bibr" target="#b36">[36]</ref> provides an objective view on the different methods used currently in power system fault detection and goes into depth on the prevalence of some time series image representation techniques over others but fails to provide an evidence-backed explanation as to why raw data is unsuitable for this use case only stating that "it is difficult to directly consider" <ref type="bibr" target="#b36">[36]</ref> them for the fault detection and classification task. This directly contradicts the statement made about the difficulty of applying signal processing tasks due to the expert knowledge required. Moreover, not every test case was explored with this approach, which was identified by the authors <ref type="bibr" target="#b36">[36]</ref> but this is to be expected since it is difficult to cover all fault types for such a complex system. On the other hand, a methodical approach was used to synthesise the proposed approach, and an evidence-backed conclusion was made to the effectiveness of their approach due to the various test cases applied and the comparison between traditional neural network models without the capsule integration. Furthermore, the weight shared capsule approach showed promise with its strong generalisation performance with the real-world dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. LSTM and Capsule hybridisation</head><p>Recently, some works have been published combining the LSTM and Capsule networks to improve performance over current state-of-the-art techniques <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. One such work <ref type="bibr" target="#b41">[41]</ref> proposes a rotating machinery fault diagnosis methodology utilising a CNN for feature extraction, a Bi-directional LSTM for denoising by dimensionality reduction and Capsules for their superior feature learning ability. The authors demonstrate the effectiveness of each proposed addition through a comparison of similar architectures with different NN combinations. The model diagnoses bearing faults in a supervised manner; this was demonstrated with an experiment on the Case Western Reserve University (CSWU) Bearing dataset <ref type="bibr" target="#b42">[42]</ref>, where raw vibration waveforms were used as the input data, and prelabelled classes as the output. The proposed model outperforms the current state-of-the-art with 98.95% accuracy whilst dramatically reducing training sample size from 5600, the sample size used by other compared methods, to just 150.</p><p>Another proposed method <ref type="bibr" target="#b44">[43]</ref> combines the LSTM and Capsule layers in a single model for EEG emotion recognition. The authors propose a channel-wise attention mechanism using a CNN to prioritise the relevant EEG channels, Capsules to extract the spatial features and LSTM layers to extract the temporal features of the data. The model was tested on a public EEG dataset, where the state-of-the-art was considerably outperformed. It was noted that NN models that use Capsules consistently outperformed models using just CNNs in all three classification categories of valence, arousal and dominance. However, the authors noted the higher computational time involved with training Capsules as opposed to CNNs.</p><p>These examples in literature demonstrate the potential advantages of hybridising the LSTM and Capsule Networks, due to the advantages that the LSTM has with temporal feature learning and the Capsules spatial feature learning ability. Furthermore, the combination of the two layers has proven to be effective in some time series applications and has potential to be used in other architectures for different use cases.</p><p>To conclude, soft sensing methods of anomaly detection are more efficient and effective methods than hardware redundancy. However, with traditional methods it is difficult to accurately model systems without in depth knowledge of their dynamics and parameters, creating a barrier to flexible and accurate system modelling which is the basis of many anomaly detection systems. However, NNs provide a solution for this issue, providing a method of easily modelling system behaviour based of previously encountered data. Using NNs such as LSTM NNs for time series data learning, researchers have been able to accurately account for long term dependency in temporal data and create robust anomaly detection systems. However, this creates another issue with requiring access to large amounts of labelled data which is expensive and time consuming to produce. To avoid labelling data, some literatures have proposed unsupervised learning techniques such as the autoencoder which is able to learn data features by transforming it into a latent space representation. However, a comprehensive training set which is fully representative of the operation of the system or device being analysed is still required to utilise this technique, and generalisation performance is weak in many &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; these methods. The Capsule was proposed to address issues with training efficiency and the shortcomings of traditional NNs with learning spatial context of data. This paper further explores these qualities found in Capsules by hybridising them with LSTMs in a NN, and addresses issues found with learning multivariate data with single channel NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED NEURAL NETWORK MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LSTM Cell</head><p>The LSTM network, proposed initially in 1997 by Hochreiter and Schmidhuber <ref type="bibr" target="#b3">[4]</ref> but popularised recently by its widespread usage in commercial environments, is a popular iteration of the RNN that can overcome the vanishing gradient issue and allows for the learning of long-term dependency. It does this using a specialised architecture that integrates "gates" to the architecture to allow the cell state to forget values and replace values, then decide which values to output and send to the next cell. A visualisation of this architecture can be seen in <ref type="figure">Fig. 1</ref>, and the equations shown in TABLE I.</p><p>The architecture can be described as follows: The "forget" gate, (1), uses a sigmoid layer to determine which irrelevant data in the cell state to remove, where a value of 1 would keep all of the data and a 0 would completely erase the information. The "replace" gate ,(2),(3),(4), is used to decide which values in the cell state to update. This gate operates by using a sigmoid <ref type="figure">Fig. 2</ref>. Autoencoder architecture visualised <ref type="bibr" target="#b45">[44]</ref> layer to decide which values to update and a tanh layer creates a vector of candidate values to add to the cell state. The old state is then multiplied by , the forget value and added to the candidate values scaled by the sigmoid. The "output" gate ,(5), <ref type="bibr" target="#b5">(6)</ref>, determines which part of the cell state to output using a sigmoid layer and is then multiplied by the cell state with a tanh layer applied to constrain the values between 1 and -1. The following equations formally define each gate:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Capsule</head><p>The Capsule network (CapsNet) <ref type="bibr" target="#b13">[14]</ref>, is a novel neural network architecture designed to address the issues the convolutional neural network has with spatial context. It does this by 'encapsulating' the spatial information between the variables using vectors which allows the neural network to learn the distances between the identified features as well as the classification of the features.</p><p>A capsule differs from the traditional artificial neuron in various ways: A traditional neuron receives scalar inputs; performs the weighted sum of the aforementioned scalars; applies an activation function and outputs a scalar dependant on the weights and biases that it has adopted through training. A capsule on the other hand, whilst operating in a similar fashion, slightly differs from the internal operation and the representation of the values that it receives. A capsule receives a vector input, where the input denotes the probability of occurrence as well as orientation and other spatial features not captured by a scalar value. It applies an "affine transformation" which is essentially a transformation matrix weight that replaces the traditional scalar weight; this operation is formally defined in (7) <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_0">?| = .<label>(7)</label></formula><p>This transformation matrix is used to represent the spatial context that is missing from the traditional method of weight application. The weighted sum of these vectors is calculated using (8) <ref type="bibr" target="#b13">[14]</ref>:</p><p>= ??| .</p><p>In order to preserve the vector information that is input to the capsule, a new type of activation is proposed known as the nonlinear "squashing" function. This operates similar to the normal x=input at current timestep, h=output of previous LSTM cell &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; activation functions discussed previously by squashing the output between 0 and 1 but does so in a way that is able to preserve the length and spatial information of the input values, so that a long vector will shrink to a value just below 1 and shorter vectors are shrunk to near 0 <ref type="bibr" target="#b13">[14]</ref>. Equation (9) <ref type="bibr" target="#b13">[14]</ref> formally defines this operation:</p><formula xml:id="formula_2">Gate Equation Forget = ( ? [? ?1 , ] + ) (1) Replace = ( ? [? ?1 , ] + ) (2) ?= tanh( ? [? ?1 , ] + ) (3) = * ?1 + * ? (4) Output = ( ? [? ?1 , ] + ) (5) ? = * tanh( )<label>(6)</label></formula><formula xml:id="formula_3">= || || 2 1+|| || 2 ? || || .<label>(9)</label></formula><p>C. Autoencoder Structure An Autoencoder is a variant of neural network architecture that aims to learn a compressed representation of the input data and copy it to the output. A compressed representation is used so that the model does not learn the noise in a data representation but only the main shapes and features of the data. An autoencoder is composed of 2 sections: An encoder and a decoder. The encoder part is used to transform the input data into a latent space representation through dimensionality reduction, which the decoder part then learns and decodes back into the input data with reduced accuracy and hence noise. A visualization of this can be seen in <ref type="figure">Fig. 2</ref>.</p><p>A formal definition of the Autoencoder operation is provided in <ref type="formula">(10)</ref> and <ref type="formula">(11) [44]</ref>:</p><p>( ) = (10) ( ( )) = ? (11) The idea is to reduce the layer width of the middle layers so that the neural network compresses the input instead of just learning the exact representation: this is known as an undercomplete Autoencoder. However, learning data that is too compressed would reduce the accuracy of the reconstruction, so when training the network, the aim is to balance the denoising ability with the accuracy of reconstruction. This is determined by the reconstruction loss, and the aim of training this type of network is to minimise this loss whilst maintaining a good generalisation performance.</p><p>This type of neural network is typically used for unsupervised deep learning, as the inputs are being copied to the outputs with no labelling required, which is useful for the use case that this paper explores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proposed Layer Architecture</head><p>The proposed layer architecture is illustrated in <ref type="figure">Fig. 3</ref> As shown in the figure, the number of input branches is entirely dependent on the number of features present in the data.</p><p>Each individual feature is first encoded through an LSTM layer for dimensionality reduction and is output as a 2D vector. This dimensionality reduction is carried out to reduce the number of degrees of freedom in the model so that the risk of overfitting on data is reduced. Additionally, representing the data in latent space also helps the NN to learn data features more easily. The Repeat Vector layer transforms the 2D tensor input back to a 3D tensor by repeating the fixed length vector n number of times; in this case the number of repetitions is set to be equal to the number of timesteps being input in the system. The 3D tensor is then input into a Capsule layer, where the number of Capsules in equal to the number of timesteps, and the width of the Capsule is equal to the LSTM hidden layer width. The output of each branch is then concatenated into a single vector which is input into a Capsule layer with an equal number of Capsules as the previous Capsule layers but with a width equal to the product of the width of the previous Capsules and the number of branches. The Time Distributed layer is then used to apply a Dense layer to each vector in the 3D output. Since this is an Autoencoder, the output should be equal to the input vectors merged into one vector with all the input features present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Fault Detection method</head><p>To be able to detect a fault, the neural network will be trained to reconstruct healthy data. This can be done when fitting by setting the input and the output as the same dataset, which would be the healthy operation of the data. While technically this is an unsupervised task as the initial data provided is unlabeled so the condition of the data is unknown, it can be framed as a supervised learning task using this method.</p><p>After this, the reconstruction error can be found using the Mean Absolute Error (MAE) of the training predictions. The maximum prediction error for the training set can be used as the reconstruction error threshold, which essentially means that the worst prediction case is being used as the threshold initially so that when applying the system to more data from the system being analysed, any predictions outside this value will be more likely to be an anomaly. The sensitivity of the anomaly detection can be adjusted by changing the threshold value, so this will be experimented with in order to find the optimal value that will minimise the false positive and false negative rate. Furthermore, each data feature will have its own error threshold to maximise the accuracy of detection as the neural network may perform better on some features than others. An example of a threshold calculation can be seen in <ref type="figure">Fig. 4</ref>. The main aim of the training process is to minimise the loss and standard deviation of this plot so that the system is able to make more confident and sensitive anomaly predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTATION</head><p>To demonstrate the effectiveness of the contributions of this paper, four different NN models have been compared:</p><p>? For the first experiment, the effectiveness of CapsNet integration is explored in terms of training performance &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; <ref type="figure">Fig. 9</ref>. Training data subset from the reference drone with the 3 outputs shown and MSE of prediction on test data. The proposed model (Design A, <ref type="figure">Fig. 5</ref>) is compared to a similar model with the Capsule layers removed and replaced with LSTM layers <ref type="figure">(Design B, Fig. 6</ref>). ? For the second experiment, the effectiveness of the branched model was observed. This was demonstrated by synthesising a non-branched model with the same layer structure as the branched model (Design C, <ref type="figure">Fig. 7</ref>). ? The final experiment will demonstrate the effectiveness of both additions being used simultaneously. This will be shown by using a nonbranched model consisting of just LSTM layers (Design D, <ref type="figure">Fig. 8</ref>). For the following experiments, each model was adjusted so that the trainable parameters are similar to the LSTMCaps model for experimental consistency. The models were then trained on the datasets with equal training iterations (epochs) 5 times each, and an average was taken. This was to observe the training stability of the respective models.</p><p>The final training and validation loss values will be used to measure the training efficiency of each NN model tested, and the Mean Squared Error (MSE), Mean Absolute Error (MAE) and F1 score of testing data predictions will be used to measure the prediction accuracy of the models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drone Data</head><p>The drone dataset was acquired from previous work conducted by a researcher in the same department. The dataset consists of 3 output features from unknown sensors. 3 subsets of data were provided, the sample sizes and respective visualisations are outlined below: Reference device: ? Sample size: 600 secs ( <ref type="figure">Fig. 9)</ref> ? Sample size: 30 secs ( <ref type="figure">Fig. 10)</ref> Test Device: ? Sample size: 30 secs <ref type="figure">(Fig. 11)   Fig. 10</ref>. Validation data subset from the reference drone with the 3 outputs shown <ref type="figure">Fig. 11</ref>. Testing data subset from the test drone with the 3 outputs shown, and anomalies circled in red  <ref type="figure">Fig. 13</ref>. Binary plot of the respective anomalies and changepoints in <ref type="figure" target="#fig_2">Fig. 12</ref> As shown in <ref type="figure">Fig. 11</ref>, it is clear to see that there are anomalies from the test device in feature 1 and feature 3 at the same points temporally. The reason for the anomalous data stems from malicious code affecting the drone controls causing the direction of the drone to differ from the intended direction input by the drone operator. Since the data was received unlabelled, the anomalies were manually labelled so that a measure of the anomaly detection performance of each NN model could be attained. The metric used for this is the F1 score, which is defined in <ref type="formula" target="#formula_4">(12)</ref>:</p><formula xml:id="formula_4">F 1 = TP TP+ 1 2 (FP+FN) ,<label>(12)</label></formula><p>where 1 = F1 Score, TP = True Positive, FP = False Positive, FN = False Negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKAB Anomaly Benchmark</head><p>The SKAB anomaly detection benchmark <ref type="bibr" target="#b46">[45]</ref> is a public benchmark available online used for offline outlier detection and changepoint detection testing. The benchmark consists of 35 subsets of data from a water circulation system which contain 8 features each from different sensors in the system. The test is conducted by looping through each subset, training the neural network on a slice of clean data from the subset then testing it on labelled anomalies that were simulated with the test rig. The metrics used to gauge the effectiveness of the anomaly detection are the F1 score <ref type="bibr" target="#b11">(12)</ref> and the NAB Changepoint </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM Activation tanh</head><p>Metric <ref type="bibr" target="#b47">[46]</ref>. <ref type="figure" target="#fig_2">Fig. 12</ref> illustrates a subset of data from the benchmark, and <ref type="figure">Fig. 13</ref> shows the plot for the anomalies in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Pre-processing</head><p>Data pre-processing can be segmented into 4 sections: Cleaning, Integration, reduction, and transformation. For the datasets acquired, most of the pre-processing procedure was not essential as the data was acquired in a format that implied that the cleaning and integration had already been carried out. Moreover, the data did not require reduction since time series data is sequential so removing or shuffling the dataset would compromise the integrity of the readings. However, it was necessary for the data to be transformed; this is an integral part of data pre-processing and is carried out due to the benefits it can have on the performance on the neural network with the speed of convergence when training as well as performance. To improve the neural network performance, it is common practice to rescale the data. For these datasets, the type of rescaling chosen was Z-Score Normalisation <ref type="bibr" target="#b12">(13)</ref>: z=</p><formula xml:id="formula_5">(X -?) ? ,<label>(13)</label></formula><p>where X = un-normalised data point, ? = mean of the dataset, ? = standard deviation of the dataset and z = normalised data point. This operation normalises the dataset so that the mean is equal to 0 and the standard deviation is equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 1: Drone Data</head><p>This experiment aims to explore the training capability of the proposed NN architecture (Design A, <ref type="figure">Fig. 5</ref>) by making a  <ref type="figure">Fig. 8</ref>), the non-branched hybridised LSTMCaps NN (Design C, <ref type="figure">Fig. 7</ref>) and the branched non-hybridised LSTM NN (Design B, <ref type="figure">Fig. 6</ref>). The models are first trained with nonoptimised hyperparameters, then each optimised for the drone dataset and tested for their anomaly detection capabilities.</p><p>The NN models were first tested with the default recommended hyperparameters in TensorFlow documentation and literature; this was to purely observe the raw effect of the inclusion of the Capsule layer as well as the introduction of the branched architecture on the training performance. The hyperparameters used are shown in <ref type="table" target="#tab_2">Table II</ref>. Each model was trained 5 times on the 5-minute subset from the reference device and the training and validation loss scores were recorded. An average was taken of these values for testing rigour: the results are shown in <ref type="table" target="#tab_2">Table III</ref>, as well as the improvement in training performance with the inclusion of the Capsule Layer. The training plot is also illustrated so that the stability of the training can be better visualised. Additionally, the percentage of overfitting is shown, which refers to the percentage difference between the training and validation losses.</p><p>The neural network models were then optimised using an iterative testing method so that the effect of changing each hyperparameter value can be seen and hence from this, the most optimal configuration of the hyperparameters for each model can be found.</p><p>The values that were monitored were the training and validation loss, and the training time. The loss value was chosen as the metric to gauge the effectiveness of the system due to the nature of the outlier detection technique. Since the anomaly threshold is calculated using the prediction residual, a low loss value allows for a lower threshold for the loss when testing the model, which can potentially lead to a more sensitive and accurate fault detection system. The validation loss scores were considered with more weight than the training loss when quantitively analysing the system as they were used to determine the error threshold, as well as them being a better indicator of the generalisation ability of the NN.</p><p>The optimal hyperparameters were then implemented into the proposed models for further testing. Before conducting the testing, each NN model was adjusted so that all networks being trained have a similar number of parameters for the purpose of experimental rigour. This will reduce the difference between each model so that the effect of the proposed architecture and hybridisation can be better observed on anomaly detection performance.</p><p>Each model was trained 5 times, and for each individual training procedure the prediction MSE and MAE thresholds were recorded, as well as the standard deviations of the latter to observe the consistency of training for each feature. Using the thresholds, the NNs were made to predict the test data, and any predictions exceeding the thresholds set were outlined as anomalies. The predicted anomalies were then compared to the real anomalies labelled during data analysis, and the precision, recall and F1 scores were calculated for each NN. The best score attained by each NN model is depicted in <ref type="table" target="#tab_4">Table IV</ref>, and the average score over the 5 runs in <ref type="table">Table V.</ref> The results in <ref type="table" target="#tab_2">Table III</ref> clearly show an improvement in performance with the proposed additions. The addition of the Capsule layer to the non-branched model variant using nonoptimised hyperparameters shows a clear improvement in the training and validation losses respectively. Training results using non-optimised hyperparameters have an overall more stable training procedure; evidence for this is shown in the training plots accompanying the results for Design A and Design C. With the addition of the branched inputs, there is a significant improvement in performance in both the hybridised and non-hybridised models with non-optimised hyperparameters. The branched model shows a clear reduction in overfitting from 475% to 150% without Capsule layers and from 115.79% to 30.77% with the Capsule layer without hyperparameter optimisation.</p><p>After optimising each NN model hyperparameters on the dataset, the results in <ref type="table" target="#tab_4">Table IV and Table V</ref> show that the proposed model, Design A, performs better than the other models tested with anomaly detection with an average F1 score of 0.64, and a best F1 score of 0.75. However, the non-branched  <ref type="figure">Fig. 6</ref>), which in this case was more beneficial for anomaly detection since data is more likely to be flagged as an outlier. Whilst this resulted in a higher F1 score, the precision of the model is weaker in comparison to both branched variants. In the case of Design B, the average MSE of prediction was the lowest out of all the      <ref type="bibr" target="#b53">[51]</ref> 0.45805 LSTM-AE <ref type="bibr" target="#b54">[52]</ref> 0.43485 MSET <ref type="bibr" target="#b49">[48]</ref> 0.42855 Isolation forest <ref type="bibr" target="#b48">[47]</ref> 0.38765 Conv-AE <ref type="bibr" target="#b52">[50]</ref> 0.3856 LSTM-VAE <ref type="bibr" target="#b55">[53]</ref> 0.38545 Autoencoder <ref type="bibr" target="#b56">[54]</ref> 0.30325 Null score `0  <ref type="bibr" target="#b51">[49]</ref> 0.47495 LSTM <ref type="bibr" target="#b53">[51]</ref> 0.4688 LSTM-AE <ref type="bibr" target="#b54">[52]</ref> 0.44885 MSET <ref type="bibr" target="#b49">[48]</ref> 0.42855 LSTM-VAE <ref type="bibr" target="#b55">[53]</ref> 0.3896 Isolation forest <ref type="bibr" target="#b48">[47]</ref> 0.38765 Conv-AE <ref type="bibr" target="#b52">[50]</ref> 0.38605 Autoencoder <ref type="bibr" target="#b56">[54]</ref> 0.30635 Null score 0 &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; models, which did not work to its favour during anomaly detection with recall but resulted in a higher precision. By utilising the hybridisation in the branched input model, the best performance was achieved across all the metrics tested, with both optimised and non-optimised hyperparameters. Furthermore, minimal overfitting was observed when training with unoptimized hyperparameters. Consequently, it can be said that minimal hyperparameter optimisation is required when using this model architecture as these results show a resilience to overfitting and relatively strong performance when applying the network on multivariate data without hyperparameter tuning.</p><p>The results attained show that both the hybridisation of the Capsule and LSTM layers and the branched input model structure are both effective methods of improving the performance of the neural network with multivariate data, especially when used in conjunction with each other. To further substantiate this performance, the proposed model was tested against common state-of-the-art anomaly detection methods on an open-source benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 2: SKAB Anomaly benchmark</head><p>This experiment aims to compare the anomaly detection and changepoint detection performance of state-of-the-art unsupervised anomaly detection methods with the proposed NN model. A selection of NNs and ML based fault detection methods were chosen to compare on the benchmark with minimal hyperparameter optimisation applied.</p><p>The same testing procedure utilised in the SKAB benchmark's GitHub repo <ref type="bibr" target="#b46">[45]</ref> was used to test the proposed model architecture. The model was trained with 100 epochs on a subset from each dataset with early stopping set at a patience of 20, and then tested on the remainder of the dataset. The F1 scores and NAB scores achieved for each dataset are averaged, which gives the final score of the benchmark. Each model compared was also tested on the same computer for experimental rigour. The results in <ref type="table" target="#tab_2">Table VI and Table VII</ref> depict the average outlier detection score and the changepoint detection scores over 5 test iterations respectively, and the results in <ref type="table" target="#tab_2">Table VIII and Table IX</ref> show the best NN performance in a single test over the outlier and changepoint scores respectively.</p><p>To better conclude the effectiveness of each anomaly detection method over both the F1 and NAB scores simultaneously, a scaled average of both metrics was calculated. This was done by scaling the NAB score between 0 and 1 and averaging the F1 score and scaled NAB scores. The results in <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> show the scaled average of the F1 and NAB score of the average performance and best performance respectively.</p><p>While testing it was found that there was an inversely proportional relationship with outlier detection score and changepoint detection score. This meant that hyperparameters optimal for a good F1 score would not perform as well in the NAB score. To demonstrate this, the hyperparameters of the LSTMCaps NN were slightly adjusted to achieve a better score in the changepoint detection benchmark, at the expense of a slightly lower outlier detection score. This NN configuration is labelled as LSTMCaps V2 in the results shown in <ref type="table" target="#tab_5">Table VI to  Table XI.</ref> The results in <ref type="table" target="#tab_2">Table VI and Table VIII</ref> show that as an outlier detector, the proposed LSTMCaps NN achieves the best F1 score and the lowest False Negative rate out of the models tested. It also achieves the second highest False Positive rate out of the models. In terms of changepoint detection, the results in <ref type="table" target="#tab_2">Table VII and Table IX</ref> indicate that the original configuration does not perform as well, coming 5th out of the 9 methods tested. However, with a slight adjustment to the hyperparameters, the LSTMCapsV2 NN was able to come 2nd out of the 9 methods tested in both the outlier detection and changepoint detection scores and performs better than all other NN based methods in the latter. Similar outcomes can be seen for the best performing test iteration, with no improvement in relation to the other NNs and ML methods. The scaled average results in <ref type="table" target="#tab_9">Table X and Table XI</ref> show that the LSTMCapsV2 configuration is overall the best performing method over the two metrics tested.</p><p>From this test, it can be concluded that for single datapoint outlier detection, the proposed LSTMCaps branched architecture provides state-of-the-art performance. However, while the changepoint detection performance is superior to other NNs with the right adjustments to the hyperparameters, significantly better performance can be attained from non-NN based algorithms, such as the Isolation Forest algorithm <ref type="bibr" target="#b48">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Across the experiments conducted, it is clear to see that both the inclusion of the Capsule Network and the branched input architecture is integral to the improvement of the performance of the Capsule Network in terms of training and anomaly detection. The evidence for this is shown clearly across the experiments, where with standard LSTM AEs, the training and anomaly detection performance is significantly weaker than with the proposed NN.</p><p>The experimental results further suggest that the Capsule Network is most effective in the training phase. Generally, it was found that models which included Capsules were training more efficiently, reaching the local minima at a faster rate in relation to networks without Capsules. Most importantly, the results in <ref type="table" target="#tab_2">Table III</ref> for training using non-optimised hyperparameters suggest that with the use of Capsules, the hyperparameter optimisation procedure can be simplified considerably due to the lack of overfitting during training on the NN models with Capsules integrated.</p><p>One significant strength of the proposed LSTMCaps NN is its ability to learn separate data features effectively in comparison to a standard single channel NN. This is shown by the difference in standard deviation in the MAE thresholds in <ref type="table" target="#tab_4">Table IV</ref> when conducting the anomaly detection test on the drone data. This is further substantiated with the anomaly detection performance on the SKAB anomaly benchmark, which contains a larger number of more complex features than the drone data. Here it is clear to see the advantage that having separate input branches per feature brings. &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; VI. CONCLUSION AND FUTURE WORKS This paper proposed a novel hybridisation of the LSTM and Capsule Networks in a branched architecture to address the issues found in the literature review with training performance of NNs, specifically on multivariate data. The motivation for this research stemmed from the growing demand for more effective unsupervised data analysis techniques regarding outlier and anomaly detection for use in industrial and commercial environments with large datasets to assist in the advancement of Industry 4.0, the automation of industrial processes.</p><p>The proposed NN was tested first in its training performance with no hyperparameter optimisation and compared to nonhybridised and non-branched variants of the NN, where it was found that the proposed NN can train more efficiently over a smaller number of epochs in comparison to the variants with no capsules integrated in the NN, and significantly reduces overfitting. After conducting hyperparameter optimisation, the NNs were retested, this time for their anomaly performance ability using an unsupervised method of reconstructing the data and using the MAE any data outlying from the expected shape in the training data. The results of this test concluded that the proposed NN performs better than the other variants tested as a result of the proposed additions and changes to the NN architecture. To substantiate these results, the proposed NN was tested against other state-of-the-art anomaly detection methods on the SKAB anomaly detection benchmark, where with slight hyperparameter adjustments the proposed method was able to perform better than all other methods tested for outlier detection and performed better than all other NN based methods in changepoint detection, only being outperformed by the Isolation Forest algorithm in the latter.</p><p>Whilst the proposed NN operated exclusively on raw data, it was found in the literature review that with different representations of data, the prominence of data features can be increased which in turn can help to improve the performance of unsupervised anomaly detection. Furthermore, the use-cases for the proposed NN were not fully explored, so future works will be exploring the use of different data representations and different unsupervised anomaly detection methods, including the grouping of encountered anomalies in an unsupervised manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .</head><label>3456</label><figDesc>Architecture proposed by this paper An example of a frequency plot of the MAEs for the drone dataset (where the signal sources are unknown), and the corresponding error thresholds for feature 1, feature 2 and feature 3 in Fig. 9. &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; Design A (The proposed design) -Branched Inputs, LSTMCaps Autoencoder Network Design B -Branched Inputs, LSTM Autoencoder Network with no Capsules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Design C -Single Input, LSTMCaps Autoencoder Network Design D -Single Input, LSTM Autoencoder Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 12 .</head><label>12</label><figDesc>Plot of a subset of data from the SKAB anomaly benchmark &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Visualisation of LSTM cell [4], where: ?=sigmoid layer, tanh= tanh layer, f t =forget gate, i t =input gate, C t =candidate gate, O t =output gate</figDesc><table><row><cell>TABLE I</cell></row><row><cell>LSTM EQUATIONS [4]</cell></row><row><cell>Fig. 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II INITIAL</head><label>II</label><figDesc>HYPERPARAMETERS USED TO TRAIN MODELS</figDesc><table><row><cell cols="2">Hyperparameter Value</cell><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Epochs</cell><cell>100</cell><cell>Loss Function</cell><cell>MSE</cell></row><row><cell>Optimiser</cell><cell>Adam</cell><cell>Dropout rate</cell><cell>0.2</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>Batch size</cell><cell>64</cell></row><row><cell>Time Steps</cell><cell>64</cell><cell>Branched layer</cell><cell>32</cell></row><row><cell>Capsule</cell><cell>squash</cell><cell>width</cell><cell></cell></row><row><cell>Activation</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>FOR TRAINING FOR EACH NEURAL NETWORK MODEL USING HYPERPARAMETERS FROM TABLE II</figDesc><table><row><cell cols="4">&gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</cell><cell></cell></row><row><cell>Model</cell><cell>Design A: Branched</cell><cell>Design B: Branched</cell><cell>Design C: Non-</cell><cell>Design D: Non-</cell></row><row><cell></cell><cell>LSTMCaps</cell><cell>LSTM</cell><cell>Branched LSTMCaps</cell><cell>Branched LSTM</cell></row><row><cell>Training Plot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg Final Train Loss</cell><cell>0.0013</cell><cell>0.0012</cell><cell>0.0019</cell><cell>0.0052</cell></row><row><cell>Avg Final Val loss</cell><cell>0.0017</cell><cell>0.0030</cell><cell>0.0041</cell><cell>0.0299</cell></row><row><cell>% Overfitting</cell><cell>31</cell><cell>150</cell><cell>116</cell><cell>475</cell></row><row><cell>% Val loss improvement</cell><cell>43</cell><cell>N/A</cell><cell>86</cell><cell>N/A</cell></row><row><cell>from non-Caps</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">comparison with the non-hybridised non-branched LSTM NN</cell><cell></cell><cell></cell></row><row><cell>(Design D,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV BEST</head><label>IV</label><figDesc>TEST RESULTS FOR ANOMALY DETECTION FROM 5 RUNS USING OPTIMISED HYPERPARAMETERS FOR EACH NN DESIGN AVERAGE TEST RESULTS FOR ANOMALY DETECTION FROM 5 RUNS USING OPTIMISED HYPERPARAMETERS FOR EACH NN DESIGN</figDesc><table><row><cell cols="9">&gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</cell><cell></cell></row><row><cell>Model</cell><cell>Trainable</cell><cell></cell><cell cols="3">MAE Threshold</cell><cell>Std Dvn of</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell></cell><cell>Parameters</cell><cell>MSE</cell><cell>Feature</cell><cell>Feature</cell><cell>Feature</cell><cell>thresholds</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Design A: Branched LSTMCaps</cell><cell>25,635</cell><cell cols="2">0.0244 0.8126</cell><cell>0.9764</cell><cell>1.1383</cell><cell>0.1330</cell><cell>0.8819</cell><cell cols="2">0.6633 0.7465</cell></row><row><cell>Design B: Branched LSTM</cell><cell>24,243</cell><cell>0.0145</cell><cell>0.6222</cell><cell>0.9034</cell><cell>0.9389</cell><cell>0.1417</cell><cell>0.8452</cell><cell cols="2">0.3333 0.3949</cell></row><row><cell>Design C: Non-Branched LSTMCaps</cell><cell>26,183</cell><cell>0.0313</cell><cell>0.5367</cell><cell>1.0195</cell><cell>0.8689</cell><cell>0.2017</cell><cell>0.6219</cell><cell cols="2">0.5533 0.5143</cell></row><row><cell>Design D: Non-Branched LSTM</cell><cell>25,338</cell><cell>0.0309</cell><cell>0.9435</cell><cell>0.7682</cell><cell>1.5487</cell><cell>0.3344</cell><cell>0.7224</cell><cell cols="2">0.5767 0.6406</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Trainable</cell><cell></cell><cell cols="3">MAE Threshold</cell><cell>Std Dvn of</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell></cell><cell>Parameters</cell><cell>MSE</cell><cell>Feature</cell><cell>Feature</cell><cell>Feature</cell><cell>thresholds</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Design A: Branched LSTMCaps</cell><cell>25,635</cell><cell>0.0187</cell><cell>0.5998</cell><cell>0.6842</cell><cell>0.8090</cell><cell>0.1119</cell><cell>0.8334</cell><cell cols="2">0.5666 0.6415</cell></row><row><cell>Design B: Branched LSTM</cell><cell>24,243</cell><cell>0.0118</cell><cell>0.8371</cell><cell>0.8588</cell><cell>0.8627</cell><cell>0.1081</cell><cell>0.8706</cell><cell>0.3387</cell><cell>0.3680</cell></row><row><cell>Design C: Non-Branched LSTMCaps</cell><cell>26,183</cell><cell>0.0587</cell><cell>1.4832</cell><cell>1.4752</cell><cell>1.5623</cell><cell>0.1161</cell><cell>0.7965</cell><cell>0.3253</cell><cell>0.3254</cell></row><row><cell>Design D: Non-Branched LSTM</cell><cell>25,338</cell><cell>0.0273</cell><cell>1.0303</cell><cell>0.6698</cell><cell>1.3450</cell><cell>0.2965</cell><cell>0.5141</cell><cell>0.4460</cell><cell>0.4663</cell></row><row><cell cols="4">LSTM model, Design D, performs better than both the non-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">branched hybridised and the branched non-hybridised models.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">This was found to be the case due to the technique used for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">anomaly detection: With a higher MSE, Design D (the standard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">LSTM AE, Fig. 8) did not learn the data features as accurately</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">as Design B (the branched LSTM variant,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI AVERAGE</head><label>VI</label><figDesc>OUTLIER DETECTION SCORES FROM 5 TEST ITERATIONS FOR EACH ANOMALY DETECTION METHOD</figDesc><table><row><cell>Algorithm</cell><cell>F1</cell><cell>FAR, %</cell><cell>MAR, %</cell></row><row><cell>Perfect score</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>LSTMCaps</cell><cell>0.74</cell><cell>21.66</cell><cell>18.74</cell></row><row><cell>MSET[48]</cell><cell>0.73</cell><cell>20.82</cell><cell>20.08</cell></row><row><cell>LSTMCapsV2</cell><cell>0.71</cell><cell>14.45</cell><cell>30.86</cell></row><row><cell>MSCRED[49]</cell><cell>0.7</cell><cell>16.82</cell><cell>31.28</cell></row><row><cell>Conv-AE [50]</cell><cell>0.66</cell><cell>5.57</cell><cell>46.16</cell></row><row><cell>LSTM [51]</cell><cell>0.65</cell><cell>14.89</cell><cell>39.4</cell></row><row><cell>LSTM-AE [52]</cell><cell>0.64</cell><cell>14.81</cell><cell>39.5</cell></row><row><cell>LSTM-VAE [53]</cell><cell>0.56</cell><cell>9.04</cell><cell>54.75</cell></row><row><cell>Autoencoder [54]</cell><cell>0.45</cell><cell>7.52</cell><cell>66.59</cell></row><row><cell>Isolation forest [47]</cell><cell>0.4</cell><cell>6.86</cell><cell>72.09</cell></row><row><cell>Null score</cell><cell>0</cell><cell>100</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII AVERAGE</head><label>VII</label><figDesc>CHANGEPOINT DETECTION SCORES FROM 5 TEST ITERATIONS FOR EACH ANOMALY DETECTION METHOD</figDesc><table><row><cell>Algorithm</cell><cell>NAB</cell><cell>NAB</cell><cell>NAB</cell></row><row><cell></cell><cell>(standard)</cell><cell>(lowFP)</cell><cell>(LowFN)</cell></row><row><cell>Perfect score</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Isolation forest</cell><cell>37.53</cell><cell>17.09</cell><cell>45.02</cell></row><row><cell>[47]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTMCapsV2</cell><cell>27.39</cell><cell>17.08</cell><cell>31.13</cell></row><row><cell>LSTM</cell><cell>26.61</cell><cell>11.78</cell><cell>32</cell></row><row><cell>MSCRED [49]</cell><cell>26.13</cell><cell>17.81</cell><cell>29.53</cell></row><row><cell>LSTM-AE [52]</cell><cell>22.97</cell><cell>20.95</cell><cell>23.93</cell></row><row><cell>LSTMCaps</cell><cell>21.58</cell><cell>5.12</cell><cell>27.49</cell></row><row><cell>LSTM-VAE [53]</cell><cell>21.09</cell><cell>17.52</cell><cell>22.73</cell></row><row><cell>Autoencoder [54]</cell><cell>15.65</cell><cell>0.48</cell><cell>21</cell></row><row><cell>MSET[48]</cell><cell>12.71</cell><cell>11.04</cell><cell>13.6</cell></row><row><cell>Conv-AE [50]</cell><cell>11.12</cell><cell>10.35</cell><cell>11.77</cell></row><row><cell>Null score</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell cols="4">BEST OUTLIER DETECTION SCORES OUT OF 5 TEST</cell></row><row><cell cols="4">ITERATIONS FOR EACH ANOMALY DETECTION METHOD</cell></row><row><cell>Algorithm</cell><cell>F1</cell><cell>FAR, %</cell><cell>MAR, %</cell></row><row><cell>Perfect score</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>LSTMCaps</cell><cell>0.74</cell><cell>21.5</cell><cell>18.74</cell></row><row><cell>MSET [48]</cell><cell>0.73</cell><cell>20.82</cell><cell>20.08</cell></row><row><cell>LSTMCapsV2</cell><cell>0.71</cell><cell>14.51</cell><cell>30.59</cell></row><row><cell>MSCRED [49]</cell><cell>0.7</cell><cell>16.2</cell><cell>30.87</cell></row><row><cell>LSTM [51]</cell><cell>0.67</cell><cell>15.42</cell><cell>36.02</cell></row><row><cell>Conv-AE [50]</cell><cell>0.66</cell><cell>5.58</cell><cell>46.05</cell></row><row><cell>LSTM-AE [52]</cell><cell>0.65</cell><cell>14.59</cell><cell>39.42</cell></row><row><cell>LSTM-VAE [53]</cell><cell>0.56</cell><cell>9.2</cell><cell>54.81</cell></row><row><cell>Autoencoder [54]</cell><cell>0.45</cell><cell>7.55</cell><cell>66.57</cell></row><row><cell>Isolation forest [47]</cell><cell>0.4</cell><cell>6.86</cell><cell>72.09</cell></row><row><cell>Null score</cell><cell>0</cell><cell>100</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX</head><label>IX</label><figDesc></figDesc><table><row><cell cols="4">BEST CHANGEPOINT DETECTION SCORES OUT OF 5 TEST</cell></row><row><cell cols="4">ITERATIONS FOR EACH ANOMALY DETECTION METHOD</cell></row><row><cell>Algorithm</cell><cell>NAB</cell><cell>NAB</cell><cell>NAB</cell></row><row><cell></cell><cell>(standard)</cell><cell>(lowFP)</cell><cell>(LowFN)</cell></row><row><cell>Perfect score</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Isolation forest [47]</cell><cell>37.53</cell><cell>17.09</cell><cell>45.02</cell></row><row><cell>LSTMCapsV2</cell><cell>27.77</cell><cell>17.14</cell><cell>31.59</cell></row><row><cell>LSTM [51]</cell><cell>26.76</cell><cell>12.92</cell><cell>31.93</cell></row><row><cell>MSCRED [49]</cell><cell>24.99</cell><cell>17.9</cell><cell>27.94</cell></row><row><cell>LSTM-AE [52]</cell><cell>24.77</cell><cell>22.69</cell><cell>25.75</cell></row><row><cell>LSTMCaps</cell><cell>24.02</cell><cell>8.14</cell><cell>29.60</cell></row><row><cell>LSTM-VAE [53]</cell><cell>21.92</cell><cell>18.45</cell><cell>23.59</cell></row><row><cell>Autoencoder [54]</cell><cell>16.27</cell><cell>1.04</cell><cell>21.62</cell></row><row><cell>MSET [48]</cell><cell>12.71</cell><cell>11.04</cell><cell>13.6</cell></row><row><cell>Conv-AE [50]</cell><cell>11.21</cell><cell>10.45</cell><cell>11.83</cell></row><row><cell>Null score</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X SCALED</head><label>X</label><figDesc>AVERAGE OF AVERAGE F1 AND NAB SCORES FROM TABLE VI AND TABLE VII RESPECTIVELY</figDesc><table><row><cell>Algorithm</cell><cell>Scaled Average of Average</cell></row><row><cell></cell><cell>Score</cell></row><row><cell>Perfect score</cell><cell>1</cell></row><row><cell>LSTMCapsV2</cell><cell>0.49195</cell></row><row><cell>MSCRED [49]</cell><cell>0.48065</cell></row><row><cell>LSTMCaps</cell><cell>0.4779</cell></row><row><cell>LSTM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI SCALED</head><label>XI</label><figDesc>AVERAGE FROM BEST F1 AND NAB SCORES FROM TABLE VIII AND TABLE IX RESPECTIVELY</figDesc><table><row><cell>Algorithm</cell><cell>Scaled Average of Best Score</cell></row><row><cell>Perfect score</cell><cell>1</cell></row><row><cell>LSTMCapsV2</cell><cell>0.49385</cell></row><row><cell>LSTMCaps</cell><cell>0.4901</cell></row><row><cell>MSCRED</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Service innovation and smart analytics for Industry 4.0 and big data environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procir.2014.02.001</idno>
	</analytic>
	<monogr>
		<title level="j">Procedia CIRP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hardware Redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubrova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-2113-9_4</idno>
	</analytic>
	<monogr>
		<title level="m">Fault-Tolerant Design</title>
		<meeting><address><addrLine>New York, NY; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="55" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural networks and statistical techniques: A review of applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2007.10.005</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection in aircraft data using Recurrent Neural Networks (RNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sherry</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICNSURV.2016.7486356</idno>
	</analytic>
	<monogr>
		<title level="m">ICNS 2016: Securing an Integrated CNS System to Meet Future Challenges</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with LSTM neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kozat</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2935975</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diverse region-based CNN for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2809606</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2623" to="2634" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Medical image classification with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICARCV.2014.7064414</idno>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Control Automation Robotics and Vision, ICARCV 2014</title>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="844" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Financial time series forecasting -a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dingli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Fournier</surname></persName>
		</author>
		<idno type="DOI">10.18178/ijmlc.2017.7.5.632</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="118" to="122" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple time-series convolutional neural network for fault detection and diagnosis and empirical study in semiconductor manufacturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10845-020-01591-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="823" to="836" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fault Detection in Sensors Using Single and Multi-Channel Weighted Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litoiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="0" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-head CNN-RNN for multi-time series anomaly detection: An industrial case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Canizo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Onieva</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.07.034</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="246" to="260" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Web traffic anomaly detection using C-LSTM neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.ESWA.2018.04.004</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="66" to="76" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">Nips</biblScope>
			<biblScope unit="page" from="3857" to="3867" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">No routing needed between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.NEUCOM.2021.08.064</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal wind speed prediction of multiple wind farms using capsule network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.renene.2021.05.023</idno>
	</analytic>
	<monogr>
		<title level="j">Renewable Energy</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="718" to="730" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Robust Weight-Shared Capsule Network for Intelligent Machinery Fault Diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2020.2964117</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on &gt; REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; Industrial Informatics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="6466" to="6475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Network and Markov Transition Field / Gramian Angular Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised Motion Representation Learning with Capsule Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<ptr target="http://arxiv.org/abs/2110.00529" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What is Industry 4.0? Everything you need to know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moore</surname></persName>
		</author>
		<ptr target="https://www.techradar.com/news/what-is-industry-40-everything-you-need-to-know" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal redundant sensor configuration for accuracy increasing in space inertial navigation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jafari</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ast.2015.09.017</idno>
	</analytic>
	<monogr>
		<title level="j">Aerospace Science and Technology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="467" to="472" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid fault detection and isolation method for UAV inertial sensor redundancy management system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3182/20050703-6-cz-1902.02005</idno>
	</analytic>
	<monogr>
		<title level="j">IFAC</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SCADA-data-based wind turbine fault detection: A dynamic model sensor method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conengprac.2020.104546</idno>
	</analytic>
	<monogr>
		<title level="j">Control Engineering Practice</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">322430</biblScope>
			<biblScope unit="page">104546</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">There and back again: Outlier detection between statistical reasoning and data mining algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Filzmoser</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1280</idno>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Signal based condition monitoring techniques for fault detection and diagnosis of induction motors: A state-of-the-art review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gangsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tiwari</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ymssp.2020.106908</idno>
	</analytic>
	<monogr>
		<title level="m">Mechanical Systems and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">106908</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ANOMALY DETECTION FOR TIME SERIES USING VAE-LSTM HYBRID MODEL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Birke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="4322" to="4326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">LSTM based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>G?hmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2019.00113</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="613" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Research on Voltage Waveform Fault Detection of Miniature Vibration Motor Based on Improved WP-LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">No routing needed between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.08.064</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with capsule network using limited training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shengyan</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18093153</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Detection in Time Series Using LSTM-Based Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Provotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Veres</surname></persName>
		</author>
		<idno type="DOI">10.1109/ATIT49449.2019.9030505</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Advanced Trends in Information Theory, ATIT 2019 -Proceedings</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="513" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Homogeneous Vector Capsules Enable Adaptive Gradient Descent in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Brain Tumor Type Classification via Capsule Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2018.8451379</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Image Processing</title>
		<meeting>-International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3129" to="3133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Capsule Networks for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2871782</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2145" to="2160" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Ensemble Capsule Network for Intelligent Compound Fault Diagnosis Using Multisensory Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIM.2019.2958010</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Robust Self-Attentive Capsule Network for Fault Diagnosis of Series-Compensated Transmission Line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R R</forename><surname>Fahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Muyeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Simoes</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPWRD.2021.3049861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Delivery</title>
		<imprint>
			<biblScope unit="volume">8977</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combined wavelet-SVM technique for fault zone detection in a series compensated transmission line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">B</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Maheshwari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPWRD.2008.919395</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Delivery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1789" to="1794" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Encoding time series as images for visual inspection and classification using tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Workshop -Technical Report</title>
		<imprint>
			<biblScope unit="page" from="40" to="46" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">WS-15-14</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep learning method for motor fault diagnosis based on a capsule network with gate-structure dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-020-04999-0</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Novel Cap-LSTM Model for Remaining Useful Life Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSEN.2021.3109623</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="23498" to="23509" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combination bidirectional long short-term memory and capsule network for rotating machinery fault diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.MEASUREMENT.2021.109208</idno>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page">109208</biblScope>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bearing Data Center | Case School of Engineering | Case Western Reserve University</title>
		<ptr target="https://engineering.case.edu/bearingdatacenter/" />
		<imprint>
			<date type="published" when="2022-01-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;gt;</forename><surname>Replace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Line</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Your</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Id Number</surname></persName>
		</author>
		<imprint>
			<publisher>DOUBLE-CLICK HERE TO EDIT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition via capsule network with channel-wise attention and LSTM models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doss</surname></persName>
		</author>
		<idno type="DOI">10.1007/S42486-021-00078-Y/FIGURES/9</idno>
	</analytic>
	<monogr>
		<title level="j">CCF Transactions on Pervasive Computing and Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="435" />
			<date type="published" when="2021-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1127647</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Katser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Kozitsin</surname></persName>
		</author>
		<ptr target="https://github.com/waico/SKAB" />
		<title level="m">Skoltech Anomaly Benchmark (SKAB)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Kaggle</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluating real-time anomaly detection algorithms -The numenta anomaly benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2015.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015</title>
		<meeting>-2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Isolation Forest ICDM08</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Tony</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Ming</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf%0Ahttps://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest" />
	</analytic>
	<monogr>
		<title level="j">Icdm</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Application of a Model-based Fault Detection System to Nuclear Plant Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Wegerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vanalstine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bockhorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent systems applications to power systems</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<ptr target="http://www.osti.gov/bridge/product.biblio.jsp?osti_id=481606" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33011409</idno>
	</analytic>
	<monogr>
		<title level="m">33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Timeseries anomaly detection using an Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vijay</surname></persName>
		</author>
		<ptr target="https://keras.io/examples/timeseries/timeseries_anomaly_detection/" />
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTMbased Predictive Data Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Filonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavrentyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorontsov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.06676" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Building Autoencoders in Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://blog.keras.io/building-autoencoders-in-keras.html" />
	</analytic>
	<monogr>
		<title level="j">The Keras Blog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2016 -20th SIGNLL Conference on Computational Natural Language Learning, Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Outlier detection with autoencoder ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974973.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th SIAM International Conference on Data Mining</title>
		<meeting>the 17th SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
