<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
							<email>gongkehong@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
							<email>zhangjianfeng@u.nus.eduelefjia@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing 3D human pose estimators suffer poor generalization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data. To address this problem, we present PoseAug, a new autoaugmentation framework that learns to augment the available training poses towards a greater diversity and thus improve generalization of the trained 2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose augmentor that learns to adjust various geometry factors (e.g., posture, body size, view point and position) of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to generate more diverse and harder poses in an online manner. Moreover, PoseAug introduces a novel part-aware Kinematic Chain Space for evaluating local joint-angle plausibility and develops a discriminative module accordingly to ensure the plausibility of the augmented poses. These elaborate designs enable PoseAug to generate more diverse yet plausible poses than existing offline augmentation methods, and thus yield better generalization of the pose estimator. PoseAug is generic and easy to be applied to various 3D pose estimators. Extensive experiments demonstrate that PoseAug brings clear improvements on both intra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK on MPI-INF-3DHP under cross-dataset evaluation setup, improving upon the previous best data augmentation based method [22] by 9.1%. Code can be found at: https://github.com/jfzhang95/PoseAug.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation aims to estimate 3D body joints in images or videos. It is a fundamental task with broad applications in action recognition <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b38">39]</ref>, human-* Equal contribution; order determined by coin toss. ? Work done during an internship at Huawei international Pte Ltd.   <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> trained with and without PoseAug. PoseAug significantly improves their performance for both the intra-and cross-dataset settings.</p><p>robot interaction <ref type="bibr" target="#b10">[11]</ref>, human tracking <ref type="bibr" target="#b28">[29]</ref>, etc. This task is typically solved using learning-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32]</ref> with ground truth annotations that are collected in the laboratorial environments <ref type="bibr" target="#b15">[16]</ref>. Despite their success in indoor scenarios, these methods are hardly generalizable to cross-scenario datasets (e.g., an in-the-wild dataset). We argue that their poor generalization is mainly due to the limited diversity of training data, such as limited variations in human posture, body size, camera view point and position. Recent works explore data augmentation to improve the training data diversity and enhance the generalization of their trained models. They either generate data through image composition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> and synthesis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>, or directly generate 2D-3D pose pairs from the available training data by applying pre-defined transformations <ref type="bibr" target="#b21">[22]</ref>. However, all of these works regard data augmentation and model training as two separate phases, and conduct data augmentation in an offline manner without interaction with model training. Consequently, they tend to generate ineffective augmented data that are too easy for model training, leading to marginal boost to the model generalization. Moreover, these methods heavily rely on pre-defined rules such as joint angle limitations <ref type="bibr" target="#b0">[1]</ref> and kinematics constraints <ref type="bibr" target="#b36">[37]</ref> for data augmentation, which limit the diversity of the generated data and make the resulting model hardly generalize to more challenging in-the-wild scenes.</p><p>To improve the diversity of augmented data, we propose PoseAug, a novel auto-augmentation framework for 3D human pose estimation. Instead of conducting data augmentation and network training separately, PoseAug jointly optimizes the augmentation process with network training endto-end in an online manner. Our main insight is that the feedback from the training process can be used as effective guidance signals to adapt and improve the data augmentation. Specifically, PoseAug exploits a differentiable augmentation module (the 'augmentor') implemented by a neural network to directly augment 2D-3D pose pairs in the training data. Considering the potential domain shift with respective to geometry in pose pairs (e.g., postures, view points) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref>, the augmentor learns to perform three types of augmentation operations to respectively control 1) the skeleton joint angle, 2) the body size, and 3) the view point and human position. In this way, the augmentor is able to produce augmented poses with more diverse geometric features and thus relieves the diversity limitation issue. With its differentiable capacity, the augmentor can be optimized together with the pose estimator end-to-end via an error feedback strategy. Concretely, by taking increasing training loss of the estimator as the learning target, the augmentor can learn to enrich the input pose pairs via enlarging data variations and difficulties; in turn, through combating such increasing difficulties, the pose estimator can become increasingly more powerful during the training process.</p><p>To ensure the plausibility of the augmented poses, we use a pose discriminator module to guide the augmentation, to avoid generating implausible joint angles <ref type="bibr" target="#b0">[1]</ref>, unreasonable positions or view points that may hamper model training. In particular, the module consists of a 3D pose discriminator for enhancing the joint angle plausibility and a 2D pose discriminator for guiding the body size, view point and position plausibility. The 3D pose discriminator adopts the Kinematic Chain Space (KCS) <ref type="bibr" target="#b43">[44]</ref> representation and extends it into a part-aware KCS for local-wise supervision. More concretely, it splits skeleton joints into several parts and focuses on joint angles in each part separately instead of the whole body pose, which yields greater flexibility of the augmented poses. By jointly training the pose augmentor, estimator and discriminator in an end-to-end manner ( <ref type="figure" target="#fig_1">Fig. 2)</ref>, PoseAug can largely improve the training data diversity, and thus boost model performance on both source and more challenging cross-scenario datasets.</p><p>Our PoseAug framework is flexible regarding the choice of the 3D human pose estimator. This is demonstrated by the clear improvements made with PoseAug on four representative 3D pose estimation models <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> over both source (H36M) <ref type="bibr" target="#b15">[16]</ref> and cross-scenario (3DHP) <ref type="bibr" target="#b28">[29]</ref> Figure 2: Overview of our PoseAug framework. The augmentor, estimator and discriminator are jointly trained endto-end with an error-feedback training strategy. As such, the augmentor learns to augment data with guidance from the estimator and discriminator. datasets ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Remarkably, it brings more than 13.1% average improvement w.r.t. MPJPE for all models on 3DHP. Moreover, it achieves 88.6% 3D PCK on 3DHP under cross-dataset evaluation setup, improving upon the previous best data augmentation based method <ref type="bibr" target="#b21">[22]</ref> by 9.1%.</p><p>Our contributions are three-fold. 1) To the best of our knowledge, we are the first to investigate differentiable data augmentation on 3D human pose estimation. 2) We propose a differentiable pose augmentor, together with the error feedback design, which generates diverse and realistic 2D-3D pose pairs for training the 3D pose estimator, and largely enhances the model's generalization ability. 3) We propose a new part-aware 3D discriminator, which enlarges the feasible region of augmented poses via local-wise supervision, ensuring both data plausibility and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D human pose estimation Recent progress of 3D human pose estimation is largely driven by the deployment of various deep neural network models <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref>. However, they all highly rely on well-annotated data for fully-supervised model training and hardly generalize to the new scenarios that present unseen patterns in the training dataset, such as new camera views and subject poses. Thus some recent works explore to leverage external information to improve their generalization ability. For example, some methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref> utilize 2D pose data collected in the wild for model training, e.g., through exploring kinematics priors for regularization or post-processing <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>, and adversarial training <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b43">44]</ref>. More recently, geometry-based self-supervised learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> has been used to train models with unlabeled data. Though effective, applying these methods is largely constrained by the availability of suitable external datasets. Instead of focusing on complex network architectures and learning schemes, we explore a learnable pose augmentation framework to enrich the 3D pose data at hand directly. Specifically, the proposed frame-work can generate 2D-3D pose pairs with both diversity and plausibility for training pose estimation models. In addition, our framework is generic and can adapt to those methods to further improve their performance. Data augmentation on 3D human poses Data augmentation is widely used to alleviate the bottleneck of training data diversity and improve model generalization ability. Some works augment data by stitching image patches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref>, and some generate new data with graphics engines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>. More recently, Li et al., <ref type="bibr" target="#b21">[22]</ref> directly augment 2D-3D pose pairs through randomly applying partial skeleton recombination and joint angle perturbation on source datasets. To ensure data plausibility, several constraints are imposed, including joint angle limitation <ref type="bibr" target="#b0">[1]</ref> and fixed augmentation range on view point and human position. Despite the good results on source data, these pre-defined rules limit the data diversity expansion and harm the model applicability to more challenging in-the-wild scenarios. Unlike all these methods, we make the first attempt to explore learnable data augmentation on 3D human pose estimation, which is shown effective for improving model generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Let x ? R 2?J denote 2D spatial coordinates of J keypoints of the human in the image, and X ? R 3?J denote the corresponding 3D joint position in the camera coordinate system. We aim to obtain a 3D pose estimator P : x ? X to recover the 3D pose information from the input 2D pose. Conventionally, the estimator P, with parameters ?, is trained on a well-annotated source dataset (e.g., well-controlled indoor environment <ref type="bibr" target="#b15">[16]</ref>) by solving the following optimization problem:</p><formula xml:id="formula_0">min ? L P (P ? , X ) = L P (P ? (x), X),<label>(1)</label></formula><p>where X = {x, X} denotes paired 2D-3D poses from the source training dataset, and the loss function L P is typically defined as mean square errors (MSE) between predicted and ground truth 3D poses. However, it is often observed that the pose estimator P trained on such an indoor dataset can hardly generalize to a new dataset (e.g., in-the-wild scenario) which features more diverse poses, body sizes, view points or human positions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b45">46]</ref>. To improve generalization ability of the model, we propose to design a pose augmentor A : X ? X , to augment the training pose pair X into a more diverse one X = {x , X } for training the model P:</p><formula xml:id="formula_1">min ? L P (P ? , A(X )).<label>(2)</label></formula><p>There are several strategies to construct the augmentor in an offline manner, e.g., random <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> or evolution-based augmentations <ref type="bibr" target="#b21">[22]</ref>. Differently, we propose to implement the augmentor A via a neural network with parameters ? A and train it jointly with the estimator in an online manner, such that the pose estimator loss can be fully exploited as a surrogate for the augmentation diversity and effectively guide the augmentor learning. In particular, the augmentor is trained to generate harder augmented samples that could increase the training loss of the current pose estimator:</p><formula xml:id="formula_2">min ? max ? A L P (P ? , A ? A (X )).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PoseAug Formulation</head><p>Our proposed framework aims to generate diverse training data, with proper difficulties for the pose estimator, to improve model generalization performance. Two challenges thus need to be tackled: how to make the augmented data diverse and beneficial for model training; and how to make them natural and realistic. To address them, we propose two novel ideas in training the augmentor. Error feedback learning for online pose augmentation Instead of performing random pose augmentation in an offline manner <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22]</ref>, the proposed pose augmentator A deploys a differentiable design which enables online joint-training with the pose estimator P. Using the training error from the pose estimator P as feedback (see Eqn. <ref type="formula" target="#formula_2">(3)</ref>), the pose augmentor A learns to generate poses that are most suitable for the current pose estimator-the augmented poses present proper difficulties and diversity due to online augmentation, thus maximally benefiting generalization of the trained 3D pose estimation model. Discriminative learning for plausible pose augmentation Purely pursuing error-maximized augmentations may result in implausible training poses that violate the biomechanical structure of human body and may hurt model performance. Previous augmentation methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22]</ref> mostly rely on pre-defined rules for ensuring plausibility (e.g., joint angle constraint <ref type="bibr" target="#b0">[1]</ref>), which however would severely limit the diversity of generated poses. For example, some harder yet plausible poses may fail to pass their rulebased plausibility check <ref type="bibr" target="#b21">[22]</ref> and will not be adopted for model training. To address this issue, we deploy a pose discriminator module over the local relation of body joints <ref type="bibr" target="#b43">[44]</ref> to assist training the augmentor, thus ensuring the plausibility of augmented poses without sacrificing the diversity. Augmentor Given a 3D pose X ? R 3?J , the augmentor first obtains its bone vector B ? R 3?(J?1) via a hierarchical transformation 1 B = H(X) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>, which can be further decomposed into a bone direction vectorB (representing the joint angle) and a bone length vector B (representing the body size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>Then the augmentor applies multi-layer perceptron (MLP) for feature extraction from the input 3D pose X. Additionally, a noise vector based on Gaussian distribution is concatenated with X in the feature extraction process to incur sufficient randomness for enhancing the feature diversity. The extracted features are then used for regressing three operation parameters (? ba , ? bl and (R, t)) to change the joint angles, body size, as well as view point and position as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Among these parameters,</p><formula xml:id="formula_3">1) ? ba ? R 3?(J?1)</formula><p>is the bone angle residual vector that is used for adjusting the Bone Angle (BA) as follows:</p><formula xml:id="formula_4">B =B + ? ba , (BA operation).<label>(4)</label></formula><p>Specifically, BA operation will rotate the input bone direction vectorB by ? ba , generating a new bone direction vectorB .</p><p>2) ? bl ? R 1?(J?1) represents the bone length ratio vector that is used for adjusting the Bone Length (BL):</p><formula xml:id="formula_5">B = B ? (1 + ? bl ), (BL operation). (5)</formula><p>BL operation modifies the input bone length vector B by ? bl to adjust the body size. Notably, to ensure biomechanical symmetry, the left and right body parts share the same parameters.</p><p>3) R ? R 3?3 and t ? R 3?1 denote the rotation and translation parameters respectively for Rigid Transformation (RT) operation to control pose view point and position:</p><formula xml:id="formula_6">X = R[H ?1 (B )] + t, (RT operation),<label>(6)</label></formula><p>where B = B ?B is the augmented bone vector from the above BA and BL operations. H ?1 is the inverse hierarchical conversion to transform B back to a 3D pose <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>By applying these operations, the augmentor can generate the augmented 3D pose X with more challenging pose, body size, view point and position from the original 3D pose X <ref type="figure" target="#fig_2">(Fig. 3)</ref>. The augmented pose is then re-projected to 2D with x = ?(X ), where ? : R 3 ? R 2 denotes perspective projection <ref type="bibr" target="#b14">[15]</ref> via the camera parameters from the original data. The augmented 2D-3D pair {x , X } is then used for further training the pose estimator.  Discriminator Due to lacking priors in the augmentation procedure, the augmented poses may present implausible joint angles that violate the bio-mechanical structure <ref type="bibr" target="#b0">[1]</ref>, or unreasonable positions and view points. Though such poses are indeed harder cases for the estimator, training on them would not benefit the model generalization ability.</p><p>To ensure the plausibility of the augmented poses, we introduce a pose discriminator module to guide the augmentation. Specifically, the module consists of a 3D pose discriminator D 3d for evaluating the joint angle plausibility and a 2D discriminator D 2d for evaluating the body size, viewpoint and position plausibility.</p><p>The key to the 3D pose discriminator design is to ensure the pose plausibility without sacrificing the diversity. Inspired by the Kinematic Chain Space (KCS) <ref type="bibr" target="#b43">[44]</ref>, we design a part-aware KCS as input to the discriminator. Instead of taking the whole body pose into consideration as in the original KCS, our part-aware KCS only focuses on local joint angle and thus enlarges the feasible region of the augmented pose, ensuring both plausibility and diversity ( <ref type="figure" target="#fig_3">Fig. 4)</ref>.</p><p>Specifically, to compute the part-aware KCS of an input pose, either X or its augmentation X , we convert the pose to its bone direction vectorB as above and separate it into 5 parts (torso and left/right arm/leg) <ref type="bibr" target="#b0">[1]</ref>, denoted asB i , i = 1, . . . , 5, respectively. We then calculate the following local joint angle matrix KCS i local for the i-th part:</p><formula xml:id="formula_7">KCS i local =B iBi ,<label>(7)</label></formula><p>which encapsulates the inter joint angle information within the i-th part. Based on the above local KCS representation, a 3D pose discriminator D 3d is constructed which takes the KCS i local as input and is trained for distinguishing the original and augmented 3D poses.</p><p>Besides the 3D discriminator, we also introduce a 2D discriminator to guide the augmentor to generate real body size, view points and positions. As the 2D poses contain information such as view point (rotation), position (translation), and body size (bone length), the 2D discriminator can learn such information through adversarial training and guide the pose augmentor in generating realistic rotation R, translation t, and bone length ratio ? bl . Estimator The pose estimator P estimates 3D poses from 2D poses. We use the original and augmented 2D-3D pose pair {x, X} and {x , X } to train the pose estimator. The pose estimator contains a feature extractor to capture internal features from 2D poses, and a regression layer to estimate the corresponding 3D poses. Moreover, any existing effective estimator can be implemented in our PoseAug framework. In Sec. 4.3, we conduct experiments to check robustness of PoseAug with different estimators, and the results show PoseAug can bring noticeable improvements on both source and cross-scenario datasets for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>Pose estimation loss We adopt the mean squared errors (MSE) of the ground truth (GT) X and predicted poses X as the pose estimation loss, which is formulated as</p><formula xml:id="formula_8">L P = X ? X 2 2 .<label>(8)</label></formula><p>We train the pose estimator using L P with both original and augmented pose pairs jointly, which can significantly boost performance for the challenging in-the-wild scenes.</p><p>Pose augmentation loss To facilitate model training, augmented data should harder than the original one, i.e., L P (X ) &gt; L P (X), but not too hard to hurt the training process. A simple way to design the loss function is to let the difference between the pose estimation loss on augmented and original data within a proper range. Inspired by <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>, we implement a controllable feedback loss as</p><formula xml:id="formula_9">L f b = |1.0 ? exp[L P (X ) ? ?L P (X)]|,<label>(9)</label></formula><p>where ? &gt; 1 controls the difficulty level for the generated poses, making the value of L P (X ) stay within a certain range w.r.t. L P (X). During training, as the pose estimator becomes increasingly more powerful, we accordingly increase ? value to generate more challenging augmentation data for training it. Additionally, to prevent extremely hard cases from causing training collapse, we introduce a rectified L2 loss for regularizing the augmentation parameters ? ba and ? bl :</p><formula xml:id="formula_10">L reg (?) = 0, if? &lt; threshold, ? 2 , otherwise,<label>(10)</label></formula><p>where ? denotes ? ba and ? bl , and? denotes the mean value over all of its elements. Combining Eqn. <ref type="bibr" target="#b8">(9)</ref> and Eqn. <ref type="formula" target="#formula_0">(10)</ref>, the overall augmentation loss L A is formulated as</p><formula xml:id="formula_11">L A = L f b + L reg .<label>(11)</label></formula><p>Pose discrimination loss For the discrimination loss L D , we adopt the LS-GAN loss <ref type="bibr" target="#b24">[25]</ref> for both 3D and 2D spaces:</p><formula xml:id="formula_12">L D = E[(D 3d (X) ? 1) 2 ] + E[D 3d (X ) 2 ] +E[(D 2d (x) ? 1) 2 ] + E[D 2d (x ) 2 ],<label>(12)</label></formula><p>where {x, X} and {x , X } denote the original (real) and the augmented (fake) pose pairs, respectively. End-to-end training strategy With the differentiable design, the pose augmentor, discriminator and estimator can be jointly trained end-to-end. We update them alternatively by minimizing losses Eqn. (11), Eqn. <ref type="bibr" target="#b11">(12)</ref> and Eqn. <ref type="bibr" target="#b7">(8)</ref>. In addition, we first pre-train the pose estimator P before training the whole framework end-to-end, which ensures stable training and produces better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We study four questions in experiments. 1) Is PoseAug able to improve performance of 3D pose estimator for both intra-dataset and cross-dataset scenarios? 2) Is PoseAug effective at enhancing diversity of training data? 3) Is PoseAug consistently effective for different pose estimators and cases with limited training data? 4) How does each component of PoseAug take effect? We experiment on H36M, 3DHP and 3DPW. Throughout the experiments, unless otherwise stated we adopt single-frame version of VPose <ref type="bibr" target="#b32">[33]</ref> as pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M (H36M) <ref type="bibr" target="#b15">[16]</ref> Following previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52]</ref>, we train our model on subjects S1, 5, 6, 7, 8 of H36M and evaluate on subjects S9 and S11. We use two evaluation metrics: Mean Per Joint Position Error (MPJPE) in millimeters and MPJPE over aligned predictions with GT 3D poses by a rigid transformation (PA-MPJPE). <ref type="bibr" target="#b28">[29]</ref> It is a large 3D pose dataset with 1.3 million frames, presenting more diverse motions than H36M. We use its test set to evaluate the model's generalization ability to unseen environments, using metrics of MPJPE, Percentage of Correct Keypoints (PCK) and Area Under the Curve (AUC). 3DPW <ref type="bibr" target="#b42">[43]</ref> It is an in-the-wild dataset with more complicated motions and scenes. To verify generalization of the proposed method to challenging in-the-wild scenarios, we use its test set for evaluation with PA-MPJPE as metric. MPII <ref type="bibr" target="#b1">[2]</ref> and LSP <ref type="bibr" target="#b16">[17]</ref> They are in-the-wild datasets with only 2D body joint annotations and used for qualitatively evaluating model generalization for unseen poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP (3DHP)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Results on H36M We compare PoseAug with state-of-theart methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref> on H36M. Similar to <ref type="bibr" target="#b21">[22]</ref>, we use 2D poses from HR-Net <ref type="bibr" target="#b39">[40]</ref> as inputs. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method outperforms SOTA methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref> by a large margin, indicating its effectiveness. Notably, compared with the previous best augmentation method <ref type="bibr" target="#b21">[22]</ref>, our PoseAug achieves lower MPJPE even though it uses external bone length data for data augmentation and nearly 3? more data than ours for model training. This clearly verifies advantages of PoseAug's online augmentation scheme-it can generate more diverse and informative data that better benefit model training.  Results on 3DHP (cross-scenario) We then evaluate how PoseAug facilitates model generalization to cross-scenario datasets. We compare PoseAug against various state-of-theart methods, including the latest one using offline data augmentation <ref type="bibr" target="#b21">[22]</ref>, the ones exploiting complex network architecture <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref> and weakly-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref> and the ones trained on the training set of 3DHP <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24]</ref>. From <ref type="table" target="#tab_2">Table 2</ref>, we can observe our method achieves the best performance w.r.t. all the metrics, outperforming previous approaches by a large margin. This verifies the effectiveness of PoseAug in improving model generalization to unseen scenarios. Moreover, PoseAug can further improve the performance (from 73.0 to 71.1 in MPJPE) by using additional in-the-wild 2D poses (MPII) to train the 2D discriminator. This demonstrates its extensibility in leveraging extra 2D poses to further enrich the diversity of augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on 3DPW (cross-scenario)</head><p>We train four 3D pose estimators <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref> without and with PoseAug on H36M and compare their generalization performance on 3DPW. As shown in <ref type="table" target="#tab_4">Table 4</ref>, on average, PoseAug brings 12.6% improvements for all the models. Qualitative results For subjective evaluation, we choose four challenging datasets, i.e., LSP, MPII, 3DHP and 3DPW, with large varieties of postures, body sizes, and view points between their data and the data from H36M. Results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. We can see our method performs fairly well, even for those unseen difficult poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis on PoseAug</head><p>Applicability to different estimators Our PoseAug framework is generic and applicable to different 3D pose estimators. To demonstrate this, we employ four representative 3D pose estimators as backbones: 1) SemGCN <ref type="bibr" target="#b51">[52]</ref>, a graphbased 3D pose estimation network; 2) SimpleBaseline <ref type="bibr" target="#b25">[26]</ref>, an effective MLP-based network; 3) ST-GCN <ref type="bibr" target="#b2">[3]</ref> (1-frame), a pioneer network that uses GCN-based architecture to encode global and local joint relations; and 4) VPose <ref type="bibr" target="#b32">[33]</ref> (1frame), a fully-convolutional network with SOTA performance. We train these models on the H36M dataset using 2D poses from four different 2D pose detectors, including CPN <ref type="bibr" target="#b6">[7]</ref>, DET <ref type="bibr" target="#b12">[13]</ref>, HR-Net <ref type="bibr" target="#b39">[40]</ref> and groundtruth (GT). We evaluate these models on the test set of H36M and 3DHP   <ref type="bibr">(-14.4)</ref> 78.4 <ref type="bibr">(-11.4)</ref> 73.2 <ref type="bibr">(-12.4)</ref> 73.0 <ref type="bibr">(-13.6)</ref> w.r.t. MPJPE metric. On H36M, we use the corresponding 2D poses for evaluation; while on 3DHP, we evaluate these models with GT 2D poses to filter out the influence of 2D pose detectors. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We can see PoseAug brings clear improvements to all models on both H36M and more challenging 3DHP datasets. Notably, they obtain more than 13.1% average improvement on 3DHP when trained with PoseAug.</p><p>Source dataset: H36M S1 S1+S5 Full 30  Effectiveness for limited training data cases 3D pose annotations are expensive to collect, making limited training data a common challenge. To demonstrate the effectiveness of our method on addressing such cases, we use pose data from H36M S1 and S1+S5 for model training which only contain 16% and 41% training samples, respectively. The results in <ref type="figure" target="#fig_6">Fig. 6</ref> show PoseAug consistently improves model performance with varying amounts of training data, on both H36M and 3DHP. Meanwhile, the improvements brought by our method are more significant for cases with less training data (e.g., MPJPE in 3DHP, S1: 116.4 ? 90.3, Full: 86.6 ? 73.0). Moreover, in cross-scenario generalization, our method trained with only S1 achieves the comparable result (MPJPE: 90.3) to baseline trained using full dataset (MPJPE: 86.6), and our method trained with S1+S5 can outperform baseline trained using full dataset by a large margin (77.9 vs 86.6 in MPJPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on the augmentor</head><p>We then check the effectiveness of each module in augmentor.  samples. Among these modules, RT contributes the most to cross-scenario performance, which implies it benefits data diversity most effectively. Analysis on diversity improvement To demonstrate effectiveness of PoseAug in enhancing data diversity, considering RT operation which augments the view point and position contributes the most to cross-scenario performance, as shown in <ref type="table" target="#tab_5">Table 5</ref>, we make diversity analysis on view point and position distribution. <ref type="figure" target="#fig_7">Fig. 7</ref> demonstrates the distributions of view point and position of H36M and the augmented data generated by Li et al. <ref type="bibr" target="#b21">[22]</ref> and our method. For H36M data, one can observe their view points concentrate near to the xz-plane with a limited diversity along the yaxis; and their positions form a small and concentrated cluster, also showing a limited diversity. This explains why the model trained on H36M hardly generalizes to in-the-wild scenarios. Similarly, we observe small divergence for the view point and position distribution of augmented data from Li et al. <ref type="bibr" target="#b21">[22]</ref>. This implies the diversity improvement from the handcrafted rule is limited. Comparably, our PoseAug can offer more plausible view points and positions using the learnable augmentor, with a much greater diversity. In addition, the diversity on human positions can be further improved with extra 2D poses, which also explains its resulted improved generalization ability in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Analysis on the discriminator We here demonstrate the effectiveness of plausibility guidance from the 2D and 3D discriminators. <ref type="table" target="#tab_7">Table 6</ref> summarizes the results. By adding one of the 2D or 3D discriminators, the performance of baseline can be boosted by 2.2/5.8 and 2.2/7.0 on H36M/3DHP, respectively. Including both discriminators into PoseAug training can further boost the performance by 3.6/13.6 on H36M/3DHP, which clearly verify the effectiveness of both discriminators and also the importance of plausibility (in augmented poses) for estimator performance. Analysis on part-aware KCS (PA-KCS) To verify its effectiveness, we replace it in PoseAug with KCS <ref type="bibr" target="#b43">[44]</ref>. <ref type="table" target="#tab_8">Table 7</ref> summarizes the results. PA-KCS clearly outperforms KCS on both 3DHP and 3DPW. This verifies our PA-KCS provides better guidance than KCS during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we develop an auto-augmentation framework, PoseAug, that learns to enrich the diversity of training data and improves performance of the trained pose estimation models. The PoseAug effectively integrates three components including the augmentor, estimator and discriminator and makes them fully interacted with each other. Specifically, the augmentor is designed to be differentiable and thus can learn to change major geometry factors of the 2D-3D pose pair to suit the estimator better by taking its training error as feedback. The discriminator can ensure the plausibility of augmented data based on a novel part-aware KCS representation. Extensive experiments justify PoseAug can augment diverse and informative data to boost estimation performance for various 3D pose estimators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Estimation error (in MPJPE) on H36M (intradataset evaluation) and 3DHP (cross-dataset evaluation) of four well established models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>summarizes our PoseAug architecture design. It includes 1) a pose augmentor that augments the input pose pair {x, X} to an augmented one {x , X } for pose estimator P training; 2) a pose discriminator module with two discriminators in 3D and 2D spaces, to ensure the plausibility of the augmented data; and 3) a 3D pose estimator, that provides pose estimation error feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Augmentation operations with PoseAug. A source 3D pose is augmented by modifying its posture (via BA operation), body size (via BL operation) and view point and position (via RT operation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustrations of the difference between original and part-aware KCS based discriminator. Given a novel and valid augmented pose, the original KCS based discriminator would wrongly classify it as fake as it does not appear in source data (H36M), while the part-aware KCS based discriminator would recognize is as real and approve it, since it inspects local joint relations. It can be seen the part-aware KCS based discriminator can help the augmentor generate more diverse and plausible pose augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example 3D pose estimations from LSP, MPII, 3DHP and 3DPW. Our results are shown in the left four columns. The rightmost column shows results of Baseline-VPose [33] trained w/o PoseAug. Errors are highlighted by black arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Ablation study on limited data setup. We report MPJPE for evaluation. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Distribution on view point (top row) and position (bottom row) for original data H36M, and augmented data from Li et al. [22], PoseAug (3rd column) and PoseAug with extra 2D poses. This distribution shows PoseAug significantly improves diversity of view point and position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Source dataset: H36M SemGCN SimpleBL ST-GCN VPose3DCross dataset: 3DHP SemGCN SimpleBL ST-GCN VPose3D</head><label></label><figDesc></figDesc><table><row><cell>44 46</cell><cell>44.4</cell><cell>43.3</cell><cell cols="2">Train w/o PoseAug Train w/ PoseAug</cell><cell cols="2">95 100 105</cell><cell>97.4</cell><cell></cell><cell cols="2">Train w/o PoseAug Train w/ PoseAug</cell></row><row><cell>36 38 40 42 MPJPE(mm)</cell><cell>41.5</cell><cell>39.4</cell><cell>41.7 36.9</cell><cell>41.8 38.2</cell><cell>MPJPE(mm)</cell><cell>65 70 75 80 85 90</cell><cell>86.1</cell><cell>85.3 76.2</cell><cell>87.8 74.9</cell><cell>86.6 73.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on H36M in terms of MPJPE and PA-MPJPE. Best results are shown in bold.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE (?) PA-MPJPE (?)</cell></row><row><cell>SemGCN (CVPR'19) [52]</cell><cell>57.6</cell><cell>-</cell></row><row><cell>Sharma et al.(CVPR'19) [38]</cell><cell>58.0</cell><cell>40.9</cell></row><row><cell>VPose (CVPR'19) [33] (1-frame)</cell><cell>52.7</cell><cell>40.9</cell></row><row><cell>Moon et al.(ICCV'19) [30]</cell><cell>54.4</cell><cell>-</cell></row><row><cell>Li et al.(CVPR'20) [22]</cell><cell>50.9</cell><cell>38.0</cell></row><row><cell>Ours</cell><cell>50.2</cell><cell>39.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on 3DHP. CE denotes cross-scenario evaluation. PCK, AUC and MPJPE are used for evaluation.</figDesc><table><row><cell>Method</cell><cell cols="3">CE PCK (?) AUC (?) MPJPE (?)</cell></row><row><cell>Mehta et al. [27]</cell><cell>76.5</cell><cell>40.8</cell><cell>117.6</cell></row><row><cell>VNect [29]</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell>Multi Person [28]</cell><cell>75.2</cell><cell>37.8</cell><cell>122.2</cell></row><row><cell>OriNet [24]</cell><cell>81.8</cell><cell>45.2</cell><cell>89.4</cell></row><row><cell>LCN [8]</cell><cell>74.0</cell><cell>36.7</cell><cell>-</cell></row><row><cell>HMR [18]</cell><cell>77.1</cell><cell>40.7</cell><cell>113.2</cell></row><row><cell>SRNet [49]</cell><cell>77.6</cell><cell>43.8</cell><cell>-</cell></row><row><cell>Li et al. [22]</cell><cell>81.2</cell><cell>46.1</cell><cell>99.7</cell></row><row><cell>RepNet [44]</cell><cell>81.8</cell><cell>54.8</cell><cell>92.5</cell></row><row><cell>Ours</cell><cell>88.6</cell><cell>57.3</cell><cell>73.0</cell></row><row><cell>Ours(+Extra2D)</cell><cell>89.2</cell><cell>57.9</cell><cell>71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results in PA-MPJPE for four estimators on 3DPW.</figDesc><table><row><cell>Method</cell><cell>PA-MPJPE (?)</cell></row><row><cell>SemGCN [52]</cell><cell>102.0</cell></row><row><cell>+ PoseAug</cell><cell>82.2 (-19.8)</cell></row><row><cell>SimpleBaseline [26]</cell><cell>89.4</cell></row><row><cell>+ PoseAug</cell><cell>78.1 (-11.3)</cell></row><row><cell>ST-GCN [3](1-frame)</cell><cell>98.0</cell></row><row><cell>+ PoseAug</cell><cell>73.2 (-24.8)</cell></row><row><cell>VPose [33] (1-frame)</cell><cell>94.6</cell></row><row><cell>+ PoseAug</cell><cell>81.6 (-13.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison in MPJPE for various pose estimators trained w/o and with PoseAug on H36M and 3DHP datasets. DET, CPN, HR and GT denote 3D pose estimation model trained on different 2D pose sources, respectively. We evaluate the model on H36M test set with the corresponding 2D pose sources. On 3DHP test set, we use GT 2D poses as input for evaluating model's generalization. We can observe PoseAug consistently decreases errors for all datasets and estimators.</figDesc><table><row><cell></cell><cell></cell><cell>H36M</cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DHP</cell><cell></cell></row><row><cell>Method</cell><cell>DET</cell><cell>CPN</cell><cell>HR</cell><cell>GT</cell><cell>DET</cell><cell>CPN</cell><cell>HR</cell><cell>GT</cell></row><row><cell>SemGCN [52]</cell><cell>67.5</cell><cell>64.7</cell><cell>57.5</cell><cell>44.4</cell><cell>101.9</cell><cell>98.7</cell><cell>95.6</cell><cell>97.4</cell></row><row><cell>+ PoseAug</cell><cell cols="6">65.2 (-2.3) 60.0 (-4.8) 55.0 (-2.5) 41.5 (-2.8) 89.9 (-11.9) 89.3 (-9.4)</cell><cell cols="2">89.1 (-6.5) 86.1 (-11.2)</cell></row><row><cell>SimpleBaseline [26]</cell><cell>60.5</cell><cell>55.6</cell><cell>53.0</cell><cell>43.3</cell><cell>91.1</cell><cell>88.8</cell><cell>86.4</cell><cell>85.3</cell></row><row><cell>+ PoseAug</cell><cell cols="8">58.0 (-2.5) 53.4 (-2.2) 51.3 (-1.7) 39.4 (-3.9) 78.7 (-12.4) 78.7 (-10.1) 76.4 (-10.1) 76.2 (-9.1)</cell></row><row><cell>ST-GCN [3] (1-frame)</cell><cell>61.3</cell><cell>56.9</cell><cell>52.2</cell><cell>41.7</cell><cell>95.5</cell><cell>91.3</cell><cell>87.9</cell><cell>87.8</cell></row><row><cell>+ PoseAug</cell><cell cols="8">59.8 (-1.5) 54.5 (-2.4) 50.8 (-1.5) 36.9 (-4.8) 83.5 (-12.1) 77.7 (-13.6) 76.6 (-11.3) 74.9 (-12.9)</cell></row><row><cell>VPose [33] (1-frame)</cell><cell>60.0</cell><cell>55.2</cell><cell>52.7</cell><cell>41.8</cell><cell>92.6</cell><cell>89.8</cell><cell>85.6</cell><cell>86.6</cell></row><row><cell>+ PoseAug</cell><cell cols="5">57.8 (-2.2) 52.9 (-2.3) 50.2 (-2.5) 38.2 (-3.6) 78.3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>summarizes the results. By gradually adding the BA, RT and BL operations, the pose estimation error can be monotonically decreased from 41.8/86.6 to 38.8/73.5 (on H36M/3DHP). Moreover, incorporating the error feedback guidance can further improve performance to 38.2 for H36M and 73.0 for 3DHP. These verify the effectiveness of each module of the augmentor in producing more effective augmented</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on components of the augmentor. We report MPJPE on H36M and 3DHP datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">BA RT BL Feedback H36M (?) 3DHP (?)</cell></row><row><cell>Baseline</cell><cell>41.8</cell><cell>86.6</cell></row><row><cell>Variant A</cell><cell cols="2">39.7 (-2.1) 85.2 (-1.4)</cell></row><row><cell>Variant B</cell><cell cols="2">39.2 (-2.6) 75.9 (-10.7)</cell></row><row><cell>Variant C</cell><cell cols="2">39.1 (-2.7) 75.5 (-11.1)</cell></row><row><cell>Variant D</cell><cell cols="2">38.8 (-3.0) 73.5 (-13.1)</cell></row><row><cell>PoseAug</cell><cell cols="2">38.2 (-3.6) 73.0 (-13.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the discriminators D 2D and D 3D on H36M and 3DHP. MPJPE is used for evaluation.</figDesc><table><row><cell>Method</cell><cell cols="2">D2D D3D H36M (?) 3DHP (?)</cell></row><row><cell>Baseline</cell><cell>41.8</cell><cell>86.6</cell></row><row><cell>Variant A</cell><cell>39.6 (-2.2)</cell><cell>80.8 (-5.8)</cell></row><row><cell>Variant B</cell><cell>39.6 (-2.2)</cell><cell>79.6 (-7.0)</cell></row><row><cell>PoseAug</cell><cell cols="2">38.2 (-3.6) 73.0 (-13.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on part-aware KCS (PA-KCS). We report MPJPE on 3DHP and PA-MPJPE on 3DPW.</figDesc><table><row><cell>Method</cell><cell cols="2">KCS PA-KCS 3DHP (?) 3DPW (?)</cell></row><row><cell>Baseline</cell><cell>86.6</cell><cell>94.6</cell></row><row><cell>Variant A</cell><cell cols="2">77.7 (-8.9) 88.4 (-6.2)</cell></row><row><cell>PoseAug</cell><cell cols="2">73.0 (-13.6) 81.6 (-13.0)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The hierarchical transformation converts the J joints of X into J ? 1 column vectors of B, each of which represents a line segment connecting two adjacent joints.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This research was partially supported by AISG-100E-2019-035, MOE2017-T2-2-151, NUS ECRA FY17 P08 and CRP20-2017-0006. JZ would like to acknowledge the support of NVIDIA AI Tech Center (NVAITC) to this research project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Cong Phuoc Huynh. Can 3d pose be learned from 2d projections alone? In ECCVw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amit Agrawal</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Human-computer interaction. An Introduction to Cyberpsychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Errity</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointaugment: an auto-augmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometry-driven selfsupervised method for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhentao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard Yi Da</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Domes to drones: Self-supervised active triangulation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksis</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural scene decomposition for multi-person motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Mathieu Salzmann, and Pascal Fua</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">Teja</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashast</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Generalizing monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyuan</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<editor>ICCVw</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
		<editor>ECCVw</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
