<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Nouranizadeh</surname></persName>
							<email>nouranizadeh@aut.ac.ir</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadjavad</forename><surname>Matinkia</surname></persName>
							<email>matinkia@aut.ac.ir</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rahmati</surname></persName>
							<email>rahmati@aut.ac.ir</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Safabakhsh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel pooling layer for graph neural networks based on maximizing the mutual information between the pooled graph and the input graph. Since the maximum mutual information is difficult to compute, we employ the Shannon capacity of a graph as an inductive bias to our pooling method. More precisely, we show that the input graph to the pooling layer can be viewed as a representation of a noisy communication channel. For such a channel, sending the symbols belonging to an independent set of the graph yields a reliable and errorfree transmission of information. We show that reaching the maximum mutual information is equivalent to finding a maximum weight independent set of the graph where the weights convey entropy contents. Through this communication theoretic standpoint, we provide a distinct perspective for posing the problem of graph pooling as maximizing the information transmission rate across a noisy communication channel, implemented by a graph neural network. We evaluate our method, referred to as Maximum Entropy Weighted Independent Set Pooling (MEWISPool), on graph classification tasks and the combinatorial optimization problem of the maximum independent set. Empirical results demonstrate that our method achieves the state-of-the-art and competitive results on graph classification tasks and the maximum independent set problem in several benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>neural networks, increasing the receptive fields of the computational neurons to capture global dependencies and attenuating the superfluous information, comprise the primary motivations for adopting pooling layers in GNNs. Various pooling methods aim to encapsulate the node-level statistics of the graph into a possibly smaller yet informative graph while preserving the structural content <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this paper we propose a novel pooling layer referred to as Maximum Entropy Weighted Independent Set Pooling (MEWISPool) which can be incorporated into GNNs in an end-to-end manner to realize effective hierarchical representations. The overarching objective of MEWISPool is to select a proper subset of nodes of the input graph which has the highest mutual information with the primary graph. We fulfill this goal using communication and information theoretic concepts.</p><p>As a key element in the proposed pooling method, Shannon capacity of a graph measures the Shannon capacity of a noisy communication channel represented by the graph <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b37">38]</ref>. This graph is called the confusability graph of the channel <ref type="bibr" target="#b15">[16]</ref>, where the nodes represent the symbols which are to be transmitted through the channel and the edges indicate the symbols which might be confused at the output due to the presence of noise. On the other hand, Shannon capacity of a noisy channel is also defined as the maximum information rate that can be transmitted through the channel and is given by the maximum mutual information between the channel's input and output <ref type="bibr" target="#b8">[9]</ref>.</p><p>Through this communication theoretic standpoint, we provide a distinct perspective for posing the problem of pooling in GNNs as maximizing the information transmission rate across a noisy communication channel, which is implemented by a GNN. In fact, this GNN is a deterministic realization of the noisy channel at its highest information transmission rate. Consequently, MEWISPool makes the assumption that the input graphs to the pooling layer manifest representations of noisy communication channels and selects the nodes contributing to the Shannon capacity of such channels. This assumption is justified by the recent studies on the over-smoothing behavior of GNNs, which demonstrate that such networks act as low-pass filters and hence, result in smooth signals over the graph <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60]</ref>. This implies that the neighboring nodes carry similar signals due to the smoothness of the signals over the graph. Thus, the input graph to the pooling layer can be viewed as a confusability graph where the neighboring nodes contain similar informational contents. Furthermore, given a confusability graph of a noisy communication channel, sending the symbols belonging to an independent set of the graph yields a reliable and error-free transmission of information. <ref type="bibr" target="#b48">[49]</ref>.</p><p>In this work, we employ the intuitions discussed above and strive to find an independent set of nodes whose signals have the maximum mutual information with the entire set of nodes signals. In order to circumvent the difficulties of solving the maximum mutual information, we utilize the Infomax principle <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref> to transform the problem into maximizing the entropy of output signals given that their corresponding nodes form an independent set. Finally, we show that this problem is equivalent to finding a maximum weight independent set (MWIS) of the graph where the weights convey entropy contents. Solving the aforementioned MWIS problem maximizes the information transmission rate which is equal to the maximum mutual information between the input and output, and realizes the Shannon capacity of the input graph.</p><p>As MWIS is considered an NP-hard problem, finding the exact solution of the problem is intractable. However, we present a neural estimator to assign a probability to each node to be included in the optimal solution. The sub-optimal solution is then excluded from the learned distribution using derandomization algorithms. We verify that the existence of such a solution is guaranteed using the probabilistic method <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>. Since MEWISPool samples the vertices following the sub-optimal solution of the MWIS problem, it does not require a pooling ratio which in turn, makes it adaptive to the graph structure and easier to fine-tune in end-to-end configurations. We evaluate the proposed method on graph classification tasks and the maximum independent set (MIS) problem.</p><p>The contributions of the present work can be summarized as, (i) devising a novel objective for pooling, based on the concepts of Shannon capacity of graphs and Infomax principle, (ii) proposing a neural execution of the combinatorial optimization (CO) problem of MWIS, (iii) introducing a pooling layer, MEWISPool, with structure-adaptive pooling ratio, and (iv) demonstrating the state-of-the-art and competitive results on graph classification tasks and the MIS problem 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>GNNs and pooling techniques. In recent years, there has been a tremendous attention on GNNs due to their achievements in learning tasks on graph-structured data in various domains such as social sciences <ref type="bibr" target="#b29">[30]</ref>, bioinformatics <ref type="bibr" target="#b20">[21]</ref>, physics <ref type="bibr" target="#b9">[10]</ref>, recommendation systems <ref type="bibr" target="#b62">[63]</ref>, etc. GNNs utilize graph-based pooling techniques to learn hierarchical representations of the input graphs. In a general perspective, one can categorize the graph pooling techniques into the methods which are based on node selection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, and the methods based on graph coarsening <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b64">65]</ref>. The node-selection-based pooling methods assign an importance score to each node of the graph and select the high score nodes as the pooled nodes. On the other hand, coarsening-based methods tend to cluster the graph and merge each cluster into one node.</p><p>Among the pooling methods, iPool <ref type="bibr" target="#b18">[19]</ref> and VIPool <ref type="bibr" target="#b33">[34]</ref>, utilize information theoretic concepts to select candidate nodes. More precisely, iPool defines an information gain criterion to quantitatively measure the conditional entropy of each node given its neighbors and selects the nodes with highest information gains. Alternatively, VIPool computes the mutual information between the neighboring nodes and selects the nodes which have the highest mutual information with their neighbors. Instead of considering the informational relationship between a node and its neighbors, MEWISPool tends to maximize the mutual information between the pooled nodes and the entire set of input vertices. Moreover, using the maximum entropy weighted independent set, MEWISPool guarantees a fair coverage of the graph, whereas iPool and VIPool might select the nodes only from a locality of the graph where the informational criteria are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual information maximization.</head><p>Recently, several research has been focused on mutual information estimation. MINE <ref type="bibr" target="#b2">[3]</ref> develops a general framework for maximization of mutual information, and introduces a consistent method for unsupervised representation learning. Similarly, <ref type="bibr" target="#b23">[24]</ref> introduces Deep InfoMax (DIM), for unsupervised representation learning while putting more focus on the intrinsic structure of image data. <ref type="bibr" target="#b55">[56]</ref> proposes the Deep Graph Infomax (DGI) which takes the graph-structured data into account and adopts the ideas from DIM to the graph domain. Also, InfoGraph <ref type="bibr" target="#b53">[54]</ref> maximizes the mutual information between the graph-level representation and the representations of substructures of the input graph.</p><p>In contrast to VIPool, which is heavily built upon the method of MINE, MEWISPool maximizes the mutual information with a completely different method by explicitly incorporating the structure of the input graphs. Indeed, MEWISPool takes this fact into account that the structure of the graph specifies how the mutual information between the pooled nodes and original nodes can be maximized. In other words, the MEWISPool's perspective to view the input graphs as the representations of noisy communication channels, enables it to directly exploit the structure of the input graphs to maximize the information transmitted through the pooling layer.</p><p>Over-smoothing in GNNs. There are numerous reports emphasizing on the characteristic of GNNs in over-smoothing the features, which results in gradual decrease in the performance of GNNs by increasing the number of layers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>. This decay is partly attributed to over-smoothing, where repeated graph convolutions eventually make node embeddings indistinguishable <ref type="bibr" target="#b66">[67]</ref>. <ref type="bibr" target="#b40">[41]</ref> develops a theoretical framework based on graph signal processing to indicate that GNNs only perform lowpass filtering on feature vectors and do not have the nonlinear manifold learning property. Similarly, <ref type="bibr" target="#b59">[60]</ref> demonstrates that such networks correspond to fixed low-pass filters. Based on this concept, MEWISPool views the input graphs as the confusability graphs of noisy communication channels. The definition of the confusability graph will be provided in the supplementary materials.</p><p>Neural execution of combinatorial optimization problems. Most of the CO problems including the MWIS problem, are considered to be NP-hard. However, the study of such problems from a neural perspective has become an engaging area of research <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>. S2V-DQN <ref type="bibr" target="#b10">[11]</ref> combines reinforcement learning and graph embedding techniques to solve CO problems using deep Q-learning. <ref type="bibr" target="#b35">[36]</ref> utilizes GNNs in combination with guided tree search, to solve certain CO problems on graphs in a supervised setting. Further examples for neural execution of CO problems can be found in <ref type="bibr" target="#b5">[6]</ref>. More recently, <ref type="bibr" target="#b26">[27]</ref> has proposed a global framework for combinatorial optimization on graphs in unsupervised settings. Their framework uses GNNs to learn distributions on the nodes of the graphs. They use the probabilistic method to guarantee the existence of a valid solution which is then extracted using the derandomization method of conditional expectation. As MEWISPool relies Then the graph with node entropies as node features is fed to a GNN to generate a probability score vector z, each of which elements represent the degree of membership of a node to the MWIS. The conditional expectation algorithm (Algorithm 1) is then applied to the probability score along with the adjacency matrix of the graph to extract the maximum entropy weighted independent set of the graph, U * . Finally, the candidate nodes U * and the adjacency matrix are utilized to reconstruct the pooled graph according to Eq.(10). The feature vector of each node in the pooled graph is set the same as the feature vector of the same node in the input graph. on extracting the MWIS of a graph, we adopt the intuition behind the work of <ref type="bibr" target="#b26">[27]</ref> to provide an approximate solution to the problem via GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In a general perspective, MEWISPool tends to pool a subset of nodes whose signals have the maximum mutual information with the entire set of input nodes signals while taking the structure of the input graph into account. Since the input graph signals to the pooling layer are the outputs of previous graph convolutional layers, due to the over-smoothing behavior of GNNs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67]</ref>, such signals mostly contain low-frequency components and exhibit smoothness in their values over the structure of the graph. Hence, the adjacent nodes signals might be confused due to low variation between their signal contents.</p><p>Given the smoothness of neighboring signals, one can view the input graph as a noisy communication channel whose nodes signals are the messages which are to be transmitted and are prone to confusion due to the presence of noise. In this graph, the edges denote the pairs of messages which might be confused through transmission. The resulting graph is called the confusability graph <ref type="bibr" target="#b15">[16]</ref> of the channel. In this perspective, sending the messages over an independent set of the graph yields a reliable and error-free transmission of messages <ref type="bibr" target="#b48">[49]</ref>. MEWISPool exploits this intuition and provides a neural implementation of the pooling layer as a reliable transmission of information across the channel while maximizing the mutual information. Here, we give a formal definition of MEWISPool's functionality. Note that the proofs of propositions 1, 2, 4, and 5 are given in the supplementary materials. Also, the preliminaries on MIS and MWIS are presented in the supplementary materials.</p><p>Let G = (V, E) be an undirected simple graph, where V denotes the set of vertices, and E denotes the set of edges, and let X ? R |V|?d be the feature matrix of the nodes of the graph. The objective of MEWISPool is to find an optimal subset of nodes, U * ? V, satisfying the following optimization problem:</p><formula xml:id="formula_0">maximize U I(X; X U ) subject to (i, j) / ? E,?i, j ? U,<label>(1)</label></formula><p>where I(X; X U ) is the mutual information between X and X U , the signals over the subset of nodes U ? V. Also, i and j denote the nodes of the graph and (i, j) denotes an edge between the nodes i and j. The mutual information between two random variables X and Y with the distributions p X and p Y is defined as the Kullback-Leibler divergence between their joint distribution and the product of their marginal distributions, i.e.,</p><formula xml:id="formula_1">I(X; Y ) = D KL (p (X,Y ) p X ? p Y ).</formula><p>The solution of the optimization problem 1 represents a subset U * of nodes whose corresponding signals have the maximum mutual information with the set of the signals on the entire graph, while the resulting selected nodes are mutually disconnected. In this work, we aim to find the optimal solution U * using GNNs.</p><p>In order to circumvent the difficulties of solving problem 1, we reformulate it to</p><formula xml:id="formula_2">maximize U H(X U ) subject to (i, j) / ? E,?i, j ? U,<label>(2)</label></formula><p>where H(X U ) is the entropy of the signals over the nodes of U.</p><p>Proposition 1. Let X be a set of random variables and X U be a subset of X denoted by the set of indices U. Let ? be a neural network, mapping X to X U . Then maximizing the mutual information I(X; X U ) is equivalent to maximizing the joint entropy H(X U ).</p><p>Formulation of problem 2 is based on the Infomax principle <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref> which states that in order to maximize the mutual information between the input and output of a neural network, one can maximize the entropy of the neural network's output. This formulation translates to finding an independent set of the graph with maximum joint entropy. Further, we show that the problem 2 can be reduced to</p><formula xml:id="formula_3">maximize U i?U H(X i ) subject to (i, j) / ? E,?i, j ? U.<label>(3)</label></formula><p>Proposition 2. Let G = (V, E) be an undirected simple graph and X be the set of random variables assigned to the nodes of G. Also, let g be a probability distribution associated to X such that for any (i, j) / ? E, it follows that X i and X j are independent. Then, problems 2 and 3 are equivalent. Proposition 3. The problem 3 is the definition of the maximum weight independent set problem where the weight of node i is defined as H(X i ).</p><p>Proof. Problem 3 states that we want to select as many nodes as possible to maximize the sum of nodes entropies while no two of the selected nodes are adjacent. Obviously, this is the definition of the MWIS problem where the weights are defined as H(X i ).</p><p>For solving the problem 3, first we need to introduce the probability distribution g over the nodes signals which satisfies the condition of proposition 2. To this end, we refer to the concept of over-smoothing in GNNs. Intuitively, since the graph signals are smooth with respect to the graph structure, the presence of a node signal with high variation with respect to its neighboring signals is less probable. Hence, we model the probability distribution of the nodes signals inversely proportional to the variations of nodes signals with respect to their neighboring signals. Mathematically speaking, for an undirected and unweighted simple graph G, and its corresponding graph signal x ? R |V| the local variation ? i at vertex i is defined as,</p><formula xml:id="formula_4">? i = ? ? j?Ni (x j ? x i ) 2 ? ? 1 2 ,<label>(4)</label></formula><p>where N i represents the neighbors of the node i <ref type="bibr" target="#b50">[51]</ref>. The local variation provides a measure of the smoothness of the graph signal x around a vertex. For the case of d-dimensional graph signal X ? R |V|?d , we calculate the local variations at each dimension, and take the L 2 norm of the local variations vector as the variation of each node:</p><formula xml:id="formula_5">? i = d k=1 (? (k) i ) 2 1 2 ,<label>(5)</label></formula><p>where ? (k) i is the local variation of the node i at the dimension k. As mentioned before, the input graph signal to MEWISPool is a smooth graph signal, meaning that the local variation of each node is supposedly small. Hence, the occurrence of a node with relatively high local variation is less probable. We mathematically model this intuition as, if v ? selected ? rejected then <ref type="bibr">4:</ref> s ? z 5:</p><formula xml:id="formula_6">p(X i ) ? exp(?? i ),<label>(6)</label></formula><formula xml:id="formula_7">s v ? 1 6: s Nv ? 0 7: if ? ? |V| i=1 H(X i )s i + (i,j)?E s i s j ? T then 8: selected ? v 9:</formula><p>rejected ? N v 10:</p><formula xml:id="formula_8">z v ? 1 11: z Nv ? 0 12: end if 13:</formula><p>end if 14: end for 15: return selected where X i ? R d is the signal on the node i. Based on Eq. <ref type="formula" target="#formula_6">(6)</ref> we propose a notion of node entropy by assigning a probability p(X i ) to the node i according to</p><formula xml:id="formula_9">p(X i ) = exp(?? i ) |V| j=1 exp(?? j ) ,<label>(7)</label></formula><p>which is the softmax function of the vector Next, we derive the node entropy as</p><formula xml:id="formula_10">?? = [?? 1 , . . . , ?? |V| ] T .</formula><formula xml:id="formula_11">H(X i ) = ?p(X i ) log(p(X i )).<label>(8)</label></formula><p>Based on proposition 3, using GNNs, MEWISPool approximately solves the MWIS problem with weights as defined in Eq. <ref type="bibr" target="#b7">(8)</ref>. To derive the appropriate loss function for MEWISPool, we adopt the framework proposed in <ref type="bibr" target="#b26">[27]</ref>, which is based on the probabilistic method <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>. More precisely, MEWISPool assigns probability scores to the nodes of a graph which represent the degree of membership of the nodes in the MWIS. Further, MEWISPool efficiently extracts the maximum entropy weighted independent set according to the computed probability scores and using the method of conditional expectation <ref type="bibr" target="#b45">[46]</ref>. Here, we only present the derived loss function and algorithm, and we elaborate the details in supplementary materials. MEWISPool utilizes a GNN which takes a graph and its corresponding nodes entropy weights as inputs and is trained by minimizing the following loss function,</p><formula xml:id="formula_12">L pool (G; ?) = ? ? |V| i=1 H(X i )z i + (i,j)?E z i z j ,<label>(9)</label></formula><p>where ? denotes the parameters of the GNN and z i is the probability score assigned to the node i generated by the GNN. Also, ? is a fixed parameter and is set to ? = i H(X i ) to ensure a positive loss value.</p><p>Once the probability scores are generated by the GNN, MEWISPool applies Algorithm 1 to incrementally extract the maximum entropy weighted independent set of the input graph. Briefly, Algorithm 1 iterates through the nodes of the graph. In each step, if the current node is neither selected nor rejected, the algorithm calculates the value of Eq.(9) conditioned on the selection of the current node and rejection of its neighboring nodes. If the calculated value is less than or equal to the value of the MEWISPool's loss function, the algorithm selects the current node and rejects its neighbors. This procedure is based on the method of conditional expectation which is elaborated in the supplementary materials.</p><p>Thus far, we have shown how to solve problem 3 and select the pooled nodes U * . After the nodes and their corresponding graph signals are pooled based on the solution of problem 3, the adjacency matrix for the pooled nodes is constructed according to</p><formula xml:id="formula_13">A pooled = ?(A, U * ),<label>(10)</label></formula><p>where the function ? is defined as ?(A,</p><formula xml:id="formula_14">U * ) = (1 |U * | ? I |U * | ) clip(A 2 U * + A 3 U * ). 1 |U * | is a square all-ones matrix of size |U * | and I |U * | is the identity matrix of size |U * |. A k U * is the submatrix of A k corresponding to the nodes of U * . The term (1 |U * | ? I |U * | )</formula><p>, simply removes any emerging self-loops. Finally, the clip function clips any values greater than 1 to 1. We justify the choice of the reconstruction function of Eq.(10) by the following proposition. To recapitulate, MEWISPool performs as follows; It extracts the pooled nodes and their signals, X pooled , by solving problem 3 and subsequently, reconstructs the pooled adjacency matrix based on Eq.(10). The complete procedure of MEWISPool as a pooling layer is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>For graph classification tasks, the whole network is trained according to L = L task + ?L pool , where L task is the cross-entropy loss for classification, and ? is a regularization factor whose effect is studied in the supplementary materials. Technically, L task is defined as ?y T log(?), where y and? are the ground truth and predicted class labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our method on several problems, namely supervised graph classification task, and the unsupervised MIS problem. Furthermore, we introduce different architectural settings used on each task and dataset. We also mention the advantage of MEWISPool comparing to other pooling techniques in being adaptive, so that it does not require the hyper-parameter of pooling ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate MEWISPool on graph classification tasks, we use the social network datasets IMDB-B, IMDB-M, and COLLAB <ref type="bibr" target="#b61">[62]</ref>, small molecules datasets FRANKENSTEIN <ref type="bibr" target="#b42">[43]</ref>, Mutagenicity <ref type="bibr" target="#b46">[47]</ref>, and MUTAG <ref type="bibr" target="#b11">[12]</ref>, and also bioinformatics datasets D&amp;D <ref type="bibr" target="#b49">[50]</ref>, and PROTEINS <ref type="bibr" target="#b12">[13]</ref>. For the MIS problem we use the citation networks datasets <ref type="bibr" target="#b47">[48]</ref>. The statistics of each dataset is reported in the supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>The implementation of MEWISPool consists of three graph convolutional layers for the graph classification task and six graph convolutional layers for the MIS problem. We use GIN <ref type="bibr" target="#b60">[61]</ref> for the graph convolutional layers. Following the convolutional layers, the conditional expectation module, as explained in Algorithm 1, extracts the MWIS of the graph. Note that in contrast with the majority of pooling layers, MEWISPool does not require a pooling ratio, which in turn facilitates the procedure by treating the ratio adaptively with respect to the graph structure. For the MIS experiments, the nodes weights are all set to 1, implying equal importance for all the vertices.</p><p>In order to conduct the experiments for graph classification tasks, we use an architecture which consists of three GIN layers, two MEWISPool layers, and two dense layers which constitute the classifier. MEWISPool is followed by each convolutional layer except the last GIN which is followed by the classifier. Furthermore, we employ batch normalization <ref type="bibr" target="#b25">[26]</ref> and dropout <ref type="bibr" target="#b52">[53]</ref> layers after each convolutional layer. The activation functions for all neurons are set to the rectified linear units (ReLU).</p><p>The whole network is trained in an end-to-end manner using the Adam <ref type="bibr" target="#b28">[29]</ref> optimizer. For the graph classification task, the objective is the minimization of the cross-entropy loss for the classification and the MEWISPool's loss as in Eq.(9), whereas in the MIS problem, the only objective is the loss function of MEWISPool. The trade-off between the cross-entropy loss and the MEWISPool's loss are controlled via a regularization factor which is set to 0.01 for all the graph classification experiments. The learning rate is set to 10 ?3 and the model is trained for 200 epochs. Additionally, all graph classification experiments are performed in a 10-fold cross-validation experimental setting. The whole experiment is implemented using PyTorch <ref type="bibr" target="#b44">[45]</ref>, PyTorch Geometric <ref type="bibr" target="#b16">[17]</ref>, and Deep Graph Library <ref type="bibr" target="#b58">[59]</ref> packages, and is executed on GeForce GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Empirical Results</head><p>Supervised graph classification task. In this experiment, we quantitatively evaluate the performance of MEWISPool. The datasets of this task contain sets of input graphs with their corresponding labels, and the task is to classify each input graph. The datasets mostly consist of feature vectors or node labels for the vertices, which we use as input features to the model. For the datasets with no node attributes, we use the one-hot encoding of node degrees as input features. We compare the performance of MEWISPool with the state-of-the-art pooling techniques, namely, Set2Set <ref type="bibr" target="#b56">[57]</ref>, ECC <ref type="bibr" target="#b51">[52]</ref>, SortPool <ref type="bibr" target="#b65">[66]</ref>, DiffPool <ref type="bibr" target="#b63">[64]</ref>, SAGPool <ref type="bibr" target="#b32">[33]</ref>, Graph U-Net <ref type="bibr" target="#b17">[18]</ref>, MinCutPool <ref type="bibr" target="#b6">[7]</ref>, Struct-Pool <ref type="bibr" target="#b64">[65]</ref>, iPool <ref type="bibr" target="#b18">[19]</ref>, and VIPool <ref type="bibr" target="#b33">[34]</ref>. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the performance of MEWISPool in comparison with the aforementioned methods. As the results suggest, MEWISPool outperforms other methods in seven benchmark datasets and achieves the second best result in the COLLAB dataset. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates sample maximum entropy weighted independent sets extracted by MEWISPool from the PROTEINS dataset.</p><p>Unsupervised combinatorial optimization problem of MIS. In this experiment, we evaluate a standalone implementation of MEWISPool, to approximately solve the NP-hard problem of MIS. The  problem is to find the largest possible subset of vertices in the graph such that no two of which are adjacent. The evaluation metric for assessing the performance of MIS-solving methods is simply the cardinality of the solution set found by the method. Here, as in <ref type="bibr" target="#b35">[36]</ref>, we compare MEWISPool with a greedy classic heuristic referred to as Classic <ref type="bibr" target="#b43">[44]</ref>, and two state-of-the-art methods for solving MIS problem, namely, S2V-DQN <ref type="bibr" target="#b10">[11]</ref>, which is based on reinforcement learning paradigms, and GCN-GTS <ref type="bibr" target="#b35">[36]</ref>, which is a supervised method based on the combination of GNNs and guided tree search heuristic algorithm. The numerical results are shown in <ref type="table" target="#tab_2">Table 2</ref>. As the results suggest, MEWISPool outperforms the Classic and S2V-DQN methods but fails to perform as desirably as GCN-GTS. However, it is worth mentioning that GCN-GTS treats the problem in a supervised manner, meaning that it requires a set of training solutions for the MIS problem, while MEWISPool tries to solve the MIS problem with no supervision. <ref type="figure" target="#fig_4">Figure 3</ref> illustrates the learning curve of MEWISPool and the evolution of solved MIS for the Cora dataset. Even though MEWISPool has a relatively high computational cost, its functionality involves finding an approximate solution for the NP-hard MWIS problem, however, due to its superior performance, it motivates further research on applying CO problems within the graph neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Pooling operation serves as an essential component in GNNs for hierarchical representation learning. In this paper, we developed a structure-adaptive pooling layer based on the combinatorial optimization problem of MWIS with entropy-based weights. We expanded the insights regarding the proposed method using the concepts of Shannon capacity of graphs in communication theory and the Infomax principle for maximizing the mutual information between the network's input and output. We proposed a neural execution to approximate the solution for the NP-hard problem of MWIS. Finally, we evaluated the method on several benchmark datasets for graph classification tasks and the MIS problem, and achieved the state-of-the-art and competitive results. <ref type="figure" target="#fig_1">Figure 4</ref>: The confusability graph of a noisy channel with input symbols of m 1 , m 2 , m 3 , m 4 , and m 5 , where m i can be confused with m i+1 across the noisy channel and i belongs to a modular arithmetic modulo 5. The vertices marked as green denote an example for an independent set for which the transmission of corresponding symbols is error-free and without confusion.</p><p>The maximum independent set problem is a special case of the maximum weight independent set problem where all the weights are set to 1. Both problems are considered as NP-hard problems and finding an exact solution for these problems is intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Shannon Capacity and the Confusability Graph</head><p>A channel is a medium that is used to convey an information signal, from one or several senders (or transmitters) to one or several receivers. In information theory, a channel refers to a theoretical channel model with certain error characteristics. The channel capacity of a given channel is the highest information rate that can be achieved with arbitrarily small error probability, and is given by the maximum of the mutual information between the input and output of the channel.</p><p>Given a communication channel in which certain signal values can be confused with each other due to the presence of noise, the Shannon capacity models the rate of information that can be transmitted through such a channel <ref type="bibr" target="#b48">[49]</ref>. Suppose each distinct message or signal is represented by a vertex in a graph and two vertices are connected by an edge if and only if their corresponding signals are confused through transmission. Mathematically speaking, a channel conveying the transmitted signal x to the received signal y can be modeled as p y|x (y|x). Let X ? {x 1 , x 2 , . . . , x n } be the transmitted signal and Y be the received signal. For a noiseless channel we have</p><formula xml:id="formula_15">p(Y = x i |X = x i ) = 1, i ? {1, . . . , n},<label>(13)</label></formula><p>meaning that the received signal Y equals to the transmitted signal X = x i . The above expression for a noisy channel can be written as,</p><formula xml:id="formula_16">p(Y = x i |X = x j ) = ij ? 0, i, j ? {1, . . . , n},<label>(14)</label></formula><p>which means that the probability of receiving Y = x i given that the signal X = x j is transmitted is ij . Now, suppose the set {x 1 , x 2 , . . . , x n } corresponds to the nodes of a graph. Nodes i and j are connected if and only if ij &gt; 0. The resulting graph is called the confusability graph of the noisy communication channel. One can simply observe that sending the symbols belonging to an independent set of the confusability graph yields a reliable and error-free transmission of information through the noisy channel. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates a simple example of this concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Proposition 1</head><p>Proof. Let X and Y be random variables, and ? be a deterministic differentiable mapping from X to Y . We prove the proposition for both cases of X and Y being discrete and continuous. For the discrete case, we have</p><formula xml:id="formula_17">I(X; Y ) = H(Y ) ? H(Y |X),<label>(15)</label></formula><p>where I(X; Y ) is the mutual information between X and Y and H(Y ) = ? y p Y (y) log p Y (y) is the entropy of the random variable Y , and H(Y |X) = ? y p Y |X (y|x) log p Y |X (y|x) is the conditional entropy of Y given X.</p><p>Since Y = ?(X), the conditional probability distribution of Y |X is given by</p><formula xml:id="formula_18">p Y |X (y|x) = 1 y = ?(x) 0 otherwise.<label>(16)</label></formula><p>Hence, we get</p><formula xml:id="formula_19">H(Y |X) = ? y p Y |X (y|x) log p Y |X (y|x) = ?p Y |X (?(x)|x) log p Y |X (?(x)|x) = 0.<label>(17)</label></formula><p>The second line of Eq.(17) is because p Y (y) is zero everywhere, except where y = ?(x). Also, the third line of Eq.(17) holds because log p Y |X (?(x)|x) = log 1 = 0. Therefore, we get</p><formula xml:id="formula_20">I(X; Y ) = H(Y ).<label>(18)</label></formula><p>Thus, maximizing I(X; Y ) is equivalent to maximizing H(Y ).</p><p>For the continuous case, we have</p><formula xml:id="formula_21">I(X; Y ) = h(Y ) ? h(Y |X),<label>(19)</label></formula><p>where h(Y ) is the differential entropy of the continuous random variable Y and is defined as</p><formula xml:id="formula_22">h(Y ) = ? f Y (y) log f Y (y)dy,<label>(20)</label></formula><p>where f Y (y) is the probability density function of Y . Similarly, h(Y |X) is the conditional differential entropy of Y |X defined as</p><formula xml:id="formula_23">h(Y |X) = ? f Y |X (y|x) log f Y |X (y|x)dy.<label>(21)</label></formula><p>Since Y = ?(X), the conditional cumulative distribution function of Y given X is given by</p><formula xml:id="formula_24">F Y |X (y|x) = p Y |X (Y ? y|x) = 1 y ? ?(x) 0 otherwise.<label>(22)</label></formula><p>Therefore, the conditional probability density function of Y given X will be</p><formula xml:id="formula_25">f Y |X (y|x) = ?F Y |X (y|x) ?y = ?(y ? ?(x)),<label>(23)</label></formula><p>where ?(.) is the continuous Dirac delta function. Thus, the conditional differential entropy of Eq.(21) will be</p><formula xml:id="formula_26">h(Y |X) = ? ?(y ? ?(x)) log ?(y ? ?(x))dy,<label>(24)</label></formula><p>which is the differential entropy of shifted Dirac delta function which is ??. Intuitively, with the mapping from X to Y assumed to be deterministic, the conditional differential entropy h(Y |X) attains its lowest possible value and diverges to ??. This result is due to the differential nature of the entropy of a continuous random variable.</p><p>Assuming that the mapping ? is parameterized by the parameters ?, based on Eq.(19), we get</p><formula xml:id="formula_27">?I(X; Y ) ?? = ?h(Y ) ?? ,<label>(25)</label></formula><p>because the conditional differential entropy h(Y |X) is independent of ?. This indicates that maximizing the differential entropy of Y is equivalent to maximizing the mutual information between Y and X, with both maximizations being performed with respect to the parameters ? of the mapping <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Since a neural network is a deterministic differentiable mapping, by replacing Y with X U , the proposition is proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Proposition 2</head><p>Proof. Here, we prove the proposition for the discrete random variables and the proof for the continuous case is carried out in a similar way.</p><p>Let G = (V, E) be an undirected simple graph and X be a set of random variables assigned to the vertices of G. Also, let g be a probability distribution associated to X such that for any (i, j) / ? E, it follows that X i and X j are statistically independent, where X i and X j denote the i th and j th elements of the set X, respectively. We have H(X i ) = ? g(X i ) log g(X i ). Using the probability chain rule, we can compute the joint entropy of X = {X 1 , . . . , X |V| } as</p><formula xml:id="formula_28">H(X) = |V| i=1 H(X i |X 1 , . . . , X i?1 ).<label>(26)</label></formula><p>For a set of independent random variables, their joint probability distribution is equal to the product of their marginal probability distributions. Accordingly, the joint entropy is equal to the sum of the entropies of each random variable. Since, in problem 2, the joint entropy is being maximized over an independent set of the graph G, where the distribution g implicates statistical independence between disconnected vertices, we have</p><formula xml:id="formula_29">H(X U ) = i?U H(X i ),<label>(27)</label></formula><p>where U is an independent set of the graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Proposition 4</head><p>Proof. We assume that the reader is familiar with the concept of Markov Random Fields (MRF) and their properties.</p><p>Definition 3. A set of random variables X over a graph G = (V, E), is said to be Gibbs Random Field (GRF), if the joint probability distribution can be written as</p><formula xml:id="formula_30">p(X) = 1 Z ci?C ? i (c i ),<label>(28)</label></formula><p>where C is the set of the cliques of the graph G, and ? i (c i ) is the potential assigned to the clique c i ? C. Also, Z is a normalizing constant which is called the partition function.</p><p>For a GRF, it is typically convenient to write the joint probability as</p><formula xml:id="formula_31">p(X) = 1 Z exp[? ci?C f i (c i )],<label>(29)</label></formula><p>where the potential of each clique is defined as an exponential function.</p><p>Lemma 1. If the node probabilities p(X i ) of a set of random variables X = {X 1 , . . . , X n } over the nodes of a graph G = (V, E) is of exponential form, then the joint probability distribution can be written as a Gibbs Random Field.</p><p>Proof. We start backwards, with all the directions being reversible. Suppose X is of Gibbs Random Field form. Then we have</p><formula xml:id="formula_32">p(X) = 1 Z ci?C ? i (c i ) = 1 Z exp[? ci?C f i (c i )].<label>(30)</label></formula><p>Then, for a clique with two members <ref type="figure" target="#fig_3">(Figure 5</ref>), the joint probability distribution is proportional to the exponential form</p><formula xml:id="formula_33">p ij ? exp(??(i, j)),<label>(31)</label></formula><p>Where ? is a function defined on the clique {i, j}. In this case, one can simply show that the marginal distributions p i (or p j ) is of the exponential form: because we can write</p><formula xml:id="formula_34">p i ? exp(??(i)),<label>(32)</label></formula><formula xml:id="formula_35">p i = jm j=j1 p ij = jm j=j1 exp(??(i, j)) = exp(??(i, j 1 ))[1 + exp(??(i, j 2 )) exp(??(i, j 1 )) + ? ? ? + exp(??(i, j m )) exp(??(i, j 1 )) ],<label>(33)</label></formula><p>where {j 1 , . . . , j m } is the set of possible values for the random variable defined on the node j. Thus we know that the marginal distribution is of exponential form. The procedure can be followed in reverse, i.e., if the marginal distributions are of exponential form, then the joint probability distribution and clique distribution are of exponential form.</p><p>The Hammersly-Clifford theorem <ref type="bibr" target="#b54">[55]</ref> provides the equivalency of a Gibbs Random Field and a Markov Random Field. Thus, according to lemma 1 and Hammersly-Clifford theorem, if we can write the nodes probability distributions in the exponential form, then the joint probability can be written as a Markov Random Field.</p><p>Given that the node probability distributions are defined in exponential and they are constructed given the values of the random variables of the neighboring nodes, i.e.,</p><formula xml:id="formula_36">p(X i ) ? exp(?? i ) = exp(??(X i , X Ni )),<label>(34)</label></formula><p>one can show that the joint probability distribution is of Gibbs Random Field, based on Lemma 1, and based on Hammersly-Clifford theorem, the assigned probability distribution to the graph constitutes a Markov Random Field. Since the node probabilities are defined given the neighboring values, one can immediately conclude that two pairs of disconnected nodes are statistically independent.</p><p>A.6 Proof of Proposition 5 Lemma 2. Let G = (V, E) be a connected simple graph, and U be an arbitrary maximal independent set of G. For any node i ? U, there is a node j ? U with the maximum geodesic distance, d G (i, j) of 3 in G.</p><p>Proof. If i, j ? U, then, since i and j are not connected, trivially we have d G (i, j) &gt; 1. Now, let i, j ? U, and there is a path of length 4 between i and j ( <ref type="figure" target="#fig_7">Figure 6</ref>). We can rapidly conclude that the nodes m and n do not belong to U since they are connected to two nodes of U. If the node w belongs to the maximal independent set U the proof is over since the geodesic distance between the node i and w would be 2 (through i ? m ? w). Thus, let's suppose that w does not belong to U. If w / ? U, it follows that w is directly connected to a node in U. In this case, three situations might happen:   (i) w is connected to i <ref type="figure" target="#fig_8">(Figure 7</ref>), (ii) or, w is connected to j <ref type="figure" target="#fig_9">(Figure 8</ref>), (iii) or, w is directly connected to another node g ? U <ref type="figure" target="#fig_10">(Figure 9</ref>).</p><p>If case (i) happens, it means that the geodesic distance between i and j is 3 (through i ? w ? n ? j) and the proof is over. If case (ii) occurs, then again, the geodesic distance between i and j is 3 (through i ? m ? w ? j) and the proof is over. Finally, if case (iii) arises, then there is a node g ? U for which the geodesic distance between i and g is 3 (through i ? m ? w ? g) and the proof is complete.</p><p>If none of the cases above happens, then w must belong to the maximal independent set U, otherwise the condition of maximality of U would be violated.</p><p>From the above lemma, we can immediately conclude the following corollary. Corollary 1. Let G = (V, E) be a connected simple graph, and U be an arbitrary maximal independent set of G. For any two nodes i, j ? U,</p><formula xml:id="formula_37">either d G (i, j) ? 3 or there is a subset S = {s 1 , . . . s k } of U such that d G (i, s 1 ) ? 3, d G (s i , s i+1 ) ? 3, 1 ? i ? k ? 1, and d G (s k , j) ? 3.</formula><p>Proof. The case where the path length between i and j is 4 is shown in the proof of Lemma 2 ( <ref type="figure" target="#fig_10">Figure  9</ref>). It is shown that either the shortest path between i and j is at most 3 or there is a subset {g} ? U where d G (i, g) ? 3 and d G (g, j) ? 3. The proof for the cases where the path length between i and j is more than 4 is carried out similarly.</p><p>Based on Corollary 1, we can present a theorem which indicates that the reconstruction function preserves the connectivity of the graph. Proposition 6. For a simple graph G = (V, E) with an associated adjacency matrix A and an arbitrary maximal independent set U, the reconstruction function of Eq.(10), preserves the connectivity of each connected component of the graph.</p><p>Proof. Without loss of generality, suppose that G has only one connected component. If U is a singleton (it contains only one vertex) then the resulting graph consists of only one vertex and is trivially connected. Now, suppose U contains n nodes. According to Corollary 1, for any two nodes in U, there is a sequence of nodes in U for which the shortest path between each pair of consecutive nodes is at most 3. Since the reconstruction function ?(A, U) connects any two nodes of U for which the shortest path length is at most 3, it follows that there is path between any two arbitrary nodes in the reconstructed graph and hence, the resulting graph is connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Neural Execution of Maximum Weight Independent Set</head><p>In this section we present a neural implementation for the approximate solution of the maximum weight independent set problem. Initially, we briefly introduce the objective function of the MWIS problem. Given a graph G = (V, E) and a weight vector w ? R |V| , MWIS problem can originally be formulated as an integer programming problem defined as 12.</p><p>We can simply replace the first constraint of problem 12 with z i z j = 0 ; (i, j) ? E. The Lagrangian of this optimization problem can be given as,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.1 Method of Conditional Expectation</head><p>Since the probabilistic method is nonconstructive, which only proves the existence of a mathematical object with the desired combinatorial structure, derandomization methods are known to provide deterministic algorithms that guarantee to construct such desired structures in a precise and efficient manner <ref type="bibr" target="#b45">[46]</ref>. Method of conditional expectation is one of such derandomization methods, which we use in this paper to retrieve the valid solution of maximum weight independent set problem. More precisely, we use the method of conditional expectation to sequentially select the nodes which belong to the maximum weight independent set.</p><p>Briefly, the method of conditional expectation works as follows; given a threshold for the expected value of a random variable, select the outcome which results in an expected value not greater than the threshold. Since the conditional expectation never increases, this ensures that we always reach to a solution which is not worse than the threshold of the expected value. For the maximum weight independent set problem the threshold is defined as Eq. <ref type="formula" target="#formula_12">(9)</ref> which is the loss of the network and the whole procedure can be translated as follows; at each step we select a node whose selection and removal of its neighbors leads to a lower expected value than the given threshold. This procedure is given at Table(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Statistics of Datasets</head><p>The statistics of the datasets that are used in graph classification tasks and the combinatorial optimization problem of maximum independent set are presented in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, respectively.  The trade-off between the cross-entropy loss for classification and the MEWISPool's loss are controlled via a regularization factor which is set to 0.01 for all the graph classification experiments. In this section we investigate the effect of the regularization factor on the performance of the model in terms of validation accuracy for model selection. The experiments are conducted on the IMDB-BINARY dataset. We sweep through the values in {1, 0.1, 0.01, 0.001, 0.0001} and record the validation accuracy. The result is illustrated in <ref type="figure" target="#fig_0">Figure 10</ref>. Note that the values for the regularization factor are represented in log-scale in the basis of 10. As it is shown, the best validation accuracy is achieved by setting the regularization factor to 0.01.</p><p>Furthermore, we depict the learning curve of the model in terms of separate losses for training. <ref type="figure" target="#fig_0">Figure 11</ref> demonstrates the learning curves corresponding to the cross-entropy loss for classification, MEWISPool loss, and the total loss with the regularization factor taken into account. The architecture <ref type="figure" target="#fig_0">Figure 10</ref>: The effect of regularization factor on the validation accuracy. As the figure shows, the best accuracy is achieved for the regularization factor set to 0.001. used for the graph classification task is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. In this architecture the MEWISPool consists of three graph convolutional layers. All the convolutional layers are graph isomorphism networks. <ref type="figure" target="#fig_0">Figure 12</ref>: The architecture used for the graph classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Maximum Entropy Weighted Independent Set Pooling (MEWISPool) diagram. As the diagram shows, the input graph and its corresponding node features X are used to compute the entropy values of the nodes, H(X) based on Eq.(8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 4 .</head><label>4</label><figDesc>The probability distribution of Eq.(7) satisfies the condition of distribution g of proposition 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of sampled nodes by MEWISPool in PROTEINS dataset<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>. The higher color intensities correspond to higher node entropies. Nodes with larger size represent the nodes sampled by MEWISPool. Note that MEWISPool adaptively samples nodes that cover the entire graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 5 .</head><label>5</label><figDesc>For a simple graph G = (V, E) with an associated adjacency matrix A and an arbitrary maximal independent set M, the reconstruction function of Eq.(10), preserves the connectivity of each connected component of the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Training loss (up) and size of learned MIS (down) for Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MIS problem.</head><label></label><figDesc>For the MIS problem, the weights of all nodes are preset to 1 and the computational complexity of MEWISPool reduces to the computational complexity of the GNN followed by the derandomization algorithm of conditional expectation as presented in Algorithm 1. The computational complexity of a graph convolutional layer is O(|V| 3 ) [65]. In the worst case, Algorithm 1 iterates through all the nodes of the graph (|V|), and for each node, it calculates the loss function of MEWISPool as in Eq.(9). Dominated by the second and third terms, the complexity of the loss function is O(|V|) + O(|E|). Therefore, the total complexity of the conditional expectation module is O(|V| 2 ) + O(|V||E|). Finally, the computational complexity of MEWISPool for the MIS problem can be written as O(|V| 3 ) + O(|V| 2 ) + O(|V||E|). Assuming that the graphs are sparse, i.e., O(|E|) ? O(|V|), the complexity reduces to O(|V| 2 ) + O(|V| 3 ) ? O(|V| 3 ) which is the complexity of a single graph convolutional layer. For the case of the Cora dataset, which contains 2708 nodes and 5429 edges, the running time of a feed-forward of the network is 2.9 seconds.Graph classification task. The computational complexity of MEWISPool for the graph classification task is similar to the MIS problem, except that we need to compute entropy-based weights beforehand. Computation of node entropies involves calculating L 2 norms of the local variation vectors for the nodes (Eq.(5)) with the order of O(|V|kd), where k is the maximum degree of the graph and d is the dimensionality of nodes features. Computing the probabilities of the nodes (Eq.(7)) is in the order of O(|V|), and finally computing the nodes entropies based on the calculated probabilities is in the order of O(|V|). The rest is similar to the case of MIS problem. Thus, the ultimate complexity of MEWISPool for the graph classification task is O(|V|kd) + O(|V| 3 ) + O(|V||E|). Assuming d |V| and the graphs are sparse, i.e., O(|E|) ? O(|V|) and O(k) is constant, the complexity reduces to O(|V|d) + O(|V| 3 ) ? O(|V| 3 ) which is the complexity of a single graph convolutional layer. For the case of the large scale D&amp;D dataset, the average running time of a feed-forward of the network for a single graph is 0.059 second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>A clique of size 2 that contains nodes i and j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>A segment of a graph where nodes i and j belong to the maximal independent set U and there is a path of length 4 between them</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The case where w is connected to i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The case where w is connected to j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>The case where w is connected to another node g ? U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>The learning curves corresponding to the cross-entropy loss, MEWISPool loss, and the total loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Maximum Entropy Weighted Independent Set ExtractionInput Adjacency Matrix A ? R |V|?|V| , Probability Scores z = [z 1 , . . . , z |V| ] T , Fixed Parameter ? = i H(X i ), Fixed Threshold T = L pool (G; ?). Output Maximum Entropy Weighted Independent Set Approximation.1:  Initialize selected = ?, and rejected = ? 2: for each node v in descending order of probability scores z do</figDesc><table /><note>3:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The empirical results of graph classification tasks. The best and second best results are demonstrated with black and blue bold-faced fonts, respectively. The results of MEWISPool consist of the means and standard deviations of the accuracies according to the 10-fold crossvalidation experimental setting. Note that the accuracy values with no standard deviations have been reported in the same fashion as their corresponding papers. Also, thesigns in thetable indicates that the result of the corresponding dataset has not been reported in the original papers. 70.83 ? 0.84 61.92 ? 0.73 86.78 ? 7.33 71.35 ? 2.1 79.33 ? 0.84 71.75 71.00 ? 7.54 49.73 ? 4.19 MEWISPool 84.33 ? 1.18 73.46 ? 0.95 96.66 ? 1.23 80.66 ? 1.72 80.71 ? 2.31 79.66 ? 4.02 82.13 ? 1.21 56.23 ? 1.04</figDesc><table><row><cell>Model</cell><cell>D&amp;D</cell><cell>FRANKENSTEIN</cell><cell>MUTAG</cell><cell>Mutagenicity</cell><cell>PROTEINS</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell>Set2Set[57]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECC[52]</cell><cell>73.68</cell><cell>63.87 ? 2.02</cell><cell>88.33</cell><cell>71.89 ? 1.20</cell><cell>72.33 ? 3.4</cell><cell>67.82 ? 2.4</cell><cell>67.70 ? 2.8</cell><cell>43.48 ? 3.0</cell></row><row><cell>SortPool[66]</cell><cell>79.37 ? 0.94</cell><cell>60.61 ? 0.77</cell><cell>85.83 ? 1.66</cell><cell>70.66 ? 1.51</cell><cell>76.26 ? 0.24</cell><cell>73.76 ? 0.49</cell><cell cols="2">70.03 ? 0.86 47.83 ? 0.85</cell></row><row><cell>DiffPool[64]</cell><cell>75.05 ? 3.4</cell><cell>60.60 ? 1.62</cell><cell>88.87 ? 6.75</cell><cell>77.6 ? 2.7</cell><cell>73.72 ? 3.5</cell><cell>74.83 ? 2.0</cell><cell>68.40 ? 6.1</cell><cell>45.62 ? 3.4</cell></row><row><cell>StructPool[65]</cell><cell>84.19</cell><cell>-</cell><cell>93.59</cell><cell>-</cell><cell>80.36</cell><cell>74.22</cell><cell>74.70</cell><cell>52.47</cell></row><row><cell>SAGPool[33]</cell><cell>78.35 ? 3.5</cell><cell>62.57 ? 0.60</cell><cell>93.32 ? 4.16</cell><cell>72.4 ? 2.4</cell><cell>78.28 ? 4.0</cell><cell>76.92 ? 1.6</cell><cell>72.80 ? 2.3</cell><cell>49.43 ? 2.6</cell></row><row><cell cols="2">Graph U-Net[18] 82.14 ? 3.0</cell><cell>61.46 ? 0.84</cell><cell>87.77 ? 6.47</cell><cell>71.9 ? 3.7</cell><cell>77.20 ? 4.3</cell><cell>77.58 ? 1.6</cell><cell>73.40 ? 3.7</cell><cell>50.27 ? 3.4</cell></row><row><cell>iPool[19]</cell><cell>79.45 ? 2.78</cell><cell>-</cell><cell>90.42 ? 4.68</cell><cell>-</cell><cell>77.36 ? 3.27</cell><cell>77.28 ? 2.17</cell><cell cols="2">73.30 ? 2.72 51.27 ? 3.44</cell></row><row><cell>MinCutPool[7]</cell><cell>80.8 ? 2.3</cell><cell>65.94 ? 1.60</cell><cell>87.34 ? 6.31</cell><cell>79.9 ? 2.1</cell><cell>76.5 ? 2.6</cell><cell>83.4 ? 1.7</cell><cell>79.0 ? 2.0</cell><cell>52.8 ? 1.69</cell></row><row><cell>VIPool[34]</cell><cell>82.68 ? 4.1</cell><cell>-</cell><cell>-</cell><cell>80.19 ? 1.02</cell><cell>79.91 ? 4.1</cell><cell>78.82 ? 1.4</cell><cell>78.60 ? 2.3</cell><cell>55.20 ? 2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The comparison of MEWISPool's performance and other methods based on the size of the solution. MEWISPool consistently performs better than the classic greedy method and the reinforcement learning method of S2V-DQN<ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b10">11]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="3">Cora Citeseer PubMed</cell></row><row><cell>Classic[44]</cell><cell>1424</cell><cell>1848</cell><cell>15852</cell></row><row><cell>S2V-DQN[11]</cell><cell>1381</cell><cell>1705</cell><cell>15705</cell></row><row><cell cols="2">GCN-GTS[36] 1451</cell><cell>1867</cell><cell>15912</cell></row><row><cell>MEWISPool</cell><cell>1433</cell><cell>1852</cell><cell>15862</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Datasets statistics for the supervised graph classification task.Name#Graphs Avg. #Nodes Avg. #Edges #Classes #Node Features Node Label</figDesc><table><row><cell>D&amp;D[13, 50]</cell><cell>1178</cell><cell>284.32</cell><cell>715.66</cell><cell>2</cell><cell>-</cell><cell>+</cell></row><row><cell>FRANKENSTEIN[43]</cell><cell>4337</cell><cell>16.90</cell><cell>17.88</cell><cell>2</cell><cell>780</cell><cell>-</cell></row><row><cell>MUTAG[12, 32]</cell><cell>188</cell><cell>17.93</cell><cell>19.79</cell><cell>2</cell><cell>-</cell><cell>+</cell></row><row><cell>Mutagenicity[47, 28]</cell><cell>4337</cell><cell>30.32</cell><cell>30.77</cell><cell>2</cell><cell>-</cell><cell>+</cell></row><row><cell>PROTEINS[13, 8]</cell><cell>1113</cell><cell>39.06</cell><cell>72.82</cell><cell>2</cell><cell>1</cell><cell>+</cell></row><row><cell>COLLAB[62]</cell><cell>5000</cell><cell>74.49</cell><cell>2457.78</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>IMDB-BINARY[62]</cell><cell>1000</cell><cell>19.77</cell><cell>96.53</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>IMDB-MULTI[62]</cell><cell>1500</cell><cell>13.00</cell><cell>65.94</cell><cell>3</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Datasets statistics for the unsupervised combinatorial optimization problem of maximum independent set.</figDesc><table><row><cell>Name</cell><cell cols="2">#Nodes #Edges</cell></row><row><cell>Cora[48]</cell><cell>2708</cell><cell>5429</cell></row><row><cell>Citeseer[48]</cell><cell>3327</cell><cell>4732</cell></row><row><cell cols="2">PubMed[48] 19717</cell><cell>44338</cell></row><row><cell>A.9 Experimental Settings</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/mewispool/mewispool</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Maximum Independent Set and the Maximum Weight Independent Set of a Graph</head><p>Let G = (V, E) be an undirected, unweighted, and simple graph, where V is the set of vertices or nodes of the graph and E denotes the set of edges. An independent set of the graph is defined as a subset of nodes I ? V such that there is no edge between any two nodes i, j ? I; i.e., for all i, j ? I,</p><p>is an independent set I with the maximum cardinality.</p><p>With this definition, a maximum independent set of a graph, is an independent set with largest possible number of vertices. The maximum independent set problem, is a combinatorial optimization problem defined over a graph whose solution determines a maximum independent set of the graph. This problem can be formulated as the following integer program</p><p>where z = [z 1 , . . . , z |V| ] T denotes an indicator vector whose element z i is a binary variable indicating whether the corresponding node i belongs to the MIS (z i = 1) or not (z i = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.</head><p>Let w ? R |V| be a weight vector assigned to the vertices of a graph G. A maximum weight independent set (MWIS) of G is an independent set I of the graph with the maximum total weight.</p><p>The maximum weight independent set problem, is a combinatorial optimization problem defined over a graph whose solution determines a maximum weight independent set of the graph. This problem can be formulated as the following integer program</p><p>where w i is the weight assigned to the vertex i.</p><p>As z i ? {0, 1} we can consider z i as a Bernoulli random variable with parameter p i . Computing the expected value of Eq. <ref type="formula">(35)</ref>, gives</p><p>Setting ? ij = 1, Eq.(36) turns into the quadratic polynomial formulation of MWIS problem as proven in <ref type="bibr" target="#b0">[1]</ref>:</p><p>where z is defined over the hypercube [0, 1] n . We employ H(z) as the objective function of a graph neural network which is supposed to learn the parameters of the Bernoulli distribution.</p><p>According to the probabilistic method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, as the objective function defined in Eq.(37) is the expected value of f (z) in problem 12, we have</p><p>This assures that there exists a valid solution in the distribution generated by the network which has no less objective value of the original problem than the objective value of the network. Similar to <ref type="bibr" target="#b26">[27]</ref>, we retrieve this solution using derandomization method of conditional expectation <ref type="bibr" target="#b1">[2]</ref>, elaborated in Section(A.7.1).</p><p>Since we intend to define a loss function for the network, we simply convert the maximization problem of 37 to a minimization problem. This yields the loss function of the network as</p><p>where ? denotes the parameters of the network and ? is an upper bound for H(z) to ensure positive values for L(G; ?). A trivial choice for ? is the sum of the weights of all nodes, ? = i w i .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding independent sets in a graph using continuous multivariable polynomial formulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Butenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio Gc</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="137" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The probabilistic method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning for combinatorial optimization: a methodological tour d&apos;horizon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Prouvost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning symbolic physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Miles D Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bistra</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph theory and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erd?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="34" to="38" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erickson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">79</biblScope>
		</imprint>
	</monogr>
	<note>Introduction to combinatorics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ipool-informationbased pooling in hierarchical graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exact combinatorial optimization with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Gasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Ch?telat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01629</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural networks and learning machines/simon haykin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon S Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning assisted heuristic tree search for the container pre-marshalling problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Hottung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunji</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tierney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">104781</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Karalias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Derivation and validation of toxicophores for mutagenicity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Bursi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08475</idno>
		<title level="m">Attention, learn to solve routing problems!</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6483</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01804</idno>
		<title level="m">Graph cross networks with vertex infomax pooling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10659</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the shannon capacity of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Lov?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topologyaware graph signal sampling for pooling in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Nouranizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadjavad</forename><surname>Matinkia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 26th International Computer Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc De</forename><surname>Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fourth international joint conference on artificial intelligence</title>
		<meeting>the twenty-fourth international joint conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
	<note>IJCAI-INT JOINT CONF ARTIF INTELL</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Combinatorial optimization: algorithms and complexity. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiglitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic construction of deterministic algorithms: approximating packing integer programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="143" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Iam graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The zero error capacity of a noisy channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A dictionary of statistics 3e</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03134</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Pointer networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning with differentiable pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Structpool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
