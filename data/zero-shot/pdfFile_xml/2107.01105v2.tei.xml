<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Efficient Meta-Learning with Large Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
							<email>dmassiceti@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<email>kahofman@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Efficient Meta-Learning with Large Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta learning approaches to few-shot classification are computationally efficient at test time, requiring just a few optimization steps or single forward pass to learn a new task, but they remain highly memory-intensive to train. This limitation arises because a task's entire support set, which can contain up to 1000 images, must be processed before an optimization step can be taken. Harnessing the performance gains offered by large images thus requires either parallelizing the meta-learner across multiple GPUs, which may not be available, or trade-offs between task and image size when memory constraints apply. We improve on both options by proposing LITE, a general and memory efficient episodic training scheme that enables meta-training on large tasks composed of large images on a single GPU. We achieve this by observing that the gradients for a task can be decomposed into a sum of gradients over the task's training images. This enables us to perform a forward pass on a task's entire training set but realize significant memory savings by back-propagating only a random subset of these images which we show is an unbiased approximation of the full gradient. We use LITE to train meta-learners and demonstrate new state-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4 parts of the challenging VTAB+MD benchmark relative to leading meta-learners. LITE also enables meta-learners to be competitive with transfer learning approaches but at a fraction of the test time computational cost, thus serving as a counterpoint to the recent narrative that transfer learning is all you need for few-shot classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Meta-learning approaches to few-shot classification are very computationally efficient. Once metatrained, they can learn a new task at test time with as few as 1-5 optimization steps <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or a single forward pass through the model <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and with minimal or no hyper-parameter tuning. In contrast, transfer learning approaches based on fine-tuning typically rely on a large pre-trained feature extractor, and instead take 100s-1000s of optimization steps at test time in order to learn a task <ref type="bibr" target="#b5">[6]</ref>, thus incurring a high computational cost for each new task encountered. This makes meta-learned solutions attractive in compute-constrained deployments, or scenarios where the model must learn multiple different tasks or update on-the-fly (e.g. in continual and online learning settings <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>).</p><p>However, a crucial barrier to progress is that meta-learning approaches are memory-intensive to train and thus cannot easily leverage large images for a performance boost, as recent fine-tuning approaches have done. This limitation arises because a meta-learner must back-propagate through all * Authors contributed equally 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2107.01105v2 [stat.ML] 26 Oct 2021 <ref type="figure" target="#fig_4">Figure 1</ref>: LITE enables meta-learners to be trained on large images with one GPU thereby significantly improving performance while retaining their test time computational efficiency. The schematic shows test time efficiency (the number of steps and number of Multiply-Accumulate operations (MACs) needed to learn a new task at test time) and whether the method can be trained on large images (required for good performance). Existing meta-learners are cheap to adapt but trained on small images (multiple GPUs are required for large images), transfer learning methods are expensive to adapt but trainable on large images. Metalearners with LITE get the best of both worlds. Note: SC + LITE is Simple CNAPS <ref type="bibr" target="#b4">[5]</ref> trained with LITE. the examples in a task's support set (i.e. the task's training set) that contribute to a prediction on a query example. In some cases, this can be as many as 1000 images <ref type="bibr" target="#b10">[11]</ref>. As a result, the amount of memory required for the computational graph grows linearly with the number of support images, and quadratically with their dimension. In contrast, transfer learning approaches can employ standard batch processing techniques to scale to larger images when under memory constraints -a feature which has contributed significantly to their recent success on few-shot benchmarks <ref type="bibr" target="#b10">[11]</ref>.</p><p>Current solutions for training meta-learners on large images include 1) parallelizing the model across multiple GPUs, which may not be available or convenient, 2) considering tasks with fewer support images, and 3) employing gradient/activation checkpointing methods <ref type="bibr" target="#b11">[12]</ref> which incur longer training times and still fall short of the task sizes required in key benchmarks. Instead, most existing work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">14]</ref> has opted for training on large tasks but small images which translates poorly into real-world applications and limits competitiveness on few-shot benchmarks.</p><p>In this work, we improve on these alternatives by proposing LITE, a Large Image and Task Episodic training scheme for meta-learning models that enables training with large images and large tasks, on a single GPU. We achieve this through the simple observation that meta-learners typically aggregate a task's support examples using a permutation invariant sum operation. This structure ensures invariance to the ordering of the support set. Consequently, the gradients for a task can be decomposed as a sum of gradient contributions from the task's support examples. This enables us to perform a forward pass on a task's entire support set, but realize significant savings in memory by back-propagating only a random subset of these images which we show is an unbiased approximation of the true gradient. <ref type="figure" target="#fig_4">Fig. 1</ref> illustrates the key trade-offs, with LITE enabling meta-learners to benefit from large images for improved classification accuracy but still remain computationally efficient at test time.</p><p>We use LITE to train key meta-learning methods and show that our best performing instantiation -Simple CNAPs <ref type="bibr" target="#b4">[5]</ref> with LITE -achieves state-of-the-art results relative to all meta-learners on two challenging few-shot benchmarks: VTAB+MD <ref type="bibr" target="#b10">[11]</ref>, an extensive suite of both meta-learning and transfer learning tasks, and ORBIT <ref type="bibr" target="#b14">[14]</ref>, an object recognition benchmark of high-variation real-world videos. Our results showcase the unique advantage of meta-learning methods -that when properly trained they can be competitive with transfer learning approaches in terms of accuracy for a fraction of the computational cost at test time -and they serve as a counterpoint to the recent narrative that transfer learning is all you need for few-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions</head><p>1. LITE, a general and memory-efficient episodic training scheme which enables meta-learning models to be trained on large images and large tasks on a single GPU. 2. A mathematical justification for approximating the true gradient with a random subset of a task's support examples which applies to common classes of meta-learning methods. 3. Instantiations of LITE on key classes of meta-learners to demonstrate its versatility. <ref type="bibr" target="#b3">4</ref>. State-of-the-art performance using Simple CNAPs with LITE compared to other leading meta-learners on two challenging few-shot benchmarks, VTAB+MD and ORBIT <ref type="bibr" target="#b1">2</ref> 2 Why Meta-Learning with Large Images and Tasks is Difficult Meta-learning preliminaries In few-shot image classification, the goal is to recognize new classes when given only a few training (or support) images of each class. Meta-learners typically achieve this through episodic training <ref type="bibr" target="#b15">[15]</ref>. Here, an episode or task ? contains a support set D ? S = {(x ? n , y ? n )} N? n=1 and a query set</p><formula xml:id="formula_0">D ? Q = {(x ? * m , y ? * m )} M? m=1 , where (x, y)</formula><p>is an image-label pair, N ? is the number of (labeled) support elements given to learn the new classes, and M ? is the number of query elements requiring predictions. Note that in a given task, elements in D ? Q are drawn from the same set of classes as the elements in D ? S . For brevity we may use the shorthand D S = {x, y} and D Q = {x * , y * }. During meta-training, a meta-learner is exposed to a large number of training tasks {? }. For each task ? , the meta-learner takes as input the support set D S and outputs the parameters of a classifier that has been adapted to the current task ? ? = ? ? (D S ). The classifier can now make task-specific probabilistic predictions f (x * , ? ? = ? ? (D S )) for any query input x * (see <ref type="figure" target="#fig_0">Fig. 2</ref>). A function L(y * , f (x * , ? ? )) computes the loss between the adapted classifier's predictions for the query input and the true label y * which is observed during meta-training. Assuming that L, f , and ? ? (D S ) are differentiable, the meta-learner can then be trained with stochastic gradient descent by back-propagating the loss and updating the parameters ?.</p><p>At meta-test time, the trained meta-learner is given a set of unseen test tasks, which typically contain classes that have not been seen during meta-training. For each task, the meta-learner is given its support set D S , and is then evaluated on its predictions for all the query inputs x * <ref type="figure" target="#fig_0">(Fig. 2</ref> Large memory requirements for meta-training The primary bottleneck to using large images (i.e. ? 224 ? 224 pixels) in meta-learning approaches is the large amount of (GPU) memory required to process a task's support set D S during meta-training. Specifically, the meta-learner ? ? (D S ) must perform a forward pass with a task's entire support set before it can back-propagate the loss for query elements (x * , y) ? D Q (and release the computation graph), thus preventing the use of conventional batch processing. The amount of memory required scales linearly with the number of support images N ? and quadratically with their dimensions. If N ? is large (e.g. the recent VTAB+MD benchmark <ref type="bibr" target="#b10">[11]</ref> requires a task's support set to be as large as 1000 images), memory on a single GPU is thus quickly exceeded for large images.</p><p>Note, the number of query elements in the task M ? is not a bottleneck when using large images as the loss decomposes over elements of the query set D Q and is therefore amenable to mini-batching. By contrast, as the classifier itself is a non-linear function of the support set, the loss does not decompose and so it is not obvious how to apply similar ideas to allow scaling of D S in a principled way.</p><p>Current ad hoc solutions to this problem are: (i) parallelize the meta-learner across multiple GPUs which may not be convenient or available and can involve significant engineering effort; (ii) train on tasks with smaller (or sub-sampled) support sets which may adversely affect performance on test tasks with more classes and/or large numbers of samples per class; (iii) train on tasks with smaller images (e.g. 84 ? 84 pixels in miniImageNet <ref type="bibr" target="#b16">[16]</ref>) which limits performance and translates poorly to many real-world applications; or (iv) trade memory usage for additional computation <ref type="bibr" target="#b11">[12]</ref> by Algorithm 1 LITE for a meta-training task ?</p><p>Require: D S : task support set; </p><formula xml:id="formula_1">D Q b ? {x * m , y * m } M b m=1 get query batch from D Q 4: H ? {(x n h , y n h )} H h=1 where {n h } H h=1 ? U(1, N ) H to back-propagate 5:</formula><p>H ? D S ? H H to not back-propagate <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_2">D S b ? H ? H 7: ? ? ? ?(D S b ) 8: L b ? 1 M b ? M b m=1 L(y * m , f (x * m , ? ? ))</formula><p>get loss of query batch <ref type="bibr">9:</ref> backward(L b ) back-propagate loss on query batch 10: end for 11: ? ? step(?, N/H) update ? using weighting factor N/H employing activation/gradient checkpointing (i.e. during training store only a subset of intermediate activations in a network needed for backpropagation and recompute the rest with additional forward computations when needed) which allows for training on larger tasks at the expense of training time, but still falls well short of the memory needed to accommodate the task sizes required for key benchmarks (e.g. VTAB+MD).</p><p>Although training meta-learners has large memory requirements, meta-testing is generally memory efficient, requiring only a small number of gradient operations, or none at all, compared to transfer learning approaches that would perform large numbers of gradient-based updates at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large Image and Task Episodic (LITE) training</head><p>In this section, we introduce our general and memory-efficient solution for training meta-learners episodically on tasks with large support sets and large images. We call our approach Large Image and Task Episodic training or LITE. In Section 3.1, we describe how LITE can be applied to key classes of meta-learners.</p><p>Approach The fundamental idea underlying LITE is to perform a forward pass using the entire support set D S , but to compute the gradient contribution on only a small random subset of the examples in the support set. By doing this, we realize large savings in memory that includes gradients, activations, and the computation graph for the elements of D S that are not back-propagated. This is an approximation of the true gradient that would result if back-propagation was performed on all of the examples in D S . In the following section, we show that this approximation is an unbiased estimate of the true gradient. The approach for a general meta-learner is detailed in Algorithm 1 and shown diagrammatically in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Mathematical justification The parameters of the meta-learner ? are found by minimizing the expected loss over all tasks.</p><formula xml:id="formula_3">argmin ? T ? =1 M? m=1 L (y ? * m , f (x ? * m , ? ? (D ? S ))) .<label>(1)</label></formula><p>In most meta-learning approaches, the support set enters into the loss through a sum over the N individual contributions from each data point it contains. This structure enables the meta-learners to be invariant to the ordering of the support set and allows all members of the support set to contribute to the adapted parameters (unlike alternative permutation invariant operators like max or min). Below we show in blue how this sum arises in popular brands of meta-learners.</p><p>In amortization methods (e.g. CNAPS <ref type="bibr" target="#b3">[4]</ref> and VERSA <ref type="bibr" target="#b17">[17]</ref>), the aggregation of support set points is built in directly via a deep set encoder e ?1 (?). This encodes the support set into an embedding vector which is mapped to the classifier parameters by a hyper-network t ?0 (?).</p><formula xml:id="formula_4">? ? (D S ) = t ?0 N n=1 e ?1 (x n , y n ) .<label>(2)</label></formula><p>In gradient-based methods (e.g. MAML <ref type="bibr" target="#b0">[1]</ref>), the classifier parameters are adapted from an initial value ? 0 using a sum of derivatives of an inner-loop loss computed for each data point in the support set. The derivatives play the role of the deep set encoder in amortization methods.</p><formula xml:id="formula_5">? ? (D S ) = ? 0 + ? 1 N n=1 d d? L inner (y n , f (x n , ?)) ?=?0<label>(3)</label></formula><p>Metric-based methods (e.g. ProtoNets <ref type="bibr" target="#b2">[3]</ref>) comprise a body formed of a feature extractor and a head formed from a distance-based classifier. The classifier's body parameters are not adapted in a task specific way ? </p><formula xml:id="formula_6">? (head) ?,c (D S ) = 1 k c kc i=1 f (x (c) i , ? 0 ) = 1 k c N n=1 1(y n = c)f (x n , ? 0 ).<label>(4)</label></formula><p>Query points can then be classified using their distance from these prototypes</p><formula xml:id="formula_7">d(f (x * n , ? 0 ), ? (head)</formula><p>?,c ). We have established that in many meta-learners, each support set affects the classifier parameters and therefore the loss through a sum of contributions from each of its elements. We now focus on the consequences of this structure on the gradients of the loss with respect to the meta-learner's parameters. To reduce clutter, we consider the contribution from just a single query point from a single task and suppress the dependence of the loss on the classifier and the query data point, writing</p><formula xml:id="formula_8">L (y * , f (x * , ? ? (D S ))) = L (e ? (D S )) where e ? (D S ) = N n=1 e ? (x n , y n ) = N n=1 e (n) ? .<label>(5)</label></formula><p>As a consequence of the summation, the derivative of the loss is given by</p><formula xml:id="formula_9">d d? L(e ? (D S )) = L (e ? (D S )) ? N n=1 de (n) ? d? where L (e ? (D S )) = dL(e)) de e=e ? (D S )<label>(6)</label></formula><p>which is a product of the sensitivity of the loss to the encoding of the data points and the sensitivity of the contribution to the encoding from each data point w.r.t. the meta-learner's parameters. This second term is the source of the memory overhead when training meta-learners, but importantly, it can be rewritten as an expectation w.r.t. a uniform distribution over the support set data-point indices,</p><formula xml:id="formula_10">d d? L(e ? (D S )) = N L (e ? (D S )) E n?U (1,N ) de (n) ? d? .<label>(7)</label></formula><p>We can now define the LITE estimator of the loss-derivative by approximating the expectation by Monte Carlo sampling H times,</p><formula xml:id="formula_11">d d? L(e ? (D S )) ? N H L (e ? (D S )) H h=1 de (n h ) ? d? = d d?L (e ? (D S )) where {n h } H h=1 ? U(1, N ). (8)</formula><p>This estimator is unbiased, converging to the true gradient as H ? ?. The estimator does not simply involve subsampling of the support set -parts of it depend on all the support set data points D S -and this is essential for it to be unbiased. The expectation and variance of this estimator are</p><formula xml:id="formula_12">E {n h }?U (1,N ) dL d? = dL d? and V {n h }?U (1,N ) dL d? = N 2 H (L ) 2 V {n h }?U (1,N ) de (n h ) ? d? .</formula><p>In Section 5.3, we empirically show that the LITE gradient estimate is unbiased and that its standard deviation is smaller than that of the naive estimator formed by sub-sampling the full support set. LITE provides memory savings by subsampling H examples from the support set, with H &lt; N , and back-propagating only them. Crucially, a forward pass is still performed with the complementary set of points, with cardinality N ? H, but these are not back-propagated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Applying LITE to key meta-learning approaches</head><p>To demonstrate its versatility, we now describe how to apply LITE to models within some of the main classes of meta-learners: CNAPS <ref type="bibr" target="#b3">[4]</ref> and Simple CNAPS <ref type="bibr" target="#b4">[5]</ref> for amortization-based methods and ProtoNets <ref type="bibr" target="#b2">[3]</ref> for metric-based methods. Note, these are a few possible instantiations. LITE can be applied to other meta-learning methods in a straightforward manner.</p><p>In the descriptions below, we consider just one query batch D Q b (i.e. one iteration of the for-loop in Algorithm 1). Note, in practice, whenever H is passed through a module, back-propagation is enabled, while for H, back-propagation is disabled. <ref type="bibr" target="#b2">3</ref> Furthermore, since typically |H| |H|, we can forward H in a single batch, however, we need to split H into smaller batches. Since H does not require gradients to be computed, this can be done without a significant impact on memory.</p><p>CNAPS <ref type="bibr" target="#b3">[4]</ref>, Simple CNAPS [5] + LITE (Appendix A.1) CNAPS variants are amortizationbased methods whose hyper-networks take a task's support set as input and generate FiLM layer <ref type="bibr" target="#b18">[18]</ref> parameters which modulate a fixed feature extractor. The classifier head can also be generated (CNAPS <ref type="bibr" target="#b3">[4]</ref>) or adopt a metric-based approach (Simple CNAPS <ref type="bibr" target="#b4">[5]</ref>), thus both variants can be adapted with just a single forward pass of the support set at test time. Meta-training them with LITE involves passing H and then H through their set-encoder e ?1 , and then averaging all the low-dimensional embeddings to get an embedding for the task. The task embedding is then input into a set of MLPs which generate FiLM layer parameters. H is passed through this configured feature extractor, followed by H, to get the task-adapted features for all support examples in D S b . For CNAPS, the task-adapted features of H and H are pooled by class and fed into a second MLP which generates the parameters of the fully-connected classification layer. For Simple CNAPS, the task-adapted features of H and H are instead used to compute class-wise distributions (i.e. class mean and covariance matrices). With back-propagation enabled, the query batch D Q b is then passed through the task-configured feature extractor and classified with the task-configured classifier (for CNAPS), or with the Mahalanobis distance <ref type="bibr" target="#b19">[19]</ref> to the class-wise distributions (for Simple CNAPS). The query batch loss is computed, and only back-propagated for H. Note that the feature extractor is pre-trained and frozen, and only the parameters of the set-encoder and generator MLPs are learned. <ref type="bibr" target="#b2">[3]</ref> is a metric-based approach which computes a set of class prototypes from the support set and then classifies query examples by their (e.g. Euclidean) distance to these prototypes. Like CNAPS variants, it requires only a single forward pass to learn a new task. Meta-training ProtoNets with LITE involves passing H through the feature extractor with back-propagation enabled, followed by H with back-propagation disabled, to obtain features for all support examples D S b . These features are averaged by class to compute the prototypes such that (with back-propagation enabled) the query batch D S b can be passed through the feature extractor and classified based on the Euclidean distance. The loss of the query batch is computed and only back-propagated for H. Note that here all the parameters of the feature extractor are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ProtoNets [3] + LITE (Appendix A.2) ProtoNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>We review the two main approaches to few-shot learning: transfer learning methods which are easy to scale to large images but are costly to adapt at test time, and meta-learning methods which are harder to scale but cheap to adapt (see <ref type="figure" target="#fig_4">Fig. 1</ref>). Note, we do not cover methods already described above.</p><p>Transfer learning approaches have demonstrated state-of-the-art performance on challenging few-shot benchmarks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b10">11]</ref>. However, they incur a high computational cost at test time as they rely on large (pre-trained) feature extractors which are fine-tuned with many optimization steps. MD-Transfer <ref type="bibr" target="#b12">[13]</ref> fine-tunes all the parameters in a ResNet18 feature extractor with a cosine classifier head for 200 optimization steps. BiT <ref type="bibr" target="#b5">[6]</ref> fine-tunes a feature extractor (pre-trained on 300M images in the JFT-300M dataset <ref type="bibr" target="#b21">[21]</ref>) with a linear head, in some cases for up to 20,000 optimization steps to achieve its state-of-the-art results on the VTAB <ref type="bibr" target="#b20">[20]</ref> benchmark. SUR <ref type="bibr" target="#b22">[22]</ref> instead trains 7 ResNet-50 feature extractors, one for each training dataset. At test time, the predictions from each are concatenated and fine-tuned to minimize a cosine-similarity loss. All of these approaches involve on the order of teras to petas of Mulitply-Accumulate operations (MACs) to learn a single new test task, and this must be repeated for each new task encountered. Furthermore, for each new task type, transfer learning approaches may need to be tuned on a validation set to obtain the optimal hyper-parameters.</p><p>In comparison, meta-learning approaches <ref type="bibr" target="#b23">[23]</ref> generally require orders of magnitude fewer MACs and steps to learn a new task at test time. Popular approaches include CNAPS <ref type="bibr" target="#b3">[4]</ref>, Simple CNAPS <ref type="bibr" target="#b4">[5]</ref>, ProtoNets <ref type="bibr" target="#b2">[3]</ref>, and MAML <ref type="bibr" target="#b0">[1]</ref> and are discussed in Section 3.1. Others include ProtoMAML <ref type="bibr" target="#b12">[13]</ref> which fuses ProtoNets and MAML by initializing the classifier weights with the prototypes and then, like MAML, takes a few optimization steps to tune the weights to the task. It therefore incurs a similar cost to adapt as MAML, except it must additionally compute the prototypes. Finally, CTX <ref type="bibr" target="#b24">[24]</ref> replaces the final average pooling layer of ProtoNets with a transformer layer that generates a series of prototypes which are spatially aware and aligned with the task. Like CNAPS, it requires just a single forward pass, however, requires more MACs to adapt since it uses a larger feature extractor (ResNet-34) and an attention module. CTX is one of the few meta-learning approaches that has been meta-trained on 224 ? 224 images, but requires 7 days of training on 8 GPUs.</p><p>Memory efficient variants of MAML have been developed. First-order MAML <ref type="bibr" target="#b0">[1]</ref> saves memory by avoiding the estimate of second-order derivatives. This is also done in Reptile <ref type="bibr" target="#b25">[25]</ref> that additionally avoids unrolling the computation graph, performing standard gradient descent at each adaptation step. Implicit MAML <ref type="bibr" target="#b26">[26]</ref>, decouples the meta-gradient from the inner loop and is able to handle many gradient steps without memory constraints. In addition <ref type="bibr" target="#b27">[27]</ref>, proposes methods to reduce the computation overhead of meta-training MAML for large tasks. Note that, in all these cases, savings arise from working around the limitations of MAML, while LITE is more general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we demonstrate that meta-learners trained with LITE achieve state-of-the-art performance among meta-learners on two challenging few-shot classification benchmarks: (i) ORBIT <ref type="bibr" target="#b14">[14]</ref> which is a real-world few-shot object recognition dataset for teachable object recognizers; and (ii) VTAB+MD <ref type="bibr" target="#b10">[11]</ref> which is composed of the Visual Task Adaptation Benchmark (VTAB) <ref type="bibr" target="#b20">[20]</ref> and Meta-Dataset (MD) <ref type="bibr" target="#b12">[13]</ref> and combines both few-shot and transfer learning tasks. We compare LITE meta-learners with state-of-the-art meta-learning and transfer learning methods in terms of classification accuracy, computational cost/time to learn a new task, and number of model parameters. ORBIT <ref type="bibr" target="#b14">[14]</ref> is a highly realistic few-shot video dataset collected by people who are blind/low-vision. It presents an object recognition benchmark task which involves personalizing (i.e. adapting) a recognizer to each individual user with just a few (support) videos they have recorded of their objects. To achieve this, the benchmark splits data collectors into disjoint train, validation, and test user sets along with their corresponding objects and videos. Models are then meta-trained on the train users, and meta-tested on how well they can learn a test user's objects given just their videos (on a user-by-user basis). The benchmark has two evaluation modes: how well the meta-trained model can recognize a test user's objects in clean videos where there is only that object present, and in clutter videos where that object appears within a realistic, multi-object scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ORBIT Teachable Object Recognition Benchmark</head><p>Experiments We meta-train ProtoNets <ref type="bibr" target="#b2">[3]</ref>, CNAPs <ref type="bibr" target="#b3">[4]</ref> and Simple CNAPs <ref type="bibr" target="#b4">[5]</ref> with LITE on tasks composed of large (224 ? 224) images. We also meta-train first-order MAML on large images as a baseline. Since first-order MAML can process task support sets in batches, we simply reduce the batch size and do not need to use LITE. We compare all of the above to meta-training on tasks of small (84 ? 84) images (i.e. the original baselines <ref type="bibr" target="#b14">[14]</ref>). We also include a transfer learning approach, FineTuner <ref type="bibr" target="#b28">[28]</ref>, which freezes a pre-trained feature extractor and fine-tunes just the linear classifier for 50 optimization steps. For each model, we consider a ResNet-18 (RN-18) and EfficientNet-B0 (EN-B0) feature extractor, both pre-trained on ImageNet <ref type="bibr" target="#b29">[29]</ref>. We follow the task sampling protocols described in <ref type="bibr" target="#b14">[14]</ref> (see Appendices B and C.1 for details). We also include analyses on meta-training with small tasks of large images in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref> images. The boost is significant for both clean and clutter videos, though absolute performance remains lower on clutter videos. This suggests that object detection or other attention-based mechanisms may be required to further exploit large images in more complex/cluttered scenes. ? All meta-learners + LITE set a new state-of-the-art on clean videos, and perform competitively with the FineTuner on cluttered videos, using an EfficientNet-B0 backbone. ? Meta-learners are competitive with transfer learning approaches in accuracy but are almost two orders of magnitude more efficient in the number of MACs and the time to learn a new task, and one order of magnitude smaller in the number of steps to adapt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VTAB+MD</head><p>VTAB+MD <ref type="bibr" target="#b10">[11]</ref> combines revised versions of the Meta-Dataset (MD) and VTAB datasets and is one of the largest, most comprehensive, and most challenging benchmarks for few-shot learning systems. The MD-v2 part of the benchmark involves testing on 8 diverse datasets while VTAB-v2 involves testing on 18 datasets grouped into three different cataegories (natural, specialized, and structured).</p><p>Results <ref type="figure">Fig. 3</ref>  This demonstrates that using large images and an approximation to the support set gradients achieves superior results compared to using small images with exact gradients. ? MD-Transfer and BiT perform strongly across VTAB+MD, however, Simple CNAPS + LITE is significantly faster to adapt to a new task requiring only a forward pass of the support set with no hyper-parameter tuning whatsoever. The transfer learners instead perform 100s of optimization steps to adapt to a new task and may require a human-in the-loop to tune hyper-parameters such as the learning rate and number of optimization steps.   Thus, the support gradient information does improve the solution, albeit by only 1-2 percentage points. Note, we report the lowest setting as |H| = 1 for Simple CNAPs but |H| = 0 for ProtoNets. This is because Simple CNAPs's adaptation network (which processes the support set) shares no parameters with the feature extractor and thus will not be learned if the support gradients are completely ignored. On the other hand, ProtoNets' adaptation network shares all its parameters with the feature extractor, thus can be metalearned even when the support gradients are neglected. Finally, in the two rightmost columns, we compare the classification accuracy for |H| = |D S | (i.e. using the full support set gradient) to |H| = 40 (i.e. using LITE). Due to memory constraints, we do this at image size 84 ? 84. Here we see that the difference in accuracy is significant. We expect that performance will smoothly interpolate as |H| is increased from 40 to the size of the largest support set (at which point the full gradient is computed). This validates how LITE can be used to trade-off GPU memory usage for classification accuracy by varying |H|. Furthermore, we conduct an empirical analysis (see <ref type="table" target="#tab_7">Table D</ref>.7) which shows that the LITE gradient estimates and the gradients when using smaller sub-sampled tasks are unbiased w.r.t. the true gradients. However, <ref type="figure" target="#fig_3">Fig. 4</ref> shows that LITE offers a significantly lower root mean square error w.r.t. the true gradients compared to using sub-sampled tasks at all but the highest values of |H|. Refer to Appendix D.4 for additional details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of varying |H|</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We propose LITE, a general and memory-efficient episodic training scheme for meta-learners that enables them to exploit large images for higher performance with limited compute resources. LITE's significant memory savings come from performing a forward pass on a task's full support set, but back-propagating only a random subset, which we show is an unbiased estimate of the full gradient. We demonstrate that meta-learners trained with LITE are state-of-the-art among meta-learners on two challenging benchmarks, ORBIT and VTAB+MD, and are competitive with transfer learning approaches at a fraction of the test time computational cost.</p><p>This offers a counterpoint to the recent narrative that transfer learning approaches are all you need for few-shot classification. Both classes of approach are worthy pursuits (and will need to exploit large images in real-world deployments) but careful consideration should be given to the data and compute available at test time to determine which class is best suited to the application under consideration. If it involves learning just a single task type (e.g. classifying natural images) with ample data and no compute or time constraints, then a fine-tuning approach would suffice and perform well. However, if a multitude of task types will be encountered at test time, each with minimal data, and new tasks need to be learned on resource-constrained devices (e.g. a mobile phone or a robot) or quickly/repeatedly (e.g. in continual or online learning settings), then a meta-learning solution will be better suited.</p><p>Finally, as the machine learning community grapples with greener solutions for training deep neural networks, LITE offers a step in the right direction by allowing meta-learners to exploit large images without an accompanying increase in compute. Future work may look toward applying the basic concept of LITE to other types of training algorithms to realize similar memory savings.</p><p>Limitations As discussed in Section 3, LITE can be applied to a wide range of meta-learners provided that they aggregate the contributions from a task's support set via a permutation-invariant operation like a sum. Because only a subset of the support set is back-propagated, however, the gradients can be more noisy and meta-training may require lower learning rates. Furthermore, LITE is a memory-efficient scheme for training meta-learners episodically and has not been tried with meta-learners trained in other ways (e.g. with standard supervised learning) or non-image datasets.</p><p>Societal impact Few-shot learning systems hold much positive potential -from personalizing object recognizers for people who are blind <ref type="bibr" target="#b14">[14]</ref> to rendering personalized avatars <ref type="bibr" target="#b30">[30]</ref> (see <ref type="bibr" target="#b23">[23]</ref> for a full review). These systems, however, also have the potential to be used in adverse ways -for example, in few-shot recognition in military/surveillance applications. Meta-trained few-shot systems may also pose risks in decision making applications as uncertainty calibration in meta-learning models has not yet been extensively explored. Careful consideration of the intended application, and further study of uncertainty quantification in meta-learning approaches will be essential in order to minimize any negative societal consequences of LITE if deployed in real-world applications. [36] TensorFlow Datasets, a collection of ready-to-use datasets. https://www.tensorflow.org/ datasets, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding Transparency Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Applying LITE to meta-learners A.1 CNAPS, Simple CNAPS + LITE</head><p>We show the LITE processing flow for both meta-training and meta-testing phases of CNAPS/Simple CNAPS in <ref type="figure" target="#fig_4">Fig. A.1</ref>. For each query set meta-training batch b, the support set images {x H , x H } are broken into batches, passed through a 2D conv net, then coalesced so that the pooling step can compute the mean of all the support set embeddings. This mean embedding is then passed into the FiLM parameter generator so that the feature extractor can be configured for the task. The support set images {x H , x H } are then passed through the adapted feature extractor in batches and the outputs are coalesced and then fed along with the support set labels {y H , y H } into the box labeled "Compute Classifier Params". For CNAPS, this box performs the class-conditional pooling operation and then uses an MLP to generate the weights and biases for the linear classifier. For Simple CNAPS, the same box computes the class-conditional means and covariances that are then used by the classifier in the Mahalanobis distance calculations. Once the classifier has been configured, the images in the query set batch {x * b } can be classified and along with the true labels {y * b }, a loss is then computed. The meta-testing flow is similar, with the exception of the loss computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ProtoNets + LITE</head><p>We show the LITE processing flow for both meta-training and meta-testing phases of ProtoNets in <ref type="figure" target="#fig_0">Fig. A.2</ref>. For each query set meta-training batch b, the support set images {x H , x H } are broken into batches, passed through the feature extractor and the resulting embeddings are then combined. The combined embeddings along with the support set labels {y H , y H } are then used to compute the class prototypes. The query batch images {x * b } are then passed through the feature extractor and the Euclidean distance from each query set image embedding to each of the class prototypes is computed. The predicted class is the one with the minimum distance. These predictions along with the true labels {y * b } are used to compute the loss. The meta-testing flow is similar, with the exception of the loss computation.</p><formula xml:id="formula_13">{ ? , ? ? } { * }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extractor Compute Prototypes</head><p>Feature Extractor Classifier</p><formula xml:id="formula_14">{ ? , ? ? }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Train</head><p>for in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Test</head><p>Combine Results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split into Batches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Simple CNAPS details</head><p>Our implementation of Simple CNAPS differs slightly from <ref type="bibr" target="#b4">[5]</ref>. Here we describe the key architecture differences which were made with the goal of reducing the number of model parameters. We verified that these modification came without a reduction in classification performance:</p><p>? We replace the ResNet18 <ref type="bibr" target="#b31">[31]</ref> feature extractor with an EfficientNet-B0 [32] since it has superior classification performance and fewer parameters (4.0M versus 11.2M for ResNet18). We pre-train the parameters of the feature extractor on ImageNet <ref type="bibr" target="#b29">[29]</ref> and then freeze them during meta-training and meta-testing. ? Like Simple CNAPS, we use Feature-wise Linear Modulation (FiLM) layers <ref type="bibr" target="#b18">[18]</ref> to adapt the feature extractor to the current task. In the EfficientNet-B0 feature extractor, we use a FiLM layer with scale parameters ? i and offset parameters ? i after every separate convolutional layer and after every depth-wise separable convolution within a inverted residual block (refer to <ref type="figure">Fig. B.3</ref>). This is a total of 18 FiLM layers (&lt;0.2% parameters in the model). ? We use a lower capacity 2-layer MLP network for generating parameters for each FiLM layer in the feature extractor (refer to <ref type="figure" target="#fig_3">Fig. B.4</ref>). This new FiLM layer generator network has less than 18% of the parameters (1.51M versus 8.45M) compared to the network used in the original Simple CNAPS. ? We do not use the Simple CNAPS Auto-regressive (AR) mode as the additional number of parameters did not yield sufficient gain.</p><p>Since the feature extractor parameters are frozen and the Mahalanobis distance based classifier has no parameters, the only learnable parameters in the model are in the set encoder and the network that generates the FiLM layer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head><p>In this section, we provide details for the LITE experiments using the ORBIT and VTAB+MD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ORBIT Teachable Object Recognition Benchmark</head><p>Meta-training and meta-testing for the ORBIT experiments were performed on a single NVIDIA Titan RTX with 24GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extractors</head><p>We use either a ResNet-18 (following <ref type="bibr" target="#b14">[14]</ref>) or an EfficientNet-B0 [32], both pre-trained on ImageNet [34]. Note, for CNAPS and Simple CNAPS, the feature extractor is frozen and only the set encoder and hyper-networks are trained, for ProtoNets and MAML all parameters are learned, and for the FineTuner the feature extractor is frozen and only the linear classifier is fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-training protocol</head><p>We train the learnable parameters in the meta-learners episodically on 50 randomly sampled tasks per train user per epoch (44 total train users). Note, each epoch samples 50 new tasks per train user. Each task is composed of clips sampled from a single user's objects (random way) and associated videos (random shot). In the case of a large task, following <ref type="bibr" target="#b14">[14]</ref>, we randomly sample 4 clips from each support and query video, where each clip is 8 frames. For a small task, we limit this to 1 clip per support video and 1 clip per query video where each clip is 8 frames, and we also cap 1) the number of objects per task to 5, and 2) the number of support/query videos per object to 2. For both large and small tasks, a clip feature is taken as the average of its frame features, where each frame in 224 ? 224 pixels. Note, the FineTuner undergoes no training -all feature extractor parameters are frozen to its pre-trained weights.</p><p>Meta-testing protocol Following <ref type="bibr" target="#b14">[14]</ref>, we evaluate the trained models on 5 tasks per test user, where each task is sampled from just that user's objects and videos. Different to training, here each task contains all the test user's objects and associated videos without caps. For each test task, we randomly sample 8 clips from each support video, and all overlapping clips from each query video. We then adapt the trained model to a task by using the task's support clips to: i) perform a forward pass for CNAPs, Simple CNAPs and ProtoNets, ii) take 15 gradient steps on all the model's parameters for MAML, or iii) take 50 gradient steps on just the linear classifier head for FineTuner.</p><p>We evaluate the adapted model predictions for every clip in each query video in the test task (in the clean video evaluation mode, the query videos show just one object on a clear surface, while in the clutter video evaluation mode, the query videos show the object in a multi-object/cluttered scene). We report all metrics averaged over a flattened list of all the query videos from all tasks from all test users (17 test users, 85 tasks in total), along with its corresponding 95% confidence interval.</p><p>Optimization hyper-parameters For CNAPs, Simple CNAPs, and ProtoNets, we use the Adam optimizer [35] and a learning rate of 10 -4 . For MAML, we use Adam and a learning rate of 10 -5 for the outer loop, and Stochastic Gradient Descent (SGD) and a learning rate of 10 -3 for the inner loop (rates reduced by 0.1 for the feature extractor in both loops). For the FineTuner, we use SGD and a learning rate of 0.1. We train Simple CNAPS with ResNet-18/EfficientNet-B0 for 10/15 epochs respectively, CNAPS for 15/15 epochs, ProtoNets for 20/20 epochs, and MAML for 20/20 epochs. These were chosen based on the number of learnable parameters in each model. <ref type="table" target="#tab_1">Table 1</ref> reports the test performance of the model with the best frame accuracy on a held-out validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LITE hyper-parameters</head><p>We train CNAPs, Simple CNAPs and ProtoNets with H = 8 clips (see Algorithm 1). We set the query batch size to M b = 8 clips across all meta-learners. Note, MAML does not use LITE since we implement only the first-order variant. We, therefore, process support (and query) sets using standard batch processing with a batch size of 32 clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VTAB+MD Benchmark</head><p>Meta-training and meta-testing for the VTAB+MD experiments were performed on a single NVIDIA V100 16GB GPU. Meta-training takes about 20 hours.</p><p>Meta-training protocol Simple CNAPS + LITE uses an EfficientNet-B0 pretrained on ImageNet for the feature extractor f and all of its parameters are frozen and not updated during meta-training. As permitted in the VTAB+MD protocol, we meta-train Simple CNAPS + LITE in an episodic manner on the training splits of following datasets: ImageNet, Omniglot, Aircraft, CU Birds, DTD, QuickDraw, and Fungi. In addition, we meta-train on the test split of MNIST as it does not overlap with any of the test datasets. We meta-train for 10,000 iterations with the Adam [35] optimizer using a fixed learning rate of 0.001, and a batch size of 40. We back-propagate after every task, but do an optimization step after every 16 tasks.</p><p>Meta-testing protocol For meta-testing on MD-v2, we generate test episodes using the Meta-Dataset episode reader with the standard evaluation settings. We test all models with 600 episodes each on all test datasets. The classification accuracy is averaged over the episodes and a 95% confidence interval is computed. For each test dataset in VTAB-v2, we use the TensorFlow Datasets API [36] and randomly sample 1000 examples from the train split for the support set and use the entire test split for the query set and report a single accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Full results on ORBIT benchmark</head><p>In the main paper, we report frame and video accuracy for test tasks, as well as the number of MACs and steps to adapt at test time and the number of model parameters. In <ref type="table" target="#tab_7">Table D</ref>.1, we include a further metric -frames to recognition or FTR -which was proposed in the original baselines <ref type="bibr" target="#b14">[14]</ref>. We also include additional results for large images (224) on small tasks without using LITE. Descriptions for the metrics are thus:</p><p>? Frame accuracy, the proportion of correct frame predictions in a query video;</p><p>? Frames-to-recognition or FTR, the number of frames before the first correct prediction, divided by the number of frames in the query video; ? Video accuracy, 1 if the most frequent frame prediction in a query video equals the true video label, otherwise 0; ? MACs to adapt, number of Multiply-Accumulate operations to learn a new task at test time (i.e. the operations required to process the whole support set); ? Steps to adapt, number of steps to learn a new task at test time (note, for gradient-based methods this involves multiple forward-backward passes through the model, while for amortization-and metric-based approaches this involves just a single forward pass); ? Number of parameters, number of learnable and frozen parameters in the model (note, this exclude the parameters that are generated by amortization-based methods)  For the no LITE, image size 224?224 pixels, and small task case, we we meta-train on 15,000 tasks using the Adam optimizer at learning rate of 0.001 on the same training datasets as Simple CNAPS + LITE. To make the number of tasks small during meta-training, we limit the maximum support set size to be 40 and the maximum classification way to be 30.</p><p>It is clear that using larger images results in a significant boost in classification accuracy, except on datasets where the images are natively small (e.g. Omniglot, Quickdraw, dSprites). Using LITE versus a smaller task size results in a significant boost in classification accuracy on VTAB-v2 where the support set size is large (1000 examples), however the results on MD-v2, where the support set sizes are smaller, are very similar in the two cases.</p><p>The trend is similar in the case of the ORBIT dataset (refer to Table D.1) where the difference between using LITE and tasks with a smaller number of examples is not great (often within the margin of error). This is likely due to the fact that in the case of ORBIT (i) the classification way is typically  <ref type="figure" target="#fig_3">Fig. 4</ref> show that the RMSE deviation of the LITE estimate is significantly smaller than that of sub-sampled small tasks at all but the highest values of |H|. Note, that these results are limited to image classification in the specific networks and network parameters that we tested. Other data types and networks are left for future work.</p><p>These experiments were carried out as follows:</p><p>? The Simple CNAPS + LITE network is initialized identically for all runs.</p><p>? Image size is 84 ? 84 pixels, so that the true gradients can be calculated.</p><p>? The same 10-way, 10-shot task (|D S | = 100) drawn from the DTD dataset is identical for all runs. ? Gradients are measured on the weights in the first (i.e. earliest) Conv2D layer in the set encoder after a single training iteration. ? Reference (exact) gradients are calculated without using LITE. ? Small task gradients are calculated by randomly sub-sampling the task (though we ensure there is at least one example per class).  <ref type="table" target="#tab_7">Table D</ref>.7, for each value of |H|, the mean of the approximate gradient runs is computed and then the mean squared error is computed between this value and the exact gradient. ? To calculate the values in <ref type="table" target="#tab_7">Table D</ref>.8, for each value of |H|, the RMSE between the approximate gradients and the exact gradients is computed and then this value is averaged over the number of runs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Additional Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Canonical meta-learner. Right: Meta-learner with LITE. The red dotted line shows the back-propagated gradients. Refer to Algorithm 1 for nomenclature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(D S ) = ? 0 . The classifier's head is adapted by averaging the activations for each class in the support set to form prototypes. Letting k c denote the number of support examples of class c, the adapted head parameters are given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Average Root Mean Square Error (RMSE) w.r.t. the true gradients versus |H| for LITE and sub-sampled tasks on 84?84 images (10-way, 10-shot, |D S | = 100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 1 :</head><label>1</label><figDesc>CNAPS<ref type="bibr" target="#b3">[4]</ref>, Simple CNAPS<ref type="bibr" target="#b4">[5]</ref> with LITE processing flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A. 2 :Figure B. 3 :Figure B. 4 :</head><label>234</label><figDesc>ProtoNets [3] with LITE processing flow. (Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a FiLM layer is used within a inverted residual block [33] of an EfficientNet [32]. Generator for i th FiLM layer. The generator takes the output of the set encoder step for each task ? and passes it through the network to generate the parameters ? i and ? i . The dimension of the vectors ? i and ? i is equal to the number of feature channels at the location where the i th FiLM layer is placed within feature extractor. The depicted generator network structure is repeated for each FiLM layer added to the feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>For each value of |H|, the number of samples used in the calculations are chosen such that 1000 examples of the support set are used. For example, for |H| = 50, 20 different one iteration training runs are done (20 runs ? 50 random examples per run = 1000). ? To calculate the values in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>D Q : task query set; N : number of support examples in D S ; H: number of elements in D S to back-propagate; M : number of query examples in D Q ; M b : batch size for D Q ; backward() ? function to back-propagate a loss; step() ? function to update parameters with a gradient step.</figDesc><table><row><cell>1: B ? ceil(M/M b )</cell><cell>number of query batches</cell></row><row><cell>2: for all b ? 1, . . . , B do</cell><cell></cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training meta-learners on large images with LITE achieves state-of-the-art accuracy with low test time adaption cost on ORBIT. Results are reported as the average (95% confidence interval) over 85 test tasks (5 tasks per test user, 17 test users). I is image size. f is model trained with/without LITE. RN-18 is ResNet-18. EN-B0 is EfficientNet-B0. T is ?10 12 MACs. F is forward pass. FB is forward-backward pass. Time is average wall clock time per task in seconds.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Clean Videos</cell><cell cols="2">Clutter Videos</cell><cell cols="3">Test-time adaption</cell><cell></cell></row><row><cell>MODEL</cell><cell>I f</cell><cell>FRAME ACC ?</cell><cell>VIDEO ACC ?</cell><cell>FRAME ACC ?</cell><cell>VIDEO ACC ?</cell><cell>MACS ?</cell><cell>STEPS ?</cell><cell>TIME ?</cell><cell>PARAMS ?</cell></row><row><cell></cell><cell>84 RN-18</cell><cell>69.5 (2.2)</cell><cell>79.7 (2.6)</cell><cell>53.7 (1.8)</cell><cell>63.1 (2.4)</cell><cell cols="2">317.70T 50FB</cell><cell cols="2">53.94s 11.17M</cell></row><row><cell>FineTuner [28]</cell><cell>224 RN-18</cell><cell>72.2 (2.2)</cell><cell>81.9 (2.6)</cell><cell>56.7 (2.0)</cell><cell>61.3 (2.5)</cell><cell cols="2">546.57T 50FB</cell><cell cols="2">96.23s 11.18M</cell></row><row><cell></cell><cell>224 EN-B0</cell><cell>78.1 (2.0)</cell><cell>85.9 (2.3)</cell><cell>63.1 (1.8)</cell><cell>66.9 (2.4)</cell><cell cols="2">121.02T 50FB</cell><cell cols="2">139.99s 4.01M</cell></row><row><cell></cell><cell>84 RN-18</cell><cell>70.6 (2.1)</cell><cell>80.9 (2.6)</cell><cell>51.7 (1.9)</cell><cell>57.9 (2.5)</cell><cell>95.31T</cell><cell>15FB</cell><cell cols="2">36.98s 11.17M</cell></row><row><cell>MAML [1]</cell><cell>224 RN-18</cell><cell>75.7 (1.9)</cell><cell>86.1 (2.3)</cell><cell>59.3 (1.9)</cell><cell>64.3 (2.4)</cell><cell cols="2">163.97T 15FB</cell><cell cols="2">65.22s 11.18M</cell></row><row><cell></cell><cell>224 EN-B0</cell><cell>79.3 (1.9)</cell><cell>87.5 (2.2)</cell><cell>64.6 (1.9)</cell><cell>69.4 (2.3)</cell><cell>36.31T</cell><cell>15FB</cell><cell cols="2">117.89s 4.01M</cell></row><row><cell></cell><cell>84 RN-18</cell><cell>65.2 (2.0)</cell><cell>81.9 (2.5)</cell><cell>50.3 (1.7)</cell><cell>59.9 (2.5)</cell><cell>3.18T</cell><cell>1F</cell><cell cols="2">0.73s 11.17M</cell></row><row><cell>ProtoNets [3]</cell><cell cols="2">224 RN-18 + LITE 76.7 (1.9)</cell><cell>86.4 (2.2)</cell><cell>61.4 (1.8)</cell><cell>68.5 (2.4)</cell><cell>5.47T</cell><cell>1F</cell><cell cols="2">1.07s 11.18M</cell></row><row><cell></cell><cell cols="2">224 EN-B0 + LITE 82.1 (1.7)</cell><cell>91.2 (1.9)</cell><cell>66.3 (1.8)</cell><cell>72.9 (2.3)</cell><cell>1.21T</cell><cell>1F</cell><cell>1.72s</cell><cell>4.01M</cell></row><row><cell></cell><cell>84 RN-18</cell><cell>66.2 (2.1)</cell><cell>79.6 (2.6)</cell><cell>51.5 (1.8)</cell><cell>59.5 (2.5)</cell><cell>3.48T</cell><cell>1F</cell><cell cols="2">0.98s 12.75M</cell></row><row><cell>CNAPs [4]</cell><cell cols="2">224 RN-18 + LITE 76.0 (1.9)</cell><cell>84.9 (2.3)</cell><cell>58.2 (1.9)</cell><cell>62.5 (2.5)</cell><cell>7.64T</cell><cell>1F</cell><cell cols="2">2.11s 12.76M</cell></row><row><cell></cell><cell cols="2">224 EN-B0 + LITE 79.6 (1.9)</cell><cell>87.6 (2.2)</cell><cell>63.3 (1.9)</cell><cell>69.2 (2.3)</cell><cell>3.38T</cell><cell>1F</cell><cell cols="2">2.85s 10.59M</cell></row><row><cell></cell><cell>84 RN-18</cell><cell>70.3 (2.1)</cell><cell>83.0 (2.5)</cell><cell>53.9 (1.8)</cell><cell>62.0 (2.5)</cell><cell>3.48T</cell><cell>1F</cell><cell cols="2">1.01s 11.97M</cell></row><row><cell>Simple CNAPs [5]</cell><cell cols="2">224 RN-18 + LITE 76.5 (2.0)</cell><cell>86.4 (2.2)</cell><cell>57.5 (1.9)</cell><cell>64.6 (2.4)</cell><cell>7.64T</cell><cell>1F</cell><cell cols="2">2.14s 11.97M</cell></row><row><cell></cell><cell cols="2">224 EN-B0 + LITE 82.7 (1.7)</cell><cell>91.8 (1.8)</cell><cell>65.6 (1.9)</cell><cell>71.9 (2.3)</cell><cell>3.39T</cell><cell>1F</cell><cell>2.92s</cell><cell>5.67M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we report frame accuracy and video accuracy, averaged over all the query videos from all tasks across all test users (17 test users, 85 tasks in total), along with their corresponding 95% confidence intervals. We also report the computational cost to learn a new task at test time in terms of the number of Multiply-Accumulate operations (MACs), the number of steps to adapt, and the wall clock time to adapt in seconds. See Appendix C.1 for results on additional metrics. The key observations from our results are:</figDesc><table /><note>? Training on larger (224 ? 224) images leads to better performance compared to smaller (84 ? 84)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Appendix D.2 provides this information along with the precise numbers and 95% confidence intervals. Appendices B and C.2 summarize all implementation and experimental details, and Appendix D.3 includes further analyses on the impact of task size on meta-training. The key observations from our results are: ? On MD-v2, Simple CNAPS + LITE has the highest average score and sets a new state-of-the-art.? On VTAB-v2, BiT scores highest overall, but among meta-learners, Simple CNAPS + LITE is the best overall and on the natural and specialized sections. It falls short of CTX on the structured section due to poor performance on dSprites which involves predicting the position and orientation Summary of results on VTAB+MD. Simple CNAPs with LITE (SC + LITE, black bar) trained on 224 ? 224 images achieves state-of-the-art accuracy on Meta-Dataset (MD-v2), and state-of-the-art accuracy among meta-learners on 3 of 4 parts of VTAB. As reference, we have included transfer learning methods (red bars), other meta-learning methods (blue bars), and Simple CNAPs without LITE trained on small images (84 ? 84, gray bar). Competitive results from<ref type="bibr" target="#b10">[11]</ref>. SeeTable D.2 for tabular results on individual datasets.</figDesc><table><row><cell></cell><cell>90</cell><cell>MD-Transfer</cell><cell>SUR</cell><cell>BiT</cell><cell>ProtoNets</cell><cell>ProtoMAML</cell><cell>CTX</cell><cell>SC(84)</cell><cell>SC+LITE</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>50 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>MD-v2</cell><cell></cell><cell>VTAB (natural)</cell><cell>VTAB (specialized)</cell><cell cols="2">VTAB (structured)</cell><cell>VTAB (all)</cell></row><row><cell cols="3">Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>compares our best meta-learner, Simple CNAPS + LITE, with 6 other competitive approaches on VTAB+MD: BiT, MD-Transfer, and SUR are transfer learning based methods while ProtoMAML, ProtoNets, and CTX are meta-learning based. Note, comparisons are not always like- for-like as methods use differing backbones, image sizes, and pre-training datasets.of small white shapes on a black background -a task quite different to image classification.? These results are significant given that CTX takes 7 days to train on 8 GPUs, whereas Simple CNAPS + LITE trains in about 20 hours on a single 16GB GPU. In addition, Simple CNAPS + LITE uses a relatively small pre-trained backbone (4.0M parameters) compared to SUR's 7 pre-trained ResNet-50s (one for each MD-v2 training set, plus ImageNet).? Simple CNAPS + LITE using 224?224 images significantly outperforms Simple CNAPS using 84?84 images, except for when the dataset images are small (e.g. Omniglot, QuickDraw, dSprites).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table /><note>the effect of varying |H|, the number of examples back-propagated per task, on the VTAB+MD benchmark. Performance is consistent across different |H|, an expected result since LITE provides an unbiased estimate of the true gradient. For both Simple CNAPs and ProtoNets + LITE, the results at the lowest values of |H| are respectable, though they fall short of what can be achieved at |H| = 40.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy results in percent on VTAB+MD using Simple CNAPs and ProtoNets with varying values of |H|. Image sizes are 224 ? 224 and 84 ? 84 pixels. For |H| &gt; 40, we used gradient/activation checkpointing methods<ref type="bibr" target="#b11">[12]</ref> in addition to LITE. For full results see Appendix D.4</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="2">Simple CNAPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ProtoNets</cell><cell></cell><cell></cell><cell cols="2">Simple CNAPs</cell></row><row><cell>Image Size</cell><cell></cell><cell></cell><cell cols="2">224 ? 224</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>224 ? 224</cell><cell></cell><cell></cell><cell cols="2">84 ? 84</cell></row><row><cell>|H|</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>100</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>40</cell><cell>|D S |</cell></row><row><cell>MD-v2</cell><cell cols="12">72.8 73.7 73.3 73.8 73.9 74.3 71.0 72.0 72.0 72.5 72.7 63.6</cell><cell>68.4</cell></row><row><cell>VTAB (all)</cell><cell cols="12">51.2 51.0 50.5 51.1 51.4 51.2 45.1 45.8 46.2 46.2 46.1 42.7</cell><cell>44.7</cell></row><row><cell>VTAB (natural)</cell><cell cols="12">64.5 65.3 64.1 65.8 65.2 66.0 58.5 60.0 60.6 60.8 60.9 47.7</cell><cell>49.5</cell></row><row><cell cols="13">VTAB (specialized) 71.8 71.4 70.5 71.3 71.9 71.6 63.5 63.9 64.2 64.5 64.2 61.0</cell><cell>63.8</cell></row><row><cell>VTAB (structured)</cell><cell cols="12">31.0 30.0 30.3 29.9 30.8 29.9 26.0 26.2 26.4 26.1 25.9 29.9</cell><cell>31.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Funding in direct support of this work: John Bronskill, Massimiliano Patacchiola and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. [32] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 6105-6114, 2019. [33] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4510-4520, 2018.</figDesc><table><row><cell>[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng</cell></row><row><cell>Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.</cell></row><row><cell>ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision</cell></row><row><cell>(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.</cell></row><row><cell>[35] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings</cell></row><row><cell>of the 3rd International Conference on Learning Representations (ICLR), 2015.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table D .</head><label>D</label><figDesc>1: Training meta-learners on large images with LITE achieves state-of-the-art accuracy with low test time adaption cost on the ORBIT Teachable Object Recognition Benchmark. Results are reported as the average (95% confidence interval) over 85 test tasks (5 tasks per test user, 17 test users). I is image size. f is model trained with/without LITE. RN-18 is ResNet-18. EN-B0 is EfficientNet-B0. T is ?10 12 MACs. F is forward pass. FB is forward-backward pass. Time is average wall clock time per task in seconds.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Clean Videos</cell><cell cols="3">Clutter Videos</cell><cell cols="3">Test-time adaption</cell><cell></cell></row><row><cell>MODEL</cell><cell>I f</cell><cell>FRAME ACC ?</cell><cell>FTR ?</cell><cell>VIDEO ACC ?</cell><cell>FRAME ACC ?</cell><cell>FTR ?</cell><cell>VIDEO ACC ?</cell><cell>MACS ?</cell><cell>STEPS ?</cell><cell>TIME ?</cell><cell>PARAMS ?</cell></row><row><cell>FineTuner [28]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D</head><label>D</label><figDesc>.2: Classification accuracy results on VTAB+MD<ref type="bibr" target="#b10">[11]</ref> using Simple CNAPS + LITE and various competing transfer learning and meta-learning approaches. All competitive results are from<ref type="bibr" target="#b10">[11]</ref>. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. Bold type indicates the highest scores (within the confidence interval). The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set. RN indicates ResNet<ref type="bibr" target="#b31">[31]</ref> and EN indicates EfficientNet[32]. The SUR results are with a linear classifier head. Simple CNAPS + LITE outperforms all approaches on MD-v2 and outperforms all meta-learning approaches on VTAB (all). Meta-training without LITE on small tasks with large imagesTable D.3 we show classification results on VTAB+MD using various ablations of Simple CNAPS including LITE on versus off, image size 84?84 versus 256?256 pixels, and small versus large sized tasks. For the no LITE, image size 84?84, and large task case, we meta-train on 35,000 tasks using the Adam optimizer at learning rate of 0.001 on the same training datasets as Simple CNAPS + LITE.</figDesc><table><row><cell></cell><cell cols="2">Transfer learning</cell><cell></cell><cell></cell><cell cols="2">Meta-Learning</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MD-Transfer</cell><cell>SUR</cell><cell>BiT</cell><cell cols="2">ProtoNets ProtoMAML</cell><cell>CTX</cell><cell>SC(84)</cell><cell>SC+LITE</cell></row><row><cell>Backbone</cell><cell>RN-18</cell><cell>RN-50 x 7</cell><cell>RN-18</cell><cell>RN-18</cell><cell>RN-18</cell><cell>RN-34</cell><cell>EN-B0</cell><cell>EN-B0</cell></row><row><cell>Params (M)</cell><cell>11.2M</cell><cell>164.6M</cell><cell>11.2M</cell><cell>11.2M</cell><cell>11.2M</cell><cell>21.3M</cell><cell>4.0M</cell><cell>4.0M</cell></row><row><cell>Image Size</cell><cell>126</cell><cell>224</cell><cell>224</cell><cell>126</cell><cell>126</cell><cell>224</cell><cell>84</cell><cell>224</cell></row><row><cell>Omniglot</cell><cell>82.0 ? 1.3</cell><cell>89.6</cell><cell cols="2">72.7 ? 4.6 85.3 ? 0.9</cell><cell>90.2 ? 0.7</cell><cell cols="3">84.6 ? 0.9 90.9 ? 0.6 86.5 ? 0.8</cell></row><row><cell>Aircraft</cell><cell>76.8 ? 1.2</cell><cell>59.7</cell><cell cols="2">73.6 ? 3.8 74.3 ? 0.8</cell><cell>82.1 ? 0.6</cell><cell cols="3">85.3 ? 0.8 77.5 ? 0.7 83.6 ? 0.7</cell></row><row><cell>Birds</cell><cell>61.2 ? 1.3</cell><cell>81.4</cell><cell cols="2">87.2 ? 1.9 68.0 ? 1.0</cell><cell>73.4 ? 0.9</cell><cell cols="3">72.9 ? 1.1 76.4 ? 0.8 88.6 ? 0.7</cell></row><row><cell>DTD</cell><cell>66.0 ? 1.1</cell><cell>83.9</cell><cell cols="2">82.6 ? 2.7 65.3 ? 0.7</cell><cell>66.3 ? 0.8</cell><cell cols="3">77.3 ? 0.7 74.3 ? 0.7 84.1 ? 0.7</cell></row><row><cell>QuickDraw</cell><cell>61.3 ? 1.1</cell><cell>81.2</cell><cell cols="2">66.3 ? 3.6 60.6 ? 1.0</cell><cell>66.4 ? 1.0</cell><cell cols="3">73.3 ? 0.8 76.5 ? 0.7 75.7 ? 0.8</cell></row><row><cell>Fungi</cell><cell>35.5 ? 1.1</cell><cell>69.2</cell><cell cols="2">53.9 ? 4.4 39.8 ? 1.1</cell><cell>46.3 ? 1.1</cell><cell cols="3">48.0 ? 1.2 51.3 ? 1.1 56.9 ? 1.2</cell></row><row><cell>Traffic Sign</cell><cell>84.7 ? 0.9</cell><cell>46.5</cell><cell cols="2">75.4 ? 4.3 49.8 ? 1.1</cell><cell>50.3 ? 1.1</cell><cell cols="3">80.1 ? 1.0 54.8 ? 1.1 65.8 ? 1.1</cell></row><row><cell>MSCOCO</cell><cell>39.6 ? 1.0</cell><cell>58.6</cell><cell cols="2">60.0 ? 2.9 39.7 ? 1.0</cell><cell>39.0 ? 1.0</cell><cell cols="3">51.4 ? 1.1 45.1 ? 1.0 50.0 ? 1.0</cell></row><row><cell>Caltech101</cell><cell>70.6</cell><cell>86.5</cell><cell>84.6</cell><cell>72.0</cell><cell>73.1</cell><cell>84.2</cell><cell>79.6</cell><cell>87.7</cell></row><row><cell>CIFAR100</cell><cell>31.3</cell><cell>34.2</cell><cell>47.1</cell><cell>27.7</cell><cell>29.7</cell><cell>37.5</cell><cell>37.1</cell><cell>48.8</cell></row><row><cell>Flowers102</cell><cell>66.1</cell><cell>71.2</cell><cell>82.7</cell><cell>57.1</cell><cell>60.2</cell><cell>81.8</cell><cell>65.5</cell><cell>83.5</cell></row><row><cell>Pets</cell><cell>49.1</cell><cell>88.7</cell><cell>83.9</cell><cell>51.0</cell><cell>56.6</cell><cell>70.9</cell><cell>69.8</cell><cell>89.3</cell></row><row><cell>Sun397</cell><cell>13.9</cell><cell>0.5</cell><cell>29.1</cell><cell>14.2</cell><cell>8.1</cell><cell>24.8</cell><cell>18.0</cell><cell>30.9</cell></row><row><cell>SVHN</cell><cell>83.2</cell><cell>24.2</cell><cell>83.4</cell><cell>41.9</cell><cell>46.8</cell><cell>67.2</cell><cell>26.7</cell><cell>51.0</cell></row><row><cell>EuroSAT</cell><cell>88.7</cell><cell>82.6</cell><cell>93.8</cell><cell>77.7</cell><cell>80.1</cell><cell>86.4</cell><cell>82.8</cell><cell>89.3</cell></row><row><cell>Resics45</cell><cell>63.7</cell><cell>67.8</cell><cell>74.1</cell><cell>50.8</cell><cell>53.5</cell><cell>67.7</cell><cell>64.5</cell><cell>76.4</cell></row><row><cell>Patch Camelyon</cell><cell>81.5</cell><cell>77.1</cell><cell>80.7</cell><cell>73.8</cell><cell>75.9</cell><cell>79.8</cell><cell>78.4</cell><cell>81.4</cell></row><row><cell>Retinopathy</cell><cell>57.6</cell><cell>37.4</cell><cell>74.5</cell><cell>28.0</cell><cell>73.2</cell><cell>35.5</cell><cell>29.4</cell><cell>40.3</cell></row><row><cell>CLEVR-count</cell><cell>40.3</cell><cell>34.1</cell><cell>55.2</cell><cell>32.0</cell><cell>32.7</cell><cell>27.9</cell><cell>30.7</cell><cell>31.4</cell></row><row><cell>CLEVR-dist</cell><cell>52.9</cell><cell>29.8</cell><cell>58.7</cell><cell>39.4</cell><cell>35.4</cell><cell>29.6</cell><cell>32.5</cell><cell>32.8</cell></row><row><cell>dSprites-loc</cell><cell>85.9</cell><cell>16.9</cell><cell>98.6</cell><cell>38.1</cell><cell>42.0</cell><cell>23.2</cell><cell>43.9</cell><cell>12.3</cell></row><row><cell>dSprites-ori</cell><cell>46.4</cell><cell>18.7</cell><cell>46.5</cell><cell>16.3</cell><cell>23.0</cell><cell>46.9</cell><cell>21.1</cell><cell>31.1</cell></row><row><cell>SmallNORB-azi</cell><cell>36.5</cell><cell>8.3</cell><cell>20.1</cell><cell>12.3</cell><cell>13.4</cell><cell>37.0</cell><cell>13.5</cell><cell>14.5</cell></row><row><cell>SmallNORB-elev</cell><cell>31.2</cell><cell>18.4</cell><cell>21.8</cell><cell>17.4</cell><cell>18.8</cell><cell>21.6</cell><cell>19.6</cell><cell>21.0</cell></row><row><cell>DMLab</cell><cell>43.0</cell><cell>33.5</cell><cell>43.7</cell><cell>31.8</cell><cell>32.5</cell><cell>31.9</cell><cell>33.9</cell><cell>39.4</cell></row><row><cell>KITTI-dist</cell><cell>58.7</cell><cell>57.5</cell><cell>78.8</cell><cell>42.1</cell><cell>54.4</cell><cell>54.3</cell><cell>58.1</cell><cell>63.9</cell></row><row><cell>MD-v2</cell><cell>63.4</cell><cell>71.3</cell><cell>71.5</cell><cell>60.3</cell><cell>64.2</cell><cell>71.6</cell><cell>68.4</cell><cell>73.9</cell></row><row><cell>VTAB (all)</cell><cell>55.6</cell><cell>43.7</cell><cell>64.3</cell><cell>40.2</cell><cell>45.0</cell><cell>50.5</cell><cell>44.7</cell><cell>51.4</cell></row><row><cell>VTAB (natural)</cell><cell>52.4</cell><cell>50.9</cell><cell>68.5</cell><cell>44.0</cell><cell>45.7</cell><cell>61.1</cell><cell>49.5</cell><cell>65.2</cell></row><row><cell>VTAB (specialized)</cell><cell>72.9</cell><cell>66.2</cell><cell>80.8</cell><cell>57.6</cell><cell>70.7</cell><cell>67.3</cell><cell>63.8</cell><cell>71.9</cell></row><row><cell>VTAB (structured)</cell><cell>49.4</cell><cell>27.2</cell><cell>53.0</cell><cell>28.7</cell><cell>31.5</cell><cell>34.1</cell><cell>31.7</cell><cell>30.8</cell></row><row><cell>D.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D .</head><label>D</label><figDesc>3: Classification accuracy results on VTAB+MD<ref type="bibr" target="#b10">[11]</ref> using various ablations of Simple CNAPS including LITE on versus off, image size 84?84 versus 256?256 pixels, and small versus large tasks. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. Bold type indicates the highest scores (within the confidence interval). The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set. A pretrained EfficientNet-B0 [32] backbone was utilized in all runs. In general, using larger images leads to better results, and using LITE on large tasks greatly improves results on VTAB-V2. than or equal to 10); and (ii) since the support frames are derived from videos, there is significant redundancy in the support sets, making the difference between having a small and large number of support examples less important. The benefits of LITE are more apparent in tasks with large way and large support set sizes, as is the case with VTAB-v2.D.4 Tabular results and additional details on the varying |H| experimentsTables D.4 to D.6 provide full tabular results for classification accuracy versus varying |H| on VTAB+MD. Note that inTable D.6, using Simple CNAPS + LITE on images of size of 84 ? 84 pixels with |H| = 40, GPU memory usage drops to roughly 8 GB, which is approximately half of that used when run without LITE (i.e. |H| = |D S |).Table D.7 shows the mean squared error between the mean of the approximate gradients and the true gradients for both LITE and sub-sampled small tasks as |H| is varied. The low mean squared error values for both training methods empirically demonstrates that both are unbiased.Table D.8 shows the average root mean squared error (RSME) of the approximate gradients and the true gradients for both LITE and sub-sampled small tasks as |H| is varied. Table D.8 and</figDesc><table><row><cell>small (less</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LITE</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Task Size</cell><cell>Large</cell><cell>Small</cell><cell>Large</cell></row><row><cell>Image Size</cell><cell>84</cell><cell>224</cell><cell>224</cell></row><row><cell>Omniglot</cell><cell cols="3">90.9 ? 0.6 91.6 ? 0.6 86.5 ? 0.8</cell></row><row><cell>Aircraft</cell><cell cols="3">77.5 ? 0.7 81.5 ? 0.7 83.6 ? 0.7</cell></row><row><cell>Birds</cell><cell cols="3">76.4 ? 0.8 88.8 ? 0.6 88.6 ? 0.7</cell></row><row><cell>DTD</cell><cell cols="3">74.3 ? 0.7 83.7 ? 0.6 84.1 ? 0.7</cell></row><row><cell>QuickDraw</cell><cell cols="3">76.5 ? 0.7 76.4 ? 0.7 75.7 ? 0.8</cell></row><row><cell>Fungi</cell><cell cols="3">51.3 ? 1.1 59.3 ? 1.1 56.9 ? 1.2</cell></row><row><cell>Traffic Sign</cell><cell cols="3">54.8 ? 1.1 60.7 ? 1.0 65.8 ? 1.1</cell></row><row><cell>MSCOCO</cell><cell cols="3">45.1 ? 1.0 52.5 ? 1.1 50.0 ? 1.0</cell></row><row><cell>Caltech101</cell><cell>79.6</cell><cell>84.9</cell><cell>87.7</cell></row><row><cell>CIFAR100</cell><cell>37.1</cell><cell>50.2</cell><cell>48.8</cell></row><row><cell>Flowers102</cell><cell>65.5</cell><cell>78.9</cell><cell>83.5</cell></row><row><cell>Pets</cell><cell>69.8</cell><cell>87.7</cell><cell>89.3</cell></row><row><cell>Sun397</cell><cell>18.0</cell><cell>32.0</cell><cell>30.9</cell></row><row><cell>SVHN</cell><cell>26.7</cell><cell>37.6</cell><cell>51.0</cell></row><row><cell>EuroSAT</cell><cell>82.8</cell><cell>86.0</cell><cell>89.3</cell></row><row><cell>Resics45</cell><cell>64.5</cell><cell>69.8</cell><cell>76.4</cell></row><row><cell>Patch Camelyon</cell><cell>78.4</cell><cell>79.1</cell><cell>81.4</cell></row><row><cell>Retinopathy</cell><cell>29.4</cell><cell>40.2</cell><cell>40.3</cell></row><row><cell>CLEVR-count</cell><cell>30.7</cell><cell>28.7</cell><cell>31.4</cell></row><row><cell>CLEVR-dist</cell><cell>32.5</cell><cell>31.4</cell><cell>32.8</cell></row><row><cell>dSprites-loc</cell><cell>43.9</cell><cell>14.7</cell><cell>12.3</cell></row><row><cell>dSprites-ori</cell><cell>21.1</cell><cell>35.8</cell><cell>31.1</cell></row><row><cell>SmallNORB-azi</cell><cell>13.5</cell><cell>12.2</cell><cell>14.5</cell></row><row><cell>SmallNORB-elev</cell><cell>19.6</cell><cell>19.0</cell><cell>21.0</cell></row><row><cell>DMLab</cell><cell>33.9</cell><cell>36.7</cell><cell>39.4</cell></row><row><cell>KITTI-dist</cell><cell>58.1</cell><cell>57.0</cell><cell>63.9</cell></row><row><cell>MD-v2</cell><cell>68.4</cell><cell>74.3</cell><cell>73.9</cell></row><row><cell>VTAB (all)</cell><cell>44.7</cell><cell>49.0</cell><cell>51.4</cell></row><row><cell>VTAB (natural)</cell><cell>49.5</cell><cell>61.9</cell><cell>65.2</cell></row><row><cell>VTAB (specialized)</cell><cell>63.8</cell><cell>68.8</cell><cell>71.9</cell></row><row><cell>VTAB (structured)</cell><cell>31.7</cell><cell>29.4</cell><cell>30.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table D .</head><label>D</label><figDesc>9 contains the results for Simple CNAPS + LITE on VTAB+MD at image size 320 ? 320 pixel with |H| = 10, demonstrating that by employing LITE, even larger images can be be used in meta-learning algorithms. Overall, these results are similar to the 224 ? 224 case as the feature extractor was pre-trained at 224 ? 224 pixels. However, on the Birds, Fungi, and Retinopathy datasets, where the original images are very large (&gt; 320 pixels), the results on this run were better than the 224 case.Table D.4: Classification accuracy results on VTAB+MD [11] using Simple CNAPS + LITE with varying values of |H|. Image size is 224 x 224 pixels. To achieve |H| &gt; 40, we used gradient/activation checkpointing methods [12] in addition to LITE. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set. ? 1.1 66.6 ? 1.1 64.7 ? 1.1 64.7 ? 1.1 65.8 ? 1.1 65.9 ? 1.1 MSCOCO 49.5 ? 1.1 50.0 ? 1.1 48.2 ? 1.2 50.2 ? 1.1 50.0 ? 1.0 51.9 ? 1.1 Table D.5: Classification accuracy results on VTAB+MD [11] using ProtoNets with varying values of |H|. Image size is 224 x 224 pixels. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set. ? 0.8 87.7 ? 0.8 88.3 ? 0.8 88.3 ? 0.7 88.3 ? 0.8 Aircraft 83.8 ? 0.7 84.6 ? 0.7 84.1 ? 0.7 85.1 ? 0.7 85.0 ? 0.7 Birds 88.8 ? 0.6 89.4 ? 0.6 89.8 ? 0.6 89.1 ? 0.7 90.2 ? 0.5 DTD 78.6 ? 0.6 79.7 ? 0.6 80.2 ? 0.7 80.6 ? 0.7 81.4 ? 0.6 QuickDraw 73.5 ? 0.8 75.0 ? 0.7 75.2 ? 0.8 75.6 ? 0.7 76.0 ? 0.7 Fungi 59.4 ? 1.2 58.9 ? 1.1 58.2 ? 1.2 58.0 ? 1.1 57.4 ? 1.1 Traffic Sign 50.0 ? 1.1 52.2 ? 1.1 52.1 ? 1.0 53.1 ? 1.1 53.5 ? 1.1 MSCOCO 47.3 ? 1.0 48.1 ? 1.0 48.1 ? 1.1 50.2 ? 1.0 49.8 ? 1.1 Table D.6: Classification accuracy results on VTAB+MD [11] using Simple CNAPS + LITE with two values of |H|. Image size is 84 x 84 pixels. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set. Dataset |H| = 40 |H| = |D S | Table D.7: Mean Squared Error (lower is better) between the mean of the gradient estimates and the true gradients for both LITE and subsampled small tasks as |H| is varied. The task used was a 10-way, 10-shot task of 84 ? 84 pixels from the DTD dataset. For each value of |H|, 1000 support set examples were used. 53E-11 9.24E-11 7.89E-11 8.48E-11 5.11E-11 5.31E-11 6.03E-11 1.02E-10 2.51E-11 Subsampled Small Task 9.23E-11 8.46E-11 7.67E-11 7.15E-11 6.45E-11 6.27E-11 5.67E-11 4.78E-11 4.30E-11Table D.8: Average root mean squared error (lower is better) with respect to the exact gradients for both LITE and subsampled small tasks as |H| is varied. The task used was a 10-way, 10-shot task of 84 ? 84 pixels from the DTD dataset. For each value of |H|, 1000 support set examples were used. 35E-03 3.20E-03 2.55E-03 2.32E-03 1.84E-03 1.63E-03 1.65E-03 1.73E-03 1.06E-03 Subsampled Small Task 5.56E-03 4.32E-03 3.43E-03 2.66E-03 2.37E-03 2.04E-03 1.77E-03 1.40E-03 1.13E-03</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Omniglot</cell><cell></cell><cell cols="4">83.7?1.0</cell><cell cols="3">90.9?0.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Aircraft</cell><cell></cell><cell cols="4">65.4?0.9</cell><cell cols="3">77.5?0.7</cell></row><row><cell></cell><cell></cell><cell>Birds</cell><cell></cell><cell></cell><cell cols="4">69.5?1.0</cell><cell cols="3">76.4?0.8</cell></row><row><cell></cell><cell></cell><cell>DTD</cell><cell></cell><cell></cell><cell cols="4">72.1?0.8</cell><cell cols="3">74.3?0.7</cell></row><row><cell></cell><cell></cell><cell cols="2">QuickDraw</cell><cell></cell><cell cols="4">70.6?0.9</cell><cell cols="3">76.5?0.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Fungi</cell><cell></cell><cell cols="4">45.5?1.2</cell><cell cols="3">51.3?1.1</cell></row><row><cell></cell><cell></cell><cell cols="2">Traffic Sign</cell><cell></cell><cell cols="4">58.2?1.0</cell><cell cols="3">54.8?1.1</cell></row><row><cell></cell><cell></cell><cell cols="2">MSCOCO</cell><cell></cell><cell cols="4">43.8?1.1</cell><cell cols="3">45.1?1.0</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">|H| = 0 Caltech101</cell><cell cols="4">|H| = 10 74.9</cell><cell cols="3">|H| = 20 79.6</cell><cell cols="2">|H| = 30</cell><cell>|H| = 40</cell></row><row><cell cols="13">Dataset Omniglot Aircraft Birds DTD QuickDraw Fungi Traffic Sign 65.9 Caltech101 |H| = 1 83.5 ? 1.0 85.2 ? 0.9 85.5 ? 0.9 85.9 ? 0.9 86.5 ? 0.8 86.2 ? 0.8 |H| = 10 |H| = 20 |H| = 30 |H| = 40 |H| = 100 82.1 ? 0.8 82.9 ? 0.8 82.5 ? 0.8 83.5 ? 0.7 83.6 ? 0.7 83.4 ? 0.8 88.0 ? 0.7 89.4 ? 0.5 88.9 ? 0.6 88.5 ? 0.7 88.6 ? 0.7 88.8 ? 0.7 84.4 ? 0.7 84.3 ? 0.7 84.2 ? 0.7 85.1 ? 0.6 84.1 ? 0.7 85.1 ? 0.7 75.3 ? 0.8 75.8 ? 0.8 75.7 ? 0.8 75.9 ? 0.8 75.7 ? 0.8 76.1 ? 0.8 53.8 ? 1.2 55.3 ? 1.2 56.8 ? 1.2 56.5 ? 1.2 56.9 ? 1.2 57.2 ? 1.2 87.9 87.8 87.1 87.5 87.7 Omniglot CIFAR100 35.4 37.1 Flowers102 69.6 65.5 Pets 55.5 69.8 Sun397 13.9 18.0 SVHN 36.7 26.7 EuroSAT 84.2 82.8 Resics45 61.5 64.5 Patch Camelyon 74.0 78.4 86.7 Caltech101 86.6 86.9 87.2 87.2 87.4 Retinopathy 24.3 29.4 88.2 CIFAR100 46.8 46.6 45.2 48.1 48.8 CIFAR100 35.5 39.6 42.0 43.4 43.1 CLEVR-count 32.2 30.7 50.1 Flowers102 82.8 83.8 82.9 83.7 83.5 Flowers102 76.6 77.9 78.3 78.5 78.2 CLEVR-dist 36.3 32.5 83.0 Pets 89.2 89.2 89.3 89.5 89.3 Pets 88.5 88.4 88.7 88.7 88.6 dSprites-loc 26.5 43.9 89.7 Sun397 28.8 31.5 30.3 32.4 30.9 Sun397 31.5 31.8 31.1 31.8 32.9 dSprites-ori 19.7 21.1 32.3 SVHN 51.4 53.0 49.6 53.5 51.0 SVHN 32.0 35.6 36.4 35.3 35.2 SmallNORB-azi 14.0 13.5 52.7 EuroSAT 88.5 88.4 88.4 88.3 89.3 88.6 Resics45 75.3 75.1 74.4 75.9 76.4 76.1 Patch Camelyon 80.4 80.2 78.7 80.2 81.4 EuroSAT 79.4 81.6 81.8 82.9 SmallNORB-elev 19.0 19.6 83.3 Resics45 65.7 67.4 68.0 69.0 DMLab 33.4 33.9 68.8 Patch Camelyon 75.9 72.8 73.7 74.2 73.3 KITTI-dist 58.1 58.1 81.9 Retinopathy 42.8 42.0 40.4 40.7 40.3 39.8 Retinopathy 32.9 33.7 33.2 31.9 31.3 MD-v2 63.6 68.4</cell></row><row><cell cols="2">CLEVR-count CLEVR-count CLEVR-dist CLEVR-dist dSprites-loc dSprites-loc dSprites-ori SmallNORB-azi SmallNORB-elev SmallNORB-elev SmallNORB-azi dSprites-ori</cell><cell cols="4">31.0 VTAB (all) 28.3 33.4 29.5 VTAB (natural) 29.0 33.4 10.7 10.5 13.3 VTAB (specialized) 27.3 29.2 14.1 30.5 29.9 14.6 14 21.3 21.3 16.3 17.1 9.4 9.4 20.4 19.6 VTAB (structured)</cell><cell></cell><cell>30.4 42.7 32.6 47.7 11.6 61.0 29.4 14.2 20.8 29.9</cell><cell cols="2">27.1 29.0 14.0 17.0 9.6 20.4</cell><cell>29.7 44.7 32.8 49.5 11.3 63.8 29.2 14.3 20.7 31.7</cell><cell>27.2 28.9 13.2 17.1 9.5 19.8</cell><cell>31.4 32.8 12.3 31.1 14.5 21</cell><cell>27.2 28.5 13.4 17.0 9.4 19.6</cell><cell>30.9 33.0 10.6 20.9 14.6 27.9</cell></row><row><cell>DMLab DMLab</cell><cell></cell><cell>40.4</cell><cell>35.2</cell><cell>40.6</cell><cell>35.5</cell><cell></cell><cell>40.3</cell><cell cols="2">35.9</cell><cell>38.5</cell><cell>35.9</cell><cell>39.4</cell><cell>35.8</cell><cell>38.8</cell></row><row><cell>KITTI-dist KITTI-dist</cell><cell></cell><cell>65.7</cell><cell>55.6</cell><cell>61.5</cell><cell>57.2</cell><cell></cell><cell>63.3</cell><cell cols="2">58.2</cell><cell>63.0</cell><cell>57.1</cell><cell>63.9</cell><cell>56.5</cell><cell>62.4</cell></row><row><cell>MD-v2 MD-v2</cell><cell></cell><cell>72.8</cell><cell>71.0</cell><cell>73.7</cell><cell>72.0</cell><cell></cell><cell>73.3</cell><cell cols="2">72.0</cell><cell>73.8</cell><cell>72.5</cell><cell>73.9</cell><cell>72.7</cell><cell>74.3</cell></row><row><cell>VTAB (all) VTAB (all)</cell><cell></cell><cell>51.2</cell><cell>45.1</cell><cell>51.0</cell><cell>45.8</cell><cell></cell><cell>50.5</cell><cell cols="2">46.2</cell><cell>51.1</cell><cell>46.2</cell><cell>51.4</cell><cell>46.1</cell><cell>51.2</cell></row><row><cell cols="2">VTAB (natural) VTAB (natural)</cell><cell>64.5</cell><cell>58.5</cell><cell>65.3</cell><cell>60.0</cell><cell></cell><cell>64.1</cell><cell cols="2">60.6 |H|</cell><cell>65.8</cell><cell>60.8</cell><cell>65.2</cell><cell>60.9</cell><cell>66.0</cell></row><row><cell cols="2">VTAB (specialized) VTAB (specialized) Training Mode 10</cell><cell>71.8</cell><cell>63.5 20</cell><cell>71.4 30</cell><cell>63.9</cell><cell>40</cell><cell>70.5</cell><cell cols="2">64.2 50</cell><cell>71.3 60</cell><cell cols="2">71.9 70 64.5</cell><cell>64.2 80</cell><cell>71.6 90</cell></row><row><cell cols="10">VTAB (structured) VTAB (structured) LITE 9.|H| 31.0 30.0 30.3 26.0 26.2 26.4</cell><cell>29.9</cell><cell>26.1</cell><cell>30.8</cell><cell>25.9</cell><cell>29.9</cell></row><row><cell>Training Mode</cell><cell>10</cell><cell></cell><cell>20</cell><cell>30</cell><cell></cell><cell>40</cell><cell></cell><cell>50</cell><cell></cell><cell>60</cell><cell cols="2">70</cell><cell>80</cell><cell>90</cell></row><row><cell>LITE</cell><cell>4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table D .</head><label>D</label><figDesc>9: Classification accuracy results on VTAB+MD<ref type="bibr" target="#b10">[11]</ref> using Simple CNAPS + LITE with |H| = 10 and image size is 320 ? 320 pixels. All figures are percentages and the ? sign indicates the 95% confidence interval over tasks. The VTAB-V2 results have no confidence interval as the testing protocol requires only a single run over the entire test set.</figDesc><table><row><cell>Dataset</cell><cell>|H| = 10, 320 x 320 pixels</cell></row><row><cell>Omniglot</cell><cell>83.2?1.0</cell></row><row><cell>Aircraft</cell><cell>82.5?0.8</cell></row><row><cell>Birds</cell><cell>91.2?0.6</cell></row><row><cell>DTD</cell><cell>85.3?0.7</cell></row><row><cell>QuickDraw</cell><cell>74.1?0.8</cell></row><row><cell>Fungi</cell><cell>58.0?1.2</cell></row><row><cell>Traffic Sign</cell><cell>62.4?1.1</cell></row><row><cell>MSCOCO</cell><cell>46.8?1.1</cell></row><row><cell>Caltech101</cell><cell>88.0</cell></row><row><cell>CIFAR100</cell><cell>45.2</cell></row><row><cell>Flowers102</cell><cell>82.6</cell></row><row><cell>Pets</cell><cell>89.5</cell></row><row><cell>Sun397</cell><cell>28.6</cell></row><row><cell>SVHN</cell><cell>51.9</cell></row><row><cell>EuroSAT</cell><cell>86.3</cell></row><row><cell>Resics45</cell><cell>72.7</cell></row><row><cell>Patch Camelyon</cell><cell>80.7</cell></row><row><cell>Retinopathy</cell><cell>46.4</cell></row><row><cell>CLEVR-count</cell><cell>30.8</cell></row><row><cell>CLEVR-dist</cell><cell>33.5</cell></row><row><cell>dSprites-loc</cell><cell>14.2</cell></row><row><cell>dSprites-ori</cell><cell>28.2</cell></row><row><cell>SmallNORB-azi</cell><cell>14.0</cell></row><row><cell>SmallNORB-elev</cell><cell>20.7</cell></row><row><cell>DMLab</cell><cell>40.3</cell></row><row><cell>KITTI-dist</cell><cell>62.3</cell></row><row><cell>MD-v2</cell><cell>72.9</cell></row><row><cell>VTAB (all)</cell><cell>50.9</cell></row><row><cell>VTAB (natural)</cell><cell>64.3</cell></row><row><cell>VTAB (specialized)</cell><cell>71.5</cell></row><row><cell>VTAB (structured)</cell><cell>30.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Source code for ORBIT experiments is available at https://github.com/microsoft/ORBIT-Dataset and for the VTAB+MD experiments at https://github.com/cambridge-mlg/LITE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In PyTorch, this can be achieved by setting torch.grad.enabled = True when passing H, and torch.grad.enabled = False when passing H.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for key suggestions and insightful questions that significantly improved the quality of the paper. Additional thanks go to Vincent Dumoulin for providing the tabular results for SUR used in <ref type="figure">Fig. 3</ref> and <ref type="table">Table D.2.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7957" to="7968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14493" to="14502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CHILD: A first step towards continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="77" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-line learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Number</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02871</idno>
		<title level="m">Online learning: A comprehensive survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Defining benchmarks for continual few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Comparing transfer and meta learning approaches on a unified few-shot classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02638</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<imprint>
			<pubPlace>Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Dataset of Datasets for Learning to Learn from Few Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta-Dataset</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Tobias</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 30th Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the generalized distance in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanta</forename><surname>Chandra Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Institute of Science of India</title>
		<meeting>the National Institute of Science of India</meeting>
		<imprint>
			<date type="published" when="1936" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selecting relevant features from a multidomain representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CrossTransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11498</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale meta-learning with continual trajectory shifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae</forename><forename type="middle">Beom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 28th Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9459" to="9468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>2016. 84 RN-18 69.5 (2.2) 7.8 (1.5) 79.7 (2.6</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tabular results on VTAB+MD benchmark In Table D.2, we show the tabular results for the VTAB+MD benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
