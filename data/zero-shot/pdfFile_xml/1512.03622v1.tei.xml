<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Feature Learning with Relative Distance Comparison for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-12-11">11 Dec 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Feature Learning with Relative Distance Comparison for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-12-11">11 Dec 2015</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition December 14, 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-identification</term>
					<term>Deep Learning</term>
					<term>Distance Comparison</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the L2 distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depends on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification, the aim of which is to match the same individual across multiple cameras, has attracted widespread attention in recent years due to its wide applications in video surveillance. It is the foundation of threat detection, behavioral understanding and other applications. Despite the considerable efforts of computer vision researchers, however, it is still an unsolved problem due to the dramatic variations caused by light, viewpoint and pose changes <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows some typical examples from two cameras. There are two crucial components, i.e. feature representations and distance metric in person re-identification systems. In these two components, feature representation is more fundamental because it is the foundation of distance learning. The features used in person re-identification range from the color histogram <ref type="bibr" target="#b1">[2]</ref>, spatial cooccurrence representation model <ref type="bibr" target="#b2">[3]</ref>, attributes model <ref type="bibr" target="#b3">[4]</ref> to combination of multiple features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. These handcrafted features can hardly be optimal in practice because of the different viewing conditions that prevail <ref type="bibr" target="#b5">[6]</ref>. Given a particular feature representation, a distance function is learned to construct a similarity measure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> with good similarity constraints . Although the effectiveness of the distance function has been demonstrated, it heavily relies on the quality of the features selected, and such selection requires deep domain knowledge and expertise <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we present a scalable distance driven feature leaning framework via the convolutional network to learn representations for the person reidentification problem. Unlike the traditional deep feature learning methods aimed at minimizing the classification error, in our framework, features are learned to maximize the relative distance. More specifically, we train the network through a set of triplets. Each triplet contains three images, i.e. a query image, one matched reference (an image of the same person as that in the query image) and one mismatched reference. The network produces features with which the L 2 distance between the matched pair and the mismatched pair should be as large as possible for each triplet. This encourages the distances between matched pairs to take smaller values than those between the mismatched pairs. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall principles. As discussed in <ref type="bibr" target="#b8">[9]</ref>, the tripletbased model is a natural model for the person re-identification problem for two main reasons. First, the intra-class and inter-class variation can vary significantly for different classes, and it may thus be inappropriate to require the distance between a matched pair or mismatched pair to fall within an absolute range. Second, person re-identification training images are relatively scarce, and the triplet-based training model can generate more constraints for distance learning, thereby helping to alleviate the over-fitting problem.</p><p>Similar to traditional neural networks, our triplet-based model also uses gradient descent algorithms in solving the parameters. Owing to limitations in memory size, it is impossible to load all the triplets for a given labeled image set into the memory to calculate the gradient. A practical means is to train the network iteratively in mini-batches, that is, in each iteration, a subset of the triplets are generated and the network parameters are then updated with the gradient derived from that batch. However, as we will see in the later sections, randomly generating the triplets at each iteration is inefficient as only a small number of distance constraints are imposed on the images within the triplets. Therefore we propose a more efficient triplet generation scheme. In each iteration, we randomly select a small number of classes (persons) from the dataset and generate the triplets using only those images, which guarantees that only a small number of images are selected in each iteration and rich distance constraints are imposed. In our proposed triplet generation scheme, one image can occur in several triplets in each iteration with a high degree of probability, and we thus design an extended network propagation algorithm to avoid recalculating the gradients of the same images. Our triplet generation scheme and the extended network propagation algorithm render the overall computational load of our model dependent mainly on the number of the training images, not on the number of triplets. Our approach also enables us to use the existing deep learning implementations to solve our model parameters with only slight modifications. In summary, we make two contributions to the literature: 1) A scalable deep feature learning method for person re-identification via maximum relative distance.</p><p>2) An effective learning algorithm for which the training cost mainly depends on the number of images rather than the number of triplets.</p><p>The remainder of this paper is organized as follows. In section two, we review the related work on person re-identification problems. In section three, we present our formulation and network architecture. In section four, we derive the algorithms for solving the model parameters using gradient descent methods for a small triplet set. In section five, we show how to train the network in batch mode with an efficient triplet generation scheme, and in section six, we present our experimental results. Section seven concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feature representation and distance metric are the two main components of person re-identification systems. The existing approaches to person re-identification tasks primarily make use of handcrafted features such as color and texture histograms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>. To increase their representative capability, features have been designed to carry spatial information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. For example, Farezena et al. utilized the symmetry property of pedestrian images to propose a method called Symmetry Driven Accumulation of Local Features (SDALF) which is robust to background clutter <ref type="bibr" target="#b4">[5]</ref>. The body configuration-based pictorial structure features have been also well studied to cope with individual variations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In addition to handcrafted feature designs, some studies addressed learning features for person re-identification tasks. For example, Gray and Tao <ref type="bibr" target="#b1">[2]</ref> proposed the use of Adaboost to learn effective representations from an ensemble of local features. Zhao et al. <ref type="bibr" target="#b12">[13]</ref> proposed the learning of mid-level features from hierarchical clusters of patches.</p><p>Another important research direction in person re-identification is distance learning. Zheng et al. <ref type="bibr" target="#b8">[9]</ref> formulated the distance learning as a Probabilistic Relative Distance Comparison model (PRDC) to maximize the likelihood that correctly matched pairs will have a smaller distance between them than incorrectly matched pairs. In addition, Mignon and Jurie proposed Pairwise Constrained Component Analysis (PCCA) to project the original data into a lower dimensional space <ref type="bibr" target="#b13">[14]</ref>, in which the distance between pairs has the desired properties. Li et al. introduced a locally adaptive thresholding rule to metric learning models (LADF), and reported that it achieved good perfor-mance on person re-identification tasks <ref type="bibr" target="#b7">[8]</ref>. RankSVM has also been proposed for learning a subspace in which the matched images have a higher rank than the mismatched images for a given query. There are also a number of general distance learning methods that have been rarely exploited in the context of person re-identification problems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>  <ref type="bibr" target="#b11">[12]</ref> adopted a cluster sampling algorithm <ref type="bibr" target="#b20">[21]</ref> for reidentifying persons with templates. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed a deep learning framework for learning filter pairs that tries to automatically encode the photometric transforms across cameras. Our work differs from these methods in its loss function and learning algorithm.</p><p>The model most similar to that proposed herein was introduced by Wang et al. <ref type="bibr" target="#b22">[23]</ref> and involved learning features for fine-grained image retrieval. They borrowed the network architecture designed by Krizhevsky et al. <ref type="bibr" target="#b23">[24]</ref>, and pretrained the network using soft-max loss function. It is unclear whether the triplet-based deep model can be effectively trained from triplets without other pre-training techniques. Here, we extend the triplet-based model to the person re-identification problem with an efficient learning algorithm and triplet generation scheme. We demonstrate its effectiveness without pre-training techniques using a relatively simple network .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Our objective is to use a deep convolutional network to learn effective feature representations that can satisfy the relative distance relationship under the L 2 distance. In other words, we apply a deep convolutional network to produce the feature for each image. And with these generated features, we require the distances between matched pairs should be smaller than those between mismatched pairs as depicted in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><formula xml:id="formula_0">F W (O i 2 ) F W (O i 1 ) F W (O i 3 ) Image Space Feature Space O i 2 O i 1 O i 3 pull push</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Network</head><p>Share parameter W Share parameter W In our model, the relative distance relationship is reflected by a set of triplet  <ref type="bibr">)</ref> or equally:</p><formula xml:id="formula_1">units {O i } where O i =&lt; O 1 i , O 2 i , O 3 i &gt;,</formula><formula xml:id="formula_2">||FW (O 1 i ) ? FW (O 2 i )|| &lt; ||FW (O 1 i ) ? FW (O 3 i )||<label>(1</label></formula><formula xml:id="formula_3">||FW (O 1 i ) ? FW (O 2 i )|| 2 &lt; ||FW (O 1 i ) ? FW (O 3 i )|| 2<label>(2)</label></formula><p>Here, we use the squared form to facilitate the partial derivative calculation.</p><p>For a given training set O={O i }, the relative distance constraints are converted to the minimization problem of the following objective, i.e. maximizing the distance between matched pairs and mismatched pairs, where n is the number of the training triplets.</p><formula xml:id="formula_4">f (W, O) = ? n i=1 max{||FW (O 1 i ) ? FW (O 2 i )|| 2 ? ||FW (O 1 i ) ? FW (O 3 i )|| 2 , C}<label>(3)</label></formula><p>The role of the max operation with the constant C is to prevent the overall value of the objective function from being dominated by easily identifiable triplets, which is similar to the technique widely used in hinge-loss functions. We set C=-1 throughout the paper.</p><p>Note the network in our model still takes one image as input both for training and testing as the conventional convolutional network does. The triplet-based loss function is introduced for parameter optimization in the training stage.</p><p>During the testing, we feed each testing image to the trained network to get its feature and use these features for performance evaluation under the normal L 2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>All existing person re-identification datasets are relatively small, and we thus designed a simplified network architecture for our model. <ref type="figure" target="#fig_5">Figure 4</ref> shows the overall network architecture, which comprises five layers. The first and third layers are convolutional layers and the second and fourth layers are pooling layers. The first convolutional layer includes 32 kernels of size 5?5?3 with a stride of 2 pixels. The second convolutional layer takes the pooled output of the first convolutional layer as input and filters it with 32 kernels of size 5?5?32 with a stride of 1 pixel. The final 400 dimensional layer is fully connected to the pooled output of the second convolutional layer with the following normalization:</p><p>Let {x i } denote the output before normalization, with the normalized output then calculated by:</p><formula xml:id="formula_5">y i = x i ?x 2 i<label>(4)</label></formula><p>Note that this normalization differs from the normalization scheme applied by Krizhevsky et al. <ref type="bibr" target="#b23">[24]</ref> over different channels. Our normalization ensures that the distance derived from each triplet cannot easily exceeds the margin C so that more triplet constraints can take effect for the whole objective function. Accordingly, the back propagation process accounts for the normalization operation using the chain rule during calculation of the partial derivative.  We use overlapped max pooling for the pooling operation. More precisely, the pooling operation can be thought of as comprising a grid of pooling units spaced s pixels apart, with each summarizing a neighborhood of size z ? z centered at the location of the pooling unit. We set s=1 and z=2 in our network.</p><p>For the neuron activation functions, we use Rectified Linear Units to transform the neuron inputs, thereby speeding up the learning process and achieving good performance, as discussed in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Algorithm</head><p>In this section, we show how to solve the network given a fixed set of training triplets. We assume the memory is sufficiently large to load all of the triplets.</p><p>The procedures for generating triplets from labeled images and training the network using the batch mode is relegated to the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Triplet-based gradient descent algorithm</head><p>We first present a direct learning algorithm derived from the definition of the objective function. For ease of exposition, we introduce d(W, O i ), which denotes the difference in distance between the matched pair and the mismatched pair <ref type="bibr" target="#b4">5)</ref> and the objective function can be rewritten as,</p><formula xml:id="formula_6">in the triplet O i . d(W, Oi) = ||FW (O 1 i ) ? FW (O 2 i )|| 2 ? ||FW (O 1 i ) ? FW (O 3 i )|| 2<label>(</label></formula><formula xml:id="formula_7">f (W, O) = ?O i max{d(W, Oi), C}<label>(6)</label></formula><p>Then the partial derivative of the objective becomes</p><formula xml:id="formula_8">?f ?Wj = ?O i h(Oi) (7) h(Oi) = ? ? ? ? ? ?d(W,O i ) ?W j , if d(W, Oi) &gt; C; 0, if d(W, Oi) &lt;= C;<label>(8)</label></formula><p>By the definition of d(W, O i ), we can obtain the gradient of d(W, O i ) as follows:</p><formula xml:id="formula_9">?d(W, Oi) ?Wj = 2(FW (O 1 i ) ? FW (O 2 i )) ? ? ?FW (O 1 i ) ? ?FW (O 2 i ) ?Wj ?2(FW (O 1 i ) ? FW (O 3 i )) ? ? ?FW (O 1 i ) ? ?FW (O 3 i ) ?Wj (9)</formula><p>We can now see that the gradient on each triplet can be easily calculated given</p><formula xml:id="formula_10">the values of F W (O 1 i ), F W (O 2 i ), F W (O 3 i ) and ?FW (O 1 i ) ?Wj , ?FW (O 2 i ) ?Wj , ?FW (O 3 i ) ?Wj</formula><p>, which can be obtained by separately running the standard forward and backward propagation for each image in the triplet. As the algorithm needs to go through all of the triplets to accumulate the gradients for each iteration, we call it the triplet-based gradient descent algorithm. Algorithm 1 shows the overall process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Triplet-based gradient descent algorithm</head><p>Input:</p><p>Training samples {O i };</p><p>Output:</p><p>The network parameters {W j } </p><formula xml:id="formula_11">Calculate F W (O 1 i ),F W (O 2 i ),F W (O 3 i ) by forward propagation; 6: Calculate ?FW (O 1 i ) ?Wj ?FW (O 2 i ) ?Wj ?FW (O 3 i ) ?Wj</formula><p>by back propagation;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Calculate ?d(W,Oi) ?Wj according to equation 9;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Increment the gradient ?f ?Wj according to equation 7, 8;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end for 10:</p><formula xml:id="formula_12">W t j = W t?1 j ? ? t ?f ?Wj ; 11: end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image-based gradient descent algorithm</head><p>In the triplet-based gradient descent algorithm, the number of network propagations depends on the number of training triplets in each iteration, with each triplet involving three rounds of forward and backward propagation during the calculation of the gradient. However, if the same image occurs in different triplets, the forward and backward propagation of that image can be reused.</p><p>Recognition of this potential shortcut inspired us to look for an optimized algorithm in which the network propagation executions depend only on the number of distinct images in the triplets. Before considering that algorithm, we first review the way in which the standard propagation algorithm is deduced in the conventional CNN learning algorithm, where the objective function often takes the following form. Here n is the number of training images.</p><formula xml:id="formula_13">f (I 1 , I 2 , ..., I n ) = 1 n ? n i=1 loss(F W (I i ))<label>(10)</label></formula><p>As the objective function is defined as the sum of the loss function on each image I i , we have:</p><formula xml:id="formula_14">?f ?W = 1 n ? n i=1 ?loss(F W (I i )) ?W<label>(11)</label></formula><p>This shows that we can calculate the gradient of the loss function for each image separately and then sum these image-based gradients to obtain the overall gradient of the objective function. In the case of a single image, the gradient can be calculated recursively by the chain rule, which is given as follows.</p><formula xml:id="formula_15">?loss(F W (I i )) ?W l = ?loss(F W (I i )) ?X l i ?X l i ?W l (12) ?loss(F W (I i )) ?X l i = ?loss(F W (I i )) ?X l+1 i ?X l+1 i ?X l i<label>(13)</label></formula><p>In the above equations, W l represents the network parameters at the l th layer and X l i represents the feature maps of the image I i at the same layer. The Equation 12 holds because X l i depends on the parameter W l and the Equation 13 holds because the feature maps at the (l + 1) th layer depend on those at the l th layer. As the partial derivative of the loss function with respect to the output feature can be simply calculated according to the loss function definition, the gradient on each image can be calculated recursively. Simple summation of the image gradients produces the overall gradient of the objective function.</p><p>We now turn to the triplet-based objective function and show that the overall gradient can also be obtained from the image-based gradients, which can be calculated separately. The difficulty lies in the impossibility of writing the objective function directly as the sum of the loss functions on the images, as in Equation 10, because it takes the following form, where n is the number of triplets:</p><formula xml:id="formula_16">f = ? n i=1 loss(F W (O 1 i ), F W (O 2 i ), F W (O 3 i ))<label>(14)</label></formula><p>However, because the loss function for each triplet is still defined on the outputs of the images in each triplet, this objective function can also be seen as follows,</p><p>where {I ? k } represents the set of all the distinct images in the triplets, i.e. {I ? k } =</p><formula xml:id="formula_17">{O 1 i } {O 2 i } {O 3</formula><p>i } and m is the number of the images in the triplets.</p><formula xml:id="formula_18">f = f (F W (I ? 1 ), F W (I ? 2 ), ..., F W (I ? m ))<label>(15)</label></formula><p>As F W (I ? k ) is some function of the feature map X l k at the l th layer, the objective function can also be seen as follows:</p><formula xml:id="formula_19">f = f (X l 1 , X l 2 , ..., X l m )<label>(16)</label></formula><p>Then the derivative rule gives us the following equations with X l k depending on W l and X l+1 k depending on X l k . </p><formula xml:id="formula_20">?f ?W l = ? m k=1 ?f ?X l k ?X l k ?W l (17) ?f ?X l k = ?f ?X l+1 k ?X l+1 k ?X l k<label>(18)</label></formula><formula xml:id="formula_21">{F W (I ? k )}. ?f ?FW (I ? k ) = ? n i=1 ? max{||FW (O 1 i ) ? FW (O 2 i )|| 2 ? ||FW (O 1 i ) ? FW (O 3 i )|| 2 , C} ?FW (I ? k )<label>(19)</label></formula><p>Algorithm 3 provides the details of calculating ?f ?FW (I ? k ) . As the algorithm shows, we need to collect the derivative from each triplet. If the triplet contains the target image I ? k and the distance d(W, O i ) is greater than the constant C (implementing the max operation in equation 3), then this triplet contributes its derivative with respect to F W (I ? k ). The form of this derivative depends on the position where the image I ? k appears in the triplet O i as listed in the algorithm. Otherwise, this triplet will be simply passed. With this image-based gradient calculation method, the whole training process is given in Algorithm 2. It is not hard to see that our optimized learning algorithm is very similar to the traditional neural network algorithm except that calculating the partial derivative with respect to the output of one image for the triplet-based loss function relies on the outputs of other images while the traditional loss function does not. This optimized learning algorithm has two obvious merits:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">We can apply a recent deep learning implementation framework such as</head><p>Caffe <ref type="bibr" target="#b24">[25]</ref> simply by modifying the loss layer.</p><p>2. The number of network propagation executions can be reduced to the number of distinct images in the triplets, a crucial advantage for large scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Batch Learning and Triplet Generation</head><p>Suppose that we have a labelled dataset with M classes (persons) and that each class has N images. The number of possible triplets would be M (M ? 1)N 2 (N ? 1). It would be impossible to load all of these triplets into the memory to train the network even for a moderate dataset. It is thus necessary to train the network using the batch mode, which allows it to be trained iteratively. In each iteration, only a small part of triplets are selected from all the possible triplets, and these triplets are used to calculate the gradient and then </p><formula xml:id="formula_22">W t = W t?1 ? ? t ?f ?W ; 12: end while</formula><p>to update the network parameters. There are several ways to select triplets from the full population of triplets. The simplest method is to select them randomly.</p><p>However, in random selection, the distinct image size is approximately three times of the selected triplet size because each triplet contains three images, and the likelihood of two triplets sharing the same image is very low. This triplet generation approach is very inefficient because only a few distance constraints are placed on the selected images in each iteration. Instead, according to our optimized gradient derivation, we know that the number of network propagations depends on the number of images contained in the triplets. So we should produce more triplets to train the model with the same number of images in each iteration. This leads to our following triplet generation scheme. In each iteration, we select a fixed number of classes (persons), and for each image in each Algorithm 3: Partial derivative with respect to the output of image I ? k Input:</p><formula xml:id="formula_23">Training triplets {O i }, image I ? k ; Output:</formula><p>The partial derivative: Labelled training images {I i };</p><formula xml:id="formula_24">?f ?FW (I ? k ) ; 1: ?f ?FW (I ? k ) = 0; 2: for all O i =&lt; O 1 i , O 2 i , O 3 i &gt; do 3: if d(W, O i ) &gt; C then 4: if I ? k =O 1 i then 5: ?f ?FW (I ? k ) + = 2(F W (O 3 i ) ? F W (O 2 i )); 6: else if I ? k =O 2 i then 7: ?f ?FW (I ? k ) ? = 2(F W (O 1 i ) ? F W (O 2 i )); 8: else if I ? k =O 3 i then 9: ?f ?FW (I ? k ) + = 2(F W (O 1 i ) ? F W (O 3 i ));</formula><p>Output:</p><p>Network Parameters W ;</p><p>1: while t &lt; T do 2:</p><p>t ? t + 1;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Randomly select a subset of classes (persons) from the training set;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Collect images from the selected classes: {I ? k } ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Construct a set of triplets from the selected classes; for all I ? k do 8:</p><p>Run forward propagation for I ? k 9:</p><p>Calculate the partial derivative of the loss function with respect to F W (I ? k ) according to Algorithm 3;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Run the standard backward propagation for I ? k ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Accumulate the gradient: ?W + = ?W (I ? k ); <ref type="bibr">12:</ref> end for 13:</p><formula xml:id="formula_25">W t = W t?1 ? ? t ?W ;</formula><p>14: end while 6. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and Evaluation Protocol</head><p>We used two well-known and challenging datasets, i.e., iLIDS <ref type="bibr" target="#b25">[26]</ref> and VIPeR <ref type="bibr" target="#b1">[2]</ref>, for our experiments. Both datasets contain a set of persons, each of whom has several images captured by different cameras. All the images were resized to 250 ? 100 pixels to train our network.</p><p>iLIDS dataset The iLIDS dataset <ref type="bibr" target="#b25">[26]</ref> was constructed from video images captured in a busy airport arrival hall. It features 119 pedestrians, with 479 images normalized to 128 ? 64 pixels. The images come from non-overlapping cameras, and were subject to quite large illumination changes and occlusions.</p><p>On average, there are four images of each individual pedestrian.</p><p>VIPeR dataset The VIPeR dataset <ref type="bibr" target="#b1">[2]</ref> contains two views of 632 pedestrians. The pair of images of each pedestrian was captured by different cameras under different viewpoint, pose and light conditions. It is the most challenging dataset in the person re-identification arena owing to the huge variance and discrepancy. Evaluation Protocol We adopted the widely used cumulative match curve (CMC) approach <ref type="bibr" target="#b26">[27]</ref>  remainder used for testing. To obtain the CMC values, we divided the testing set into a gallery set and a probe set, with no overlap between them. The gallery set comprised one image for each person. For each image in the probe set, we returned the n nearest images in the gallery set using the L2 distance with the features produced by the trained network. If the returned list contained an image featuring the same person as that in the query image, this query was considered as success of rank n. We repeated the procedure 10 times, and used the average rate as the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Performance Comparison</head><p>Training Setting The weights of the filters and the full connection parameters were initialized from two zero-mean Gaussian distributions with standard deviation 0.01 and 0.001 respectively. The bias terms were set with the constant 0. We generated the triplets as follows. In each iteration, we selected 40 persons and generate 80 triplets for each person. When there were less than 10 triplets whose distance constraints could not be satisfied, i.e. the distance between the matched pair is larger than the distance between the mismatched pair, the learning process was taken as converged.  Comparison on iLIDS dataset Using the iLIDS dataset, we compared our method with PRDC <ref type="bibr" target="#b8">[9]</ref> and other metric learning methods (i.e. Adaboost <ref type="bibr" target="#b1">[2]</ref>, Xing's <ref type="bibr" target="#b14">[15]</ref>, LMNN <ref type="bibr" target="#b15">[16]</ref>, ITML <ref type="bibr" target="#b16">[17]</ref>, PLS <ref type="bibr" target="#b27">[28]</ref>, Bhat. <ref type="bibr" target="#b1">[2]</ref>, L1-norm <ref type="bibr" target="#b2">[3]</ref> and MCC <ref type="bibr" target="#b28">[29]</ref>). The features were an ensemble of color histograms and texture histograms, as described in <ref type="bibr" target="#b8">[9]</ref>. We used 69 persons for training and the rest for testing (the same setting as used by the compared methods). <ref type="figure" target="#fig_8">Figure 5</ref> shows the curves of the various models, and <ref type="table">Table 1</ref>  Comparison on VIPeR dataset Using the VIPeR dataset, we compared our method with such state-of-the-art methods as MtMCML <ref type="bibr" target="#b29">[30]</ref>, LMLF <ref type="bibr" target="#b12">[13]</ref>, SDALF <ref type="bibr" target="#b4">[5]</ref>, eBiCov <ref type="bibr" target="#b30">[31]</ref>, eSDC <ref type="bibr" target="#b31">[32]</ref>, PRDC <ref type="bibr" target="#b8">[9]</ref>, aPRDC <ref type="bibr" target="#b32">[33]</ref>, PCCA <ref type="bibr" target="#b13">[14]</ref>, KISSME <ref type="bibr" target="#b33">[34]</ref>, LF <ref type="bibr" target="#b34">[35]</ref> and SalMatch <ref type="bibr" target="#b35">[36]</ref>. Half of the persons were used for training, and the rest for testing (the same setting as used by the compared methods). <ref type="figure" target="#fig_10">Figure 7</ref> presents the CMC curves of the various models, and <ref type="table">Ta</ref>   does contain the same person as the query), the images ranked higher than the matched one often look more closer to the query image as in columns 2-7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Studies of Learning</head><p>In this section, we explore the learning details on the VIPeR dataset, as it is more challenging and contains more images.</p><p>Data Augmentation Data augmentation is an important mechanism for alleviating the over-fitting problem. In our implementation, we crop a center region 230 ? 80 in size with a small random perturbation for each image to augment the training data. Such augmentation is critical to the performance, particularly when the training dataset is small. In our experiment, the performance declined by 33 percent without it.</p><p>Normalization Normalization is a common approach in CNN networks <ref type="bibr" target="#b23">[24]</ref>, but these networks normalize the feature map over different channels. In our model, the output feature is normalized to 1 under the L2 norm. Without this normalization, the top 1 performance drops by 25 percent. Normalization also helps to reduce the convergence time. In our experiment, the learning process roughly converged in four 4,000 iterations with normalization and in roughly 7,000 without it.</p><p>feature maps of the rst convolutional layer feature maps of the second convolutional layer <ref type="figure">Figure 9</ref>: Visualization of feature maps generated by our approach.</p><p>Triplet Generation The triplet generation scheme affects the convergence time and matching rate, as pointed out in previous sections. We compared the model's performance under two triplet generation schemes. In the first scheme, we selected 40 persons in each iteration, and constructed 80 triplets for each person using the images of those 40 persons. In the second scheme, we again selected 40 persons in each iteration, but constructed only one triplet for each person (approximating random selection). The first scheme achieved its best performance in about 4,000 iterations while the second scheme achieved its best performance (90 percent matching rate of the first scheme) in 20,000 iterations.</p><p>However, the training time in each iteration for these two schemes is almost the same as we expected.</p><p>Implementation Detail We implemented our model based on the Caffe framework <ref type="bibr" target="#b23">[24]</ref>, with only the data layer and loss layer replaced. We trained the network on a GTX 780 GPU with 2G memory. When there were fewer than 10 triplets whose distance constraints had been violated, the model was taken as converged. Our model usually converged in less than one hour thanks to its simplified network architecture and effective triplet generation scheme.</p><p>Feature map visualization In addition, we visualize the intermediate fea-</p><p>tures generated by our model to validate the effectiveness of representation learning. <ref type="figure">Figure 9</ref> shows two examples, where we present some feature maps of the first and the second convolutional layers, respectively. As we expect, the lower layer feature maps tend to have strong responses at the edges, showing some characteristics of low level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a scalable deep feature learning model for person re-identification via relative distance comparison. In this model, we construct a CNN network that is trained by a set of triplets to produce features that can satisfy the relative distance constraints organized by that triplet set. To cope with the cubically growing number of triplets, we present an effective triplet generation scheme and an extended network propagation algorithm to efficiently train the network iteratively. Our learning algorithm ensures the overall computation load mainly depends on the number of training images rather than the number of triplets. The results of extensive experiments demonstrate the superior performance of our model compared with the state-of-the-art methods. In future research, we plan to extend our model to more datasets and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Typical examples of pedestrians shot by different cameras. Each column corresponds to one person. Huge variations exist due to the light, pose and view point changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of deep feature learning via relative distance maximization. The network is trained by a set of triplets to produce effective feature representations with which the true matched images are closer than the mismatched images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>Inspired by the success of deep learning, there are also some literatures applying neural network models to address the person re-identification problems. Dong Yi et al. [19] applied a deep neural network to learn pair-wise similarity and achieved state-of-the-art performance. Hao Liu et al. [20] presented a Set-Label Model, which applies DBN (Deep Belief Network) and NCA (Neighborhood Component Analysis) on the proposed concatenated features of the query image and the gallery image to improve the person re-identification performance. Xu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of maximizing the distance for person re-identification. The L 2 distance in the feature space between the matched pair should be smaller than the mismatched pair in each triplet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>An illustration of the network architecture. The first and third layers are convolutional layers and the second and fourth layers are max pooling layers. The final layer is a full connection layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 4 :</head><label>4</label><figDesc>end for class, we randomly construct a large number of triplets, in which the matched references are randomly selected from the same class and the mismatched references are randomly selected from the remaining selected classes. This policy ensures large amounts of distance constraints are posed on a small number of images, which can be loaded into the limited memory in each iteration. And with the increasing number of iterations are executed, the sampled triplets still can cover all the possible triplet pattern, ensuring the model to converge to a local minimum.As a comparison, suppose the memory can only load 300 images (a typical case for 2G GPU memory device). Then in the random triplet generation scheme, only about 100 triplets can be applied to train the model in one iteration. However, our proposed scheme can use thousands of triplets to train the model without obvious computation load increase. Algorithm 4 gives the complete batch training process. As described in the ablation study section, our proposed triplet generation scheme shows obvious advantages both in convergence time and matching rate. Learning deep features from relative distance comparison in the batch mode Input:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison using CMC curves on i-LIDS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Search examples on iLIDS dataset. Each column represents a ranking result with the top image being the query and the rest images being the returned list. The image with the red bounding box is the matched one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Performance comparison using CMC curves on VIPeR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>shows the top 1 , top 5, top 10, top 15, top 20 and top 30 performance. Our method achieved rank-1 accuracy 52.1%, which clearly outperformed the other methods. Figure 6 shows several query examples for the iLIDS dataset. In this figure, each column represents a ranking result with the top image being the query image. The matched one in the returned list is marked by a red bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>most available benchmarking methods. Figure 8 shows some query examples for the VIPeR dataset. Each column represents a ranking result with the top image being the query image and the rest being the result list. The matched one in the returned list is highlighted by a red bounding box. This figure shows the difficulty of this dataset. Actually, in the failed examples (rank 1 image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>in which O 1 i and O 2 i are a matched pair and O 1 i and O 3 i are a mismatched pair. Let W = {W j } denote the network parameters and F W (I) denote the network output of image I, i.e. feature representation for image I. For a training triplet O i , the desired feature should satisfy the following condition under the L 2 norm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The first equation shows the gradient of the loss function with respect to the network parameters takes image-based form (summation over images) and tells us how to get this gradient given ?f ?W l whose computation only relies on image I ? k . If we get ?f ?W l for all the layers, then we get the overall gradient of the triplet-based loss function, i.e. ?W = ?f ?W . The second equation tells us how to get the partial derivative of the loss function with respect to the feature map of each image I ? k at the l th layer, i.e. So if we get the partial derivative of the loss function with respect to the output (feature map of the top layer) of each image, i.e. Luckily, the derivative with respect to the output of each image can be easily obtained as follows since it is defined analytically on</figDesc><table><row><cell cols="2">by ?? k</cell><cell>?f k ?X l</cell><cell cols="2">with ? k =</cell><cell>?X l k</cell><cell>?X l k</cell><cell>for all k. Actually, ?f ?W l can be obtained</cell></row><row><cell>?f k ?X l</cell><cell cols="6">recursively. More precisely, if we have known the partial derivative with</cell></row><row><cell cols="7">respect to the feature maps of the upper layer, say the (l + 1) th layer, then</cell></row><row><cell cols="7">the derivative with respect to this layer can be derived by simply multiplying a</cell></row><row><cell cols="2">matrix</cell><cell cols="2">?X l+1 k ?X l k</cell><cell cols="3">which can also be calculated for each image I ? k separately.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?f ?FW (I ? k ) , we can get the</cell></row><row><cell cols="7">gradient ?f ?W by applying Equation 18 and Equation 17 recursively (standard</cell></row><row><cell cols="5">backward propagation).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 2 :</head><label>2</label><figDesc>Image-based gradient descent algorithm Input:</figDesc><table><row><cell></cell><cell cols="2">Training triplets {O i };</cell></row><row><cell cols="2">Output:</cell><cell></cell></row><row><cell></cell><cell cols="2">The network parameters W ;</cell></row><row><cell cols="3">1: Collect all the distinct images {I ? k } in {O i };</cell></row><row><cell cols="3">2: while t &lt; T do</cell></row><row><cell>3:</cell><cell>t ? t + 1;</cell><cell></cell></row><row><cell>4:</cell><cell>?f ?W = 0;</cell><cell></cell></row><row><cell>5:</cell><cell cols="2">Calculate the outputs for each image I ? k by forward propagation;</cell></row><row><cell>6:</cell><cell cols="2">for all I ? k do</cell></row><row><cell>7:</cell><cell>Calculate</cell><cell>?f ?FW (I ? k ) for image I ? k according to Algorithm 3;</cell></row><row><cell>8:</cell><cell cols="2">Calculate ?f ?W (I ? k ) using back propagation;</cell></row><row><cell>9:</cell><cell cols="2">Increment the partial derivative: ?f ?W += ?f ?W (I ? k );</cell></row><row><cell>10:</cell><cell>end for</cell><cell></cell></row><row><cell>11:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for quantitive evaluation. We randomly selected about half of the persons for training (69 for iLIDS and 316 for VIPeR), with the</figDesc><table><row><cell>Method</cell><cell cols="6">Top1 Top5 Top10 Top15 Top20 Top30</cell></row><row><cell>Ours</cell><cell cols="2">52.1 68.2</cell><cell>78.0</cell><cell>83.6</cell><cell>88.8</cell><cell>95.0</cell></row><row><cell>Adaboost</cell><cell>29.6</cell><cell>55.2</cell><cell>68.1</cell><cell>77.0</cell><cell>82.4</cell><cell>92.1</cell></row><row><cell>LMNN</cell><cell>28.0</cell><cell>53.8</cell><cell>66.1</cell><cell>75.5</cell><cell>82.3</cell><cell>91.0</cell></row><row><cell>ITML</cell><cell>29.0</cell><cell>54.0</cell><cell>70.5</cell><cell>81.0</cell><cell>86.7</cell><cell>95.0</cell></row><row><cell>MCC</cell><cell>31.3</cell><cell>59.3</cell><cell>75.6</cell><cell>84.0</cell><cell>88.3</cell><cell>95.0</cell></row><row><cell>Xing's</cell><cell>27.0</cell><cell>52.3</cell><cell>63.4</cell><cell>74.8</cell><cell>80.7</cell><cell>93.0</cell></row><row><cell>PLS</cell><cell>22.1</cell><cell>46.0</cell><cell>60.0</cell><cell>70.0</cell><cell>78.7</cell><cell>87.5</cell></row><row><cell>L1-norm</cell><cell>30.7</cell><cell>55.0</cell><cell>68.0</cell><cell>75.0</cell><cell>83.0</cell><cell>90.0</cell></row><row><cell>Bhat.</cell><cell>28.4</cell><cell>51.1</cell><cell>64.3</cell><cell>72.0</cell><cell>78.8</cell><cell>89.0</cell></row><row><cell>PRDC</cell><cell>37.8</cell><cell>63.7</cell><cell>75.1</cell><cell>82.8</cell><cell>88.4</cell><cell>95.0</cell></row><row><cell cols="7">Table 1: Performance of different models on i-LIDS dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Search examples on VIPeR dataset. Each column represents a ranking result with the top image being the query and the rest images being the returned list. The image with the red bounding box is the matched one.</figDesc><table><row><cell>Ours MtMCML SDALF eBiCov eSDC PRDC aPRDC PCCA KISSME LF SalMatch Figure 8: Method LMLF</cell><cell>Top1 Top5 Top10 Top15 Top20 Top30 40.5 60.8 70.4 78.3 84.4 90.9 28.8 59.3 75.8 83.4 88.5 93.5 19.9 38.4 49.4 58.5 66.0 74.4 20.7 42.0 56.2 63.3 68.0 76.0 26.3 46.4 58.6 66.6 72.8 80.5 15.7 38.4 53.9 63.3 70.1 78.5 16.1 37.7 51.0 59.5 66.0 75.0 19.3 48.9 64.9 73.9 80.3 87.2 19.6 48.0 62.2 70.9 77.0 83.7 24.2 52.3 67.1 76.1 82.2 87.9 30.2 52.3 66.0 73.4 79.2 86.0 29.1 52.3 66.0 73.9 79.9 87.9</cell></row><row><cell></cell><cell>-</cell></row><row><cell cols="2">ble 2 presents the top 1 , top 5, top 10, top 15, top 20 and top 30 ranking</cell></row><row><cell cols="2">results. Our method achieved rank-1 accuracy 40.5% that clearly outperformed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Performance of different models on VIPeR dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corresponding authors of this work are L. Lin and H. Chao.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A stochastic graph grammar for compositional object representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1297" to="1307" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards person identification and re-identification with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive scene category discovery with generative learning and compositional sampling, Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="260" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representing and recognizing objects with massive local image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminatively trained and-or graph models for object shape detection, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="959" to="972" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3152" to="3159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning mid-level filters for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ACM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a mahalanobis distance metric for data clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3600" to="3612" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep metric learning for practical person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1407.4979</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Set-label modeling and deep metric learning on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="1283" to="1292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Layered graph matching with composite cluster sampling, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1426" to="1442" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks, in: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discriminative appearance-based models using partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Image Processing (SIBGRAPI), 2009 XXII Brazilian Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="322" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Person re-identification: what features are important?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3318" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
