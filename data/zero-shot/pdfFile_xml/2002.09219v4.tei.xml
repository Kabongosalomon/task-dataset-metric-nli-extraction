<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Latent Residual Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Delasalles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<title level="a" type="main">Stochastic Latent Residual Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent networks, which raises several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and temporal dynamics. However, no such model for stochastic video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model whose dynamics are governed in a latent space by a residual update rule. This first-order scheme is motivated by discretization schemes of differential equations. It naturally models video dynamics as it allows our simpler, more interpretable, latent model to outperform prior stateof-the-art methods on challenging datasets. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being able to predict the future of a video from a few conditioning frames in a self-supervised manner has many applications in fields such as reinforcement learning <ref type="bibr" target="#b17">(Gregor et al., 2019)</ref> or robotics <ref type="bibr" target="#b0">(Babaeizadeh et al., 2018)</ref>. More generally, it challenges the ability of a model to capture visual and dynamic representations of the world. Video prediction has received a lot of attention from the computer vision community. However, most proposed methods are deterministic, reducing their ability to capture video dynamics, which are intrinsically stochastic <ref type="bibr">(Denton &amp; Fergus, 2018)</ref>.</p><p>Stochastic video prediction is a challenging task which has been tackled by recent works. Most state-of-the-art approaches are based on image-autoregressive models <ref type="bibr">(Denton &amp; Fergus, 2018;</ref><ref type="bibr" target="#b0">Babaeizadeh et al., 2018)</ref>, built around Recurrent Neural Networks (RNNs), where each generated frame is fed back to the model to produce the next frame. However, performances of their temporal models innately depend on the capacity of their encoder and decoder, as each generated frame has to be re-encoded in a latent space. Such autoregressive processes induce a high computational cost, and strongly tie the frame synthesis and temporal models, which may hurt the performance of the generation process and limit its applicability <ref type="bibr" target="#b17">(Gregor et al., 2019;</ref><ref type="bibr">Rubanova et al., 2019</ref>).</p><p>An alternative approach consists in separating the dynamic of the state representations from the generated frames, which are independently decoded from the latent space. In addition to removing the aforementioned link between frame synthesis and temporal dynamics, this is computationally appealing when coupled with a low-dimensional latent space. Moreover, such models can be used to shape a complete representation of the state of a system, e.g. for reinforcement learning applications <ref type="bibr" target="#b17">(Gregor et al., 2019)</ref>, and are more interpretable than autoregressive models <ref type="bibr">(Rubanova et al., 2019)</ref>. Yet, these State-Space Models (SSMs) are more difficult to train as they require non-trivial inference schemes <ref type="bibr">(Krishnan et al., 2017)</ref> and a careful design of the dynamic model <ref type="bibr" target="#b29">(Karl et al., 2017)</ref>. This leads most successful SSMs to only be evaluated on small or artificial toy tasks.</p><p>In this work, we introduce a novel stochastic dynamic model for the task of video prediction which successfully leverages structural and computational advantages of SSMs that operate on low-dimensional latent spaces. Its dynamic component determines the temporal evolution of the system through residual updates of the latent state, conditioned on learned stochastic variables. This formulation allows us to implement an efficient training strategy and process in an interpretable manner complex high-dimensional data such as videos. This residual principle can be linked to recent advances relating residual networks and Ordinary Differential Equations (ODEs) <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>. This interpretation opens new perspectives such as generating videos at different frame rates, as demonstrated in our experiments. The proposed approach outperforms current state-of-theart models on the task of stochastic video prediction, as demonstrated by comparisons with competitive baselines on representative benchmarks. arXiv:2002.09219v4 [cs.CV] 7 Aug 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video synthesis covers a range of different tasks, such as video-to-video translation , superresolution <ref type="bibr" target="#b3">(Caballero et al., 2017)</ref>, interpolation between distant frames <ref type="bibr" target="#b26">(Jiang et al., 2018)</ref>, generation <ref type="bibr">(Tulyakov et al., 2018)</ref>, and video prediction, which is the focus of this paper.</p><p>Deterministic models. Inspired by prior sequence generation models using RNNs <ref type="bibr" target="#b16">(Graves, 2013)</ref>, a number of video prediction methods <ref type="bibr">(Srivastava et al., 2015;</ref><ref type="bibr">Villegas et al., 2017;</ref><ref type="bibr">van Steenkiste et al., 2018;</ref><ref type="bibr" target="#b35">Wichers et al., 2018;</ref><ref type="bibr" target="#b27">Jin et al., 2020)</ref> rely on LSTMs (Long Short-Term Memory networks, <ref type="bibr" target="#b22">Hochreiter &amp; Schmidhuber, 1997</ref><ref type="bibr">), or, like Ranzato et al. (2014</ref>, <ref type="bibr" target="#b25">Jia et al. (2016)</ref> and <ref type="bibr" target="#b37">Xu et al. (2018a)</ref>, on derived networks such as ConvLSTMs <ref type="bibr">(Shi et al., 2015)</ref>. Indeed, computer vision approaches are usually tailored to high-dimensional video sequences and propose domainspecific techniques such as pixel-level transformations and optical flow <ref type="bibr">(Shi et al., 2015;</ref><ref type="bibr" target="#b31">Walker et al., 2015;</ref><ref type="bibr">Finn et al., 2016;</ref><ref type="bibr" target="#b25">Jia et al., 2016;</ref><ref type="bibr" target="#b32">Walker et al., 2016;</ref><ref type="bibr">Vondrick &amp; Torralba, 2017;</ref><ref type="bibr">Liang et al., 2017;</ref><ref type="bibr">Liu et al., 2017;</ref><ref type="bibr">Lotter et al., 2017;</ref><ref type="bibr">Lu et al., 2017a;</ref><ref type="bibr">Fan et al., 2019;</ref><ref type="bibr" target="#b13">Gao et al., 2019</ref>) that help to produce high-quality predictions. Such predictions are, however, deterministic, thus hurting their performance as they fail to generate sharp long-term video frames <ref type="bibr" target="#b0">(Babaeizadeh et al., 2018;</ref><ref type="bibr">Denton &amp; Fergus, 2018)</ref>. Following <ref type="bibr">Mathieu et al. (2016)</ref>, some works proposed to use adversarial losses <ref type="bibr" target="#b15">(Goodfellow et al., 2014)</ref> on the model predictions to sharpen the generated frames <ref type="bibr">(Vondrick &amp; Torralba, 2017;</ref><ref type="bibr">Liang et al., 2017;</ref><ref type="bibr">Lu et al., 2017a;</ref><ref type="bibr" target="#b38">Xu et al., 2018b;</ref><ref type="bibr" target="#b36">Wu et al., 2020)</ref>. Nonetheless, adversarial losses are notoriously hard to train <ref type="bibr" target="#b14">(Goodfellow, 2016)</ref>, and lead to mode collapse, thereby preventing diversity of generations.</p><p>Stochastic and image-autoregressive models. Some approaches rely on exact likelihood maximization, using pixellevel autoregressive generation (van den <ref type="bibr">Oord et al., 2016;</ref><ref type="bibr" target="#b28">Kalchbrenner et al., 2017;</ref><ref type="bibr" target="#b34">Weissenborn et al., 2020)</ref> or normalizing flows through invertible transformations between the observation space and a latent space <ref type="bibr">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr">Kumar et al., 2020)</ref>. However, they require careful design of complex temporal generation schemes manipulating high-dimensional data, thus inducing a prohibitive temporal generation cost. More efficient continuous models rely on Variational Auto-Encoders (VAEs, <ref type="bibr">Kingma &amp; Welling, 2014;</ref><ref type="bibr">Rezende et al., 2014)</ref> for the inference of low-dimensional latent state variables. Except <ref type="bibr" target="#b39">Xue et al. (2016)</ref> and <ref type="bibr">Liu et al. (2019)</ref> who learn a one-frame-ahead VAE, they model sequence stochasticity by incorporating a random latent variable per frame into a deterministic RNN-based image-autoregressive model. <ref type="bibr" target="#b0">Babaeizadeh et al. (2018)</ref> integrate stochastic variables into the ConvLSTM architecture of <ref type="bibr">Finn et al. (2016)</ref>. Concurrently with <ref type="bibr" target="#b19">He et al. (2018)</ref>, <ref type="bibr">Denton &amp; Fergus (2018)</ref> use a prior LSTM conditioned on previously generated frames in order to sample random variables that are fed to a predictor LSTM; performance of such methods were improved in follow-up works by increasing networks capacities <ref type="bibr" target="#b4">(Castrejon et al., 2019;</ref><ref type="bibr" target="#b18">Villegas et al., 2019)</ref>. Finally, <ref type="bibr" target="#b35">Lee et al. (2018)</ref> combine the ConvLSTM architecture and this learned prior, adding an adversarial loss on the predicted videos to sharpen them at the cost of a diversity drop. Yet, all these methods are image-autoregressive, as they feed their predictions back into the latent space, thereby tying the frame synthesis and temporal models and increasing their computational cost. Concurrently to our work, <ref type="bibr">Minderer et al. (2019)</ref> propose to use the autoregressive VRNN model <ref type="bibr" target="#b7">(Chung et al., 2015)</ref> on learned image key-points instead of raw frames. It remains unclear to which extent this change could mitigate the aforementioned problems. We instead tackle these issues by focusing on video dynamics, and propose a model that is state-space and acts on a small latent space. This approach yields better experimental results despite weaker video-specific priors.</p><p>State-space models. Many latent state-space models have been proposed for sequence modelization <ref type="bibr" target="#b1">(Bayer &amp; Osendorfer, 2014;</ref><ref type="bibr" target="#b11">Fraccaro et al., 2016;</ref><ref type="bibr">Krishnan et al., 2017;</ref><ref type="bibr" target="#b29">Karl et al., 2017;</ref><ref type="bibr" target="#b18">Hafner et al., 2019)</ref>, usually trained by deep variational inference. These methods, which use locally linear or RNN-based dynamics, are designed for low-dimensional data, as learning such models on complex data is challenging, or focus on control or planning tasks. In contrast, our fully latent method is the first one to be successfully applied to complex high-dimensional data such as videos, thanks to a temporal model based on residual updates of its latent state. It falls within the scope of a recent trend linking differential equations with neural networks <ref type="bibr">(Lu et al., 2017b;</ref><ref type="bibr">Long et al., 2018)</ref>, leading to the integration of ODEs, that are seen as continuous residual networks <ref type="bibr" target="#b20">(He et al., 2016)</ref>, in neural network architectures <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>. However, the latter work as well as follow-ups and related works <ref type="bibr">(Rubanova et al., 2019;</ref><ref type="bibr" target="#b41">Y?ld?z et al., 2019;</ref><ref type="bibr">Le Guen &amp; Thome, 2020)</ref> are either limited to low-dimensional data, prone to overfitting or unable to handle stochasticity within a sequence. Another line of works considers stochastic differential equations with neural networks <ref type="bibr">(Ryder et al., 2018;</ref><ref type="bibr" target="#b8">De Brouwer et al., 2019)</ref>, but are limited to continuous Brownian noise, whereas video prediction additionally requires to model punctual stochastic events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>We consider the task of stochastic video prediction, consisting in approaching, given a number of conditioning video</p><formula xml:id="formula_0">y 1 y 2 y 3 x 1 x 2 x 3 z 2 z 3 (a) Generative model p. y 1 y 2 y 3 x 1 x 2 x 3 z 2 z 3 (b) Inference model q.</formula><p>x 1 x 2</p><formula xml:id="formula_1">x 1 x 2 z 2 LSTM y 1 y 2 x 1 x 2 ? z ? , ? z ? w g ? g ? h ? h ? f ? ? ? , ? ? z 3 f ? x 3 y 3 g ? LSTM ? y ? , ? y ? q p q x 3 x 3 h ? LSTM ? z ? , ? z ? q ? ? , ? ? p (c)</formula><p>Model and inference architecture on a test sequence. The transparent block on the left depicts the prior, and those on the right correspond to the full inference performed at training time. frames, the distribution of possible future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Latent Residual Dynamic Model</head><p>Let x 1:T be a sequence of T video frames. We model their evolution by introducing latent variables y that are driven by a dynamic temporal model. Each frame x t is then generated from the corresponding latent state y t only, making the dynamics independent from the previously generated frames.</p><p>We propose to model the transition function of the latent dynamic of y with a stochastic residual network. State y t+1 is chosen to deterministically depend on the previous state y t , conditionally to an auxiliary random variable z t+1 . These auxiliary variables encapsulate the randomness of the video dynamics. They have a learned factorized Gaussian prior that depends on the previous state only. The model is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(a), and defined as follows:</p><formula xml:id="formula_2">? ? ? ? ? ? ? ? ? ? ? y 1 ? N (0, I), z t+1 ? N ? ? (y t ), ? ? (y t )I , y t+1 = y t + f ? (y t , z t+1 ), x t ? G g ? (y t ) ,<label>(1)</label></formula><p>where ? ? , ? ? , f ? and g ? are neural networks, and G g ? (y t ) is a probability distribution parameterized by g ? (y t ). In our experiments, G is a normal distribution with mean g ? (y t ) and constant diagonal variance. Note that y 1 is assumed to have a standard Gaussian prior, and, in our VAE setting, will be inferred from conditioning frames for the prediction task, as shown in Section 3.3.</p><p>The residual update rule takes inspiration in the Euler dis-cretization scheme of differential equations. The state of the system y t is updated by its first-order movement, i.e., the residual f ? (y t , z t+1 ). Compared to a regular RNN, this simple principle makes our temporal model lighter and more interpretable. Equation <ref type="formula" target="#formula_2">(1)</ref>, however, differs from a discretized ODE because of the introduction of the stochastic discrete-time variables z. Nonetheless, we propose to allow the Euler step size ?t to be smaller than 1, as a way to make the temporal model closer to a continuous dynamics. The updated dynamics becomes, with 1 ?t ? N to synchronize the step size with the video frame rate:</p><formula xml:id="formula_3">y t+?t = y t + ?t ? f ? y t , z t +1 .</formula><p>(2)</p><p>For this formulation, the auxiliary variable z t is kept constant between two integer time steps. Note that a different ?t can be used during training or testing. This allows our model to generate videos at an arbitrary frame rate since each intermediate latent state can be decoded in the observation space. This ability enables us to observe the quality of the learned dynamic as well as challenge its ODE inspiration by testing its generalization to the continuous limit in Section 4. In the following, we consider ?t as a hyperparameter. For the sake of clarity, we consider that ?t = 1 in the remaining of this section; generalizing to a smaller ?t is straightforward as <ref type="figure" target="#fig_0">Figure 1</ref>(a) remains unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Content Variable</head><p>Some components of video sequences can be static, such as the background or shapes of moving objects. They may not impact the dynamics; we therefore model them separately, in the same spirit as <ref type="bibr" target="#b10">Denton &amp; Birodkar (2017)</ref> and <ref type="bibr" target="#b40">Yingzhen &amp; Mandt (2018)</ref>. We compute a content variable w that remains constant throughout the whole generation process and is fed together with y t into the frame generator. It enables the dynamical part of the model to focus only on movement, hence being lighter and more stable. Moreover, it allows us to leverage architectural advances in neural networks, such as skip connections <ref type="bibr">(Ronneberger et al., 2015)</ref>, to produce more realistic frames.</p><p>This content variable is a deterministic function c ? of a fixed number k &lt; T of frames x</p><formula xml:id="formula_4">(k) c = x i1 , . . . , x i k : ? ? ? w = c ? x (k) c = c ? x i1 , . . . , x i k x t ? G g ? (y t , w) .<label>(3)</label></formula><p>During testing, x</p><p>c are the last k conditioning frames (usually between 2 and 5).</p><p>This content variable is not endowed with any probabilistic prior, contrary to the dynamic variables y and z. Thus, the information it contains is not constrained in the loss function (see Section 3.3), but only architecturally. To prevent temporal information from leaking in w, we propose to uniformly sample these k frames within x 1:T during training. We also design c ? as a permutation-invariant function <ref type="bibr" target="#b42">(Zaheer et al., 2017)</ref>, consisting in an MLP fed with the sum of individual frame representations, following <ref type="bibr">Santoro et al. (2017)</ref>.</p><p>This absence of prior and its architectural constraint allows w to contain as much non-temporal information as possible, while preventing it from containing dynamic information. On the other hand, due to their strong standard Gaussian priors, y and z are encouraged to discard unnecessary information. Therefore, y and z should only contain temporal information that could not be captured by w.</p><p>Note that this content variable can be removed from our model, yielding a more classical deep state-space model. An experiment in this setting is presented in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variational Inference and Architecture</head><p>Following the generative process depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(a), the conditional joint probability of the full model, given a content variable w, can be written as:</p><formula xml:id="formula_6">p(x 1:T , z 2:T , y 1:T | w) = p(y 1 ) T t=2 p(z t , y t | y t?1 ) T t=1 p(x t | y t , w),<label>(4)</label></formula><p>with</p><formula xml:id="formula_7">p(z t , y t | y t?1 ) = p(z t | y t?1 )p(y t | y t?1 , z t ). (5)</formula><p>According to the expression of y t+1 in Equation <ref type="formula" target="#formula_2">(1)</ref>,</p><formula xml:id="formula_8">p(y t | y t?1 , z t ) = ? y t ? y t?1 ? f ? (y t?1 , z t ) ,</formula><p>where ? is the Dirac delta function centered on 0. Hence, in order to optimize the likelihood of the observed videos p(x 1:T | w), we need to infer latent variables y 1 and z 2:T . This is done by deep variational inference using the inference model parameterized by ? and shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), which comes down to considering a variational distribution q Z,Y defined and factorized as follows:</p><formula xml:id="formula_9">q Z,Y q(z 2:T , y 1:T | x 1:T , w) = q(y 1 | x 1:k ) T t=2 q(z t | x 1:t ) q(y t | y t?1 , z t ) =p(yt | yt?1,zt) ,<label>(6)</label></formula><p>with q(y t | y t?1 , z t ) = p(y t | y t?1 , z t ) being the aforementioned Dirac delta function. This yields the following evidence lower bound (ELBO), whose full derivation is given in Appendix A:</p><formula xml:id="formula_10">log p(x 1:T | w) ? L(x 1:T ; w, ?, ?) ? D KL q(y 1 | x 1:k ) p(y 1 ) + E ( z 2:T , y 1:T )?q Z,Y ? ? T t=1 log p(x t | y t , w) ? T t=2 D KL q(z t | x 1:t ) p(z t | y t?1 ) ? ? ,<label>(7)</label></formula><p>where D KL denotes the Kullback-Leibler (KL) divergence <ref type="bibr">(Kullback &amp; Leibler, 1951)</ref>.</p><p>The sum of KL divergence expectations implies to consider the full past sequence of inferred states for each time step, due to the dependence on conditionally deterministic variables y 2:T . However, optimizing L(x 1:T ; w, ?, ?) with respect to model parameters ? and variational parameters ? can be done efficiently by sampling a single full sequence of states from q Z,Y per example, and computing gradients by backpropagation <ref type="bibr">(Rumelhart et al., 1988)</ref> trough all inferred variables, using the reparameterization trick <ref type="bibr">(Kingma &amp; Welling, 2014;</ref><ref type="bibr">Rezende et al., 2014)</ref>. We classically choose q(y 1 | x 1:k ) and q(z t | x 1:t ) to be factorized Gaussian so that all KL divergences can be computed analytically.</p><p>We include an 2 regularization term on residuals f ? applied to y which stabilizes the temporal dynamics of the residual network, as noted by <ref type="bibr" target="#b2">Behrmann et al. (2019</ref><ref type="bibr" target="#b9">), de B?zenac et al. (2019</ref> and <ref type="bibr">Rousseau et al. (2019)</ref>. Given a set of videos X , the full optimization problem, where L is defined as in Equation <ref type="formula" target="#formula_10">(7)</ref>, is then given as:</p><formula xml:id="formula_11">arg max ?,?,? x?X ? ? E x (k) c L x 1:T ; c ? x (k) c , ?, ? ?? ? E (z 2:T ,y 1:T )?q Z,Y T t=2 f ? (y t?1 , z t ) 2 ? ? .<label>(8)</label></formula><p>Stochastic Latent Residual Video Prediction  The first latent variables are inferred with the conditioning framed and are then predicted with the dynamic model. In contrast, during training, each frame of the input sequence is considered for inference, which is done as follows. Firstly, each frame x t is independently encoded into a vector-valued representation x t , with x t = h ? (x t ). y 1 is then inferred using an MLP on the first k encoded frames x 1:k . Each z t is inferred in a feed-forward fashion with an LSTM on the encoded frames. Inferring z this way experimentally performs better than, e.g., inferring them from the whole sequence x 1:T ; we hypothesize that this follows from the fact that this filtering scheme is closer to the prediction setting, where the future is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section exposes the experimental results of our method on four standard stochastic video prediction datasets. <ref type="bibr">1</ref> We compare our method with state-of-the-art baselines on stochastic video prediction. Furthermore, we qualitatively study the dynamics and latent space learned by our model. 2 Training details are described in Appendix C.</p><p>The stochastic nature and novelty of the task of stochastic video prediction make it challenging to evaluate (Lee et al., 2018): since videos and models are stochastic, comparing the ground truth and a predicted video is not adequate. We thus adopt the common approach <ref type="bibr">(Denton &amp; Fergus, 2018;</ref><ref type="bibr" target="#b35">Lee et al., 2018)</ref> consisting in, for each test sequence, sampling from the tested model a given number (here, 100) of possible futures and reporting the best performing sample against the true video. We report this discrepancy for three commonly used metrics that are computed frame-wise and averaged over time: Peak Signal-to-Noise Ratio (PSNR, 1 Code and datasets are available at https://github. com/edouardelasalles/srvp. Pretrained models are downloadable at https://data.lip6.fr/srvp/.</p><p>2 Animated video samples are available at https://sites. google.com/view/srvp/.</p><p>higher is better), Structured Similarity (SSIM, higher is better), and Learned Perceptual Image Patch Similarity (LPIPS, lower is better, <ref type="bibr" target="#b43">Zhang et al., 2018)</ref>. PSNR greatly penalizes errors in predicted dynamics, as it is a pixel-level measure derived from the 2 distance, but might also favor blurry predictions. SSIM (only reported in Appendix D for the sake of concision) rather compares local frame patches to circumvent this issue, but loses some dynamics information. LPIPS compares images through a learned distance between activations of deep CNNs trained on image classification tasks, and has been shown to better correlate with human judgment on real images. Finally, the recently proposed Fr?chet Video Distance (FVD, lower is better, Unterthiner et al., 2018) aims at directly comparing the distribution of predicted videos with the ground truth distribution through the representations computed by a deep CNN trained on action recognition tasks. It has been shown, independently from LPIPS, to better capture the realism of predicted videos than PSNR and SSIM. We treat all four metrics as complementary, as they capture different scales and modalities.</p><p>We present experimental results on a simulated dataset and three real-world datasets, that we briefly present in the following and detail in Appendix B. The corresponding numerical results can be found in Appendix D. For the sake of concision, we only display a handful of qualitative samples in this section, and refer to Appendix H and our website for additional samples. We compare our model against several variational state-of-the-art models: SV2P <ref type="bibr" target="#b0">(Babaeizadeh et al., 2018)</ref>, SVG <ref type="bibr">(Denton &amp; Fergus, 2018)</ref>, SAVP (Lee et al., 2018), and <ref type="bibr">StructVRNN (Minderer et al., 2019)</ref>. Note that SVG has the closest training and architecture to ours among the state of the art. Therefore, we use the same neural architecture as SVG for our encoders and decoders in order to perform fair comparisons with this method.</p><p>All baseline results are presented only on the datasets on which they were tested in the original articles. They were obtained with pretrained models released by the authors, except those of SVG on the Moving MNIST dataset and StructVRNN on the Human3.6M dataset, for which we  . Conditioning frames and corresponding ground truth, best samples from SVG, SAVP and our method, and worst sample from our method, for a video of the KTH dataset. Samples are chosen according to their LPIPS with respect to the ground truth. SVG fails to make a person appear, unlike SAVP and our model. The latter better predicts the subject pose and produces more realistic predictions.</p><p>trained models using the code and hyperparameters provided by the authors (see Appendix B). Unless specified otherwise, our model is tested with the same ?t as in training (see Equation <ref type="formula">(2)</ref>).</p><p>Stochastic Moving MNIST. This dataset consists of one or two MNIST digits (LeCun et al., 1998) moving linearly and randomly bouncing on walls with new direction and velocity sampled randomly at each bounce <ref type="bibr">(Denton &amp; Fergus, 2018)</ref>.</p><p>Figure 3 (left) shows quantitative results with two digits. Our model outperforms SVG on both PSNR and SSIM; LPIPS and FVD are not reported as they are not relevant for this synthetic task. Decoupling dynamics from image synthesis allows our method to maintain temporal consistency despite high-uncertainty frames where crossing digits become indistinguishable. For instance in <ref type="figure" target="#fig_1">Figure 2</ref>, the digits shapes change after they cross in the SVG prediction, while our model predicts the correct digits. To evaluate the predictive ability on a longer horizon, we perform experiments on the deterministic version of the dataset <ref type="bibr">(Srivastava et al., 2015)</ref> with only one prediction per model to compute PSNR and SSIM. We show the results up to t + 95 in <ref type="bibr">Figure 3 (right)</ref>. We can see that our model better captures the dynamics of the problem compared to SVG as its performance decreases significantly less over time, especially at a long-term horizon.</p><p>We also compare to two alternative versions of our model in <ref type="figure">Figure 3</ref>, where the residual dynamic function is replaced by an MLP or a GRU (Gated Recurrent Unit, <ref type="bibr" target="#b6">Cho et al., 2014)</ref>. Our residual model outperforms both versions on the stochastic, and especially on the deterministic version of the dataset, showing its intrinsic advantage at modeling long-term dynamics. Finally, on the deterministic version of Moving MNIST, we compare to an alternative where z is entirely removed, resulting in a temporal model close to the one presented by <ref type="bibr" target="#b5">Chen et al. (2018)</ref>. The loss of performance of this alternative model is significant, showing that our stochastic residual model offers a substantial advantage even when used in a deterministic environment.</p><p>KTH Action dataset (KTH). This dataset is composed of real-world videos of people performing a single action per video in front of different backgrounds <ref type="bibr">(Sch?ldt et al., 2004)</ref>. Uncertainty lies in the appearance of subjects, the actions they perform, and how they are performed.</p><p>We substantially outperform on this dataset every considered baseline for each metric, as shown in <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In some videos, the subject only appears after the conditioning frames, requiring the model to sample the moment and location of the subject appearance, as well as its action. This critical case is illustrated in <ref type="figure">Figure 5</ref>. There, SVG fails to even generate a moving person; only SAVP and our model manage to do so, and our best sample is closer to the subject's poses compared to SAVP. Moreover, the worst sample of our model demonstrates that it captures the diversity of the dataset by making a person appear at different time steps and with different speeds. An additional experiment on this dataset in Appendix G studies the influence of the encoder and decoder architecture on SVG and our model.</p><p>Finally, <ref type="table" target="#tab_0">Table 1 and appendix Table 3</ref> compare our method to its MLP and GRU alternative versions, leading to two conclusions. Firstly, it confirms the structural advantage of residual dynamics observed on Moving MNIST. Indeed, both MLP and GRU lose on all metrics, and especially in terms of realism according to LPIPS and FVD. Secondly, all three versions of our model (residual, MLP, GRU) outperform prior methods. Therefore, this improvement is due to their common inference method, latent nature and content variable, strengthening our motivation to propose a non-autoregressive model.</p><p>Human3.6M. This dataset is also made of videos of subjects performing various actions <ref type="bibr" target="#b23">(Ionescu et al., 2011;</ref>. While there are more actions and details to capture with less training subjects than in KTH, the video backgrounds are less varied, and subjects always remain within the frames.</p><p>As reported in <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_0">Table 1</ref>, we significantly outperform, with respect to all considered metrics, StructVRNN, which is the state of the art on this dataset and has been shown to surpass both SAVP and SVG by <ref type="bibr">Minderer et al. (2019)</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> shows the dataset challenges; in particular, both methods do not capture well the subject appearance. Nonetheless, our model better captures its movements, and produces more realistic frames.</p><p>Comparisons to the MLP and GRU versions demonstrate once again the advantage of using residual dynamics. GRU obtains low scores on all metrics, which is coherent with similar results for SVG reported by <ref type="bibr">Minderer et al. (2019)</ref>. While the MLP version remains close to the residual model on PSNR, SSIM and LPIPS, it is largely beaten by the latter in terms of FVD.</p><p>BAIR robot pushing dataset (BAIR). This dataset contains videos of a Sawyer robotic arm pushing objects on a tabletop <ref type="bibr">(Ebert et al., 2017)</ref>. It is highly stochastic as the arm can change its direction at any moment. Our method    achieves similar or better results compared to state-of-theart models in terms of PSNR, SSIM and LPIPS, as shown in <ref type="figure">Figure 4</ref>, except for SV2P that produces very blurry samples, as seen in Appendix H, yielding good PSNR but prohibitive LPIPS scores. Our method obtains second-best FVD score, close to SAVP whose adversarial loss enables it to better model small objects, and outperforms SVG, whose variational architecture is closest to ours, demonstrating the advantage of non-autoregressive methods. Recent advances <ref type="bibr" target="#b18">(Villegas et al., 2019)</ref> indicate that performance of such variational models can be improved by increasing networks capacities, but this is out of the scope of this paper.</p><p>Varying frame rate in testing. We challenge here the ODE inspiration of our model. Equation <ref type="formula">(2)</ref> amounts to learning a residual function f z t +1 over t ? t , t + 1 . We aim at testing whether this dynamics is close to its continuous generalization:</p><formula xml:id="formula_12">dy dt = f z t +1 (y),<label>(9)</label></formula><p>which is a piecewise ODE. To this end, we refine this Euler approximation during testing by halving ?t; if this maintains the performance of our model, then the dynamic rule of the latter is close to the piecewise ODE. As shown in <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_0">Table 1</ref>, prediction performances overall remain stable while generating twice as many frames (cf. Appendix F for further discussion). Therefore, the justification of the proposed update rule is supported by empirical evidence. This property can be used to generate videos at a higher frame rate, with the same model, and without supervision. We show in <ref type="figure" target="#fig_6">Figure 7</ref> and Appendix F frames generated at a double and quadruple frame rate on KTH, Human3.6M and BAIR.</p><p>Disentangling dynamics and content. Let us show that the proposed model actually separates content from dynamics as discussed in Section 3.2. To this end, two sequences x s and x t are drawn from the Human3.6M testing set. While x s is used for extracting our content variable w s , dynamic states y t are inferred with our model from x t . New frame sequences x are finally generated from the fusion of the content vector and the dynamics. This results in a content corresponding to the first sequence x s and a movement following the dynamics of the second sequence x t , as observed in <ref type="figure" target="#fig_7">Figure 8</ref>. More samples for KTH, Human3.6M, and BAIR can be seen in Appendix H.</p><p>Interpolation of dynamics. Our state-space structure allows us to learn semantic representations in y t . To highlight this feature, we test whether two deterministic Moving MNIST trajectories can be interpolated by linearly interpolating their inferred latent initial conditions. We begin by generating two trajectories x s and x t of a single moving digit. We infer their respective latent initial conditions y s 1 and y t 1 . We then use our model to generate frame sequences from latent initial conditions linearly interpolated between y s 1 and y t 1 . If it learned a meaningful latent space, the resulting trajectory should also be a smooth interpolation between the directions of reference trajectories x s and x t , and this is what we observe in <ref type="figure" target="#fig_8">Figure 9</ref>. Additional examples can be found in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a novel dynamic latent model for stochastic video prediction which, unlike prior image-autoregressive models, decouples frame synthesis and dynamics. This temporal model is based on residual updates of a small latent state that is showed to perform better than RNN-based models. This endows our method with several desirable properties, such as temporal efficiency and latent space interpretability. We experimentally demonstrate the performance and advantages of the proposed model, which outperforms prior state-of-the-art methods for stochastic video prediction. This work is, to the best of our knowledge, the first to propose a latent dynamic model that scales for video prediction. The proposed model is also novel with respect to the recent line of work dealing with neural networks and ODEs for temporal modeling; it is the first such residual model to scale to complex stochastic data such as videos.</p><p>We believe that the general principles of our model (statespace, residual dynamic, static content variable) can be generally applied to other models as well. Interesting future works include replacing the VRNN of Minderer et al. <ref type="formula" target="#formula_2">(2019)</ref> with our residual dynamics in order to model the evolution of key-points, supplementing our model with more videospecific priors, or leveraging its state-space nature in modelbased reinforcement learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evidence Lower Bound</head><p>We develop in this section the computations of the variational lower bound for the proposed model.</p><p>Using the original variational lower bound of <ref type="bibr">Kingma &amp; Welling (2014)</ref> in Equation <ref type="formula" target="#formula_2">(10)</ref>:</p><formula xml:id="formula_13">log p(x 1:T | w) ? E ( z 2:T , y 1:T )?q Z,Y log p(x 1:T | z 2:T , y 1:T , w) ? D KL q Z,Y p(y 1:T , z 2:T | w) (10) = E ( z 2:T , y 1:T )?q Z,Y log p(x 1:T | z 2:T , y 1:T , w) ? D KL q(y 1 , z 2:T | x 1:T ) p(y 1 , z 2:T ) (11) = E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 , z 2:T | x 1:T ) p(y 1 , z 2:T ) ,<label>(12)</label></formula><p>where:</p><p>? Equation <ref type="formula" target="#formula_2">(11)</ref> is given by the forward and inference models factorizing p and q in Equations (4) to (6) and illustrated by, respectively <ref type="figure" target="#fig_0">, Figures 1(a) and 1(b)</ref>:</p><p>the z variables and y 1 are independent from w with respect to p and q; the y 2:T variables are deterministic functions of y 1 and z 2:T with respect to p and q;</p><p>? Equation <ref type="formula" target="#formula_2">(12)</ref> results from the factorization of p(x 1:T | y 1:T , z 1:T , w) in Equation <ref type="formula" target="#formula_6">(4)</ref>.</p><p>From there, by using the integral formulation of D KL :</p><formula xml:id="formula_14">log p(x 1:T | w) ? E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) + ? ? ?</formula><p>y1,z 2:T q(y 1 , z 2:T | x 1:T ) log p(y 1 , z 2:T ) q(y 1 , z 2:T | x 1:T ) dz 2:T dy 1</p><formula xml:id="formula_15">(13) = E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:T ) p(y 1 ) + E y1?q(y1 | x 1:T ) ? ? ? z 2:T q(z 2:T | x 1:T , y 1 ) log p(z 2:T | y 1 ) q(z 2:T | x 1:T , y 1 ) dz 2:T (14) = E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:k ) p(y 1 ) + E y1?q(y1 | x 1:k ) ? ? ? z 2:T q(z 2:T | x 1:T , y 1 ) log p(z 2:T | y 1 ) q(z 2:T | x 1:T , y 1 ) dz 2:T (15) = E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:k ) p(y 1 ) + E y1?q(y1 | x 1:k ) ? ? ? ? ? z 2:T T t=2 q(z t | x 1:t ) T t=2 log p(z t | y 1 , z 2:t?1 ) q(z t | x 1:t ) dz 2:T ? ? (16) = E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:k ) p(y 1 ) ? E y1?q(y1 | x 1:k ) D KL q(z 2 | x 1:t ) p(z 2 | y 1 ) + E y1?q(y1 | x 1:k ) E z2?q(z2 | x1:2) ? ? ? ? ? z 3:T T t=3 q(z t | x 1:t ) T t=3 log p(z t | y 1 , z 2:t?1 ) q(z t | x 1:t ) dz 3:T ? ? ,<label>(17)</label></formula><p>where:</p><p>? Equation <ref type="formula" target="#formula_2">(15)</ref> follows from the inference model of Equation <ref type="formula" target="#formula_9">(6)</ref>, where y 1 only depends on x 1:k ;</p><p>? Equation <ref type="formula" target="#formula_2">(16)</ref> is obtained from the factorizations of Equations (4) to (6).</p><p>By iterating Equation <ref type="formula" target="#formula_2">(17)</ref>'s step on z 3 , . . . , z T and factorizing all expectations, we obtain:</p><formula xml:id="formula_16">log p(x 1:T | w) ? E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:k ) p(y 1 ) ? E y1?q(y1 | xc) E zt?q(zt | x1:t) T t=2 T t=2 D KL q(z t | x 1:t ) p(z t | y 1 , z 1:t?1 ) ,<label>(18)</label></formula><p>and we finally retrieve Equation <ref type="formula" target="#formula_10">(7)</ref> by using the factorization of Equation <ref type="formula" target="#formula_9">(6)</ref>:</p><formula xml:id="formula_17">log p(x 1:T | w) ? E ( z 2:T , y 1:T )?q Z,Y T t=1 log p(x t | y t , w) ? D KL q(y 1 | x 1:k ) p(y 1 ) ? E ( z 2:T , y 1:T )?q Z,Y T t=2 D KL q(z t | x 1:t ) p(z t | y t?1 ) .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets Details</head><p>We detail in this section the datasets used in our experimental study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Data Representation</head><p>For all datasets, video frames are represented by greyscale or RGB pixels with values within [0, 1] obtained by dividing by 255 their original values lying in 0, 255 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Stochastic Moving MNIST</head><p>This monochrome dataset consists in one or two training MNIST digits <ref type="bibr">(LeCun et al., 1998)</ref> of size 28 ? 28 moving linearly within a 64 ? 64 frame and randomly bouncing against its border, sampling a new direction and velocity at each bounce <ref type="bibr">(Denton &amp; Fergus, 2018)</ref>. We use the same settings as <ref type="bibr">Denton &amp; Fergus (2018)</ref>, train all models on 15 timesteps, condition them at testing time on 5 frames, and predict either 20 (for the stochastic version) or 95 (for the deterministic version) frames. Note that we adapted the dataset to sample more coherent bounces: the original dataset computes digit trajectories that are dependent on the chosen framerate, unlike our corrected version of the dataset. We consequently retrained SVG on this dataset, obtaining comparable results as those originally presented by <ref type="bibr">Denton &amp; Fergus (2018)</ref>. Test data were produced by generating a trajectory for each testing digit, and randomly pairwise combining these trajectories to produce 5000 testing sequences containing each two digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. KTH Action Dataset (KTH)</head><p>This dataset is composed of real-world 64 ? 64 monochrome videos of 25 people performing one of six actions (walking, jogging, running, boxing, handwaving and handclapping) in front of different backgrounds <ref type="bibr">(Sch?ldt et al., 2004)</ref>. Uncertainty lies in the appearance of subjects, the action they perform and how it is performed. We use the same settings as <ref type="bibr">Denton &amp; Fergus (2018)</ref>, train all models on 20 timesteps, condition them at testing time on 10 frames, and predict 30 frames. The training set is formed with actions from the first 20 subjects, the remaining five being used for testing. Training is performed by sampling sub-sequences of size 20 in the training set. The test set is composed of 1000 randomly sampled sub-sequences of size 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Human3.6M</head><p>This dataset is also made of videos of subjects performing various actions <ref type="bibr" target="#b23">(Ionescu et al., 2011;</ref>. While there are more actions and details to capture with less training subjects than in KTH, the video backgrounds are less varied, and subjects always remain within the frames. We use the same settings as <ref type="bibr">Minderer et al. (2019)</ref> to train both our model and StructVRNN, for which there is no available pretrained model. We train all models on 16 timesteps, condition them at testing time on 8 frames, and predict 45 frames. Videos used in our experiment are subsampled from the original videos at 6.25Hz, center-cropped from 1000 ? 1000 to 800 ? 800 and resized to 64 ? 64 using the Lanczos filter of the Pillow library 3 . The training set is composed of videos of subjects 1, 5, 6, 7, and 8, and the testing set is made from subjects 9 and 11; videos showing more than one action, marked by "ALL" in the dataset, are excluded. Training is performed by sampling sub-sequences of size 16 in the training set. The test set is composed of 1000 randomly sampled sub-sequences of size 53 from the testing videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. BAIR Robot Pushing Dataset (BAIR)</head><p>This dataset contains 64 ? 64 videos of a Sawyer robotic arm pushing objects on a tabletop <ref type="bibr">(Ebert et al., 2017)</ref>. It is highly stochastic as the arm can change its direction at any moment. We use the same settings as <ref type="bibr">Denton &amp; Fergus (2018)</ref>, train all models on 12 timesteps, condition them at testing time on 2 frames, and predict 28 frames. Training and testing sets are the same as those used by <ref type="bibr">Denton &amp; Fergus (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>We expose in this section further information needed for the reproduction of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Specifications</head><p>We Encoder and decoder architecture. Both g ? and h ? are chosen to have the same mirrored architecture that depends on the dataset. We used the same architectures as <ref type="bibr">Denton &amp; Fergus (2018)</ref>: a DCGAN discriminator and generator architecture <ref type="bibr">(Radford et al., 2016)</ref> for Moving MNIST, and a VGG16 (Simonyan &amp; Zisserman, 2015) architecture (mirrored for h ? ) for the other datasets. In both cases, the output of h ? (i.e., x) is a vector of size 128, and g ? and h ? weights are initialized using a centered Gaussian distribution with a standard deviation of 0.02 (except for biases initialized to 0, and batch normalization layers weights drawn from a Gaussian distribution with unit mean and a standard deviation of 0.02). Additionally, we supplement g ? with a last sigmoid activation in order to ensure its outputs lie within [0, 1] like the ground truth data.</p><p>Note that, during testing, predicted frames are directly generated by g ? (y t , w) without sampling from the observation probability distribution G g ? (y t , w) = N g ? (y t , w), ?I . This is a common practice for Gaussian decoders in VAEs that is adopted by our competitors <ref type="bibr" target="#b35">(Lee et al., 2018;</ref><ref type="bibr">Denton &amp; Fergus, 2018;</ref><ref type="bibr">Minderer et al., 2019)</ref>.</p><p>Content variable. For the Moving MNIST dataset, the content variable w is obtained directly from x and is a vector of size 128. For KTH, Human3.6M, and BAIR, we supplement this vectorial variable with skip connections from all layers of the encoder g ? that are then fed to the decoder h ? to handle complex backgrounds. For Moving MNIST, the number of frames k used to compute the content variable is 5; for KTH and Human3.6M, it is 3; for BAIR, it is 2.</p><p>The vectorial content variable w is computed from k input frames x (k) c = x i1 , . . . , x i k with c ? defined as follows:</p><formula xml:id="formula_18">w = c ? x (k) c = c 2 ? ? ? k j=1 c 1 ? x ij ? ? .<label>(20)</label></formula><p>In other words, c ? transforms each frame representation using c 1 ? , sums these transformations and outputs the application of c 2 ? to this sum. Since frame representations x ij = h ? x ij are computed independently from each other, c ? is indeed permutation-invariant. In practice, c 1 ? consists in a linear layer of output size 256 followed by a rectified linear unit (ReLU) activation, while c 2 ? is a linear layer of output size 256 (making w of size 256) followed by a hyperbolic tangent activation.</p><p>LSTM architecture. The LSTM used for all datasets has a single layer of LSTM cells with a hidden state size of 256.</p><p>MLP architecture. All MLPs used in inference (with parameters ?) have three linear layers with hidden size 256 and ReLU activations. All MLPs used in the forward model (with parameters ?) have four linear layers with hidden size 512 and ReLU activations. Any MLP outputting Gaussian distribution parameters (?, ?) additionally includes a softplus <ref type="bibr">(Dugas et al., 2001)</ref> applied to its output dimensions that are used to obtain ?. Weights of f ? are orthogonally initialized with a gain of 1.2 for KTH and Human3.6M, and 1.41 for the other datasets (except for biases which are initialized to 0), while the other MLPs are initialized with the default weight initialization of PyTorch.</p><p>Sizes of latent variables. The sizes of the latent variables in our model are the following: for Moving MNIST, y and z have size 20; for KTH, Human3.6M, and BAIR, y and z have size 50.</p><p>Euler step size Models are trained with ?t = 1 on Moving MNIST, and with ?t = 1 2 on the others datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Optimization</head><p>Models are trained using the Adam optimizer <ref type="bibr" target="#b30">(Kingma &amp; Ba, 2015)</ref> with learning rate 3 ? 10 ?4 , and decay rates ? 1 = 0.9 and ? 2 = 0.999.</p><p>Loss function. The batch size is chosen to be 128 for Moving MNIST, 100 for KTH and Human3.6M, and 192 for BAIR. The regularization coefficient ? is always set to 1. Logarithms used in the loss are natural logarithms.</p><p>For the Moving MNIST dataset, we follow <ref type="bibr" target="#b21">Higgins et al. (2017)</ref> by weighting the KL divergence terms on z (i.e., the sum of KL divergences in Equation <ref type="formula" target="#formula_10">(7)</ref>) with a multiplication factor ? = 2.</p><p>Variance of the observation. The variance ? considered in the observation probability distribution G g ? (y, w) = N g ? (y t , w), ?I is chosen as follows:</p><p>? for Moving MNIST, ? = 1;</p><p>? for KTH and Human3.6M, ? = 4 ? 10 ?2 ;</p><p>? for BAIR, ? = 1 2 .</p><p>Number of optimization steps. The number of optimization steps for each dataset is the following:</p><p>? Stochastic Moving MNIST: 1 000 000 steps, with additional 100 000 steps where the learning rate is linearly decreased to 0;</p><p>? Deterministic Moving MNIST: 800 000 steps, with additional 100 000 steps where the learning rate is linearly decreased to 0;</p><p>? KTH: 150 000 steps, with additional 50 000 steps where the learning rate is linearly decreased to 0;</p><p>? Human3.6M: 325 000 steps, with additional 25 000 steps where the learning rate is linearly decreased to 0;</p><p>? BAIR: 1 000 000 steps, with additional 500 000 steps where the learning rate is linearly decreased to 0.</p><p>Furthermore, the final models for KTH and Human3.6M are chosen among several checkpoints, computed every 5000 iterations for KTH and 20 000 iterations for Human3.6M, as the ones obtaining the best evaluation PSNR. This evaluation score differs from the test score as we extract from the training set an evaluation set by randomly selecting 5% of the training videos from the training set of each dataset. More precisely, the evaluation PSNR for a checkpoint is computed as the mean best prediction PSNR for 400 (for KTH) or 200 (for Human3.6M) randomly extracted sequences of length 30 (for KTH) or 53 (for Human3.6M) from the videos of the evaluation set.  29.43 ? 0.33 0.8633 ? 0.0049 0.0790 ? 0.0034 Ours -MLP 28.91 ? 0.34 0.8527 ? 0.0051 0.0799 ? 0.0032 Ours -GRU 29.14 ? 0.33 0.8590 ? 0.0050 0.0790 ? 0.0032</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Numerical Results</head><p>Tables 2 to 5 present, respectively, numerical results for PSNR, SSIM and LPIPS averaged over all time steps for our methods and considered baselines on the Moving MNIST, KTH, Human3.6M, and BAIR datasets, corresponding to Figures 3 and 4.</p><p>Note that we choose the learned prior version of SVG for all datasets but KTH, for which we choose the fixed prior version, as done by its authors <ref type="bibr">(Denton &amp; Fergus, 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pendulum Experiments</head><p>We test the ability of our model to model the dynamics of a common dataset used in the literature of state-space models <ref type="bibr" target="#b29">(Karl et al., 2017;</ref><ref type="bibr" target="#b12">Fraccaro et al., 2017)</ref>, Pendulum <ref type="bibr" target="#b29">(Karl et al., 2017)</ref>. It consists of noisy observations of a dynamic torque-controlled pendulum; it is stochastic as the information of this control is not available. We test our model, without the content variable w, in the same setting as DVBF <ref type="bibr" target="#b29">(Karl et al., 2017)</ref> and KVAE <ref type="bibr" target="#b12">(Fraccaro et al., 2017)</ref> and report the  corresponding ELBO scores in <ref type="table">Table 6</ref>. The encoders and decoders for all methods are MLPs.</p><p>Our model outperforms DVBF and is merely beaten by KVAE. This can be explained by the nature of the KVAE model, whose sequential model is learned using a Kalman filter rather than a VAE, allowing exact inference in the latent space. On the contrary, DVBF is learned, like our model, by a sequential VAE, and is thus closer to our model than KVAE. This result then shows that the dynamic model that we propose in the context of sequential VAEs is more adapted on this dataset than the one of DVBF, and achieve results close to a method taking advantage of exact inference using adapted tools such as Kalman filters.</p><p>F. Influence of the Euler step size <ref type="table">Table 7</ref> details the numerical results of our model trained on BAIR with ?t = 1 2 and tested with different values of ?t. It shows that, when refining the Euler approximation, our model maintains its performances in settings unseen during training.</p><p>Tables 8 and 9 detail the numerical results of our model trained on KTH with, respectively, ?t = 1 and ?t = 1 2 , and tested with different values of ?t. They show that if ?t is chosen too high when training (here, ?t = 1), the model performance drops when refining the Euler approximation. We assume that this phenomenon arises because the Euler approximation used in training is too rough, making the model adapt to an overly discretized dynamic that cannot be transferred to smaller Euler step sizes. Indeed, when training with a smaller step size (here, ?t = 1 2 ), results in the training settings are equivalent while results obtained with a lower ?t are now much closer, if not equivalent, to the nominal ones. This shows that the model learns a continuous dynamic if learned with a small enough step size.</p><p>Note that the loss of performance using a higher ?t in testing than in training, like in Tables 7 and 9, is expected as it corresponds to loosening the Euler approximation compared to training. However, even in this challenging setting, our model maintains state-of-the-art results, demonstrating the quality of the learned dynamic as it can be further discretized if needed at the cost of a reasonable drop in performance.</p><p>G. Autoregressivity and Impact of Encoder and Decoder Architecture <ref type="figure" target="#fig_0">Figure 10</ref> and <ref type="table" target="#tab_0">Table 10</ref> expose the numerical results on KTH of our model trained with ?t = 1 and SVG for two choices of encoder and decoder architectures: DCGAN and VGG.</p><p>Since DCGAN is a less powerful architecture than VGG, results of each method with VGG are expectedly better than those of the same method with DCGAN. Moreover, our model outperforms SVG for any fixed choice of encoder and decoder 28.98 ? 0.37 0.8496 ? 0.0057 0.0939 ? 0.0045 ?t = 1 5</p><p>28.95 ? 0.37 0.8490 ? 0.0058 0.0948 ? 0.0045 <ref type="table">Table 9</ref>. Numerical results for PSNR, SSIM, and LPIPS on KTH of our model trained with ?t = 1 2 and tested with different values of ?t.</p><p>Step size ?t   <ref type="figure" target="#fig_0">Figure 11</ref>. Conditioning frames and corresponding ground truth and best samples with respect to PSNR from SVG and our method, and worst and random samples from our method, for an example of the Stochastic Moving MNIST dataset.</p><p>architecture, which is coherent with <ref type="figure">Figure 4</ref>.</p><p>We observe, however, that the difference between a method using VGG and its DCGAN counterpart differs depending on the model. Ours shows more robustness to the choice of encoder and decoder architecture, as it its performance decreases less than SVG when switching to a less powerful architecture. This loss is particularly pronounced with respect to PSNR, which is the metric that most penalizes dynamics errors. This shows that reducing the capacity of the encoders and decoders of SVG not only hurts its ability to produce realistic frames, as expected, but also substantially lowers its ability to learn a good dynamic. We assume that this phenomenon is caused by the autoregressive nature of SVG, which makes it reliant on the performance of its encoders and decoders. This supports our motivation to propose a non-autoregressive model for stochastic video prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Additional Samples</head><p>This section includes additional samples corresponding to experiments described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Stochastic Moving MNIST</head><p>We present in <ref type="figure" target="#fig_0">Figures 11 to 14</ref> additional samples from SVG and our model on Stochastic Moving MNIST.</p><p>In particular, <ref type="figure" target="#fig_0">Figure 13</ref> shows SVG changing a digit shape in the course of a prediction even though it does not cross another digit, whereas ours maintain the digit shape. We assume that the advantage of our model comes from the latent nature of its dynamic and the use of a static content variable that is prevented from containing temporal information. Indeed, even when the best sample from our model is not close from the ground truth of the dataset, like in <ref type="figure" target="#fig_0">Figure 14</ref>, it still maintains the shapes of the digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. KTH</head><p>We present in <ref type="figure" target="#fig_0">Figures 15 to 19</ref>    <ref type="figure" target="#fig_0">Figure 11</ref>).  <ref type="figure" target="#fig_0">Figure 13</ref>. Additional samples for the Stochastic Moving MNIST dataset (cf. <ref type="figure" target="#fig_0">Figure 11</ref>). SVG fails to maintain the shape of a digit, while ours is temporally coherent.  <ref type="figure" target="#fig_0">Figure 14</ref>. Additional samples for the Stochastic Moving MNIST dataset (cf. <ref type="figure" target="#fig_0">Figure 11</ref>). This example was chosen in the worst 1% testing examples of our model with respect to PSNR. Despite this adversarial criterion, our model maintains temporal consistency as digits are not deformed in the course of the video.  <ref type="figure" target="#fig_0">Figure 15</ref>. Conditioning frames and corresponding ground truth, best samples from SVG, SAVP and our method, and worst and random samples from our method, for an example of the KTH dataset. Samples are chosen according to their LPIPS with respect to the ground truth. On this specific task (clapping), all methods but SV2P (which produces blurry predictions) perform well, even though ours stays closer to the ground truth.  <ref type="figure" target="#fig_0">Figure 16</ref>. Additional samples for the KTH dataset (cf. <ref type="figure" target="#fig_0">Figure 15</ref>). In this example, the shadow of the subject is visible in the last conditioning frames, foreshadowing its appearance. This is a failure case for SV2P and SVG which only produce an indistinct shadow, whereas SAVP and our model make the subject appear. Yet, SAVP produces the wrong action and an inconsistent subject in its best sample, while ours is correct.  <ref type="figure" target="#fig_0">Figure 17</ref>. Additional samples for the KTH dataset (cf. <ref type="figure" target="#fig_0">Figure 15</ref>). This example is a failure case for all methods: SV2P produces blurry frames, SVG and SAVP are not consistent (change of action or subject appearance in the video), and our model produces a ghost image at the end of the prediction on the worst sample only.  <ref type="figure" target="#fig_0">Figure 18</ref>. Additional samples for the KTH dataset (cf. <ref type="figure" target="#fig_0">Figure 15</ref>). Our model is the only one to make a subject appear, like in the ground truth.  <ref type="figure" target="#fig_0">Figure 19</ref>. Additional samples for the KTH dataset (cf. <ref type="figure" target="#fig_0">Figure 15</ref>). The subject in this example is boxing, which is a challenging action in the dataset as all methods are far from the ground truth, even though ours remain closer in this case as well.  <ref type="figure" target="#fig_1">Figure 20</ref>. Conditioning frames and corresponding ground truth, best samples from StructVRNN and our method, and worst and random samples from our method, for an example of the Human3.6M dataset. Samples are chosen according to their LPIPS with respect to the ground truth. We better capture the movements of the subject as well as their diversity, predict more realistic subjects, and present frames with less artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. Human3.6M</head><p>We present in Figures 20 and 21 additional samples from StructVRNN and our model on Human3.6M, with additional insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4. BAIR</head><p>We present in Figures 22 to 24 additional samples from SV2P, SVG, SAVP and our model on BAIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.5. Oversampling</head><p>We present in <ref type="figure" target="#fig_1">Figure 25</ref> additional examples of video generation at a doubled and quadrupled frame rate by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.6. Content Swap</head><p>We present in <ref type="figure" target="#fig_0">Figures 26 to 31</ref> additional examples of content swap as in <ref type="figure" target="#fig_7">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7. Interpolation in the Latent Space</head><p>We present in <ref type="figure" target="#fig_1">Figures 32 and 33</ref> additional examples of interpolation in the latent space between two trajectories as in <ref type="figure" target="#fig_8">Figure 9</ref>.  <ref type="figure" target="#fig_0">Figure 21</ref>. Additional samples for the Human3.6M dataset (cf. <ref type="figure" target="#fig_1">Figure 20)</ref>. This action is better captured by our model, which is able to produce diverse realistic predictions.  <ref type="figure" target="#fig_1">Figure 22</ref>. Conditioning frames and corresponding ground truth, best samples from SVG, SAVP and our method, and worst and random samples from our method, for an example of the BAIR dataset. Samples are chosen according to their LPIPS with respect to the ground truth.  <ref type="figure" target="#fig_1">Figure 23</ref>. Additional samples for the BAIR dataset (cf. <ref type="figure" target="#fig_1">Figure 22</ref>).  <ref type="figure" target="#fig_1">Figure 24</ref>. Additional samples for the BAIR dataset (cf. <ref type="figure" target="#fig_1">Figure 22</ref>).   <ref type="figure" target="#fig_1">Figure 26</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Latent Residual Video Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic</head><p>Content Swap <ref type="figure" target="#fig_1">Figure 28</ref>. Additional example of content swap (cf. <ref type="figure" target="#fig_1">Figure 26</ref>). In this example, the extracted content is the video background, which is successfully transferred to the target video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic</head><p>Content Swap <ref type="figure" target="#fig_1">Figure 29</ref>. Additional example of content swap (cf. <ref type="figure" target="#fig_1">Figure 26</ref>). In this example, the extracted content is the video background and the subject appearance, which are successfully transferred to the target video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic</head><p>Content Swap <ref type="figure">Figure 30</ref>. Additional example of content swap (cf. <ref type="figure" target="#fig_1">Figure 26</ref>). This example shows a failure case of content swapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic</head><p>Content Swap <ref type="figure" target="#fig_0">Figure 31</ref>. Additional example of content swap (cf. <ref type="figure" target="#fig_1">Figure 26</ref>).  <ref type="figure" target="#fig_1">Figure 32</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a), (b) Proposed generative and inference models. Diamonds and circles represent, respectively, deterministic and stochastic states. (c) Corresponding architecture with two parts: inference on conditioning frames on the left, generation for extrapolation on the right. h ? and g ? are deep Convolutional Neural Networks (CNNs), and other named networks are Multilayer Perceptrons (MLPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Conditioning frames and corresponding ground truth and best samples with respect to PSNR from SVG and our method for an example of the Stochastic Moving MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1</head><label>1</label><figDesc>(c) depicts the full architecture of our temporal model, showing how the model is applied during testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Mean PSNR scores with respect to t for all tested models on the Moving MNIST dataset, with their 95%-confidence intervals. Vertical bars mark the length of training sequences. PSNR and LPIPS scores with respect to t for all tested models on the KTH (left column), Human3.6M (center) and BAIR (right) datasets, with their 95%-confidence intervals. Vertical bars mark the length of training sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Conditioning frames and corresponding ground truth, best samples from StructVRNN and our method, and worst sample from our method, with respect to LPIPS, for a video of the Human3.6M dataset. Our method better captures the dynamic of the subject and produces less artefacts than StructVRNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Cropped KTH sample. (b) Cropped Human3.6M sample. (c) Cropped BAIR sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Generation examples at doubled or quadrupled frame rate, using a halved ?t compared to training. Frames including a bottom red dashed bar are intermediate frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Video (bottom right) generated from the dynamic latent state y inferred with a video (top) and the content variable w computed with the conditioning frames of another video (left). The generated video keeps the same background as the bottom left frames, while the subject moves accordingly to the top frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>From left to right, x s , x s (reconstruction of x s by the VAE of our model), results of the interpolation in the latent space between x s and x t , x t and x t . Each trajectory is materialized in shades of grey in the frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>used Python 3.7.6 and PyTorch 1.4.0 (Paszke et al., 2019) to implement our model. Each model was trained on Nvidia GPUs with CUDA 10.1 using mixed-precision training (Micikevicius et al., 2018) with Apex. 4 C.2. Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>PNSR, SSIM and LPIPS scores with respect to t, with their 95%-confidence intervals, on the KTH dataset for SVG and our model with two choices of encoder and decoder architecture for each model: DCGAN and VGG. Vertical bars mark the length of training sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Additional samples for the Stochastic Moving MNIST dataset (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Cropped KTH sample, centered on the subject.(b) Cropped Human3.6M sample, centered on the subject. (c) Cropped BAIR sample, centered on the robot arm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 25 .Figure 26 .Figure 27 .</head><label>252627</label><figDesc>Generation examples at doubled or quadrupled frame rate, using a halved ?t compared to training. Frames including a bottom red dashed bar are intermediate frames. Video (bottom right) generated from the combination of dynamic variables (y, z) inferred with a video (top) and a content variable (w) computed with the conditioning frames of another video (bottom left). Additional example of content swap (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 32 .Figure 33 .</head><label>3233</label><figDesc>From left to right, x s , x s (reconstruction of x s by the VAE of our model), results of the interpolation in the latent space between x s and x t , x t and x t . Each trajectory is materialized in shades of grey in the frames. (a) Ref 1 (b) Rec 1 (c) Interpolation (d) Rec 2 (e) Ref 2 Additional example of interpolation in the latent space between two trajectories (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>FVD scores for all tested methods on the KTH, Human3.6M and BAIR datasets with their 95%-confidence intervals over five different samples from the models. Bold scores indicate the best performing method for each dataset.</figDesc><table><row><cell>Dataset</cell><cell>SV2P</cell><cell>SAVP</cell><cell>SVG</cell><cell>StructVRNN</cell><cell>Ours</cell><cell>Ours -?t 2</cell><cell cols="2">Ours -MLP Ours -GRU</cell></row><row><cell>KTH</cell><cell>636 ? 1</cell><cell>374 ? 3</cell><cell>377 ? 6</cell><cell>-</cell><cell>222 ? 3</cell><cell>244 ? 3</cell><cell>255 ? 4</cell><cell>240 ? 5</cell></row><row><cell>Human3.6M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>556 ? 9</cell><cell>416 ? 5</cell><cell>415 ? 3</cell><cell>582 ? 4</cell><cell>1050 ? 20</cell></row><row><cell>BAIR</cell><cell cols="2">965 ? 17 152 ? 9</cell><cell>255 ? 4</cell><cell>-</cell><cell>163 ? 4</cell><cell>222 ? 42</cell><cell>162 ? 4</cell><cell>178 ? 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Proceedings of the 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pp. 344-356. PMLR, November 2017. Fan, H., Zhu, L., and Yang, Y. Cubic LSTMs for video prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 8263-8270, 2019. Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 10215-10224. Curran Associates, Inc., 2018. Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3208-3216, Stockholmsm?ssan, Stockholm Sweden, July 2018. PMLR. Lotter, W., Kreiman, G., and Cox, D. Deep predictive coding networks for video prediction and unsupervised learning. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In Xing, E. P. and Jebara, T. (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1278-1286, Bejing, China, June 2014. PMLR. Advances in Neural Information Processing Systems 28, pp. 802-810. Curran Associates, Inc., 2015. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.</figDesc><table><row><cell>Kingma, D. P. and Welling, M. Auto-encoding variational</cell><cell>In International Conference on Learning Representations, Srivastava, N., Mansimov, E., and Salakhudinov, R. Unsu-</cell></row><row><cell>Bayes. In International Conference on Learning Repre-Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolu-</cell><cell>2017. pervised learning of video representations using LSTMs.</cell></row><row><cell>sentations, 2014. Krishnan, R. G., Shalit, U., and Sontag, D. Structured in-ference networks for nonlinear state space models. In Proceedings of the AAAI Conference on Artificial Intelli-tional networks for biomedical image segmentation. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F. (eds.), Medical Image Computing and Computer-Assisted Springer International Publishing. Intervention -MICCAI 2015, pp. 234-241, Cham, 2015.</cell><cell>In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd In-Lu, C., Hirsch, M., and Sch?lkopf, B. Flexible spatio-ternational Conference on Machine Learning, volume 37 temporal networks for video prediction. In The IEEE of Proceedings of Machine Learning Research, pp. 843-Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2137-2145, July 2017a. 852, Lille, France, July 2015. PMLR.</cell></row><row><cell>gence, volume 31, pp. 2101-2109, 2017. Kullback, S. and Leibler, R. A. On information and suf-ficiency. The Annals of Mathematical Statistics, 22(1): 79-86, 1951. Rousseau, F., Drumetz, L., and Fablet, R. Residual networks as flows of diffeomorphisms. Journal of Mathematical Imaging and Vision, May 2019.</cell><cell>Lu, Y., Zhong, A., Li, Q., and Dong, B. Beyond fi-nite layer neural networks: Bridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121, 2017b. Tulyakov, S., Liu, M.-Y., Yang, X., and Kautz, J. MoCo-GAN: Decomposing motion and content for video gener-ation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1526-1535, June 2018.</cell></row><row><cell></cell><cell>Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R.,</cell></row><row><cell></cell><cell>Michalski, M., and Gelly, S. Towards accurate generative</cell></row><row><cell></cell><cell>models of video: A new metric &amp; challenges. arXiv</cell></row><row><cell></cell><cell>preprint arXiv:1812.01717, 2018.</cell></row><row><cell></cell><cell>van den Oord, A., Kalchbrenner, N., Espeholt, L.,</cell></row><row><cell></cell><cell>Kavukcuoglu, K., Vinyals, O., and Graves, A. Condi-</cell></row><row><cell></cell><cell>International Conference on Learning Representations, tional image generation with PixelCNN decoders. In Lee,</cell></row><row><cell>696-</cell><cell>2018. D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and</cell></row><row><cell>699. MIT Press, Cambridge, MA, USA, 1988. Ryder, T., Golightly, A., McGough, A. S., and Prangle, D. Black-box variational inference for stochastic differential</cell><cell>Garnett, R. (eds.), Advances in Neural Information Pro-Minderer, M., Sun, C., Villegas, R., Cole, F., Murphy, K., and Lee, H. Unsupervised learning of object structure and dynamics from videos. In Wallach, H., Larochelle, H., cessing Systems 29, pp. 4790-4798. Curran Associates, Inc., 2016.</cell></row><row><cell>equations. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,</cell><cell>Beygelzimer, A., d'Alch? Buc, F., Fox, E., and Garnett, van Steenkiste, S., Chang, M., Greff, K., and Schmidhuber, R. (eds.), Advances in Neural Information Processing J. Relational neural expectation maximization: Unsu-Systems 32, pp. 92-102. Curran Associates, Inc., 2019. pervised discovery of objects and their interactions. In</cell></row><row><cell>pp. 4423-4432, Stockholmsm?ssan, Stockholm Sweden, July 2018. PMLR.</cell><cell>International Conference on Learning Representations, Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, 2018.</cell></row><row><cell>The IEEE International Conference on Computer Vision (ICCV), pp. 1762-1770, October 2017. Santoro, A., Raposo, D., Barrett, D. G. T., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. A simple neural network module for relational reasoning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances</cell><cell>L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai-son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Wal-lach, H., Larochelle, H., Beygelzimer, A., d'Alch? Buc, Villegas, R., Yang, J., Hong, S., Lin, X., and Lee, H. De-composing motion and content for natural video sequence prediction. In International Conference on Learning Rep-resentations, 2017.</cell></row><row><cell>Liu, Z., Yeh, R. A., Tang, X., Liu, Y., and Agarwala, A. Video frame synthesis using deep voxel flow. In The IEEE in Neural Information Processing Systems 30, pp. 4967-4976. Curran Associates, Inc., 2017. Sch?ldt, C., Laptev, I., and Caputo, B. Recognizing human International Conference on Computer Vision (ICCV), pp. actions: A local SVM approach. In Proceedings of the 4473-4481, October 2017. 17th International Conference on Pattern Recognition,</cell><cell>F., Fox, E., and Garnett, R. (eds.), Advances in Neural In-formation Processing Systems 32, pp. 8026-8037. Curran Villegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q. V., and Lee, H. High fidelity video prediction with large Associates, Inc., 2019. stochastic recurrent neural networks. In Wallach, H., Radford, A., Metz, L., and Chintala, S. Unsupervised rep-Larochelle, H., Beygelzimer, A., d'Alch? Buc, F., Fox, E., resentation learning with deep convolutional generative and Garnett, R. (eds.), Advances in Neural Information</cell></row><row><cell>Liu, Z., Wu, J., Xu, Z., Sun, C., Murphy, K., Freeman, W. T., 2004. ICPR 2004., volume 3, pp. 32-36, August 2004.</cell><cell>adversarial networks. In International Conference on Processing Systems 32, pp. 81-91. Curran Associates,</cell></row><row><cell>and Tenenbaum, J. B. Modeling parts, structure, and</cell><cell>Learning Representations, 2016. Inc., 2019.</cell></row><row><cell>system dynamics via predictive learning. In International</cell><cell></cell></row><row><cell>Conference on Learning Representations, 2019.</cell><cell></cell></row></table><note>Denton, E. and Fergus, R. Stochastic video generation with a learned prior. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Ma- chine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1174-1183, Stockholmsm?ssan, Stockholm, Sweden, July 2018. PMLR. Dugas, C., Bengio, Y., B?lisle, F., Nadeau, C., and Garcia, R. Incorporating second-order functional knowledge for better option pricing. In Leen, T. K., Dietterich, T. G., and Tresp, V. (eds.), Advances in Neural Information Processing Systems 13, pp. 472-478. MIT Press, 2001. Ebert, F., Finn, C., Lee, A. X., and Levine, S. Self- supervised visual planning with temporal skip connec- tions. In Levine, S., Vanhoucke, V., and Goldberg, K. (eds.),Finn, C., Goodfellow, I., and Levine, S. Unsupervised learning for physical interaction through video prediction. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 64-72. Curran Associates, Inc., 2016.Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L., and Kingma, D. VideoFlow: A conditional flow-based model for stochastic video generation. In International Conference on Learning Representations, 2020. Le Guen, V. and Thome, N. Disentangling physical dynam- ics from unknown factors for unsupervised video predic- tion. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11474-11484, June 2020. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, November 1998. Lee, A. X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., and Levine, S. Stochastic adversarial video prediction. arXiv preprint arXiv:1804.01523, 2018. Liang, X., Lee, L., Dai, W., and Xing, E. P. Dual motion GAN for future-flow embedded video prediction. InLong, Z., Lu, Y., Ma, X., and Dong, B. PDE-net: Learn- ing PDEs from data. In Dy, J. and Krause, A. (eds.),Mathieu, M., Couprie, C., and LeCun, Y. Deep multi-scale video prediction beyond mean square error. In Interna- tional Conference on Learning Representations, 2016. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H. Mixed precision training. InRanzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert, R., and Chopra, S. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.Rubanova, Y., Chen, R. T. Q., and Duvenaud, D. Latent ordinary differential equations for irregularly-sampled time series. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch? Buc, F., Fox, E., and Garnett, R. (eds.), Ad- vances in Neural Information Processing Systems 32, pp. 5320-5330. Curran Associates, Inc., 2019. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Neuro- computing: Foundations of Research, chapter Learning Representations by Back-Propagating Errors, pp.Shi, X., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-k., and Woo, W.-c. Convolutional LSTM network: A ma- chine learning approach for precipitation nowcasting. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.),Vondrick, C. and Torralba, A. Generating the future with adversarial transformers. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2992-3000, July 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Numerical results (mean and 95%-confidence interval) for PSNR and SSIM of tested methods on the two-digits Moving MNIST dataset. Bold scores indicate the best performing method for each metric and, where appropriate, scores whose means lie in the confidence interval of the best performing method.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell>Stochastic SSIM</cell><cell>Deterministic PSNR SSIM</cell></row><row><cell>SVG</cell><cell cols="3">14.50 ? 0.04 0.7090 ? 0.0015 12.85 ? 0.03 0.6185 ? 0.0011</cell></row><row><cell>Ours</cell><cell cols="3">16.93 ? 0.07 0.7799 ? 0.0020 18.25 ? 0.06 0.8300 ? 0.0017</cell></row><row><cell cols="4">Ours -MLP 16.55 ? 0.06 0.7694 ? 0.0019 16.70 ? 0.05 0.7876 ? 0.0015</cell></row><row><cell cols="4">Ours -GRU 15.80 ? 0.05 0.7464 ? 0.0016 13.17 ? 0.03 0.6237 ? 0.0011</cell></row><row><cell>Ours -w/o z</cell><cell>-</cell><cell>-</cell><cell>14.99 ? 0.03 0.4757 ? 0.0019</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Numerical results (mean and 95%-confidence interval, when relevant) for PSNR, SSIM, and LPIPS of tested methods on the KTH dataset. Bold scores indicate the best performing method for each metric and, where appropriate, scores whose means lie in the confidence interval of the best performing method.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>SV2P</cell><cell cols="3">28.19 ? 0.31 0.8141 ? 0.0050 0.2049 ? 0.0053</cell></row><row><cell>SAVP</cell><cell cols="3">26.51 ? 0.29 0.7564 ? 0.0062 0.1120 ? 0.0039</cell></row><row><cell>SVG</cell><cell cols="3">28.06 ? 0.29 0.8438 ? 0.0054 0.0923 ? 0.0038</cell></row><row><cell>Ours</cell><cell cols="3">29.69 ? 0.32 0.8697 ? 0.0046 0.0736 ? 0.0029</cell></row><row><cell>Ours -?t 2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Numerical results (mean and 95%-confidence interval, when relevant) for PSNR, SSIM, and LPIPS of tested methods on the Human3.6M dataset. Bold scores indicate the best performing method for each metric and, where appropriate, scores whose means lie in the confidence interval of the best performing method.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell cols="4">StructVRNN 24.46 ? 0.17 0.8868 ? 0.0025 0.0557 ? 0.0013</cell></row><row><cell>Ours</cell><cell cols="3">25.30 ? 0.19 0.9074 ? 0.0022 0.0509 ? 0.0013</cell></row><row><cell>Ours -?t 2</cell><cell cols="3">25.14 ? 0.21 0.9049 ? 0.0024 0.0534 ? 0.0015</cell></row><row><cell>Ours -MLP</cell><cell cols="3">25.00 ? 0.19 0.9047 ? 0.0021 0.0529 ? 0.0013</cell></row><row><cell>Ours -GRU</cell><cell cols="3">23.54 ? 0.18 0.8868 ? 0.0022 0.0683 ? 0.0014</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Numerical results (mean and 95%-confidence interval, when relevant) with respect to PSNR, SSIM, and LPIPS of tested methods on the BAIR dataset. Bold scores indicate the best performing method for each metric and, where appropriate, scores whose means lie in the confidence interval of the best performing method. ELBO, in nats, achieved by DVBF, KVAE and our model on the Pendulum dataset. The bold score indicates the best performing method.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell cols="2">SSIM</cell><cell>LPIPS</cell></row><row><cell>SV2P</cell><cell cols="4">20.39 ? 0.27 0.8169 ? 0.0086 0.0912 ? 0.0053</cell></row><row><cell>SAVP</cell><cell cols="4">18.44 ? 0.25 0.7887 ? 0.0092 0.0634 ? 0.0026</cell></row><row><cell>SVG</cell><cell cols="4">18.95 ? 0.26 0.8058 ? 0.0088 0.0609 ? 0.0034</cell></row><row><cell>Ours</cell><cell cols="4">19.59 ? 0.27 0.8196 ? 0.0084 0.0574 ? 0.0032</cell></row><row><cell>Ours -?t 2</cell><cell cols="4">19.45 ? 0.26 0.8196 ? 0.0082 0.0579 ? 0.0032</cell></row><row><cell cols="5">Ours -MLP 19.56 ? 0.26 0.8194 ? 0.0084 0.0572 ? 0.0032</cell></row><row><cell cols="5">Ours -GRU 19.41 ? 0.26 0.8170 ? 0.0084 0.0585 ? 0.0032</cell></row><row><cell></cell><cell cols="2">DVBF KVAE</cell><cell>Ours</cell></row><row><cell></cell><cell cols="3">798.56 807.02 806.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Numerical results for PSNR, SSIM, and LPIPS on BAIR of our model trained with ?t = 1 2 and tested with different values of ?t. Numerical results for PSNR, SSIM, and LPIPS on KTH of our model trained with ?t = 1 and tested with different values of ?t.</figDesc><table><row><cell>Step size ?t</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>?t = 1</cell><cell cols="3">18.95 ? 0.25 0.8139 ? 0.0081 0.0640 ? 0.0036</cell></row><row><cell>?t = 1 2 ?t = 1 3 ?t = 1 4 ?t = 1 5</cell><cell cols="3">19.59 ? 0.27 0.8196 ? 0.0084 0.0574 ? 0.0032 19.49 ? 0.25 0.8201 ? 0.0082 0.0574 ? 0.0032 19.45 ? 0.26 0.8196 ? 0.0082 0.0579 ? 0.0032 19.46 ? 0.26 0.8197 ? 0.0082 0.0584 ? 0.0032</cell></row><row><cell>Step size ?t</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>?t = 1</cell><cell cols="3">29.77 ? 0.33 0.8681 ? 0.0046 0.0742 ? 0.0029</cell></row><row><cell>?t = 1 2 ?t = 1 3 ?t = 1 4</cell><cell cols="3">29.18 ? 0.35 0.8539 ? 0.0054 0.0882 ? 0.0040 29.05 ? 0.36 0.8509 ? 0.0056 0.0924 ? 0.0043</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>FVD scores for SVG and our method on KTH, trained either with DCGAN or VGG encoders and decoders, with their 95%-confidence intervals over five different samples from the models.</figDesc><table><row><cell></cell><cell></cell><cell>SVG</cell><cell></cell><cell>Ours</cell></row><row><cell></cell><cell>VGG</cell><cell>DCGAN</cell><cell>VGG</cell><cell>DCGAN</cell></row><row><cell></cell><cell cols="4">377 ? 6 542 ? 6 220 ? 2 371 ? 3</cell></row><row><cell>Ground</cell><cell>Truth</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SVG</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>(Best)</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>(Worst)</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>(Random)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>additional samples from SV2P, SVG, SAVP and our model on KTH, with additional insights.</figDesc><table><row><cell></cell><cell>Stochastic Latent Residual Video Prediction</cell></row><row><cell>Ground</cell><cell>Truth</cell></row><row><cell></cell><cell>SVG</cell></row><row><cell>Ours</cell><cell>(Best)</cell></row><row><cell>Ours</cell><cell>(Worst)</cell></row><row><cell>Ours</cell><cell>(Random)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pillow.readthedocs.io/ 4 https://github.com/nvidia/apex.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank all members of the MLIA team from the LIP6 laboratory of Sorbonne Universit? for helpful discussions and comments, as well as Matthias Minderer for his help to process the Human3.6M dataset and reproduce StructVRNN results.</p><p>We acknowledge financial support from the LOCUST ANR project (ANR-15-CE23-0027). This work was granted access to the HPC resources of IDRIS under the allocation 2020-AD011011360 made by GENCI (Grand Equipement National de Calcul Intensif).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7610</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved conditional VRNNs for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="7608" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, Oc</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Continuous modeling of sporadicallyobserved time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gru-Ode-Bayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch? Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optimal unsupervised domain translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De B?zenac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">; I</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
	<note>Guyon</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A disentangled recognition and nonlinear dynamics model for unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamronn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>V. N., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3601" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="9006" to="9015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal difference variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davidson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic video generation using holistic attribute control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="466" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Super</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring spatial-temporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="4554" to="4563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep variational Bayes filters: Unsupervised learning of state space models from raw data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soelch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B., Matas, J., Sebe, N., and Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, N., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1144" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="6038" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Future video synthesis with object motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="5539" to="5548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video prediction via selective sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1705" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yingzhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5670" to="5679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ODE 2 VAE: Deep generative second order odes with Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Y?ld?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lahdesmaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch? Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13412" to="13421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>V. N., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
