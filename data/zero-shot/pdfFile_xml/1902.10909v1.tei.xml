<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT for Joint Intent Classification and Slot Filling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhuo</surname></persName>
							<email>zhuozhu.zz@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
							<email>w.wang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BERT for Joint Intent Classification and Slot Filling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, a variety of smart speakers have been deployed and achieved great success, such as Google Home, Amazon Echo, Tmall Genie, which facilitate goal-oriented dialogues and help users to accomplish their tasks through voice interactions. Natural language understanding (NLU) is critical to the performance of goal-oriented spoken dialogue systems. NLU typically includes the intent classification and slot filling tasks, aiming to form a semantic parse for user utterances. Intent classification focuses on predicting the intent of the query, while slot filling extracts semantic concepts. <ref type="table" target="#tab_0">Table 1</ref> shows an example of intent classification and slot filling for user query "Find me a movie by Steven Spielberg". * Ongoing work.</p><p>Query Find me a movie by Steven Spielberg  Intent classification is a classification problem that predicts the intent label y i and slot filling is a sequence labeling task that tags the input word sequence x = (x 1 , x 2 , ? ? ? , x T ) with the slot label sequence y s = (y s 1 , y s 2 , ? ? ? , y s T ). Recurrent neural network (RNN) based approaches, particularly gated recurrent unit (GRU) and long short-term memory (LSTM) models, have achieved state-ofthe-art performance for intent classification and slot filling. Recently, several joint learning methods for intent classification and slot filling were proposed to exploit and model the dependencies between the two tasks and improve the performance over independent models <ref type="bibr" target="#b4">(Guo et al., 2014;</ref><ref type="bibr" target="#b5">Hakkani-T?r et al., 2016;</ref><ref type="bibr" target="#b9">Liu and Lane, 2016;</ref><ref type="bibr" target="#b3">Goo et al., 2018)</ref>. Prior work has shown that attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> helps RNNs to deal with long-range dependencies. Hence, attention-based joint learning methods were proposed and achieved the state-of-the-art performance for joint intent classification and slot filling <ref type="bibr" target="#b9">(Liu and Lane, 2016;</ref><ref type="bibr" target="#b3">Goo et al., 2018)</ref>.</p><p>Lack of human-labeled data for NLU and other natural language processing (NLP) tasks results in poor generalization capability. To address the data sparsity challenge, a variety of techniques were proposed for training general purpose language representation models using an enormous amount of unannotated text, such as ELMo <ref type="bibr" target="#b12">(Peters et al., 2018)</ref> and Generative Pre-trained Transformer (GPT) <ref type="bibr" target="#b13">(Radford et al., 2018)</ref>. Pre-trained models can be fine-tuned on NLP tasks and have achieved significant improvement over training on task-specific annotated data. More recently, a pretraining technique, Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>, was proposed and has created stateof-the-art models for a wide variety of NLP tasks, including question answering (SQuAD v1.1), natural language inference, and others.</p><p>However, there has not been much effort in exploring BERT for NLU. The technical contributions in this work are two folds: 1) we explore the BERT pre-trained model to address the poor generalization capability of NLU; 2) we propose a joint intent classification and slot filling model based on BERT and demonstrate that the proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to attentionbased RNN models and slot-gated models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Deep learning models have been extensively explored in NLU. According to whether intent classification and slot filling are modeled separately or jointly, we categorize NLU models into independent modeling approaches and joint modeling approaches.</p><p>Approaches for intent classification include CNN <ref type="bibr" target="#b6">(Kim, 2014;</ref><ref type="bibr" target="#b22">Zhang et al., 2015)</ref>, LSTM (Ravuri and Stolcke, 2015), attentionbased CNN <ref type="bibr" target="#b24">(Zhao and Wu, 2016)</ref>, hierarchical attention networks <ref type="bibr" target="#b20">(Yang et al., 2016</ref>), adversarial multi-task learning <ref type="bibr" target="#b10">(Liu et al., 2017)</ref>, and others. Approaches for slot filling include CNN <ref type="bibr" target="#b17">(Vu, 2016)</ref>, deep LSTM <ref type="bibr" target="#b21">(Yao et al., 2014)</ref>, RNN-EM <ref type="bibr" target="#b11">(Peng et al., 2015)</ref>, encoder-labeler deep LSTM <ref type="bibr" target="#b8">(Kurata et al., 2016)</ref>, and joint pointer and attention <ref type="bibr" target="#b23">(Zhao and Feng, 2018)</ref>, among others.</p><p>Joint modeling approaches include CNN-CRF <ref type="bibr" target="#b19">(Xu and Sarikaya, 2013)</ref>, RecNN <ref type="bibr" target="#b4">(Guo et al., 2014)</ref>, joint RNN-LSTM (Hakkani- <ref type="bibr" target="#b5">T?r et al., 2016)</ref>, attention-based BiRNN <ref type="bibr" target="#b9">(Liu and Lane, 2016)</ref>, and slot-gated attention-based model <ref type="bibr" target="#b3">(Goo et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>We first briefly describe the BERT model <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> and then introduce the proposed joint model based on BERT. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates a highlevel view of the proposed model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT</head><p>The model architecture of BERT is a multi-layer bidirectional Transformer encoder based on the original Transformer model <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. The input representation is a concatenation of WordPiece embeddings , positional embeddings, and the segment embedding. Specially, for single sentence classification and tagging tasks, the segment embedding has no discrimination. A special classification embedding ([CLS]) is inserted as the first token and a special token ([SEP]) is added as the final token. Given an input token sequence</p><formula xml:id="formula_0">x = (x 1 , . . . , x T ), the out- put of BERT is H = (h 1 , . . . , h T ).</formula><p>The BERT model is pre-trained with two strategies on large-scale unlabeled text, i.e., masked language model and next sentence prediction. The pre-trained BERT model provides a powerful context-dependent sentence representation and can be used for various target tasks, i.e., intent classification and slot filling, through the finetuning procedure, similar to how it is used for other NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Intent Classification and Slot Filling</head><p>BERT can be easily extended to a joint intent classification and slot filling model. Based on the hidden state of the first special token ([CLS]), denoted h 1 , the intent is predicted as:</p><formula xml:id="formula_1">y i = softmax(W i h 1 + b i ) ,<label>(1)</label></formula><p>For slot filling, we feed the final hidden states of other tokens h 2 , . . . , h T into a softmax layer to classify over the slot filling labels. To make this procedure compatible with the WordPiece tokenization, we feed each tokenized input word into a WordPiece tokenizer and use the hidden state corresponding to the first sub-token as input to the  </p><formula xml:id="formula_2">y s n = softmax(W s h n + b s ) , n ? 1 . . . N (2)</formula><p>where h n is the hidden state corresponding to the first sub-token of word x n . To jointly model intent classification and slot filling, the objective is formulated as:</p><formula xml:id="formula_3">p(y i , y s |x) = p(y i |x) N n=1 p(y s n |x) ,<label>(3)</label></formula><p>The learning objective is to maximize the conditional probability p(y i , y s |x). The model is finetuned end-to-end via minimizing the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conditional Random Field</head><p>Slot label predictions are dependent on predictions for surrounding words. It has been shown that structured prediction models can improve the slot filling performance, such as conditional random fields (CRF). <ref type="bibr" target="#b25">Zhou and Xu (2015)</ref> improves semantic role labeling by adding a CRF layer for a BiLSTM encoder. Here we investigate the efficacy of adding CRF for modeling slot label dependencies, on top of the joint BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>We evaluate the proposed model on two public benchmark datasets, ATIS and Snips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>The ATIS dataset <ref type="bibr" target="#b15">(T?r et al., 2010)</ref> is widely used in NLU research, which includes audio recordings of people making flight reservations. We use the same data division as <ref type="bibr" target="#b3">Goo et al. (2018)</ref> for both datasets. The training, development and test sets contain 4,478, 500 and 893 utterances, respectively. There are 120 slot labels and 21 intent types for the training set. We also use Snips <ref type="bibr" target="#b1">(Coucke et al., 2018)</ref>, which is collected from the Snips personal voice assistant. The training, development and test sets contain 13,084, 700 and 700 utterances, respectively. There are 72 slot labels and 7 intent types for the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We use English uncased BERT-Base model 1 , which has 12 layers, 768 hidden states, and 12 heads. BERT is pre-trained on BooksCorpus (800M words) <ref type="bibr" target="#b26">(Zhu et al., 2015)</ref> and English Wikipedia (2,500M words). For fine-tuning, all hyper-parameters are tuned on the development set. The maximum length is 50. The batch size is 128. Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate of 5e-5. The dropout probability is 0.1. The maximum number of epochs is selected from <ref type="bibr">[1,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">30,</ref><ref type="bibr">40]</ref>.   <ref type="table" target="#tab_3">Table 2</ref> shows the model performance as slot filling F1, intent classification accuracy, and sentence-level semantic frame accuracy on the Snips and ATIS datasets.  The first group of models are the baselines and it consists of the state-of-the-art joint intent classification and slot filling models: sequencebased joint model using BiLSTM <ref type="bibr" target="#b5">(Hakkani-T?r et al., 2016)</ref>, attention-based model <ref type="bibr" target="#b9">(Liu and Lane, 2016)</ref>, and slot-gated model <ref type="bibr" target="#b3">(Goo et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Epochs Intent Slot</head><p>The second group of models includes the proposed joint BERT models. As can be seen from <ref type="table" target="#tab_3">Table 2</ref>, joint BERT models significantly outperform the baseline models on both datasets. On Snips, joint BERT achieves intent classification accuracy of 98.6% (from 97.0%), slot filling F1 of 97.0% (from 88.8%), and sentence-level semantic frame accuracy of 92.8% (from 75.5%). On ATIS, joint BERT achieves intent classification accuracy of 97.5% (from 94.1%), slot filling F1 of 96.1% (from 95.2%), and sentence-level semantic frame accuracy of 88.2% (from 82.6%). Joint BERT+CRF replaces the softmax classifier with CRF and it performs comparably to BERT, probably due to the self-attention mechanism in Transformer, which may have sufficiently modeled the label structures.</p><p>Compared to ATIS, Snips includes multiple domains and has a larger vocabulary. For the more complex Snips dataset, joint BERT achieves a large gain in the sentence-level semantic frame accuracy, from 75.5% to 92.8% (22.9% relative). This demonstrates the strong generalization capability of joint BERT model, considering that it is pre-trained on large-scale text from mismatched domains and genres (books and wikipedia). On ATIS, joint BERT also achieves significant improvement on the sentence-level semantic frame accuracy, from 82.6% to 88.2% (6.8% relative).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Analysis and Case Study</head><p>We conduct ablation analysis on Snips, as shown in <ref type="table" target="#tab_5">Table 3</ref>. Without joint learning, the accuracy of intent classification drops to 98.0% (from 98.6%), and the slot filling F1 drops to 95.8% (from 97.0%). We also compare the joint BERT model with different fine-tuning epochs. The joint BERT model fine-tuned with only 1 epoch already outperforms the first group of models in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We further select a case from Snips, as in <ref type="table" target="#tab_6">Table 4</ref>, showing how joint BERT outperforms the slot-gated model <ref type="bibr" target="#b3">(Goo et al., 2018)</ref> by exploiting the language representation power of BERT to improve the generalization capability. In this case, "mother joan of the angels" is wrongly predicted by the slot-gated model as an object name and the intent is also wrong. However, joint BERT correctly predicts the slot labels and intent because "mother joan of the angels" is a movie entry in Wikipedia. The BERT model was pre-trained partly on Wikipedia and possibly learned this information for this rare phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a joint intent classification and slot filling model based on BERT, aiming at addressing the poor generalization capability of traditional NLU models. Experimental results show that our proposed joint BERT model outperforms BERT models modeling intent classification and slot filling separately, demonstrating the efficacy of exploiting the relationship between the two tasks. Our proposed joint BERT model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on ATIS and Snips datasets over previous state-of-the-art models. Future work includes evaluations of the proposed approach on other large-scale and more complex NLU datasets, and exploring the efficacy of combining external knowledge with BERT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A high-level view of the proposed model. The input query is "play the song little robin redbreast".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Query</head><label></label><figDesc>need to see mother joan of the angels in one second Gold, predicted by joint BERT correctly Intent SearchScreeningEvent Slots O O O B-movie-name I-movie-name I-movie-name I-movie-name I-movie-name B-timeRange I-timeRange I-timeRange Predicted by Slot-Gated Model (Goo et al., 2018) Intent BookRestaurant Slots O O O B-object-name I-object-name I-object-name I-object-name I-object-name B-timeRange I-timeRange I-timeRange</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example from user query to semantic frame.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>NLU performance on Snips and ATIS datasets. The metrics are intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy (%). The results for the first group of models are cited from<ref type="bibr" target="#b3">Goo et al. (2018)</ref>.</figDesc><table /><note>softmax classifier.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation Analysis for the Snips dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>A case in the Snips dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google-research/bert</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
		<idno>abs/1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chih-Wen Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Kai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chieh</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint semantic utterance classification and slot filling with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?khan</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2014.7078634</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="554" to="559" />
			<date type="published" when="2014-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?khan</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Asli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-402</idno>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08" />
			<biblScope unit="page" from="715" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A meeting of SIGDAT, a Special Interest Group of the ACL</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>EMNLP 2014</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Leveraging sentence-level information with encoder LSTM for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1601.01530</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-1352</idno>
		<imprint>
			<date type="published" when="2016-09-08" />
			<biblScope unit="page" from="685" to="689" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1001</idno>
		<imprint>
			<date type="published" when="2017-07-30" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with external memory for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-25207-0_3</idno>
	</analytic>
	<monogr>
		<title level="m">NLPCC 2015</title>
		<meeting><address><addrLine>Nanchang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-09" />
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA; Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report</title>
		<meeting><address><addrLine>OpenAI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network and LSTM models for lexical utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2015</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-06" />
			<biblScope unit="page" from="135" to="139" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What is left to be understood in atis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?khan</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2010.5700816</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Spoken Language Technology Workshop, SLT 2010</title>
		<meeting><address><addrLine>Berkeley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12-12" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6000" to="6010" />
			<pubPlace>Long Beach, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequential convolutional neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-395</idno>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016, 17th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08" />
			<biblScope unit="page" from="3250" to="3254" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular CRF for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU.2013.6707709</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting><address><addrLine>Olomouc, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-08" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2014.7078572</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="189" to="194" />
			<date type="published" when="2014-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving slot filling in spoken language understanding with joint pointer and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="426" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attentionbased convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-354</idno>
		<imprint>
			<date type="published" when="2016-09-08" />
			<biblScope unit="page" from="705" to="709" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2015</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
