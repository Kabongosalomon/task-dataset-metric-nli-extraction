<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
							<email>liuwen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
							<email>liandz@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events. All codes are released in https://github. com/StevenLiuWen/ano_pred_cvpr2018.</p><p>We also compare the video prediction network based and Auto-Encoder network based anomaly detection. Here for Auto-Encoder network based anomaly detection, we use the Conv-AE [13] which is the latest work and achieves state-of-the-art performance for anomaly detection. Because of the capacity of deep neural network, Auto-Encoder</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection in videos refers to the identification of events that do not conform to expected behavior <ref type="bibr" target="#b2">[3]</ref>. It is an important task because of its applications in video surveillance. However, it is extremely challenging because abnormal events are unbounded in real applications, and it is almost infeasible to gather all kinds of abnormal events and tackle the problem with a classification method. * The authors contribute equally and are listed in alphabetical order. ? Corresponding author. Lots of efforts have been made for anomaly detection <ref type="bibr" target="#b19">[20]</ref>[13] <ref type="bibr" target="#b22">[23]</ref>. Of all these work, the idea of feature reconstruction for normal training data is a commonly used strategy. Further, based on the features used, all existing methods can be roughly categorized into two categories: i) hand-crafted features based methods <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b19">[20]</ref>. They represent each video with some hand-crafted features including appearance and motion ones. Then a dictionary is learnt to reconstruct normal events with small reconstruction errors. It is expected that the features corresponding to abnormal events would have larger reconstruction errors. But since the dictionary is not trained with abnormal events and it is usually overcomplete, we cannot guarantee the expectation. ii) deep learning based methods <ref type="bibr" target="#b12">[13]</ref>[5] <ref type="bibr" target="#b25">[26]</ref>. They usually learn a deep neural network with an Auto-Encoder way and they enforce it to reconstruct normal events with small reconstruction errors. But the capacity of deep neural network is high, and larger reconstruction errors for abnormal events do not necessarily happen. Thus, we can see that almost all training data reconstruction based methods cannot guaran-  <ref type="figure">Figure 2</ref>. The pipeline of our video frame prediction network. Here we adopt U-Net as generator to predict next frame. To generate high quality image, we adopt the constraints in terms of appearance (intensity loss and gradient loss) and motion (optical flow loss).</p><p>Here Flownet is a pretrained network used to calculate optical flow. We also leverage the adversarial training to discriminate whether the prediction is real or fake.</p><p>tee the finding of abnormal events.</p><p>It is interesting that even though anomaly is defined as those events do not conform the expectation, most existing work in computer vision solve the problem within a framework of reconstructing training data <ref type="bibr" target="#b19">[20]</ref>[38] <ref type="bibr" target="#b12">[13]</ref>. We presume it is probable that the video frame prediction is far from satisfactory at that time. Recently, as the emergence of Generative Adversarial Network (GAN) <ref type="bibr" target="#b11">[12]</ref>, the performance of video prediction has been greatly advanced <ref type="bibr" target="#b24">[25]</ref>. In this paper, rather than reconstructing training data for anomaly detection, we propose to identify abnormal events by comparing them with their expectation, and introduce a future video frame prediction based anomaly detection method. Specifically, given a video clip, we predict the future frame based on its historical observation. We first train a predictor that can well predict the future frame for normal training data. In the testing phase, if a frame agrees with its prediction, it potentially corresponds to a normal event. Otherwise, it potentially corresponds to an abnormal event. Thus a good predictor is a key to our task. We implement our predictor with an U-Net <ref type="bibr" target="#b27">[28]</ref> network architecture given its good performance at image-to-image translation <ref type="bibr" target="#b14">[15]</ref>. First, we impose a constraint on the appearance by enforcing the intensity and gradient maps of the predicted frame to be close to its ground truth; Then, motion is another important feature for video characterization, and a good prediction should be consistent with real object motion. Thus we propose to introduce a motion constraint by enforcing the optical flow between predicted frames to be close to their ground truth. Further, we also add a Generative Adversarial Network (GAN) <ref type="bibr" target="#b11">[12]</ref> module into our framework in light of its success for video generation <ref type="bibr" target="#b24">[25]</ref> and image generation <ref type="bibr" target="#b8">[9]</ref>.</p><p>We summarize our contributions as follows: i) We propose a future frame prediction based framework for anomaly detection. Our solution agrees with the concept of anomaly detection that normal events are predictable while abnormal ones are unpredictable. Thus our solution is more suitable for anomaly detection. To the best of our knowledge, it is the first work that leverages video prediction for anomaly detection; ii) For the video frame prediction framework, other than enforcing predicted frames to be close to their ground truth in spatial space, we also enforce the optical flow between predicted frames to be close to their optical flow ground truth. Such a temporal constraint is shown to be crucial for video frame prediction, and it is also the first work that leverages a motion constraint for anomaly detection; iii) Experiments on toy dataset validate the robustness to the uncertainty for normal events, which validates the robustness of our method. Further, extensive experiments on real datasets show that our method outperforms all existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hand-crafted Features Based Anomaly Detection</head><p>Hand-crafted features based anomaly detection is mainly comprised of three modules: i) extracting features; In this module, the features are either hand-crafted or learnt on training set; ii) learning a model to characterize the distribution of normal scenarios or encode regular patterns; iii) identifying the isolated clusters or outliers as anomalies. For feature extraction module, early work usually utilizes low-level trajectory features, a sequence of image coordinates, to represent the regular patterns <ref type="bibr" target="#b31">[32]</ref> <ref type="bibr" target="#b34">[35]</ref>. However, these methods are not robust in complex or crowded scenes with multiple occlusions and shadows, because trajectory features are based on object tracking and it is very easy to fail in these cases. Taking consideration of the shortcom-ings of trajectory features, low-level spatial-temporal features, such as histogram of oriented gradients (HOG) <ref type="bibr" target="#b26">[27]</ref>, histogram of oriented flows (HOF) <ref type="bibr" target="#b6">[7]</ref> are widely used. Based on spatial-temporal features, Zhang et al. <ref type="bibr" target="#b36">[37]</ref> exploit a Markov random filed (MRF) for modeling the normal patterns. Adam et al. <ref type="bibr" target="#b1">[2]</ref> characterize the regularly local histograms of optical flow by an exponential distribution. Kim and Grauman <ref type="bibr" target="#b15">[16]</ref> model the local optical flow pattern with a mixture of probabilistic PCA (MPPCA). Mahadevan et al. <ref type="bibr" target="#b22">[23]</ref> fit a Gaussian mixture model to mixture of dynamic textures (MDT). Besides these statistic models, sparse coding or dictionary learning is also a popular approach to encode the normal patterns <ref type="bibr" target="#b37">[38]</ref>[20] <ref type="bibr" target="#b5">[6]</ref>. The fundamental underlying assumption of these methods is that any regular pattern can be linearly represented as a linear combination of basis of a dictionary which encodes normal patterns on training set. Therefore, a pattern is considered as an anomaly if its reconstruction error is high and vice verse. However, optimizing the sparse coefficients is usually timeconsuming in sparse reconstruction based methods. In order to accelerate both in training and testing phase, Lu et al <ref type="bibr" target="#b19">[20]</ref> propose to discard the sparse constraint and learn multiple dictionaries to encode normal scale-invariant patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning Based Anomaly Detection.</head><p>Deep learning approaches have demonstrated their successes in many computer vision tasks <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b10">[11]</ref> as well as anomaly detection <ref type="bibr" target="#b12">[13]</ref>. In the work <ref type="bibr" target="#b35">[36]</ref>, Xu et al. design a multi-layer auto-encoder for feature learning, which demonstrates the effectiveness of deep learning features. In another work <ref type="bibr" target="#b12">[13]</ref>, a 3D convolutional auto-encoder (Conv-AE) is proposed by Hasan to model regular frames. Further, motivated by the observation that Convolutional Neural Networks (CNN) has strong capability to learn spatial features, while Recurrent Neural Network (RNN)and its long short term memory (LSTM) variant have been widely used for sequential data modeling. Thus, by taking both advantages of CNN and RNN, <ref type="bibr" target="#b4">[5]</ref>[21] leverage a Convolutional LSTMs Auto-Encoder (ConvLSTM-AE) to model normal appearance and motion patterns at the same time, which further boosts the performance of the Conv-AE based solution. In <ref type="bibr" target="#b21">[22]</ref>, Luo et al. propose a temporally coherent sparse coding based method which can map to a stacked RNN framework. Besides, Ryota et al. <ref type="bibr" target="#b13">[14]</ref> combine detection and recounting of abnormal events. However, all these anomaly detections are based on the reconstruction of regular training data, even though all these methods assume that abnormal events would correspond to larger reconstruction errors, due to the good capacity and generalization of deep neural network, this assumption does not necessarily hold. Therefore, reconstruction errors of normal and abnormal events will be similar, resulting in less discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video Frame Prediction</head><p>Recently, prediction learning is attracting more and more researchers' attention in light of its potential applications in unsupervised feature learning for video representation <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b28">[29]</ref>, Shi et al. propose to modify original LSTM with ConvLSTM and use it for precipitation forecasting. In <ref type="bibr" target="#b24">[25]</ref>, a multi-scale network with adversarial training is proposed to generate more natural future frames in videos. In <ref type="bibr" target="#b18">[19]</ref>, a predictive neural network is designed and each layer in the network also functions as making local predictions and only forwarding deviations. All aforementioned work focuses on how to directly predict future frames. Different from these work, recently, people propose to predict transformations needed for generating future frames <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b3">[4]</ref>, which further boosts the performance of video prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Future Frame Prediction Based Anomaly Detection Method</head><p>Since anomaly detection is the identification of events that do not conform the expectation, it is more natural to predict future video frames based on previous video frames, and compare the prediction with its ground truth for anomaly detection. Thus we propose to leverage video prediction for anomaly detection. To generate a high quality video frame, most existing work <ref type="bibr" target="#b14">[15]</ref>[25] only considers appearance constraints by imposing intensity loss <ref type="bibr" target="#b24">[25]</ref>, gradient loss <ref type="bibr" target="#b24">[25]</ref>, or adversarial training loss <ref type="bibr" target="#b14">[15]</ref>. However, only appearance constraints cannot guarantee to characterize the motion information well. Besides spatial information, temporal information is also an important feature of videos. So we propose to add an optical flow constraint into the objective function to guarantee the motion consistency for normal events in training set, which further boosts the performance for anomaly detection, as shown in the experiment section (section 4.5 and 4.6). It is worth noting abnormal events can be justified by either appearance (A giant monster appears in a shopping mall) or motion (A pickpocket walks away from an unlucky guy), and our future frame prediction solution leverages both the appearance and motion loss for normal events, therefore these abnormal events can be easily identified by comparing the prediction and ground truth. Thus the appearance and motion losses based video prediction are more consistent with anomaly detection.</p><p>Mathematically, given a video with consecutive t frames I 1 , I 2 , . . . , I t , we sequentially stack all these frames and use them to predict a future frame I t+1 . We denote our prediction as? t+1 . To make? t+1 close to I t+1 , we minimize their distance regarding intensity as well as gradient. To preserve the temporal coherence between neighboring frames, we enforce the optical flow between I t+1 and I t and that between? t+1 and I t to be close. Finally, the difference  between a future frame's prediction and itself determines whether it is normal or abnormal. The network architecture of our framework is shown in <ref type="figure">Fig. 2</ref>. Next, we will introduce all the components of our framework in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Future Frame Prediction</head><p>The network commonly used for frame generation or image generation in existing work <ref type="bibr" target="#b24">[25]</ref>[13] usually contains two modules: i) an encoder which extracts features by gradually reducing the spatial resolution; and ii) a decoder which gradually recovers the frame by increasing the spatial resolution. However, such a solution confronts with the gradient vanishing problem and information imbalance in each layer. To avoid this, U-Net <ref type="bibr" target="#b27">[28]</ref> is proposed by adding a shortcut between a high level layer and a low level layer with the same resolution. Such a manner suppresses gradient vanishing and results in information symmetry. We slightly modify U-Net for future frame prediction in our implementation. Specifically, for each two convolution layers, we keep output resolution unchanged. Consequently, it does not need the crop and resize operations anymore when adding shortcuts. The details of this network are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The kernel sizes of all convolution and deconvolution are set to 3 ? 3 and that of max pooling layers are set to 2 ? 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Constraints on Intensity and Gradient</head><p>To make the prediction close to its ground truth, following the work <ref type="bibr" target="#b24">[25]</ref>, intensity and gradient difference are used. The intensity penalty guarantees the similarity of all pixels in RGB space, and the gradient penalty can sharpen the generated images. Specifically, we minimize the 2 distance between a predicted frame? and its ground true I in intensity space as follows:</p><formula xml:id="formula_0">L int (?, I) = ? ? I 2 2<label>(1)</label></formula><p>Further, we define the gradient loss by following previous work <ref type="bibr" target="#b24">[25]</ref> as follows:</p><formula xml:id="formula_1">L gd (?, I) = i,j |? i,j ?? i?1,j | ? |I i,j ? I i?1,j | 1 + |? i,j ?? i,j?1 | ? |I i,j ? I i,j?1 | 1<label>(2)</label></formula><p>where i, j denote the spatial index of a video frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Constraint on Motion</head><p>Previous work <ref type="bibr" target="#b24">[25]</ref> only considers the difference between intensity and gradient for future frame generation, and it can not guarantee to predict a frame with the correct motion. This is because even a small change occurs in terms of the pixel intensity of all pixels in a predicted frame, even though it corresponds to a small prediction error in terms of gradient and intensity, it may result in totally different optical flow, which is a good estimator of motion <ref type="bibr" target="#b29">[30]</ref>. So it is desirable to guarantee the correctness of motion prediction. Especially for anomaly detection, the coherence of motion is an important factor for the evaluation of normal events. Therefore, we introduce a temporal loss defined as the difference between optical flow of prediction frames and ground truth. However, the calculation of optical flow is not easy. Recently, a CNN based approach has been proposed for optical flow estimation <ref type="bibr" target="#b7">[8]</ref>. Thus we use the Flownet <ref type="bibr" target="#b7">[8]</ref> for optical flow estimation. We denote f as the Flownet, then the loss in terms of optical flow can be expressed as follows:</p><formula xml:id="formula_2">L op = f (? t+1 , I t ) ? f (I t+1 , I t ) 1<label>(3)</label></formula><p>In our implementation, f is pre-trained on a synthesized dataset <ref type="bibr" target="#b7">[8]</ref>, and all the parameters in f are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adversarial Training</head><p>Generative adversarial networks (GAN) have demonstrated its usefulness for image and video generation <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b24">[25]</ref>. By following <ref type="bibr" target="#b24">[25]</ref>, we also leverage a variant of GAN (Least Square GAN <ref type="bibr" target="#b23">[24]</ref>) module for generating a more realistic frame. Usually GAN contains a discriminative network D and a generator network G. G learns to generate frames that are hard to be classified by D, while D aims to discriminate the frames generated by G. Ideally, when G is well trained, D cannot predict better than chance. In practice, adversarial training is implemented with an alternative update manner. Moreover, we treat the U-Net based prediction network as G. As for D, we follow <ref type="bibr" target="#b14">[15]</ref> and utilize a patch discriminator which means each output scalar of D corresponds a patch of an input image. Totally, the training schedule is illustrated as follows:</p><p>Training D. The goal of training D is to classify I t+1 into class 1 and G(I 1 , I 2 , ..., I t ) =? t+1 into class 0, where 0 and 1 represent fake and genuine labels, respectively. When training D, we fix the weights of G, and a Mean Square Error (MSE) loss function is imposed:</p><formula xml:id="formula_3">L D adv (?, I) = i,j 1 2 L M SE (D(I) i,j , 1) + i,j 1 2 L M SE (D(?) i,j , 0)<label>(4)</label></formula><p>where i, j denotes the spatial patches indexes and L M SE is a MSE function, which is defined as follows:</p><formula xml:id="formula_4">L M SE (? , Y ) = (? ? Y ) 2<label>(5)</label></formula><p>where Y takes values in {0,1} and? ? [0, 1] Training G. The goal of training G is to generate frames where D classify them into class 1. When training G, the weights of D are fixed. Again, a MSE function is imposed as follows:</p><formula xml:id="formula_5">L G adv (?) = i,j 1 2 L M SE (D(?) i,j , 1)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective Function</head><p>We combine all these constraints regarding appearance, motion, and adversarial training, into our objective function, and arrive at the following objective function:</p><formula xml:id="formula_6">L G =? int L int (? t+1 , I t+1 ) + ? gd L gd (? t+1 , I t+1 ) + ? op L op + ? adv L G adv (? t+1 )<label>(7)</label></formula><p>When we train D, we use the following loss function:</p><formula xml:id="formula_7">L D = L D adv (? t+1 , I t+1 )<label>(8)</label></formula><p>To train the network, the intensity of pixels in all frames are normalized to [-1, 1] and the size of each frame is resized to 256 ? 256. We set t = 4 and use a random clip of 5 sequential frames which is the same with <ref type="bibr" target="#b24">[25]</ref>. Adam <ref type="bibr" target="#b16">[17]</ref> based Stochastic Gradient Descent method is used for parameter optimization. The mini-batch size is 4. For gray scale datasets, the learning rate of generator and discriminator are set to 0.0001 and 0.00001, respectively. While for color scale datasets, the learning rate of generator and discriminator start from 0.0002 and 0.00002, respectively. For different datasets, the coefficient factors of ? int , ? gd , ? op and ? adv are slightly different. An easy way is to set ? int , ? gd , ? op and ? adv as 1.0, 1.0, 2.0 and 0.05, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Anomaly Detection on Testing Data</head><p>We assume that normal events can be well predicted. Therefore, we can use the difference between predicted frame? and its ground truth I for anomaly prediction. MSE is one popular way to measure the quality of predicted images by computing a Euclidean distance between the prediction and its ground truth of all pixels in RGB color space. However, Mathieu <ref type="bibr" target="#b24">[25]</ref> shows that Peak Signal to Noise Ratio (PSNR) is a better way for image quality assessment, shown as following:</p><formula xml:id="formula_8">P SN R(I,?) = 10 log 10 [max? ] 2 1 N N i=0 (I i ?? i ) 2</formula><p>High PSNR of the t-th frame indicates that it is more likely to be normal. After calculating each frame's PSNR of each testing video, following the work <ref type="bibr" target="#b24">[25]</ref>, we normalize PSNR of all frames in each testing video to the range [0, 1] and calculate the regular score for each frame by using the following equation:</p><formula xml:id="formula_9">S(t) = P SN R(I t ,? t ) ? min t P SN R(I t ,? t ) max t P SN R(I t ,? t ) ? min t P SN R(I t ,? t )</formula><p>Therefore, we can predict whether a frame is normal or abnormal based its score S(t). One can set a threshold to distinguish regular or irregular frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed method as well as the functionalities of different components on three publicly available anomaly detection datasets, including the CUHK Avenue dataset <ref type="bibr" target="#b19">[20]</ref>, the UCSD Pedestrian dataset <ref type="bibr" target="#b22">[23]</ref> and the ShanghaiTech dataset <ref type="bibr" target="#b21">[22]</ref>. We further use a toy dataset to validate the robustness of our method, i.e., even if there exists some uncertainties in normal events, our method can still correctly classify normal and abnormal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Here we briefly introduce the datasets used in our experiments. Some samples are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>? CUHK Avenue dataset contains 16 training videos and 21 testing ones with a total of 47 abnormal events, including throwing objects, loitering and running. The size of people may change because of the camera position and angle.</p><p>? The UCSD dataset contains two parts: The UCSD Pedestrian 1 (Ped1) dataset and the UCSD Pedestrian 2 (Ped2) dataset. The UCSD Pedestrian 1 (Ped1) dataset includes 34 training videos and 36 testing ones with CUHK Avenue UCSD Ped1 UCSD Ped2 ShanghaiTech MPPCA <ref type="bibr" target="#b15">[16]</ref> N/A 59.0% 69.3% N/A MPPC+SFA <ref type="bibr" target="#b22">[23]</ref> N/A 66.8% 61.3% N/A MDT <ref type="bibr" target="#b22">[23]</ref> N/A 81.8% 82.9% N/A Conv-AE <ref type="bibr" target="#b12">[13]</ref> 80.0% 75.0% 85.0% 60.9% Del et al. <ref type="bibr" target="#b9">[10]</ref> 78.3% N/A N/A N/A ConvLSTM-AE <ref type="bibr" target="#b20">[21]</ref> 77.0% 75.5% 88.1% N/A Unmasking <ref type="bibr" target="#b30">[31]</ref> 80.6% 68.4% 82.2% N/A Hinami et al. <ref type="bibr" target="#b13">[14]</ref> N/A N/A 92.2% N/A Stacked RNN <ref type="bibr" target="#b21">[22]</ref> 81  Usually different methods are evaluated on these two parts separately.</p><p>? The ShanghaiTech dataset is a very challenging anomaly detection dataset. It contains 330 training videos and 107 testing ones with 130 abnormal events. Totally, it consists of 13 scenes and various anomaly types. Following the setting used in <ref type="bibr" target="#b21">[22]</ref>, we train the model on all scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>In the literature of anomaly detection <ref type="bibr" target="#b19">[20]</ref>[23], a popular evaluation metric is to calculate the Receiver Operation Characteristic (ROC) by gradually changing the threshold of regular scores. Then the Area Under Curve (AUC) is cumulated to a scalar for performance evaluation. A higher value indicates better anomaly detection performance. In this paper, following the work <ref type="bibr" target="#b21">[22]</ref>, we leverage frame-level AUC for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Methods</head><p>In this section, we compare our method with different hand-craft features based method <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b21">[22]</ref>. The AUC of different methods is listed in <ref type="table" target="#tab_1">Table 1</ref>. We can see that our method outperforms all existing methods (around (3-5)% on all datasets), which demonstrates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Design of Prediction Network</head><p>In our anomaly detection framework, the future frame prediction network is an important module. To evaluate how different prediction networks affect the performance of anomaly detection, we compare our U-Net prediction network with Beyond Mean Square Error (Beyond-MSE) <ref type="bibr" target="#b24">[25]</ref> which achieves state-of-the-art performance for video generation. Beyond-MSE leverages a multi-scale prediction network to gradually generate video frames with larger spatial resolution. Because of its multi-scale strategy, it is much slower than U-Net. To be consistent with Beyond-MSE, we adapt our network architecture by removing the motion constraint and only use the intensity loss, the gradient loss and adversarial training in our U-Net based solution.</p><p>Quantitative comparison for anomaly detection. We first compute the gap between average score of normal frames and that of abnormal frames, denoted as ? s . We compare the result of U-Net with that of Beyond-MSE on the Ped1 and Ped2 datasets, respectively. Larger ? s means the network can be more capable to distinguish normal and abnormal patterns. Then, we also compare the U-Net based solution and Beyond-MSE with the AUC metric on the Ped1 and Ped2 datasets, respectively. We demonstrate the results in <ref type="table" target="#tab_4">Table 2</ref>. We can see that our method both achieves a larger ? s and higher AUC than Beyond-MSE, which show that our network is more suitable for anomaly detection than Beyond-MSE. Therefore, we adapt U-Net architecture as our prediction network. As we aforementioned, the results listed here do not contain motion constraint, which would further boost the AUC. ? !" P P P P   <ref type="figure" target="#fig_5">Figure 6</ref>. We firstly compute the average score for normal frames and that for abnormal frames in the testing set of the Ped1, Ped2 and Avenue datasets. Then, we calculate the difference of these two scores(?s) to measure the ability of our method and Conv-AE to discriminate normal and abnormal frames. A larger gap(?s) corresponds to small false alarm rate and higher detection rate.</p><formula xml:id="formula_10">? #$ - P P P ? %$&amp; - - P P ? '( - - - P )*</formula><p>The results show that our method consistently outperforms Conv-AE in term of the score gap between normal and abnormal events. To evaluate the importance of motion constraint for video frame generation as well as anomaly detection, we conduct the experiment by removing the constraint from the objective in the training. Then we compare such a baseline with our method.</p><p>Evaluation of motion constraint with optical flow maps.</p><p>We show the optical flow maps generated with/without motion constraint in <ref type="figure" target="#fig_6">Fig. 7</ref>, we can see that the optical flow generated with motion constraint is more consistent with ground truth, which shows that such motion constraint term helps our prediction network to capture motion information more precisely. We also compare the MSE between optical flow maps generated with/without motion constraint and the ground truth, which is 7.51 and 8.26, respectively. This further shows the effectiveness of motion constraint.</p><p>Quantitatively evaluation of motion with anomaly detection. The result in <ref type="table" target="#tab_6">Table 3</ref> shows that the model trained with motion constraint consistently achieves higher AUC than that without the constraint on Ped1 and Ped2 dataset. This also proves that it is necessary to explicitly impose the motion consistency constraint into the objective for anomaly detection.   based methods may well reconstruct normal and abnormal frames in the testing phase. To evaluate the performance of prediction network and Auto-Encoder one, we also utilize the aforementioned gap(? s ) between normal and abnormal scores. The result in <ref type="figure" target="#fig_5">Fig. 6</ref> shows that our solution always achieves higher gaps than Conv-AE, which validates the effectiveness of video prediction for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Evaluation with A Toy Dataset</head><p>We also design a toy pedestrian dataset for performance evaluation. In the training set, only a pedestrian walks on the road and he/she can choose different directions when he/she comes to a crossroad. In the testing set, there are some abnormal cases such as vehicles intruding, humans fighting, etc.. We have uploaded our toy dataset in the supplementary material. Totally, the training data contains 210 frames and testing data contains 1242 frames.</p><p>It is interesting that the motion direction is sometimes also uncertain for normal events, for example, a pedestrian stands at the crossroad. Even though we cannot predict the motion well, we only cannot predict the next frame at a moment which leads a slightly instant drop in terms of PSNR. After observing the pedestrian for a while when the pedestrian has made his or her choice, it becomes predictable and PSNR would go up, shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. Therefore the uncertainty of normal events does not affect our solution too much. However, for the real abnormal events, for example, a truck breaks into the scene and hits the pedestrian, it would leads to a continuous lower PSNR, which facilitates the anomaly prediction. Totally, the AUC is 98.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Running Time</head><p>Our framework is implemented with NVIDIA GeForce TITAN GPUs and Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The average running time is about 25 fps, which contains both the video frame generation and anomaly prediction. We also report the running time of other methods such as 20 fps in <ref type="bibr" target="#b30">[31]</ref>, 150 fps <ref type="bibr" target="#b19">[20]</ref> and 0.5 fps in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Since normal events are predictable while abnormal events do not conform to the expectation, therefore we propose a future frame prediction network for anomaly detection. Specifically, we use a U-Net as our basic prediction network. To generate a more realistic future frame, other than adversarial training and constraints in appearance, we also impose a loss in temporal space to ensure the optical flow of predicted frames to be consistent with ground truth. In this way, we can guarantee to generate the normal events in terms of both appearance and motion, and the events with larger difference between prediction and ground truth would be classified as anomalies. Extensive experiments on three datasets show our method outperforms existing methods by a large margin, which proves the effectiveness of our method for anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some predicted frames and their ground truth in normal and abnormal events. Here the region is walking zone. When pedestrians are walking in the area, the frames can be well predicted. While for some abnormal events (a bicycle intrudes/ two men are fighting), the predictions are blurred and with color distortion. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of our main prediction network (U-Net). The resolutions of input and output are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Some samples including normal and abnormal frames in the UCSD, CUHK Avenue and ShanghaiTech datasets are illustrated. Red boxes denote anomalies in abnormal frames. 40 irregular events. All of these abnormal cases are about vehicles such as bicycles and cars. The UCSD Pedestrian 2 (Ped2) dataset contains 16 training videos and 12 testing videos with 12 abnormal events. The definition of anomaly for Ped2 is the same with Ped1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The evaluation of different components in our future frame prediction network in the Avenue dataset. Each column in the histogram corresponds to a method with different loss functions. We calculate the average scores of normal and abnormal events in the testing set. The gap is calculated by subtracting the abnormal score from the normal one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 6 .</head><label>6</label><figDesc>Impact of Different Losses for Anomaly Detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>The visualization of optical flow and the predicted images on the Ped1 dataset. The red boxes represent the difference of optical flow predicted by the model with/without motion constraint. We can see that the optical flow predicted by the model with motion constraint is closer to ground truth. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FightingFigure 8 .</head><label>8</label><figDesc>The visualization of predicted testing frames in our toy pedestrian dataset. There are two abnormal cases including vehicle intruding(left column) and humans fighting(right column). The orange circles correspond to normal events with uncertainty in prediction while the red ones correspond to abnormal events. It is noticeable that the predicted truck is blurred, because no vehicles appear in the training set. Further, in the fighting case, two persons cannot be predicted well because fighting motion never appear in the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>AUC of different methods on the Avenue, Ped1, Ped2 and ShanghaiTech datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>The gap (?s) and AUC of different prediction networks in the Ped1 and Ped2 datasets.</figDesc><table><row><cell>Ped1</cell><cell>Ped2</cell></row><row><cell cols="2">? s Beyond-MSE 0.200 75.8% 0.396 88.5% AUC AUC ? s U-Net 0.243 81.8% 0.435 93.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>AUC for anomaly detection of networks with/wo the motion constraint in Ped1 and Ped2.</figDesc><table><row><cell>Ped1</cell><cell>Ped2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We also analyze the impact of different loss functions for anomaly detection by ablating different terms gradually. We combine different losses to conduct experiments on the Avenue dataset. To evaluate how different losses affect the performance of anomaly detection, we also utilize the score gap(? s ) mentioned above. The larger gap represents the more discriminations between normal and abnormal frames. The results in <ref type="figure">Figure 5</ref> show more constraints usually achieve a higher gap as well as AUC value, and our method achieves the highest value under all settings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixedlocation monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video imagination from a single image with transformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04124</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01546</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<publisher>Winter semester</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
	<note>Multimedia and Expo (ICME</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">249</biblScope>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Goal-based trajectory analysis for unusual behaviour detection in intelligent surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="230" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Histograms of optical flow orientation for abnormal events detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Evaluation of Tracking and Surveillance (PETS), 2013 IEEE International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised adapted hmms for unusual event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
