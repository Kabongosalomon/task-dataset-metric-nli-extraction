<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Zand</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Etemad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Greenspan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ingenuity Labs Research Institute Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>6 DoF pose estimation, keypoint voting</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel keypoint voting scheme based on intersecting spheres, that is more accurate than existing schemes and allows for fewer, more disperse keypoints. The scheme is based upon the distance between points, which as a 1D quantity can be regressed more accurately than the 2D and 3D vector and offset quantities regressed in previous work, yielding more accurate keypoint localization. The scheme forms the basis of the proposed RCVPose method for 6 DoF pose estimation of 3D objects in RGB-D data, which is particularly effective at handling occlusions. A CNN is trained to estimate the distance between the 3D point corresponding to the depth mode of each RGB pixel, and a set of 3 disperse keypoints defined in the object frame. At inference, a sphere centered at each 3D point is generated, of radius equal to this estimated distance. The surfaces of these spheres vote to increment a 3D accumulator space, the peaks of which indicate keypoint locations. The proposed radial voting scheme is more accurate than previous vector or offset schemes, and is robust to disperse keypoints. Experiments demonstrate RCVPose to be highly accurate and competitive, achieving state-of-theart results on the LINEMOD (99.7%) and YCB-Video (97.2%) datasets, notably scoring +4.9% higher (71.1%) than previous methods on the challenging Occlusion LINEMOD dataset, and on average outperforming all other published results from the BOP benchmark for these 3 datasets. Our code is available at http://www.github.com/aaronwool/rcvpose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object pose estimation is an enabling technology for many applications including robot manipulation, human-robot interaction, augmented reality, and autonomous driving <ref type="bibr">[36,</ref><ref type="bibr">35,</ref><ref type="bibr">45]</ref>. It is challenging due to background clutter, occlusions, sensor noise, varying lighting conditions, and object symmetries. Traditional methods have tackled the problem by establishing correspondences between a known 3D model and image features <ref type="bibr" target="#b16">[15,</ref><ref type="bibr">40]</ref>. They have generally relied on hand-crafted features and therefore fail when objects are featureless or when scenes are very cluttered and occluded <ref type="bibr" target="#b19">[18,</ref><ref type="bibr">36]</ref>. Recent methods use deep learning and train end-to-end networks to directly regress an input image to a 6 DoF pose <ref type="bibr" target="#b20">[19,</ref><ref type="bibr">49]</ref>. For example, CNN-based techniques have been proposed which <ref type="figure" target="#fig_8">Fig. 1</ref>: Radial voting scheme: 3D scene point P i at depth d i projects to 2D image pixel p i . The network estimates radial distance r i from p i . Sphere S i is centered at P i with radius r i , and all accumulator space A voxels on the surface of S i are incremented. Keypoint k lies at the intersection of S 1 ? S 2 , and all other S i regress 2D keypoints and use Perspective-n-Point (PnP) to estimate the 6 DoF pose parameters <ref type="bibr">[35,</ref><ref type="bibr">43]</ref>. As an alternate to directly regressing keypoint coordinates, methods which vote for keypoints have been shown to be highly effective <ref type="bibr">[36,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr">37]</ref>, especially when objects are partially occluded. These schemes regress a distinct geometric quantity that relates positions of 2D pixels to 3D keypoints, and for each pixel casts this quantity into an accumulator space. As votes accumulate independently per pixel, these methods perform especially well in challenging occluded scenes.</p><p>While recent voting methods have shown great promise and leading performance, they require the regression of either a 2-channel (for 2D voting) [36] or 3-channel (for 3D voting ) <ref type="bibr" target="#b15">[14]</ref> activation map where voting quantities are accumulated in order to vote for keypoints. The activation map is the image shaped tensor where voting quantities are saved. The dimensionality of the activation map follows from the formulation of the geometric quantity being regressed, and the estimation errors in each channel tend to compound. This leads to reduced localization accuracy for higher dimensional activation maps when voting for keypoints. This observation has motivated our novel radial voting scheme, which regresses a one dimensional activation map for RGB-D data, leading to more accurate localization. The increase in keypoint localization accuracy also allows us to disperse our keypoint set farther, which increases the accuracy of transformation estimation, and ultimately that of 6 DoF pose estimation.</p><p>Our proposed method, RCVPose, trains a CNN to estimate the distance between a 3D keypoint, and the 3D scene point corresponding to each 2D RGB pixel. At inference, this distance is estimated for each 2D scene pixel, which is a 1D quantity and therefore has the potential to be more accurate than higherdimension quantities regressed in previous methods. For each pixel, a sphere of radius equal to this regressed distance is centered at each corresponding 3D scene point. Those 3D accumulator space cells (voxels) that intersect with the surface of these spheres are incremented, and peaks indicate keypoint locations, as illustrated in <ref type="figure" target="#fig_8">Fig. 1</ref>. Executing this for minimally 3 keypoints allows the unique recovery of the 6 DoF object pose.</p><p>Our main contribution is a novel radial voting scheme (based on a 1D regression) which we experimentally show to be more accurate than previous voting schemes (which are based on 2D and 3D regressions). Based on our radial voting scheme, a further contribution is a novel 6 DoF pose estimation method, called RCVPose. Notably, RCVPose requires only 3 keypoints per object, which is fewer than existing methods that use 4 or more keypoints <ref type="bibr">[36,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr">37]</ref>. We experimentally characterize the performance of RCVPose on 3 standard datasets, and show that it outperforms previous peer-reviewed methods, performing especially well in highly occluded scenes. We also conduct experiments to justify certain design decisions and hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Estimating 6 DoF pose has been extensively addressed in the literature <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b2">3]</ref>. Recent deep learning-based methods use CNNs to generate pose and can be generally classified into the three categories of viewpoint-based <ref type="bibr" target="#b16">[15]</ref>, keypointbased <ref type="bibr">[49]</ref>, and voting-based methods <ref type="bibr">[37]</ref>.</p><p>Viewpoint-based methods predict 6 DoF poses by matching 3D or projected 2D templates. In <ref type="bibr" target="#b34">[33]</ref>, a generative auto-encoder architecture used a GAN to convert RGB images into 3D coordinates, similar to the image-to-image translation task. Generated pixel-wise predictions were used in multiple stages to form 2D to 3D correspondences to estimate poses with RANSAC-based PnP. Manhardt et al. <ref type="bibr" target="#b28">[27]</ref> proposed predicting several 6 DoF poses for each object instance to estimate the pose distribution generated by symmetries and repetitive textures. Each predicted hypothesis corresponded to a single 3D translation and rotation, and estimated hypotheses collapsed onto the same valid pose when the object appearance was unique. Recent variations include <ref type="bibr">Trabelsi et al. [44]</ref>, who used a multi-task CNN-based encoder/multi-decoder network, and Wang et al.</p><p>[47] and <ref type="bibr" target="#b21">[20,</ref><ref type="bibr">34,</ref><ref type="bibr">42]</ref>, who used a rendering method by a self-supervised model on unannotated real RGB-D data to find an optimal alignment.</p><p>Keypoint-based methods detect specified object-centric keypoints and apply PnP for final pose estimation. Hu et al. <ref type="bibr" target="#b19">[18]</ref> proposed a segmentationdriven 6 DoF pose estimation method which used the visible parts of objects for local pose prediction from 2D keypoint locations. They then used the output confidence scores of a YOLO-based [39] network to establish 2D to 3D correspondences between the image and the object's 3D model. <ref type="bibr">Zakharov et al. [50]</ref> proposed a dense pose object detector to estimate dense 2D to 3D correspondence maps between an input image and available 3D models, recovering 6 DoF pose using PnP and RANSAC. In addition to RGB data, depth information was used in <ref type="bibr" target="#b15">[14]</ref> to detect 3D keypoints with a Deep Hough Voting network, with the 6 DoF pose parameters then fit with a least-squares method.</p><p>Voting-based methods have a long history in pose estimation. Before artificial intelligence became widespread, first the Hough Transform <ref type="bibr" target="#b8">[8]</ref> and RANSAC <ref type="bibr" target="#b10">[10]</ref> and subsequently methods such as pose clustering <ref type="bibr" target="#b33">[32]</ref> , image retrieval <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">41]</ref> and geometric hashing <ref type="bibr" target="#b22">[21]</ref> were widely used to localize simple geometric shapes, objects in images and full 6 DoF object pose. Hough Forests <ref type="bibr" target="#b11">[11]</ref>, while learning-based, still required hand-crafted feature descriptors. Voting was also extended to 3D point cloud images, such as 4PCS <ref type="bibr" target="#b0">[1]</ref> and its variations <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b30">29]</ref>, to estimate affine-invariant poses.</p><p>Following the advent of CNNs, hybrid methods emerged combining aspects of both data-driven and classical voting approaches. Both <ref type="bibr" target="#b19">[18]</ref> and [36] conclude with RANSAC-based keypoint voting, whereas Deep Hough Voting <ref type="bibr">[37]</ref> proposed a complete MLP pipeline of keypoint localization using a series of convolutional layers as the voting module. To estimate keypoints, two different deep learning-based voting schemes have appeared <ref type="bibr">[36,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr">37]</ref>, the proposed scheme introducing a third. At training, all voting schemes regress a distinct quantity that relates positions of pixels to keypoints. At inference, this quantity is estimated for each pixel, and is cast into an accumulator space in a voting process. Accumulator spaces can cover the 2D [49, <ref type="bibr" target="#b19">18,</ref><ref type="bibr">37]</ref> image space, or more recently the 3D [36] camera reference frame. After voting, peaks in accumulator space indicate positions of keypoints in the 2D image or 3D camera frame.</p><p>While only a few hybrid voting-based methods exist for 6 DoF pose estimation, they have outstanding performance, which has motivated us to develop RCVPose as a further advance of this class of hybrid method. Specifically, our method is inspired by PVNet [36], and is most closely related to the recently proposed PVN3D of He et al. <ref type="bibr" target="#b15">[14]</ref>, which combined PVNet and Deep Hough Voting [37] with a 3D accumulator space, utilizing the offset voting scheme of [49].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Keypoint Voting Scheme Alternatives</head><p>The three keypoint voting schemes are illustrated in 2D in <ref type="figure">Fig. 2a</ref>, for image pixel p and keypoint k to be estimated. The grid represents the (initially empty) accumulator space bins, which are the voxel space elements where votes are cast. In offset voting, the values of ?x and ?y are estimated from forward inference through the network. These values are used to offset p to reference that accumulator bin (shown in blue) containing k, the value of which is then incremented. Alternately, in vector voting, the direction ? n is estimated, and all bins (shown in green and blue) that intersect with ? n are incremented. Finally, in radial voting, the scalar r is estimated, and all bins (shown outlined in red) are incremented that intersect with the perimeter of the circle of radius r centered at p. When repeated for all image pixels, the bin containing k will contain the maximum accumulator space value, irrespective of which scheme is used, so long as the quantities estimated by network inference are sufficiently accurate. In <ref type="figure">Fig. 2b</ref>, circles generated by radial voting are illustrated for three image pixels. Each bin contains a count of the number of circle perimeters that it intersects, (a) Votes cast (in 2D) for offset, vector, and radial voting (b) Accumulator space values after radial voting for 3 points <ref type="figure">Fig. 2</ref>: Keypoint Voting Schemes in 2D: a) Pixel p casts votes for keypoint k at blue bin (offset and vector voting), green bins (vector voting), and red bins (radial voting). b) Radial votes cast for pixels p 1 , p 2 , and p 3 result in bin peaks at the intersection of the circles, with the peak occuring at keypoint k such that the peak value of 3 indicates the location of keypoint k. The above three voting schemes extend directly to 3D space, in which the accumulator space is a grid of voxels, the offset scheme contains an additional ?z component, ? n is a 3-dimensional vector, and the radial scheme casts votes on the surfaces of 3D spheres rather than 2D circles.</p><p>Formally, let p i be pixel from RGB-D image I with 2D image coordinate (u i , v i ) and corresponding 3D camera frame coordinate (x i , y i , z i ). Further let k ? j = (x j , y j , z j ) denote the camera frame coordinate of the j th keypoint of an object located at 6 DoF pose ?. The quantity m o regressed in the first offset scheme <ref type="bibr" target="#b19">[18,</ref><ref type="bibr">37]</ref> is the displacement between the two 3D points, denoted as m o = (?x, ?y, ?z) = (x i ?x j , y i ?y j , z i ?z j ). Alternately, the 3D quantity m v from the second vector scheme [36,49] is the unit vector pointing to k ? j from p i , denoted as m v = (dx, dy, dz) = mo ?mo? . The 3D vector scheme can alternately be parametrized into a 2D polar scheme, denoted as m p = (?, ?) = (cos ?1 dz, tan ?1 dy dx ). Finally, the 1D quantity m r from the radial scheme proposed here is simply the Euclidean distance between the points, i.e. m r = ?m o ?.</p><p>The above quantities encode different information about the relationship between p i and k ? j . For example, m v , m p , and m r can be derived directly from m o , whereas m o cannot be derived from the others. Also, m r and m v (and m p ) are independent of one another. This difference in geometric information leads to their different dimensionality, and ultimately the greater accuracy of radial voting, as discussed in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Keypoint Estimation Pipeline</head><p>The above described voting schemes can be used interchangeably within a keypoint estimation pipeline. The training inputs (  </p><formula xml:id="formula_0">L = L S + L M1 ,<label>(1)</label></formula><formula xml:id="formula_1">L S = 1 N N i=1 S i ? S i ,<label>(2)</label></formula><formula xml:id="formula_2">L M1 = 1 N N i=1 | M 1i ? M 1i | ,<label>(3)</label></formula><p>with summations over all N pixels. The network output is estimate S of S, and (unsegmented) estimate M 0 of M 1 . At inference ( <ref type="figure" target="#fig_0">Fig. 3</ref>), I RGB is fed to the network which returns estimates S and M 0 , the element-wise multiplication of which yields segmented estimate M 1 . Each pixel (u i , v i ) of M 1 , with corresponding 3D coordinate (x i , y i , z i ) drawn from the depth field I D of I, then independently casts a vote through the voting module into the initially empty 3D accumulator space A.</p><p>Vote casting is performed for each (u i , v i ), and is distinct for each voting scheme. In offset voting, accumulator space</p><formula xml:id="formula_3">A bin A[x i + M 1 [u i , v i , 0], y i + M 1 [u i , v i , 1], z i + M 1 [u i , v i , 2]</formula><p>] is incremented, thereby voting for the specific bin of A that contains keypoint k ? j . In vector and polar voting, every A bin is incremented that intersects with the ray ?(</p><formula xml:id="formula_4">x i + M 1 [u i , v i , 0], y i + M 1 [u i , v i , 1], z i + M 1 [u i , v i , 2])</formula><p>, for ? &gt; 0, thereby casting a vote for every bin along the ray that intersects with (x i , y i , z i ) and k ? j . Finally, in radial voting, every A bin is incremented that intersects with the sphere of radius M 1 [u i , v i ] centered at (x i , y i , z i ), thereby voting for every bin that lies on the surface of a sphere upon which k ? j resides. Whichever scheme is used, at the conclusion of vote casting for all (u i , v i ), a global peak will exist in the A bin containing k ? j , and a simple peak detection operation is then sufficient to estimate keypoint position k ? j , within the precision of A. The radial voting scheme has been shown to be more accurate than the other schemes at keypoint estimation, as shown in the experiments in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RCVPose</head><p>The above keypoint voting method formed the core of RCVPose. Radial voting was used, based on its superior accuracy as demonstrated in Sec. 4.4. The network of <ref type="figure" target="#fig_0">Fig. 3</ref> was used with ResNet-152 as the FCN-ResNet module. The minimal K = 3 keypoints were used for each object, selected from the corners of each object's bounding box. Based on Sec. 4.5, keypoints were scaled to lie beyond the surface of each object, ? 2 object radius units from its centroid.</p><p>The network structure was based on a Fully Convolutional ResNet-152 <ref type="bibr" target="#b13">[12]</ref>, similar to PVNet [36], albeit with two main differences. First, we replaced LeakyReLU with ReLU as the activation function. This was because our radial voting scheme only includes positive values, in contrast to the vector voting scheme of PVNet which also admits negative values. Second, we increased the number of skip connections linking the downsampling and upsampling layers from three to five, to include extra local features when upsampling <ref type="bibr" target="#b25">[24]</ref>.</p><p>All voxels were initialized to zero, with their values incremented as votes were cast. The voting process is similar to 3D sphere rendering, wherein those voxels that intersect with the sphere surface have their values incremented. The process is based on Andre's circle rendering algorithm <ref type="bibr" target="#b1">[2]</ref>. We generate a series of 2D slices of A parallel to the x-y plane, that fall within the sphere radius from the sphere center in both directions of the z-axis. For each slice, the radius of the circle formed by the intersection of the sphere and that slice is calculated, and all voxels that intersect with this circumference are incremented. The algorithm is accurate and efficient, requiring that only a small portion of the voxels be visited for each sphere rendering. It was implemented in Python and parallelized at the thread level, and executes with an efficiency similar to forward network inference.</p><p>Once the K= 3 keypoint locations are estimated for an image, it is straightforward to determine the object's 6 DoF rigid transformation ?, from the corresponding estimated scene and ground truth object keypoint coordinates <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b26">25]</ref>. This is analogous to the approach of <ref type="bibr" target="#b15">[14]</ref>, and is efficient compared to previous pure RGB approaches <ref type="bibr">[36]</ref> which employ an iterative PnP method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The LINEMOD dataset <ref type="bibr" target="#b16">[15]</ref> includes 1200 images per object. The training set contains only 180 training samples using the standard 15%/85% training/testing split <ref type="bibr">[49,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b19">18]</ref>. We augmented the dataset by rendering the objects with a random rotation and translation, transposed using the BOP rendering kit <ref type="bibr" target="#b17">[16]</ref> onto a background image drawn from the MSCOCO dataset <ref type="bibr" target="#b24">[23]</ref>. An additional 1300 augmented images were generated for each object in this way, inflating the training set to 1480 images per object.</p><p>The LINEMOD depth images have an offset compared to the ground-truth pose values, for unknown reasons <ref type="bibr" target="#b29">[28]</ref>. To reduce the impact of this offset, we regenerated the depth field for each training image from the ground truth pose, by reprojecting the depth value drawn from the object pose at each 2D pixel coordinate. The majority (1300) of the resulting training set were in this way purely synthetic images, and the minority (180) comprised real RGB and synthetic depth. All test images were original, real and unaltered.</p><p>Occlusion LINEMOD <ref type="bibr" target="#b2">[3]</ref> is a re-annotation of LINEMOD comprising a subset of 1215 challenging test images of partially occluded objects. The protocol is to train using LINEMOD images only, and then test on Occlusion LINEMOD to verify robustness.</p><p>YCB-Video [49] is a much larger dataset, containing 130K key frames of 21 objects over 92 videos. We split 113K frames for training and 27K frames for testing, following PVN3D <ref type="bibr" target="#b15">[14]</ref>. For data augmentation, YCB-Video provides 80K synthetic images with random object poses, rendered on a black background. We repeated here the process described above, by rendering random MSCOCO images as background. The complete training dataset therefore comprised 113K real + 80K synthetic = 193K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Prior to training, each RGB image is shifted and scaled to adhere to the Ima-geNet mean and standard deviation <ref type="bibr" target="#b5">[6]</ref>. The 3D coordinates were calculated from the image depth fields and represented in decimeter units, as all LINEMOD and YCB-Video objects are at most 1.5 decimeters in diameter and the backbone network can estimate better when the output is within a normalized range. The loss functions of Eqs. 1-3 were used with an Adam optimizer, with initial learning rate lr=1e-4. The lr was adjusted on a fixed schedule, re-scaled by a factor of 0.1 every 70 epochs. The network trained for 300 and 500 epochs for each object in the LINEMOD and YCB-Video datasets respectively, with batch size 32.</p><p>The accumulator space A is represented as a flat 3D integer array, i.e. an axis-aligned grid of voxel cubes. The size of A was set for each test image to the bounding box of the 3D data. The voxel resolution was set to 5 mm, which was found to be a good tradeoff between memory expense and keypoint localization accuracy (see Supplementary Material Sec. S.4.5).</p><p>For each object, 3 instances of the network were trained, one for each keypoint. We also implemented a version in which all 3 keypoints were trained simultaneously, within a single network. In this version, theM 0 ,M 1 , and M 1 representations of <ref type="figure" target="#fig_0">Fig. 3</ref> are replicated 3 times, and the FCN-ResNet weights are shared. Our experiments (detailed in the supplementary material) showed that the accuracy was poorer for this version, than when using separate networks for each keypoint. The only two methods that have used a combined network for all keypoints and all objects are GDRNet [48] and SOPose <ref type="bibr" target="#b7">[7]</ref>, against which our performance compares favourably (see Sec. 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We follow the ADD(s) metric defined by <ref type="bibr" target="#b16">[15]</ref> to evaluate LINEMOD, whereas YCB-Video is evaluated based on both ADD(s) and AUC as proposed by <ref type="bibr">[49]</ref>. All metrics are based on the distances between corresponding points as objects are transformed by the ground truth and estimated transformations. ADD measures the average distance between corresponding points, whereas ADDs averages the minimum distance between closest points, and is more forgiving for symmetric objects. A pose is considered correct if its ADD(s) falls within 10% of the object radius. AUC applies the ADD(s) values to determine the success of an estimated transformation, integrating these results over a varying 0 to 100 mm threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison of Keypoint Voting Schemes</head><p>We first conducted an experiment to evaluate the relative accuracies of the four voting schemes at keypoint localization, using the process from Sec. 3.2. Each scheme used the same 15%/85% train/test split of a subset of objects from the LINEMOD dataset. All four schemes used the exact same backbone network and hyperparameters. Specifically, they all used a fully convolutional ResNet-18 <ref type="bibr" target="#b25">[24]</ref>, batch size 48, initial learning rate 1e-3, and Adam optimizer, with accumulator space resolution of 1 mm. They were all trained with a fixed learning rate reduction schedule, which reduced the rate by a factor of 10 following every 70 epochs, and all trials trained until they fully converged. The only difference between trials, other than the selective use of either m o , m v , m p or m r in training M 1 , was a slight variation in the loss functions. For m o and m r , the L1 loss from Eqs. 1-3 was used, identical to the offset voting in PVN3D <ref type="bibr" target="#b15">[14]</ref>. Alternately, for m v and m p , the Smooth L1 equivalents of Eqs. 2 and 3 (with ?=1) were used, as in PVNet [36] (albeit therein using a 2D accumulator space).</p><p>Surface Keypoints: Sets of size K= 4 surface keypoints were selected for each object tested, using the Farthest Point Sampling (FPS ) method <ref type="bibr" target="#b9">[9]</ref>. FPS selects points on the surface of an object which are well separated, and is a popular keypoint generation strategy <ref type="bibr">[36,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr">38,</ref><ref type="bibr">37]</ref>. Following training, each keypoint's location k ?i j was estimated by passing each test image I i through the network, as in <ref type="figure" target="#fig_0">Fig. 3</ref>. The error ? i,j for each estimate was its Euclidean distance from its ground truth location, i.e. ? i,j = ? k ?i j ?k ?i j ?. The average of ? i,j for an object over all test images and keypoints was the keypoint estimation error, denoted as?.</p><p>Each voting scheme was implemented with care, so that they were numerically accurate and equivalent. To test the correctness of voting in isolation, ground truth values of M 1 calculated for each object and voting scheme were passed directly into the voting module, effectively replacing M 1 with M 1 in the inference stage of <ref type="figure" target="#fig_0">Fig. 3</ref>. For each voting scheme, the average? for all objects was similar and less than the accumulator space resolution of 1 mm, indicating that the implementations were correct and accurate.</p><p>The? values were evaluated for the four voting schemes for the ape, driller and eggbox LINEMOD objects as summarized in <ref type="table" target="#tab_0">Table 1</ref>. These three particular objects were chosen as the ape is the smallest and the driller the largest of the objects, whereas the eggbox includes a rotational symmetry. <ref type="table" target="#tab_0">Table 1</ref> includes a measure of the average distancer of the ground truth keypoints to each object centroid. Radial voting is seen to be the most accurate method, with a mean value 1.9-4.3x more accurate than the next most accurate polar voting, with smaller standard deviations. Notably, the ordinal relationship between the four schemes remains consistent across the scheme dimensionality, which indicates that dimensionality impacts keypoint localization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disperse Keypoints:</head><p>We repeated this experiment for keypoints selected from the corners of each object's bounding box, which was first scaled by a factor of 2 so that the keypoints were dispersed to fall outside of the object's surface. The results in <ref type="table" target="#tab_0">Table 1</ref> indicate that radial voting still outperforms the other two schemes by a large margin. Whereas the other two methods decrease in accuracy sharply as the mean keypoint distancer increases, radial voting accuracy degrades more gracefully. For example, for the ape, the 232% increase inr from 61.2 to 142.1 mm, reduced accuracy for offset voting by 80% (from 5.8 to 10.4 mm), but only by 23% (from 2.2 to 2.7 mm) for radial voting.</p><p>The improved accuracy of radial voting is likely due to the fact that the radial scheme regresses a 1D quantity, compared with the 2D polar, and the 3D offset and vector scheme quantities. It seems likely that the errors in each independent dimension compound during voting. This is further supported by the recognition that the polar scheme is simply a reduced dimensionality parametrization of the vector scheme, and yet its performance is far superior, with between 1.7-2.4x greater accuracy. Radial voting also has a degree of resilience to rotations, which is lacking in the other schemes. Specifically, the three voting quantities m o , m v , and m p are all sensitive to object in-plane rotations, whereas only radius scheme m r is invariant to in-plane rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Keypoint Dispersion</head><p>Impact on Transformation Estimation: It was suggested in [36] that 6 DoF pose estimation accuracy is improved by selecting keypoints that lie on the object surface, rather than the bounding box corners which lie just beyond the object surface. This may be the case when keypoint localization error increases signficantly with keypoint disperson, as occurs with vector and offset voting. There is, however, an advantage to dispersing the keypoints farther apart when using radial voting, which has a lower estimation error.</p><p>To demonstrate this, we conducted an experiment in which the keypoint locations were dispersed to varying degrees under a constant keypoint estimation error, with the impact measured on the accuracy of the resulting estimated transformation. We first selected a set K={k j } 4 j=1 keypoints on the surface of an object, using the FPS strategy. This set was then rigidly transformed by T , comprising a random rotation (within 0 ? to 360 ? for each axis) and a random translation (within 1/2 of the object radius), to form keypoint set K T . Each keypoint in K T was then independently pertubed by a magnitude of 1.5 mm in a random direction, to simulate the keypoint estimation error of the radial voting scheme, resulting in (estimated) keypoint set K T .</p><p>Next, the estimated transformation T between K T and the original (ground truth) keypoint set K was calculated using the Horn method <ref type="bibr" target="#b18">[17]</ref>. This process simulates the pose estimation that would occur between estimated keypoint locations, each with some error, and their corresponding ground truth model keypoints. The surface points of the object were then transformed by both the ground truth T and the estimated T transformations, and the distances separating corresponding transformed surface points were compared, as a measure of the accuracy of the estimated transformation.</p><p>The above process was repeated for versions of K that were dispersed by scaling an integral factor of the object radius from the object centroid. The exact same error perturbations (i.e. magnitudes and directions) were applied to each keypoint for each new scale value. The scaled trials therefore represented keypoints that were dispersed more distant from the object centroid, albeit with the exact same localization error.</p><p>This process was executed for all Occlusion LINEMOD objects, with 100 trials for each scale factor value from 1 to 5. The means of the corresponding point distances (i.e. the ADD metric as defined in <ref type="bibr" target="#b16">[15]</ref>) are plotted in <ref type="figure" target="#fig_2">Fig. 4a</ref>. It can be seen that ADD decreases for the first few scale factor increments for all objects, indicating an improved transformation estimation accuracy for larger keypoint dispersions. This increase in accuracy stems from improved rotational estimates, as the same positional perturbation error of a keypoint under a larger moment arm will result in a smaller angular error. The translational component of the transformation is not impacted by the scaling, as the Horn method starts by centering the two point clouds. After a certain increase in scale factor of 3 or 4, the unaffected translational error dominates, and the error plateaus.</p><p>This experiment shows that the transformation estimate from corresponding ground truth and estimated keypoints will be more accurate, when the keypoints are dispersed further (? 1 object radius, i.e. a scale of 2x) from the object's surface, when keypoint estimation error itself remains small (?1.5 cm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact on 6 DoF Pose Estimation:</head><p>The above result can be leveraged to further improve the accuracy of 6 DoF pose estimation when using radial voting. An experiment was executed for all Occlusion LINEMOD objects for varying keypoint dispersions. The keypoints were first selected to lie on the surface of each object using FPS, and the complete RCVPose inference pipeline was executed, yielding an ADD(s) value for each trial image. The keypoints were then projected outward from each object's centroid to a distance of 1, 2 and 3 object radius values, and RCVPose inference was once again executed and ADD(s) recalculated.</p><p>The results are plotted in <ref type="figure" target="#fig_2">Fig. 4b</ref>. Of the 8 objects, 4 had a higher ADD(s) value at a dispersion of 2x, as did the average over all objects. It seems that the decreased transformation estimation error <ref type="figure" target="#fig_2">(Fig. 4a</ref>) at 2x radius dispersion more than compensates for the gradual increase in keypoint localization error exhibited by radial voting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with SOTA</head><p>We next compared RCVPose against other recent competitive methods in the literature. We achieved state-of-art results on all three datasets, under a moderate training effort (i.e. hyper-parameter adjustment). The most challenging dataset was Occlusion LINEMOD, with results in <ref type="table" target="#tab_1">Table 2</ref>. RCVPose+ICP outperformed all other methods on average, achieving 71.1% mean accuracy, exceeding the next closest method PVN3D by 7.9%. It achieved the top performance on all objects except duck, where PVNet had the best result. Even without ICP refinement, RCVPose achieved close to the same results at 70.2% mean accuracy.</p><p>One strength of RCVPose is scale tolerance. Unlike most other methods whose performance reduced with smaller objects, our method was not impacted much. Significantly, accuracy improved over FFB6D from 47.2%, 45.7% to 61.3%, 51.2% for the ape and cat, respectively. Another advantage is that it accumulates votes independently for each pixel and is therefore robust to partial occlusions, capable of recognizing objects that undergo up to 70% occlusion (see <ref type="figure" target="#fig_3">Fig. 5</ref>). The LINEMOD dataset is less challenging, as objects are unoccluded. As listed in <ref type="table" target="#tab_1">Table 2</ref>, RCVPose+ICP still achieved the highest mean accuracy of 99.7%, slightly exceeding the tie between RCVPose (without ICP) and PVN3D. RCV-Pose+ICP was the only method to achieve 100% accuracy for more than one object. Again the RGB-D methods outperformed all other data modes, and the a) ape b) driller c) duck d) eggbox top RGB method that included depth refinement <ref type="bibr" target="#b34">[33]</ref> outperformed the best pure RGB method <ref type="bibr" target="#b28">[27]</ref>, supporting the benefits of the added depth mode. The YCB-Video results in <ref type="table" target="#tab_2">Table 3</ref> list AUC and ADD(s), with and without depth refinement. RCVPose is the top performing method, achieving from 95.2% to 95.9% ADD(s) and from 96.6% to 97.2% AUC accuracy, outperforming the next best method FFB6D by 2.8% ADD(s) and 0.2% AUC. Notably, RCVPose increased ADD(s) of the relatively small tuna fish can by a full 6% compared to the second best PVN3D. We also evaluated RCVPose on the BOP challenge benchmark <ref type="bibr" target="#b17">[16]</ref>, which is a standardized split of a number of datasets. Our results on their LINEMOD, Occlusion LINEMOD, and YCB-Video splits showed that RCVPose outperformed all other published results tested on this benchmark, when averaged over all 3 datasets (see Supplementary Material Sec. S.3). RCVPose runs at 18 fps on a server with an Intel Xeon 2.3 GHz CPU and RTX8000 GPU for a 640?480 image input. This compares well to other votingbased methods, such as PVNet at 25 fps, and PVN3D at 5 fps. The backbone network forward path, radial voting process, and Horn transformation solver take approximately 10, 41, and 4 msecs. per image respectively at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed RCVPose, a hybrid 6 DoF pose estimator with a ResNet-based radial estimator and a novel keypoint radial voting scheme. Our radial voting scheme is shown to be more accurate than previous schemes, especially when the keypoints are more dispersed, which leads to more accurate pose estimation requiring only 3 keypoints. We achieved state-of-the-art results on three popular benchmark datasets, YCB-Video, LINEMOD and the challenging Occlusion LINEMOD, ranking high on the BOP Benchmark, with an 18 fps runtime. A limitation is that training and inference are executed separately for each object and keypoint (also true for other recent competitive approaches) and that the 3D voting space is memory intensive, which will be the focus of future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1 Overview</head><p>We document here some addition implementation details, results and further hyperparameter experiments. In Sec. S.2, the complete details of the Fully Convolutional ResNet backbone are provided, in sufficient detail to recreate the network. In Sec. S.3, the 6 DoF pose estimation accuracy results for each individual object in the three data sets are presented, as well as some extra bounding box image samples. Finally in Sec. S.4, six additional experiments are included investigating the impact of the number of keypoints, the number of skip connections in the backbone network, the use of a combined vs. separate networks for each keypoint, the depth of the backbone network, the accumulator space resolution, as well as the impact of combining the three different voting schemes into various multi-scheme voting configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2 ResNet Backbone Structure</head><p>As shown in <ref type="table" target="#tab_4">Table S</ref>.10, we modified ResNet into a Fully Convolutional Network.</p><p>To start with, we replaced the Fully Connected Layer with a convolutional layer for the following up sampling layers. We then applied up-sampling to the feature map with a combination of convolution, bilinear interpolations, and skip concatenations from the residual blocks. We apply more skip layers than did PVNet [S.17], under the assumption that the convolutional feature maps would preserve more local features than the alternative bilinear interpolation, especially for deeper small scale feature maps. This design choice was supported by the experiment described in Sec. S.4.2.</p><p>We conducted an experiment on three objects, ape, driller and eggbox in Occlusion LINEMOD with different fully convolutional ResNets structures. Each network is trained until fully convergence with consistent hyper parameter sets. The resutls are shown in <ref type="table" target="#tab_4">Table S</ref>.1. Deeper ResNet has a tiny performance improvement on three objects tested with a minor sacrifice of speed. We ended up with ResN et152 32s when conducting the full test on all three datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3 Accuracy Results Per Object and BOP Benchmark</head><p>The detailed LINEMOD and Occlusion LINEMOD ADD(s) results, and the YCB-Video ADD(s) and AUC results categorized per object are listed in Table S.8, <ref type="table" target="#tab_4">Table S.7 and Table S</ref> As can be seen in <ref type="table" target="#tab_4">Table S</ref>.8, the original LINEMOD dataset is mostly saturated, with results from a number of different methods that are close to perfect. Nevertheless, RCVPose+ICP outperformed all alternatives at 99.7%, with 100% ADD(s) for three objects, including the only perfect scores for the driller and holepuncher objects.</p><p>The results in <ref type="table" target="#tab_4">Table S.7</ref> show Occlusion LINEMOD to be quite challenging. This is not only because of the occluded scenes, but is also due to the fact that the meshes are not very precisely modelled, and that some ground truth poses are not accurate for some cases.</p><p>The YCB-Video dataset has two evaluation metrics, as shown in <ref type="table" target="#tab_4">Table S</ref>.9. In general, AUC is more foregiving than ADD(s) since AUC has a tolerance of up to 10 cm [S.26]. For some objects like the master chef can and the power drill, RCVPose performs slightly worse in AUC compared to PVN3D [S.4], while still performing better in ADD(s).</p><p>All three datasets were also evaluated by the standardized metrics proposed by BOP [S.7]. The results in <ref type="table">Table.</ref> S.2 show that our average recall outperformed CosyPose [S.10] on LINEMOD and occlusion LINEMOD. Although we did not perform better on YCB-Video, we did perform better for the average results over all three datasets. Our method also runs at 18 fps which is also more time efficient compared to 0.36 fps for CosyPose.  We examined the impact of the number of keypoints on pose estimation accuracy. Sets of 3, 4 and 8 keypoints were selected for the ape, driller and eggbox LINEMOD objects, using the Bounding Box selection method described in Sec. 4.5 in the main paper. The results indicate that increasing the number of RCVPose keypoints does not impact pose estimation accuracy, which changed at most only 0.4% between these settings for all three objects. This is likely due to the high accuracy of keypoint location estimation under radial voting, which removes the added benefit of redundant keypoints. Given that the time and memory expense scale linearly with the number of keypoints, we settled upon the use of the minimal 3 keypoints for RCVPose for all of our experiments. We conducted an experiment which examined the impact of the number of skip connections on mean keypoint estimation error?. We increased the number of skip connections for ResNet-18, from 3 to 5. Such skip connections serve to improve the influence of image features during upsampling. The results are displayed in <ref type="table" target="#tab_4">Table S</ref>.3, and show that increasing the skip connections from 3 to 5, decreased both the mean and the standard deviation of the keypoint estimation error by a large margin, in all cases. We included 5 skip connections in our architecture, for all experiments, as shown in <ref type="figure">Fig S.</ref>1.  . We conducted an experiment on the optimal configuration of the number of networks. As shown in <ref type="table">Table.</ref> S.4, The radii regression is more accurate when a single network is trained separately on each keypoint compared to training simultaneously on all three keypoints per object. Therefore, we trained separate networks for each keypoint among each objects to achieve the best performance, with a small sacrifice of the time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4 Extra Hyperparameter Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.2 Number of Skip Connections</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.4 ResNet Backbone Depth</head><p>A further experiment tested different ResNet depths, from 18 to 152 layers. The results are plotted in <ref type="figure">Fig. S.2</ref>, and indicate that the substantially deeper networks exhibit only a minor reduction in average keypoint estimation error?. Despite the rather minor improvement due to increased depth, we nevertheless used ResNet-152 with 5 skip connections in the RCVPose in our experiments, as shown in <ref type="figure" target="#fig_8">Fig. S.1</ref> compared to PVNet. It is likely that we would have received very similar results had we based our backbone network on ResNet-18, albeit with a faster training cycle and smaller memory footprint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.5 Accumulator Space Resolution</head><p>We varied the accumulator space resolution to evaluate the balance of accuracy and efficiency. Resolution ? refers to the linear dimension of a voxel edge (i.e. voxel volume = ? 3 ). We selected 6 different resolutions from ? = 1 mm to 16 mm, and ran the voting module for each ? value with the same system, for all 3 scaled bounding box keypoints of all test images of the LINEMOD ape object. The results are listed in <ref type="table" target="#tab_4">Table S</ref>.5 which shows the means ? r and standard deviations ? r of the keypoint estimation errors? and ADD metric, and both the time and space efficiencies, for varying voxel resolutions. As expected, the voting module was faster and smaller, and the keypoint estimation error was greater, at coarser resolutions. The ADD value, which is the main metric used to identify a successful pose estimation event, remains nearly constant up to a resolution of 5 mm. The ? = 5 mm voxel size therefore achieved both an acceptable speed of 24 fps, an efficient memory footprint of 3.4 Mbtyes, and close to the highest ADD value, and so it was subsequently used throughout the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.6 Ensemble Multi-scheme Voting</head><p>The accumulator space is represented exactly the same for all three voting schemes, and is handled in exactly the same manner to extract keypoint locations through peak detection, once the voting has been completed. It is therefore possible and straightforward to combine voting schemes, by simply adding their resulting accumulator spaces prior to peak detection. We implemented this and compared the impact of all possible combinations of offset, vector, and radial voting schemes. The results are shown in <ref type="table" target="#tab_4">Table S</ref>.6, which also includes the results from each individual voting scheme for comparison. It can be seen that the radial voting scheme outperforms all other alternatives, yielding a lower mean and standard deviation of keypoint estimation error?. The next best alternative was the combination of all three schemes, which was greater than 3.5X less accurate than pure radial voting. Combing radial and offset voting slightly improved results over pure offset voting in two of the three objects. Curiously, combining radial and vector voting degraded results for all objects compared to pure vector voting, as did combining vector and offset voting. Based on these results, it seems possible that there may be better ways than simply adding the individual accumulator spaces to ensemble the information from these three voting schemes to reduce error further.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 )</head><label>3</label><figDesc>are: RGB fields I RGB of image I; ground truth binary segmented image S of the foreground object at pose ?; ground truth keypoint coordinate k ? j , and; the ground truth voting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>RCVPose training and inference. M 0 , M 1 , and M 1 have channel depth D = 1 for radial, D = 2 for polar, or D = 3 for offset or vector voting schemes scheme values (i.e. one of m o , m v , m p or m r ) for each pixel in S, represented by matrix M 1 . M 1 is calculated for a given k ? j using one of the voting scheme values, and has either channel depth D = 3 for m o or m v , D = 2 for m p , or D = 1 for m r . Both S and M 1 are assessed to compute the loss L as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Transformation error (?) (b) 6 DoF pose ADD(s) (?) Impact of keypoint dispersion on (a) Transformation estimation error, and (b) 6 DoF pose estimation ADD(s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>RCVPose sample Occlusion LINEMOD results: Blue box = ground truth, green box = estimate. RCVPose shows robustness to (even severe) occlusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>34. Park, K., Patten, T., Vincze, M.: Neural object learning for 6d pose estimation using a few cluttered images. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision -ECCV 2020. pp. 656-673. Springer International Publishing, Cham (2020) 35. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K.G., Daniilidis, K.: 6-dof object pose from semantic keypoints. In: 2017 IEEE international conference on robotics and automation (ICRA). pp. 2011-2018. IEEE (2017) 36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561-4570 (2019) 37. Qi, C.R., Litany, O., He, K., Guibas, L.J.: Deep hough voting for 3d object detection in point clouds. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019) 38. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413 (2017) 39. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 (2018) 40. Rothganger, F., Lazebnik, S., Schmid, C., Ponce, J.: 3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. International journal of computer vision 66(3), 231-259 (2006) 41. Sch?nberger, J.L., Price, T., Sattler, T., Frahm, J.M., Pollefeys, M.: A vote-andverify strategy for fast spatial verification in image retrieval. In: Asian Conference on Computer Vision. pp. 321-337. Springer (2016) 42. Shao, J., Jiang, Y., Wang, G., Li, Z., Ji, X.: Pfrl: Pose-free reinforcement learning for 6d pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 43. Tekin, B., Sinha, S.N., Fua, P.: Real-time seamless single shot 6d object pose prediction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 292-301 (2018) 44. Trabelsi, A., Chaabane, M., Blanchard, N., Beveridge, R.: A pose proposal and refinement network for better 6d object pose estimation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2382-2391 (2021) 45. Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., Birchfield, S.: Deep object pose estimation for semantic robotic grasping of household objects. arXiv preprint arXiv:1809.10790 (2018) 46. Wang, C., Xu, D., Zhu, Y., Mart?n-Mart?n, R., Lu, C., Fei-Fei, L., Savarese, S.: Densefusion: 6d object pose estimation by iterative dense fusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3343-3352 (2019) 47. Wang, G., Manhardt, F., Shao, J., Ji, X., Navab, N., Tombari, F.: Self6d: Selfsupervised monocular 6d object pose estimation. In: European Conference on Computer Vision. pp. 108-125. Springer (2020) 48. Wang, G., Manhardt, F., Tombari, F., Ji, X.: Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16611-16621 (2021) 49. Xiang, Y., Schmidt, T., Narayanan, V., Fox, D.: Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes (2018) 50. Zakharov, S., Shugurov, I., Ilic, S.: Dpod: 6d pose object detector and refiner. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1941-1950 (2019) 51. Zhou, G., Wang, H., Chen, J., Huang, D.: Pr-gcn: A deep graph convolutional network with point refinement for 6d pose estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2793-2802 (2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>.9, respectively. Some additional successful images showing recovered and ground truth bounding boxes are displayed in Figure S.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>S. 4 . 1</head><label>41</label><figDesc>Number of Keypoints Previous works have used between 4 [S.16], and up to 8 [S.4, S.17] or more [S.18] keypoints per object, selected from bounding box corners [S.20, S.21, S.13] or</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>There were five different network architectures proposed in the intial ResNet paper [S.3]. While some 6 DoF pose recovery works use variations of ResNet-18 [S.17, S.23, S.27, S.22] others use ResNet-50 [S.24, S.15]. Some customize the structure by converting it to an encoder [S.22, S.15, S.27, S.23], adding extra layers and skip connections [S.17] while others use the original ResNet unaltered [S.4, S.14].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. S. 1 :</head><label>1</label><figDesc>Backbone network structure for (a) RCVPose and (b) PVNet: Denser skip connections allow more local image features to be kept during upsampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>S. 9 :</head><label>9</label><figDesc>YCB Video AUC [S.26] and ADD(s) [S.5] results:Non-symmetric objects are evaluated with ADD, and symmetric objects (annotated with * ) are evaluated with ADD-s. The AUC metrics is based on the curve with ADD for non-symmetries and ADDs with 76.9 84.2 81.0 90.4 88.0 79.1 87.2 78.5 86.0 77.0 71.6 69.6 78.2 72.7 64.3 56.9 71.7 50.2 44.1 88.0 75.8 DF(per-pixel)[S.23] 95.3 92.5 95.1 93.8 95.8 95.7 94.3 97.2 89.3 90.0 93.6 94.4 86.0 95.3 92.1 89.5 90.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. S. 3 :</head><label>3</label><figDesc>Occluded LINEMOD sample results: Blue box = ground truth, green box = estimate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Keypoint localization error?, for surface (FPS) and disperse keypoints: mean ? and standard deviation ? for 4 voting schemes {v, o, p, r}, withr = mean keypoint distance to object centroid?</figDesc><table><row><cell></cell><cell></cell><cell>[mm]</cell></row><row><cell></cell><cell></cell><cell>vector (3D) offset (3D) polar (2D) radial (1D) r [mm] ?v ?v ?o ?o ?p ?p ?r ?r</cell></row><row><cell>ape driller eggbox</cell><cell>FPS</cell><cell>61.2 10.0 5.8 5.8 2.6 5.6 2.4 1.3 0.7 129.4 10.0 2.3 6.5 4.7 5.3 2.5 2.2 1.0 82.5 11.8 5.3 5.2 2.7 4.9 1.9 2.0 0.7</cell></row><row><cell>ape driller eggbox</cell><cell>disperse</cell><cell>142.1 12.5 7.6 10.4 5.3 5.7 2.5 1.8 0.8 318.8 11.3 8.2 9.5 3.5 5.2 2.6 2.7 0.8 197.3 13.7 8.5 11.4 4.7 7.2 3.4 2.4 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LINEMOD and Occlusion LINEMOD accuracy results</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ADD(s) [%]</cell></row><row><cell cols="2">Mode Method</cell><cell cols="2">LM O-LM</cell></row><row><cell></cell><cell>SSD6D [19]</cell><cell>9.1</cell><cell>-</cell></row><row><cell></cell><cell>Oberweger [31]</cell><cell>-</cell><cell>27.1</cell></row><row><cell></cell><cell>Hu et al. [18]</cell><cell>-</cell><cell>30.4</cell></row><row><cell>RGB</cell><cell>Pix2Pose [33]</cell><cell cols="2">72.4 32.0</cell></row><row><cell></cell><cell>DPOD [50]</cell><cell cols="2">83.0 32.8</cell></row><row><cell></cell><cell>PVNet [36]</cell><cell cols="2">86.3 40.8</cell></row><row><cell></cell><cell>DeepIM [22]</cell><cell>88.6</cell><cell>-</cell></row><row><cell></cell><cell>PPRN [44]</cell><cell cols="2">93.9 58.4</cell></row><row><cell></cell><cell>GDR-Net [48]</cell><cell cols="2">93.7 62.2</cell></row><row><cell></cell><cell>SO-Pose [7]</cell><cell cols="2">96.0 62.3</cell></row><row><cell></cell><cell>YOLO6D [43]</cell><cell cols="2">56.0 6.4</cell></row><row><cell>RGB</cell><cell cols="3">SSD6D+ref [19] 34.1 27.5</cell></row><row><cell>+D ref</cell><cell>PoseCNN [49]</cell><cell>-</cell><cell>24.9</cell></row><row><cell></cell><cell cols="3">DPOD+ref [50] 95.2 47.3</cell></row><row><cell></cell><cell cols="2">DenseFusion [46] 94.3</cell><cell>-</cell></row><row><cell></cell><cell>PVN3D [14]</cell><cell cols="2">99.4 63.2</cell></row><row><cell>RGB-D</cell><cell>PR-GCN [51] FFB6D [13]</cell><cell cols="2">99.6 65.0 99.7 66.2</cell></row><row><cell></cell><cell>RCVPose</cell><cell cols="2">99.4 70.2</cell></row><row><cell></cell><cell cols="3">RCVPose+ICP 99.7 71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>YCB-Video accuracy results</figDesc><table><row><cell cols="2">D ref? Method</cell><cell cols="2">ADD(s) AUC</cell></row><row><cell></cell><cell>PoseCNN [49]</cell><cell>59.9</cell><cell>75.8</cell></row><row><cell></cell><cell cols="2">DF (per-pixel) [46] 82.9</cell><cell>91.2</cell></row><row><cell></cell><cell>SO-Pose [7]</cell><cell>56.8</cell><cell>90.9</cell></row><row><cell></cell><cell>GDR-Net [48]</cell><cell>60.1</cell><cell>91.6</cell></row><row><cell>No</cell><cell>PVN3D [14]</cell><cell>91.8</cell><cell>95.5</cell></row><row><cell></cell><cell>PR-GCN [51]</cell><cell>-</cell><cell>95.8</cell></row><row><cell></cell><cell>FFB6D [13]</cell><cell>92.7</cell><cell>96.6</cell></row><row><cell></cell><cell>RCVPose</cell><cell cols="2">95.2 96.6</cell></row><row><cell></cell><cell>PoseCNN [49]</cell><cell>85.4</cell><cell>93.0</cell></row><row><cell></cell><cell>DF (iterative) [46]</cell><cell>86.1</cell><cell>93.2</cell></row><row><cell>Yes</cell><cell>PVN3D [14]+ICP</cell><cell>92.3</cell><cell>96.1</cell></row><row><cell></cell><cell>FFB6D [13]+ICP</cell><cell>93.1</cell><cell>97.0</cell></row><row><cell></cell><cell>RCVPose+ICP</cell><cell cols="2">95.9 97.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S .</head><label>S</label><figDesc>1: ADD(S) metrics for 3 LMO objects on different ResNet backbones with ICP.</figDesc><table><row><cell>LMO</cell><cell>ape driller eggbox</cell></row><row><cell cols="2">ResNet18 32s 60.2 77.9 81.9</cell></row><row><cell cols="2">ResNet34 32s 60.2 77.9 81.9</cell></row><row><cell cols="2">ResNet50 32s 60.8 78.4 81.9</cell></row><row><cell cols="2">ResNet101 32s 61.3 78.4 81.9</cell></row><row><cell cols="2">ResNet152 32s 61.3 78.8 82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S .</head><label>S</label><figDesc>2: LINEMOD, Occlusion LINEMOD and YCB-Video evaluated based on BOP</figDesc><table><row><cell cols="2">Average Recall metrics [S.7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ARV SD</cell><cell></cell><cell>ARMSSD</cell><cell></cell><cell>ARMSP D</cell><cell></cell><cell>Average</cell></row><row><cell>Method</cell><cell cols="7">LM LMO YCB-V LM LMO YCB-V LM LMO YCB-V AR</cell></row><row><cell>PVNet [S.17]</cell><cell>-0.43</cell><cell>-</cell><cell>-0.54</cell><cell>-</cell><cell>-0.75</cell><cell>-</cell><cell>-</cell></row><row><cell>EPOS [S.6]</cell><cell cols="2">-0.39 0.63</cell><cell cols="2">-0.50 0.68</cell><cell cols="2">-0.75 0.78</cell><cell>-</cell></row><row><cell>SO-Pose [S.1]</cell><cell cols="2">-0.44 0.65</cell><cell cols="2">-0.58 0.73</cell><cell cols="2">-0.82 0.76</cell><cell>-</cell></row><row><cell cols="7">CosyPose [S.10] 0.67 0.58 0.83 0.81 0.75 0.90 0.84 0.83 0.85</cell><cell>0.78</cell></row><row><cell cols="7">RCVPose+ICP 0.74 0.68 0.86 0.83 0.77 0.86 0.83 0.79 0.86</cell><cell>0.80</cell></row><row><cell cols="8">using the FPS algorithm [S.18, S.4, S.17]. It has been suggested that a greater</cell></row><row><cell cols="8">number of keypoints is preferable to improve robustness and accuracy [S.4, S.17],</cell></row><row><cell cols="8">especially for pure RGB methods in which at least 3 keypoints need to be visible</cell></row><row><cell cols="8">for any view of an object to satisfy the constraints of the P3P algorithm [S.19,</cell></row><row><cell>S.2].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S . 3 :</head><label>S3</label><figDesc>Average keypoint estimation error mean (? [mm]) and standard devia-</figDesc><table><row><cell cols="2">tion (? [mm]) for different ResNet-18 backbone skip connections. Increasing the skip</cell></row><row><cell>connections reduced the error of the estimation</cell><cell></cell></row><row><cell cols="2"># of skip connections</cell></row><row><cell>3</cell><cell>5</cell></row><row><cell>? ? ?</cell><cell>?</cell></row><row><cell>ape 2.4 1.1 1.8</cell><cell>0.8</cell></row><row><cell>driller 3.6 1.2 2.7</cell><cell>0.8</cell></row><row><cell>eggbox 3.5 1.7 2.4</cell><cell>1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S</head><label>S</label><figDesc></figDesc><table><row><cell cols="3">.4: Keypoint localization error, for training all three keypoints' radii simul-</cell></row><row><cell cols="3">taneously in one network and separately in three networks:? mean (? {sim|sep} ) and standard deviation (? {sim|sep} ) for radial voting schemes ? [mm]</cell></row><row><cell cols="3">simultaneously separately ?sim ?sim ?sep ?sep</cell></row><row><cell>ape 1.7</cell><cell>0.9</cell><cell>1.3 0.7</cell></row><row><cell>driller 2.6</cell><cell>1.4</cell><cell>2.2 1.0</cell></row><row><cell>eggbox 2.5</cell><cell>1.3</cell><cell>2.0 0.7</cell></row><row><cell>S.4.3 Number of Networks</cell><cell></cell><cell></cell></row><row><cell cols="3">Some of the 6 DoF pose estimators trained a single distinct network for each</cell></row><row><cell cols="3">individual object [S.4, S.17, S.23] whereas other multi-class methods trained a</cell></row><row><cell cols="3">single network for all classes combined [S.9, S.25, S.1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Mean keypoint estimation error [mm] vs. ResNet depth Table S.5: Accumulator space resolution ? [mm] impact on accuracy? [mm], ADD [%], processing speed [fps], and memory [Mbyte], for LINEMOD ape test images. The processing speed includes only the accumulator space time performance</figDesc><table><row><cell>av. keypoint estimation error [mm]</cell><cell>1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>driller eggbox ape</cell></row><row><cell></cell><cell>18</cell><cell>34</cell><cell>50</cell><cell cols="2">ResNet depth</cell><cell>101</cell><cell>152</cell></row><row><cell></cell><cell cols="5">Fig. S.2: ?? [mm] ADD speed</cell><cell>memory</cell></row><row><cell></cell><cell></cell><cell cols="4">[mm] ?r ?r [%] [fps]</cell><cell>[Mbyte]</cell></row><row><cell></cell><cell></cell><cell cols="5">0.5 1.65 0.63 61.5 1.6 4?957 3 = 3517</cell></row><row><cell></cell><cell></cell><cell cols="3">1 1.75 0.81 61.5</cell><cell cols="2">5 4?479 3 = 440.61</cell></row><row><cell></cell><cell></cell><cell cols="5">2 2.33 0.52 61.3 12 4?239 3 =</cell><cell>54.81</cell></row><row><cell></cell><cell></cell><cell cols="5">4 6.27 0.72 61.3 20 4?118 3 =</cell><cell>6.57</cell></row><row><cell></cell><cell></cell><cell cols="5">5 6.33 0.69 61.3 24 4?95 3 =</cell><cell>3.43</cell></row><row><cell></cell><cell></cell><cell cols="5">8 11.73 2.37 55.2 32 4?58 3 =</cell><cell>0.78</cell></row><row><cell></cell><cell></cell><cell cols="5">16 17.92 5.52 45.7 40 4?28 3 =</cell><cell>0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S .</head><label>S</label><figDesc>6: Combined Accumulator Space:? mean (? {v|o|r} ) and standard deviation (? {v|o|r} ) for different combination of 3 voting schemes, withr = mean distance of keypoints to object centroid? [mm] ? v ? v ? o ? o ? r ? r ? r ? r ? r ? r ? r ? r ? r ? r Table S.7: Occlusion LINEMOD Accuracy Results. Non-symmetric objects are evaluated with ADD, and symmetric objects (annotated with * ) are evaluated with ADD-s</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[mm]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">vector vector radial + offset + offset + radial + offset + radial vector offset radial vector r ape 142.1 20.2 12.4 12.7 6.7 9.8 6.2 7.2 1.2 12.5 7.6 10.4 5.3 1.8 0.8</cell></row><row><cell cols="11">driller 318.8 22.3 11.7 13.3 7.9 8.7 3.4 5.7 2.3 11.3 8.2 9.5 3.5 2.7 0.8</cell></row><row><cell cols="11">eggbox 197.3 21.6 13.5 17.4 10.5 12.1 5.2 6.4 3.3 13.7 8.5 11.4 4.7 2.4 1.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Object</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mode</cell><cell>Method</cell><cell cols="9">ape can cat driller duck eggbox  *  glue  *  holepuncher ADD(s)[%]</cell></row><row><cell></cell><cell cols="8">Oberweger [S.12] 12.1 39.9 8.2 45.2 17.2 22.1 35.8</cell><cell>36.0</cell><cell>27.1</cell></row><row><cell></cell><cell cols="8">Hu et al. [S.8] 17.6 53.9 3.3 62.4 19.2 25.9 39.6</cell><cell>21.3</cell><cell>30.4</cell></row><row><cell>RGB</cell><cell cols="8">Pix2Pose [S.14] 22.0 44.7 22.7 44.7 15.0 25.2 32.4 DPOD [S.27] -------</cell><cell>49.5 -</cell><cell>32.0 32.8</cell></row><row><cell></cell><cell cols="8">PVNet [S.17] 15.8 63.3 16.7 25.2 65.7 50.2 49.6</cell><cell>39.7</cell><cell>40.8</cell></row><row><cell></cell><cell>PPRN [S.22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.4</cell></row><row><cell></cell><cell>YOLO6D [S.21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.4</cell></row><row><cell>RGB</cell><cell cols="2">SSD6D+ref [S.9] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.5</cell></row><row><cell>+D ref</cell><cell cols="8">PoseCNN [S.26] 9.6 45.2 0.9 41.4 19.6 22.0 38.5</cell><cell>22.1</cell><cell>24.9</cell></row><row><cell></cell><cell cols="2">DPOD+ref [S.27] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.3</cell></row><row><cell></cell><cell>PVN3D [S.4]</cell><cell cols="7">33.9 88.6 39.1 78.4 41.9 80.9 68.1</cell><cell>74.7</cell><cell>63.2</cell></row><row><cell>RGB-D</cell><cell>RCVPose</cell><cell cols="7">60.3 92.5 50.2 78.2 52.1 81.2 72.1</cell><cell>75.2</cell><cell>70.2</cell></row><row><cell></cell><cell cols="8">RCVPose+ICP 61.3 93 51.2 78.8 53.4 82.3 72.9</cell><cell>75.8</cell><cell>71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S .</head><label>S</label><figDesc>8: LINEMOD Accuracy Results: Non-symmetric objects are evaluated with ADD, and symmetric objects (annotated with * 99.5 99.8 99.3 98.2 99.8 100.0 99.9 99.7 99.8 99.5 99.4 RCVPose+ICP 99.6 99.7 99.7 99.3 99.7 100 99.7 99.3 100.0 100 99.9 99.5 99.7 99.7</figDesc><table><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>are evaluated with ADD-s</cell><cell>Object</cell><cell>bench-hole-</cell><cell>Method ape vise camera can cat driller duck eggbox  *  glue  *  puncher iron lamp phone mean Mode</cell><cell>SSD6D+ref [S.9] 2.6 15.1 6.1 27.3 9.3 12.0 1.3 2.8 3.4 3.1 14.6 11.4 9.7 9.1</cell><cell>Pix2Pose [S.14] 58.1 91 60.9 84.4 65 76.3 43.8 96.8 79.4 74.8 83.4 82 45 72.4</cell><cell>DPOD [S.27] 53.3 95.3 90.4 94.1 60.4 97.7 66 99.7 93.8 65.8 99.8 88.1 74.2 83.0 RGB PVNet [S.17] 43.62 99.9 86.9 95.5 79.3 96.4 52.6 99.2 95.7 81.9 98.9 99.3 92.4 86.3</cell><cell>PPRN [S.22] 84.5 98.7 93.7 97.8 87.3 96.9 88.5 98.5 99.5 84.5 99.1 98.7 92.5 93.9</cell><cell>DeepIM [S.11] 77 97.5 93.5 96.5 82.1 95.0 77.7 97.1 99.4 52.8 98.3 97.5 87.7 88.6</cell><cell>YOLO6D [S.21] 21.6 81.8 36.6 68.8 41.8 63.5 27.2 69.6 80 42.6 75 71.1 47.7 56.0 RGB SSD6D+ref [S.9] -------------34.1 +D ref DPOD+ref [S.27] 87.7 98.5 96.1 99.7 94.7 98.8 86.3 99.9 96.8 86.8 100 96.8 94.7 95.2</cell><cell>DenseFusion [S.23] 92.3 93.2 94.4 93.1 96.5 87.0 92.3 99.8 100.0 92.1 97.0 95.3 92.8 94.3</cell><cell>99 99.4 99.7 99.4 98.7 99.7 99.8 99.9 99.2 99.1 99.43 99.2 99.6 99.7 99.6 RCVPose PVN3D [S.4] 97.3 99.7 RGB-D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>1 95.1 71.5 70.2 92.2 91.2 PVN3D[S.4] 96.0 96.1 97.4 96.2 97.5 96 97.1 97.7 93.3 96.6 97.4 96.0 90.2 97.6 96.7 90.4 96.7 96.7 93.6 88.4 96.8 95.5 AUC RCVPose 95.7 97.2 97.6 98.2 97.9 98.2 97.7 97.7 97.9 97.9 96.2 99.2 95.2 98.4 96.2 89.1 96.2 95.9 95.2 94.7 95.7 96.6 PoseCNN [S.26] 50.2 53.1 68.4 66.2 81.0 70.7 62.7 75.2 59.5 72.3 53.3 50.3 69.6 58.5 55.3 64.3 35.8 58.3 50.2 44.1 88.0 59.9 No DF(per-pixel)[S.23] 70.7 86.9 90.8 84.7 90.9 79.6 89.3 95.8 79.6 76.7 87.1 87.5 86.0 83.8 83.7 89.5 77.4 89.1 71.5 70.2 92.2 82.9 PVN3D[S.4] 80.5 94.8 96.3 88.5 96.2 89.3 95.7 96.1 88.6 93.7 96.5 93.2 90.2 95.4 95.1 90.4 92.7 91.8 93.6 88.4 96.8 91.8 ADD (s) RCVPose 93.6 95.7 97.2 94.7 97.2 96.4 97.1 96.5 90.2 96.7 95.7 97.8 94.9 96.3 95.4 89.3 94.7 92.4 96.4 94.7 95.7 95.2 PoseCNN [S.26] +ICP 95.8 92.7 98.2 94.5 98.6 97.1 97.9 98.8 92.7 97.1 97.8 96.9 81.0 94.9 98.2 87.6 91.7 97.2 75.2 64.4 97.2 93.0 DF(iterative)[S.23] 96.4 95.8 97.6 94.5 97.3 97.1 96.0 98.0 90.7 96.2 97.5 95.9 89.5 96.7 96.0 92.8 92.0 97.6 72.5 69.9 92.0 93.2 PVN3D[S.4]+ICP 95.2 94.4 97.9 95.9 98.3 96.7 98.2 98.8 93.8 98.2 97.6 97.2 92.8 97.7 97. 1 91.1 95.0 98.1 95.6 90.5 98.2 96.1 AUC RCVPose+ICP 96.2 97.9 97.9 99 98.2 98.6 98.1 98.4 98.4 98.3 97.2 99.6 96.9 98.7 96.4 90.7 96.4 96.6 96.2 95.1 96.6 97.2 PoseCNN [S.26] +ICP 68.1 83.4 97.1 81.8 98.0 83.9 96.6 98.1 83.5 91.9 96.9 92.5 81.0 81.1 97.7 87.6 78.4 85.3 75.2 64.4 97.2 85.4 Yes DF(iterative)[S.23] 73.2 94.1 96.5 85.5 94.7 81.9 93.3 96.7 83.6 83.3 96.9 89.9 89.5 88.9 92.7 92.8 77.9 93.0 72.5 69.9 92.0 86.1 PVN3D[S.4]+ICP 79.3 91.5 96.9 89.0 97.9 90.7 97.1 98.3 87.9 96.0 96.9 95.9 92.8 96.0 95.7 91.1 87.2 91.6 95.6 90.5 98.2 92.3 ADD (s) RCVPose+ICP 94.7 96.4 97.6 95.4 97.7 96.7 97.4 97.9 92.6 97.2 96.7 98.4 95.3 97.1 96.2 91.2 94.9 93.2 96.7 94.9 96.6 95.9</figDesc><table><row><cell>(a) ape</cell><cell>(e) cat</cell><cell>(h) can</cell><cell>(l) driller</cell></row><row><cell>(b) ape</cell><cell>(f) cat</cell><cell>(i) can</cell><cell>(m) driller</cell></row><row><cell>(c) duck</cell><cell>(g) eggbox</cell><cell>(j) glue</cell><cell>(n) holepuncher</cell></row><row><cell>(d) duck</cell><cell>(g) eggbox</cell><cell>(k) glue</cell><cell>(o) holepuncher</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S .</head><label>S</label><figDesc>10: ResNet Backbone structure compared to PVNet</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Thanks to Bluewrist Inc. and NSERC for their support of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">4-points congruent sets for robust surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discrete circles, rings and spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="695" to="706" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast local spatial verification for feature-agnostic large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6892" to="6905" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficientpose-an efficient, accurate and scalable end-toend 6d multi object pose estimation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bukschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04307</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">So-pose: Exploiting self-occlusion for direct 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12396" to="12405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Use of the hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The farthest point strategy for progressive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1315" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hough forests for object detection, tracking, and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2188" to="2202" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2011.70</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2011.70" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ffb6d: A full flow bidirectional fusion network for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3003" to="3013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep pointwise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">Bop challenge 2020 on 6d object localization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="577" to="594" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using orthonormal matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Hilden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Negahdaripour</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSAA.5.001127</idno>
		<ptr target="http://josaa.osa.org/abstract.cfm?URI=josaa-5-7-1127" />
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1127" to="1135" />
			<date type="published" when="1988-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgbbased 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometric hashing: A general and efficient model-based recognition scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lamdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Wolfson</surname></persName>
		</author>
		<idno type="DOI">10.1109/CCV.1988.589995</idno>
		<ptr target="https://doi.org/10.1109/CCV.1988.589995" />
	</analytic>
	<monogr>
		<title level="m">Proceedings] Second International Conference on Computer Vision</title>
		<meeting>] Second International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A comparison of four algorithms for estimating 3-D rigid transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorusso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explaining the ambiguity of object detection and 6d pose from visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Super generalized 4pcs for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohamad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rappaport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2015.74</idno>
		<ptr target="https://doi.org/10.1109/3DV.2015.74" />
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized 4-points congruent sets for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohamad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rappaport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2014.21</idno>
		<ptr target="https://doi.org/10.1109/3DV.2014.21" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Olson</surname></persName>
		</author>
		<title level="m">Efficient pose clustering using a randomized algorithm</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7668" to="7677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">So-pose: Exploiting self-occlusion for direct 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12396" to="12405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Complete solution classification for the perspective-three-point problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="930" to="943" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bop challenge 2020 on 6d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="577" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7668" to="7677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural object learning for 6d pose estimation using a few cluttered images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="656" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Linear n-point camera pose determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="774" to="780" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A pose proposal and refinement network for better 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaabane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2382" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self6d: Self-supervised monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16611" to="16621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1941" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
