<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Attention Aware Feature Learning for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Addx Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology Beijing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Addx Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Attention Aware Feature Learning for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification. However, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the attention learning as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attentions have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) makes the feature maps obtained by backbone focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) makes the extracted features be decoupled into several groups and be separately responsible for different body parts (i.e., keypoints), thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet [10]  and Bag of Tricks <ref type="bibr" target="#b14">[15]</ref>) and observed significant performance improvement on five widely used datasets. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ReID with human mask and keypoints</head><p>With the development of human mask and keypoint detection, there are many ReID methods proposed to use mask</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) targets at matching the same person at different locations across different cameras. A common solution is to extract features directly from the whole person <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. For example, using a deep network pre-trained on ImageNet and fine-tuned over specific ReID <ref type="bibr">Figure 1</ref>. An illustration of heatmaps generated by our attention aware feature learning. (a) original images; (b) holistic attention heatmaps; (c) partial attention heatmaps. Note that how our method effectively alleviates the background and makes the heatmaps focus on the person and the body parts, respectively. datasets. Although the ReID problem has been extensively studied in recent years, it is still an open problem due to pose variation, background clutters, occlusion, etc.</p><p>To address these challenges, many solutions have been proposed in the literature. Among them, attention learning is attractive for its potential in removing background clutter <ref type="bibr" target="#b18">[19]</ref>, or enhancing local discriminability for different body parts <ref type="bibr" target="#b29">[30]</ref>. Currently, most strategies using attention have to incorporate a separate stream as attention function to reweight the feature maps which will increase computational complexity and model size in turn. Alternatively, we consider a more practical way to incorporate attention without changing the basic ReID networks. Our key assumption is that such attention can be implicitly embedded in the feature maps that are used for extracting person appearance representations. If feature maps contain such information, then they could be used to predict some attention-related information subsequently.</p><p>Specifically, this paper proposes an attention aware feature learning method for ReID task. We believe that CNN with proper constraints can obtain attention itself because of its powerful non-linearity. Therefore, if we can add appropriate constraints in the train stage, the attention aware features could be obtained without adding additional structures as previous works did. To this end, we present a holis-tic attention branch (HAB) to introduce the global attention information in the learned feature maps and a partial attention branch (PAB) to generate local attention aware feature maps, see <ref type="figure">Figure 1</ref> for example. By predicting the mask of a person, HAB is designed to restrict the backbone network focus on person bodies instead of the background. PAB further forces different feature channels focus on different body parts, by explicitly using different feature groups to predict different keypoints. While channel wise attention has recently been explored for segmentation <ref type="bibr" target="#b7">[8]</ref>, to the best of our knowledge, it has not been used in ReID. Meanwhile, the channel wise attention in PAB is quite different from the one used in <ref type="bibr" target="#b7">[8]</ref> which is basically to re-weight the feature channels. For comparison, PAB interprets channel wise attention as a way to make specific channels focus on different spatial parts and implicitly achieves the decoupling of feature channels.</p><p>The main contributions of this work can be summarized as: <ref type="bibr" target="#b0">(1)</ref> We propose a method to incorporate attention in feature learning without changing any structure of the original model, simply by adding supervisions during training. In other words, our approach adds no extra computing complexity during inference. (2) Both global and local attentions have been considered simultaneously in our method to improve the ReID performance. Especially, the partial attention implemented by performing channel wise decoupling supplies a new perspective to attention learning for ReID. (3) Our method has been extensively evaluated on widely used benchmarks with ablation study to analyze how different modules in our method work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention learning in person ReID</head><p>Attention learning has recently been introduced to person ReID task <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref> for obtaining more discriminative features due to its successful in other computer vision tasks like object detection <ref type="bibr" target="#b2">[3]</ref> and image segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. The common strategy to use attention in ReID is to incorporate a separate stream of regional attention into a deep convolutional ReID model. For example, Zhao et al. <ref type="bibr" target="#b29">[30]</ref> exploit the Spatial Transformer Network <ref type="bibr" target="#b10">[11]</ref> as hard attention to search discriminative parts. Li et al. <ref type="bibr" target="#b13">[14]</ref> propose a multi-granularity attention selection mechanism to address the problem caused by poor located bounding-boxes and noisy information at pixel level. Song et al. <ref type="bibr" target="#b18">[19]</ref> propose a method generating a pair of body-aware and background-aware spatial attention maps for background clutter removal.</p><p>The methods mentioned above could be regarded as spatial-wise attention learning. Channel-wise attention are also introduced in this area by <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref> to decide which channel is more important. Wang et al. <ref type="bibr" target="#b24">[25]</ref> incor-porate channel-wise attention by using a series of threedimensional masks , which could be obtained through multi-task learning, to re-weight the feature map not only on spatial-wise but also on channel wise. Chen et al. <ref type="bibr" target="#b4">[5]</ref> use channel attention module which could calculate correlation coefficient among different channels to realize attention mechanism on channel-wise. Due to the additional streams used for attention, all these methods achieve better performance at the cost model size as well as the inference time. On the contrary, our method achieves attention learning only in the training stage and does not alter the inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local features in person ReID</head><p>Extracting the appearance representations of a whole person image has been well studied for person ReID, and encouraging performance is achieved in the past years with the fast development of CNNs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref>. However, these global features usually do not perform well in cases of pose variation, occlusions and missing parts. Methods based on local features have been proposed to handle such issues. The most common way to extract local features is through image dicing, which directly divides person image into horizontal stripes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> or grids <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1]</ref>. Then the extracted local features are assembled to form person appearance representations.</p><p>Another solution is performing part-alignment based on either keypoints or body parts which are estimated by separate models. For example, Varior et al. <ref type="bibr" target="#b23">[24]</ref> applies affine transformation to align the same keypoints. Kalayeh et al. <ref type="bibr" target="#b11">[12]</ref> extract local features for different human body structure ROIs (Region-Of-Interest) to obtain local features and concatenate them with global features. Zhao et al. <ref type="bibr" target="#b29">[30]</ref> obtain part-alignment representations according to the body parts detection.</p><p>All of these part-alignment methods require an additional model (either for keypoints regression or for semantic parsing), which leads to extra computation complexity. Furthermore, since all the exiting local feature based methods have to concatenate many feature embeddings, either from different image regions or body parts, it finally results in a very high dimensional embedding which are less efficiency to be dealt with in practice. For comparison, the proposed PAB achieves local feature learning in a significantly different way. In essential, it implicitly extracts the locally spatial features by decoupling feature maps at channel dimension which has never been explored before. What is more, our method does not need additional models to detect local features, thus keeping high efficiency in inference time.</p><p>or keypoints for getting more robust feature representations by excluding the background clutter or conducting partalignment when extracting features. By simultaneously predicting human mask and extracting feature representations, MGCAM <ref type="bibr" target="#b18">[19]</ref> and SPReID <ref type="bibr" target="#b11">[12]</ref> have reported improved results with the help of human mask. Su et al. <ref type="bibr" target="#b19">[20]</ref> also use keypoints to guide division of human image so as to extract part-aligned local features. Different from these methods, the keypoints prediction in our method is separately conducted on different groups of feature channels, aiming to decouple the learned feature maps channel wise, which is a new way to implement local attention. We will show later that directly predict keypoints like most keypoint prediction methods does not work well in our ablation study (Section 4.3.2). Due to the decoupling learning of feature channels, our method improves robustness to occlusions and pose variance, thus better performance could be expected. As for the human mask prediction, our method uses it to guide holistic attention learning through back propagating, while previous works need to combine with a separate stream for predicting mask during inference. Finally, for existing ReID methods using these auxiliary information like human mask and keypoints, they use either existing detectors or separately trained ones, no co-training is involved as our do. They also have to change the original network both for training and test stages, and so the model size and computational cost in the inference stage are increased due to the added branches, such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Our work is also related to the multi-task learning in the perspective that we use two additional branches as implicit constraints to adjust the feature map learning in the backbone network. However, the most commonly used additional task for person ReID is adding classification loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>, which has the same purpose to ReID, i.e., distinguishing different persons. In this paper, we show two tasks with somehow contradicting purposes to ReID 2 can be further used and lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>As shown in <ref type="figure">Fig. 2</ref>, our attention aware feature learning method can be applied to existing CNNs designed for person ReID (whose structure is called as base network in this paper), by simply adding two branches from the backbone network during the training process. While for the inference using the trained network, the two added branches are removed and only the base network is used. Therefore, the proposed attention aware feature learning method could be considered as a general framework to adjust a ReID network by retraining with the two additional losses defined on the proposed branches respectively, while keeping the test network structure unchanged as the originial ReID network. On one hand, one branch named Holistic Attention Branch (HAB) is designed for guiding the learned features being aware of the global human body in the clutter background, so as to make the backbone network pay more attention to the persons rather than the background. This is achieved by back propagating the supervision information about the human body mask through this branch to the backbone network. On the other hand, the Partical Attention Branch (PAB) is proposed to make the learned features from backbone could be decoupled into different groups, each of which is capable of predicting several human body keypoints that are predefined according to their positions. In this way, the learned features are implicitly part-aligned, improving the robustness to occlusions and pose variations. As a result, our objective for learning feature embedding is</p><formula xml:id="formula_0">L = L r + ? h L h + ? p L p<label>(1)</label></formula><p>where L r stands for a ReID loss (e.g., the triplet loss for hard examples if we use TriNet <ref type="bibr" target="#b9">[10]</ref> as the base network), L h is the loss computed on the HAB and L p is the one computed on PAB. ? h and ? p are two trade-off parameters. The details about the structures and the related losses are elaborated in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Holistic Attention Feature Learning</head><p>In order to make the learned features be able to focus more on the body part instead of the background, we use these features to predict the human body mask. The basic hypothesis for doing so is that the ability for generating the mask of a human body is highly correlated to the features' awareness of body part in the cluttered background. Therefore, if the mask can be predicted well, the features used for this task are considered being aware of body part in the background clutter. Inspired by this hypothesis, we introduce the Holistic Attention Branch to use the feature maps generated from the first convolution block (i.e., the low-level features) as input, which are forwarded through some encoding and decoding layers to output the predicted human body mask. The structure of HAB is illustrated in the right top part of <ref type="figure">Fig. 2</ref>, where the encoder has the identical structure to the remaining part of the backbone network and the decoder consists of four deconvolution layers <ref type="bibr" target="#b28">[29]</ref> and one 1 ? 1 convolution layer. The underlying reasons behind the design of encoding part of this branch are two folds. Firstly, the features extracted by CNNs gradually represent low-level to high-level ones. The low-level features are considered to be common ones for various tasks, while the high-level features are mostly task specific ones. For this reason, we need to build the mask prediction branch from the low-level features since human mask prediction is a different task to person ReID. Secondly, keeping the encoder has the identical structure to the backbone network <ref type="figure">Figure 2</ref>. Structure of the proposed deep attention aware feature learning for person ReID. Our method adds two branches from the backbone network at training stage so as to guide the backbone being able to learn global and local attention aware features. The PAB forces the separated groups of feature channels focus on predefined body parts by predicting their corresponding keypoints. The HAB branch predicts the mask of a person and restricts the backbone network to focus on person bodies instead of background. PAB and HAB do not influence the testing stage.</p><p>can expect a good performance of human mask prediction as well as further imposes constraint on the shared low-level features to be general enough for different tasks. The detailed structure of the decoder in this branch is given in Table 1. The loss L h related to this branch is</p><formula xml:id="formula_1">L h = 1 N N n=1 z n ? m n 2<label>(2)</label></formula><p>where N is the batch size, z n denotes the output of HAB for the n-th input, and m n is a binary image as the groundtruth body mask of the n-th input:</p><formula xml:id="formula_2">m n (x , y ) = 1 if (x , y ) is within a person body 0 else (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Partial Attention Feature Learning</head><p>While the holistic attention feature learning could lead to a feature map focusing more on the human body out of the background clutter, local attention can be further helpful for person ReID. Lots of previous works have been proposed to explore the local features for ReID task. For example, PCB <ref type="bibr" target="#b21">[22]</ref> dices image into horizon stripes and extracts local features from each parts. SPReID <ref type="bibr" target="#b11">[12]</ref> uses a human semantic parsing branch to obtain local features. PDC <ref type="bibr" target="#b19">[20]</ref> uses keypoints explicitly to extract local features of different body part regions. All of these methods finally obtain feature representations by concatenating local features of different body parts (and some methods even have to combine global features), thus inevitably resulting in a very high dimensional embedding vector for person ReID. On the contrary, we realize the partial attention feature learning by separating the feature maps extracted by the backbone network into different groups, each of which is trained to be responsible for predicting a specific group of keypoints. Compared to the previous works, our solution is more efficient as it does not increase the final feature embedding length. Moreover, since we use the same decoder to each group of feature maps for keypoint prediction, these groups are considered to be decoupled in the ideal case where each group can be used to predict the related keypoints perfectly. Although this can not be achieved in real case, the learning process is designed towards generating features with this property, thus our partial attention feature learning could also be considered as a kind of decoupled feature learning. The advantage of such decoupled feature learning is that it improves robustness to occlusions and pose variance. In case of occlusion happens, the disappeared body parts can only affect the corresponding feature channels, while other feature channels can still work well, thus the influence of partial occlusions to all feature channels could be limited to a small content.</p><p>Under this basic idea, the partial attention feature learning module takes the feature maps outputted by the backbone network as input, and manually separates them into several groups to predict different groups of keypoints. The learning procedure and the network structure are shown in the <ref type="figure">Fig. 2</ref> and <ref type="figure" target="#fig_0">Fig. 3(a)</ref>. Supposing a is the input image, F is the mapping function of backbone network and x = F (a) is the output feature maps of a. If we divide x into M groups, then x = {x 1 , x 2 , ? ? ? , x M } = {F 1 (a), F 2 (a), ? ? ? , F M (a)}, where x p = F p (a) denotes the feature maps of the p-th group. Given x p as input, four deconvolution layers are followed to decoding the input as output feature maps with the size of input image, which is then convolved with a 1?1 convolution to generate the keypoint prediction results (i.e., in terms of heatmaps). Like other keypoint detection methods <ref type="bibr" target="#b5">[6]</ref>, K keypoints are pre- dicted with K heatmaps respectively, each of which corresponds to one specific keypoint. Therefore, besides the sharing weights deconvolution layers, the 1?1 convolution layers of different groups need to be independent since their role is to map the extracted features to different numbers of heatmaps.</p><p>Intuitively, different groups of feature maps can be used to predict different body parts, instead of the keypoints. For example, through image dicing, one can divide person image into horizontal stripes equally so that each stripe coarsely represents a part of human body. A straightforward of implementing the partial attention feature learning is to predict these divided horizontal stripes, given different groups of feature maps. However, such a method will be influenced by the background as the divided stripes contain background inevitably. What is worse, predicting the background as the body part will be harmful to the previously introduced holistic attention feature learning, whose role is to learning features focus on body part out of the background clutter. These are the reasons why we choose to predict the human body keypoints to realize the partial attention feature learning. In addition, since the PAB is built on feature maps after the HAB, it also benefits from the human body aware feature representations moduled by learning with HAB. This further emphasizes the importance of holistic attention learning.</p><p>Based on the above analysis, the proposed PAB is optimized according to the following regression loss for keypoint prediction:</p><formula xml:id="formula_3">L p = 1 N ? K N n=1 K k=1 p n,k ? g n,k 2<label>(4)</label></formula><p>where N denotes the number of training samples in a batch, K denotes the number of keypoints of a person; k denotes the k-th output channel that corresponds to the k-th keypoint, p and g are the output and groundtruth respectively. For K keypoints, there are K output channels with the same size of input image, each of which predicts the location of a specific keypoint. Thus, p n,k denotes the k-th prediction channel for the n-th input image and g n,k is the groundtruth probability map of the k-th keypoint in the n-th image. Similar to the keypoint prediction methods <ref type="bibr" target="#b5">[6]</ref>, the groundtruth probability map of a keypoint is represented as a Gaussian distribution with the center in the position of keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementations and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Network structure</head><p>Although most of existing ReID networks can be used in our method as the base network, we implement our method on the basis of the widely used TriNet <ref type="bibr" target="#b9">[10]</ref> without loss of generality. Trinet <ref type="bibr" target="#b9">[10]</ref> consists of backbone network i.e. ResNet-50 <ref type="bibr" target="#b8">[9]</ref> and 2 fully connected layers in the end, which is simple and concise. The parameters of decoders in HAB and PAB are listed in <ref type="table" target="#tab_0">Table 1</ref>. In addition, to demonstrate the universality of our method and how good performance can it achieve, we also implement our method on Bag of Tricks <ref type="bibr" target="#b14">[15]</ref> due to its state of the art performance. The structure of Bag of Tricks is as same as Trinet except for a normalization layer and it is trained by optimize the triplet loss and cross entropy loss. The two trade-off parameters in the learning object (Eq. (1)) is set as ? h = ? p = 0.003. The initial learning rate is 0.001. Note that the two additional branches are only used for network training. The original base network structure is used for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Weakly supervision of human body mask and keypoint</head><p>By adding two attention aware branches, our approach needs human mask and keypoint annotations to supervise the network training. However, manually labeling these annotations for the existing ReID dataset is impractical and labor expensive. Fortunately, there are many human mask detectors as well as keypoint detectors trained on the COCO dataset. We can freely use their detection results as weakly supervised information for these two branches. Although the human masks and keypoints generated in this way are not as accurate enough, our method can still benefit from the limited accurate annotations. In fact, our experiments show that the better the used detectors are, the higher ReID performance will our method achieve. Since the accuracy of these mask and keypoint detectors is still far from to reach the satisfactory level, it implies a big potential of using our method to improve the ReID performance. It is no doubt that our method will benefit from the development of these two techniques in the future. In this paper, we use CPN <ref type="bibr" target="#b5">[6]</ref> to generate keypoints of a person and ResNet-50 with 4-layer deconvolution head to generate human masks. Note that leveraging on existing detection results is not new in this paper, previous works <ref type="bibr" target="#b19">[20]</ref> have been proposed to rely on keypoint alignment to improve ReID performance. Our method explores more on this direction and further improves previous results. The keypoint prediction in our method is only added in training stage and removed during inference. Thus, our method needs no extra computation cost on keypoints detection during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Keypoint grouping</head><p>In this paper, the keypoints of a person are divided into 6 groups according to their positions, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. Accordingly, the feature maps used to predict the keypoints are also divided into 6 groups along the channel dimension, each of which is associated with a keypoint group. The output of ResNet-50 Block-4 has 2048 channels and so each group has 2048/6 = 341 channels. The omitting 2 channels do not participate in keypoint prediction. We will show later in the ablation study that our method is insensitive to the specific way to group the keypoints (cf. <ref type="table">Table 6</ref>). What is important is to separate the feature channels into different groups for predicting keypoints respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use three widely used holistic datasets to validate the effectiveness of the proposed method, i.e. Market-1501 <ref type="bibr" target="#b31">[32]</ref>, CUHK-03 <ref type="bibr" target="#b12">[13]</ref>, DukeMTMC <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref>. Market-1501 has 32668 annotated bounding boxes of 1501 identities collected by six cameras. There are 12936 images used for training. The query and gallery sets have 3368 and 19732 images respectively. CUHK-03 is a dataset consisting of 1467 identities for 13164 images. It is divided into two parts, CUHK-03(labeled) with manually labeled pedestrian bounding boxes and CUHK-03(detected) with DPM detected pedestrian bounding boxes. According to Zhong et al. <ref type="bibr" target="#b36">[37]</ref>'s split, we divide the dataset into a training set and a testing set similar to Market-1501. DukeMTMC consists of 16522 training images of 702 identities, and 19889 images of the other 702 identities as testing set. The number of query images is 2228 and the number of gallery images is 17661.</p><p>Additionally, two partial datasets, i.e. Partial-REID <ref type="bibr" target="#b34">[35]</ref> and Partial-iLIDS <ref type="bibr" target="#b33">[34]</ref> are used to evaluate the robustness of the proposed method to occlusions. Partial-REID has 60 identities, each of which has 5 partial images as query and 5 holistic images as gallery. Partial-iLIDS is a dataset consisting of 119 identities where one partial image is in query set and one holistic image is in gallery set for each person. Since there is no training data for Partial-REID and Partial-iLIDS, we test on these two partial datasets using the model trained on Market-1501.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results on holistic datasets</head><p>To demonstrate the effectiveness of the proposed deep attention aware feature learning (DAAF), we apply it to two typical ReID models respectively. The first one is the TriNet <ref type="bibr" target="#b9">[10]</ref>, which shows that the feature embedding for person ReID can be simply learned by a triplet loss using ResNet with hard examples mining. It is one of the most influential works in deep learning based ReID. The other one is the recently proposed Bag of Tricks <ref type="bibr" target="#b14">[15]</ref> which achieves the state of the art performance by collecting several practical training methods on ReID task and implanting a normalization layer to release the inconsistency between metric learning loss (i.e. triplet) and classification loss (i.e. cross entropy). For comparison, we also report the results of some other methods from their original papers. These methods include three attention based methods (HA-CNN <ref type="bibr" target="#b13">[14]</ref>, Mancs <ref type="bibr" target="#b24">[25]</ref>, AACN <ref type="bibr" target="#b26">[27]</ref>), two ReID methods based on local features (PCB <ref type="bibr" target="#b13">[14]</ref> and PDC <ref type="bibr" target="#b19">[20]</ref>), and two methods using human mask as additional information for ReID (MG-CAM <ref type="bibr" target="#b18">[19]</ref> and SPReID <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b14">[15]</ref> with re-ranking <ref type="bibr" target="#b36">[37]</ref> achieves the state of the art performance on these benchmarks. * means re-ranking. Note that how DAAF improves existing methods.</p><p>The quantitative results are reported in <ref type="table">Table 2</ref>. It is obvious that the proposed DAAF effectively improves over TriNet and Bag of Tricks in all cases. Especially, DAAF-Bag of Tricks with re-ranking achieves the state of the art performance in terms of both mAP and Rank-1 accuracy. When comparing to the base network, DAAF-TriNet improves over TriNet by 4.92%-6.82% for mAP and 3.36%-7.16% for Rank-1 accuracy on Market-1501, CUHK-03, and DukeMTMC-reID datasets. When using a better base network, DAAF-BoT achieves the state of the art performance on these benchmarks and finally DAAF-BoT* reaches the best results with re-ranking on all these tested datasets.</p><p>The proposed DAAF is designed as additional objectives in the training stage in order to make the learned feature maps for ReID be aware of the person and the body parts. The greatest difference compared to other existing attention based methods lies in that this implicit manner of attention learning does not need to reweight a separate stream to the feature maps. Thus, our DAAF could be easily incorporated into other ReID networks. It could be seen from <ref type="table">Table 2</ref>, when added to Bag of Tracks, which already achieves the state of the art perfromance, our DAAF still could bring significant improvements on all these tested datasets, especially on the CUHK-03 dataset. Compared to existing methods based on local features or using human mask, our method takes both global and local attentions into consideration simultaneously. Particularly, the channel wise decoupling design in partial attention learning leads to the learned features for ReID more robust to pose variation and partial occlusion, which will be further verified in the following experiment results on partial datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Partial-iLIDS Partial-REID  <ref type="table">Table 3</ref>. Results on two partial ReID datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results on partial datasets</head><p>One advantage of our method is that the final feature embeddings are from those decoupled feature channels implicitly learned by PAB, therefore, if occlusion happens, only partial feature channels will be influenced, instead of all. As a result, our method is capable of dealing with partial occlusion in ReID. To show this point, we test our method on two additional partial ReID datasets, i.e., Partial-iLids and Partical-REID. For comparison, besides the base networks used in our method, we also report the results of PCB, which is a typical method based on local features. All the tested models are trained on Market-1501. Since the purpose of this paper is not for partial ReID, here we do not compare to those methods specially designed for partial ReID tasks that adopt different solutions to normal ReID tasks, which in turn has inferior ReID performance on general cases, such as the three benchmarks on <ref type="table">Table 2</ref>. The results shown in <ref type="table">Table 3</ref> confirm that the proposed method is effective for handling the occlusion problem, with improved performance to the base network, either TriNet or Bag of Tricks. It also outperforms other local feature based methods such as PCB, even with a simple base network like TriNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Visualization results of attention learning</head><p>In order to visualize our attention learning, we use Grad-CAM (Gradient-weighted class activation maps) <ref type="bibr" target="#b16">[17]</ref> to show the active part of our feature map. Slightly different from gradient-weighted class activation maps, we show the activation maps of feature embedding. Assume the embedding vector is x = [x 1 , ..., x N ] T . According to the Grad-CAM <ref type="bibr" target="#b16">[17]</ref>, the gradient-weighted activation maps of x n can be presented by weighted sum of the feature maps:</p><formula xml:id="formula_4">L n Grad?CAM = 1 K k Abs(F n k A k )<label>(5)</label></formula><p>where A k means k-th channel of feature map, K represents the number of channels, represent element-wise multiply and</p><formula xml:id="formula_5">F n k = ?x n ?A k<label>(6)</label></formula><p>where the derivative of x n with respect to feature maps discribe the importance of each element from feature maps to the embedding component x n .</p><p>We add gradient-weighted activation maps of all channels to represent heatmap of holistic attention (Holistic Attention Map):</p><formula xml:id="formula_6">L HAM = 1 N n L n Grad?CAM<label>(7)</label></formula><p>Using this method, we visualize the activation maps of our backbone network. In Equation 5, we use final embedding vector as x and take A equal to ResNet conv1 feature map. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, the activation map of ResNet conv1 concentrate more on the whole person instead of background clutter or only a specific small part of body.</p><p>Similarly, instead of using the final embedding vector to visualize holistic attention, we use the vector generated by average pooling on different groups of the feature maps to visualize partial attention. The activation map of i-th group could be calculated by: where S i is the set containing all serial number of channels in i-th group. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, through partial attention learning, different groups of channels could concentrate on different body parts. </p><formula xml:id="formula_7">L i P AM = 1 Card(S i ) n?Si L n Grad?CAM<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Learning results of HAB and PAB</head><p>Additionally, although HAB and PAB are removed during inference, and the quality of their outputs is not related to ReID task directly. We still care about what HAB and PAB could output.</p><p>We add HAB to backbone network during inference and output the mask prediction results of HAB. To make comparison, we use the same mask detector which is used to obtain training labels to generate segmentation heatmap. As shown in <ref type="figure">Figure 7</ref>, the outputs of HAB are highly similar to the outputs of mask detector, which suggest that HAB obtain much useful information from the mask detector through using labels generated by the mask detector in training stage. The results also show that even though a small coefficient of mask loss is used to conduct weak supervision of human body, the learning results of HAB is not bad. <ref type="figure">Figure 7</ref>. Outputs of HAB during inference. For each group, the left is image in testing set during inference, and the middle is the segmentation heatmap generated by the same mask detector as training sets'. The right is segmentation heatmap generated by HAB.</p><p>Similarly, we add PAB to backbone network during inference and output the keypoints prediction results of PAB. <ref type="figure" target="#fig_4">Figure 8</ref> shows that the shape of keypoints prediction results varies from standard Gaussian Window, but the position of high response region is corresponding to the position of keypoints accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The effectiveness of HAB and PAB</head><p>We conduct experiment on Market-1501 to verify the effectiveness of the proposed HAB and PAB, whose results are shown in <ref type="table">Table 4</ref>. To this end, we apply HAB and PAB separately to a base network (TriNet) so as to observe how the performance changes. Since PAB aims to incorporate the partial attention about human keypoints into the feature learning process, the learned features will be aware of the global human body to some extent too. Therefore, compared to HAB that only enforces the feature learning towards perception of the human body, training with PAB is somewhat better as it enables the learned features not only being useful for predicting local information such as keypoints but also for the global human body. This analysis is consistent to the results shown in <ref type="table">Table 4</ref>, where using PAB individually is better than using HAB. On the other hand, as other keypoint detection methods, PAB implements the keypoint prediction by heatmap regression where the groundtruth is generated by applying a Gaussianwindow centered at the groundtruth keypoint. This kind of regression target inevitably contains some background areas, which can not be entirely suppressed by PAB alone. Therefore, HAB is required as a complementary to further alleviate the distraction of the background. Experimental results obtained when combining HAB and PAB together achieve the best results in <ref type="table">Table 4</ref>   <ref type="table">Table 4</ref>. Ablation study of the PAB and HAB modules based on Market-1501 dataset. The numbers in the brackets are the improvements relative to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Channel grouping for local attention</head><p>In PAB, we propose a new method to learn the local attention aware features by channel decoupling. Each channel group is explicitly forced to learn features being responsible for a group of keypoints in a specific body part. In this way, the learning process will encourage different feature groups focus on different keypoints, while being less useful in predicting keypoints in other groups. Thus, achieving the decoupling of feature channels in term of keypoint prediction ability. Alternatively, a typical technique used in the literature for learning features responsible for keypoint prediction is to take all the feature channels together as input to a decoder, and output K probability maps, each of which corresponds to a specific keypoint. To show the advantage of our method, we implement this widely used technique to replace the proposed one in PAB. From the results reported in <ref type="table" target="#tab_4">Table 5</ref>, we can see that such a method to introduce local attention is not helpful. On the contrary, with our grouping strategy, the performance is significantly improved. This study validates that the boosted performance by the proposed PAB is not simply due to the incorporation of keypoint prediction in ReID, but more importantly, is because of the novel way (decouple the feature channels) we proposed to use it. Note that in this experiment, we only apply PAB to the base network so as to reduce the influence of HAB for our analysis. To further show the importance of decoupling feature channels, we conduct another experiment on PAB by replacing the keypoint prediction with the part image prediction. Specifically, we divide the input image into 6 equal parts horizontally as PCB <ref type="bibr" target="#b21">[22]</ref> does. As shown in <ref type="figure" target="#fig_0">Fig. 3(b)</ref>, the groundtruth consists of blank area and specific image part, in which only 1/6 image could be seen. Then we separate the output feature channels of backbone into 6 groups, each of which is responsible for predicting a corresponding body part image. As shown in <ref type="table" target="#tab_4">Table 5</ref>, even with such a simple supervision, PAB still works well and improves the baseline substantially. Compared to using keypoint prediction as supervision information, predicting the partial body image is inferior because the horizontal body part image inevitably contains a larger portion of background than keypoint and the part appearance is more complex than the heatmap of keypoints. Therefore, although the feature channel decoupling is key to the performance improvement in PAB, it can still benefit from a well designed task relying on local features, such as the keypoint prediction used in this paper. How to incorporate a better task for realizing the local attention is beyond the scope of this paper, and will be studied as our future work.</p><p>Finally, although PAB relies on keypoint grouping to implement the channel wise decoupling, the specific way of keypoint grouping only has a limited influence on its performance according to our experiments (see <ref type="table">Table 6</ref> for the results of different grouping schemes shown in <ref type="figure" target="#fig_1">Fig. 4</ref>). The possible reason could be that different keypoints of a human body are mostly related to each other, it is hard to manually find a good grouping of them so as to make different groups are potentially unrelated, in which case the performance may benefit most from the proposed decoupling learning. In common cases, there is little difference when there are correlations among different groups. Among the tested grouping schemes, as shown in <ref type="table">Table 6</ref>, the best performance is achieved when using 6 parts which is closely followed by utilizing 7 parts. This results is relatively consistent to some other methods which use similar body part grouping schemes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22]</ref>.  <ref type="table">Table 6</ref>. Performance on Market-1501 for different keypoint grouping schemes shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. "all" refers to use all keypoints together as one group, while "each" is to use each keypoint separately for one group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Weights sharing mechanism</head><p>Shared-weights deconvolution layers are incorporated into PAB to conduct channel grouping, which means six groups of keypoints prediction use the same decoder. The reason why we don't use 6 independent decoders is that decoders should not be related to the group of keypoints or they may have partial information of human body. The goal of partial attention learning is to let different channels of feature map focus on different parts rather than let different decoders focus on different parts. Therefore, using the same decoder for all channel groups will guarantee the decoder not related to a specific body part, which could let different channels of feature maps focus more on corresponding body parts. For comparison, we use 6 independent branches to conduct the keypoints or diced image prediction. The results are shown in <ref type="table">Table 7</ref>. As shown in <ref type="table">Table 7</ref>, partial attention learning using 6 independent branches performs worse than using the same one. The results based on keypoints and based on part images are consistent, which suggest that using the same decoder is essential to decoupled feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">1x1 conv shuffle in PAB</head><p>Because of different number of keypoints in each group, the 1x1 convolution layers are different while the deconvolution   <ref type="table">Table 7</ref>. We use 6 independent decoders to predict keypoints or part images in comparsion with the method using the same decoder. No HAB is used in order to explore PAB design.</p><p>layers for each group are same. We will show that, to a great extent, 1x1 convolution layers contains little part-related information. We conduct 1x1 convolution layers shuffle experiments. As shown in <ref type="figure">Figure 10</ref>, we combine different channel groups of feature map with different 1x1 convolution layers during inference. If each group of the output heatmaps corresponds to the 1x1 convolution layers no matter which group of feature maps input, 1x1 convolution layers instead of the feature maps are responsible for predicting the keypoints of different body parts. This suggests each group of feature maps doesn't well focused on the cor-responding body part. On the contrary, if each group of the output heatmaps corresponds to the input feature maps no matter which group of 1x1 convolution layers is used, the 1x1 convolution would include little information of body parts.</p><p>The results are shown in <ref type="figure" target="#fig_5">Figure 9</ref>. The results show that the outputs are mainly determined by inputs feature maps instead of 1x1 convolution layers, which suggests that the feature are possibly highly decoupled on channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an attention aware feature learning method for person re-identification. The proposed method consists of a partial attention branch (PAB) and a holistic attention branch (HAB) that are jointly optimized with the base re-identification feature extractor. Since the two branches are built on the backbone network, it does not introduce additional structure for ReID feature extraction. Therefore, our method is able to maintain the same inference time as the original network, while most previous works realize the attention mechanism at the cost of higher <ref type="figure">Figure 10</ref>. 1x1 conv shuffle in PAB. We combine the 1x1 convolution layer of different groups with different feature maps to conduct keypoints prediction. All 1x1 convolution layer and feature maps are trained normally, and we only shuffle them during inference. inference time. Experimental results show that the proposed feature learning method is general enough by integrating it into two different base networks and consistent performance improvement is observed. With a strong base network such as Bag of Tricks proposed recently, our method achieves the state of the art performance on various ReID benchmarks. In addition, due to the channel wise decoupling features learned through PAB, the proposed method also works well on ReID tasks with partial occlusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Structure of PAB branch. Keypoints or horizontal strips of different body parts could be predicted based on different feature channel groups. (a) is used for this work, and we will compare (a) and (b) in Section 4.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of different grouping schemes of human keypoints. (c) is used in this work, while other configurations are considered in our ablation study ( Section 4.3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of holistic attention feature learning. For each group, the left is original image. The middle is the output activation maps of our baseline Trinet<ref type="bibr" target="#b9">[10]</ref> without holistic attention learning, while the right is the output activation maps of Trinet<ref type="bibr" target="#b9">[10]</ref> with holistic attention learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of partial attention feature learning. The left is original image. And from left to right, there are activation maps of 6 channel groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Outputs of PAB during inference. The heatmaps output by PAB which could be used for keypoints prediction. The high response region of heatmap is accurately corresponding to the position of keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Results of 1x1 conv shuffle. In each row, a fixed set of feature maps is combined with different 1x1 convolution layers. The diagonal of the image represents feature map and 1x1 convolution layer are matched. The outputs are mainly determined by inputs feature maps instead of 1x1 convolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>layer</cell><cell>#channels in</cell><cell>#channels out</cell><cell cols="2">kernel size stride</cell></row><row><cell cols="2">deconv 1 2048 (HAB) 341 (PAB)</cell><cell>64</cell><cell>3 ? 3</cell><cell>2</cell></row><row><cell>deconv 2</cell><cell>64</cell><cell>64</cell><cell>3 ? 3</cell><cell>2</cell></row><row><cell>deconv 3</cell><cell>64</cell><cell>64</cell><cell>3 ? 3</cell><cell>2</cell></row><row><cell>deconv 4</cell><cell>64</cell><cell>64</cell><cell>3 ? 3</cell><cell>2</cell></row><row><cell>1 ? 1 conv</cell><cell>64</cell><cell>1 (HAB) keypoint groups (PAB)</cell><cell>1 ? 1</cell><cell>1</cell></row></table><note>. Parameters of deconvolution layers and 1 ? 1 convolution layer in HAB and PAB.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The effectiveness of channel wise decoupling in PAB. The results are reported on the Market-1501 dataset. The numbers are the relative increase/decrease compared to the base network, i.e., TriNet. No HAB is used in order to focus on different choices of PAB.</figDesc><table><row><cell>Methods</cell><cell cols="2">with Group w/o Group mAP Rank-1</cell></row><row><cell>keypoint</cell><cell>+0.92</cell><cell>+0.03</cell></row><row><cell>part image</cell><cell>+2.74</cell><cell>+1.66</cell></row><row><cell>keypoint</cell><cell>+3.83</cell><cell>+1.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Mask/keypoints prediction is to find common features among different people (unable to distinguish persons), while ReID targets at finding discriminative features for each person.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5363" to="5372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiat-Pin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmili</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Deconvolutional networks</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person reidentification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Partial person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4678" to="4686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
