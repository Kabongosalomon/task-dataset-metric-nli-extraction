<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oriented RepPoints for Aerial Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
							<email>jkzhu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Oriented RepPoints for Aerial Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In contrast to the generic object, aerial targets are often non-axis aligned with arbitrary orientations having the cluttered surroundings. Unlike the mainstreamed approaches regressing the bounding box orientations, this paper proposes an effective adaptive points learning approach to aerial object detection by taking advantage of the adaptive points representation, which is able to capture the geometric information of the arbitrary-oriented instances. To this end, three oriented conversion functions are presented to facilitate the classification and localization with accurate orientation. Moreover, we propose an effective quality assessment and sample assignment scheme for adaptive points learning toward choosing the representative oriented reppoints samples during training, which is able to capture the non-axis aligned features from adjacent objects or background noises. A spatial constraint is introduced to penalize the outlier points for roust adaptive learning. Experimental results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD and DIOR-R, demonstrate the efficacy of our proposed approach. The source code is availabel at: https://github.com/LiWentomng/OrientedRepPoints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being an important computer vision task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>, aerial object detection has recently attracted increasing attention, which plays the significant role in the remote image understanding. Different from the generic object detection, aerial target localization has its own difficulties, including non-axis aligned objects with arbitrary orientations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref> and densely packed distribution with complex context <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>The mainstreamed approaches typically treat aerial object detection as a problem of rotated object localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. Among them, the direct angle-based orienta-* Corresponding author is Jianke Zhu.</p><p>(a) Orientation-regressed method-RetinaNet <ref type="bibr" target="#b18">[19]</ref> (b) Proposed Oriented RepPoints framework <ref type="figure">Figure 1</ref>. (a) denotes the common baseline-RetinaNet <ref type="bibr" target="#b18">[19]</ref> of orientation regression-based methods, (b) is the baseline method of our Oriented RepPoints. Comparing to the direct orientation regression, our approach can estimate more accurate orientations by learning the adaptive points that are marked as red. tion regression methods dominate this research area, which are derived from the general vanilla detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref> with the extra orientation parameter. Although having achieved promising performance, the direct orientation prediction still has some issues including the discontinuity of loss and regression inconsistency <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. This is mainly due to the bounded periodic nature of the angular orientation and the orientation definition of the rotated bounding box. Despite of their attractive localization results, the orientation regression-based detectors may not accurately predict the orientations, as shown in <ref type="figure">Fig. 1-(a)</ref>.</p><p>To effectively address the above issues, we revisit the representation for aerial objects in order to avoid the sensitive orientation estimation. As a fine-grained object representation, point set shows the great potential to capture the key semantic features in conventional generic detector like RepPoints <ref type="bibr" target="#b45">[46]</ref>. However, its simple conversion functions only produce the upright-horizontal bounding boxes, which cannot precisely estimate the aerial objects' orientations. Moreover, RepPoints only regresses the key points according to the semantic features while ignoring to effectively measure the quality of learned points. This may lead to the inferior performance for the non-axis aligned objects with dense-distribution and complex scene in aerial images.</p><p>In this work, we proposed an oriented object detector for aerial images, named Oriented RepPoints, which introduces the adaptive points representation for diverse orientations, shapes and poses. In contrast to conventional orientation regression approach, our proposed method not only achieves the precise aerial detection with accurate orientation, but also captures the underlying geometric structure of the arbitrary-oriented aerial instances, as shown in <ref type="figure">Fig. 1</ref>. Specifically, the initial adaptive points are generated from the center point, which are further refined to adapt for the aerial object. To obtain the oriented bounding box, three oriented conversion functions are presented according to the layouts of the learned points. Moreover, an effective adaptive points assessment and assignment (APAA) scheme is proposed for point set learning, which measures the quality of oriented reppoints not only from the classification, localization but also from their orientation and point-wise feature correlation during training. Such scheme enables the detector to capture the non-axis aligned features from adjacent objects or background noises toward assigning the representative oriented reppoints samples. Furthermore, a spatial constraint is proposed to facilitate the vulnerable points to find their instance owner from the complex context in the aerial scene. Compared with the orientation regressionbased methods, our framework obtains more precise detection performance with accurate orientation.</p><p>In summary, the main contributions of this paper are: (1) an effective aerial object detector named Oriented Rep-Points, where the flexible adaptive points are introduced as the representation to achieve the oriented object detection; (2) a novel quality assessment and sample assignment scheme for adaptive points learning, which selects the points sample not only from the classification, localization but also from the orientation, point-wise feature correlation; (3) extensive experiments on four challenging datasets showing promising qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unlike most of generic object detectors with horizontal bounding boxes, the targets in aerial images are often arbitrary-oriented and dense-distributed. We discuss the related work in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Oriented Object detection</head><p>The recent aerial object detection methods are mainly derived from the classical object detectors by introducing the orientation regression.</p><p>SCRDet <ref type="bibr" target="#b44">[45]</ref>, CAD-Net <ref type="bibr" target="#b46">[47]</ref>, DRN <ref type="bibr" target="#b23">[24]</ref>, R3Det <ref type="bibr" target="#b41">[42]</ref>, ReDet <ref type="bibr" target="#b8">[9]</ref> and Oriented R-CNN <ref type="bibr" target="#b35">[36]</ref> achieve the promising performance by predicting the rotation angles of bounding boxes. Gliding Vertex <ref type="bibr" target="#b36">[37]</ref> and RSDet <ref type="bibr" target="#b24">[25]</ref> improve the detection results through regressing quadrilateral. To address the boundary discontinuity in the angel-based orientation estimation, Yang et al. <ref type="bibr" target="#b40">[41]</ref> transform the angular regression to angular classification <ref type="bibr" target="#b39">[40]</ref>. Later, Yang et al. <ref type="bibr" target="#b42">[43]</ref> convert the parameterization of the rotated bounding box into 2-D Gaussian distributions, which gains more robust results for the oriented object detection. These methods are devoted to improving orientation estimation using rotation angle representation. Alternatively, we introduce a more effective representation using adaptive points in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Non-axis Aligned Features Learning</head><p>Most of conventional object detection methods <ref type="bibr">[?, ?, 26, 28,31,46,51]</ref> focus on either upright or axis-aligned objects, which may have difficulties with the non-axis aligned targets densely distributed in the complex background. To address this issue, Ding et al. <ref type="bibr" target="#b2">[3]</ref> adopt the spatial transformations on axis-align RoIs and learn the non-axis aligned representation under the supervision of oriented bounding box. SCRDet++ <ref type="bibr" target="#b43">[44]</ref> enhances the non-axis aligned features and bring the higher object response to train the network. Han et al. <ref type="bibr" target="#b7">[8]</ref> design a feature alignment module to alleviate the misalignment between axis-aligned convolutional features and arbitrary oriented objects. DRN <ref type="bibr" target="#b23">[24]</ref> proposes the feature selection module to aggregate the non-axis aligned information from the different kernel sizes, shapes and orientations, and employs the dynamic filter generator for further regression. Guo et al. <ref type="bibr" target="#b6">[7]</ref> employ the convex-hull representation to learn the irregular shapes and layouts, which intend to avoid the feature aliasing via learnable feature adaption. Our point set-based method amis to capture the key features for non-axis aligned aerial objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Samples Assignment for Object Detection</head><p>A large amount of detection methods adopt a simple way to set the IoU threshold for selecting the positive samples. However, such scheme cannot guarantee the overall quality of training samples due to the potential noise and hard cases <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. Some recent samples assignment methods in general object detection, such as the ATSS <ref type="bibr" target="#b47">[48]</ref>, FreeAnchor <ref type="bibr" target="#b48">[49]</ref>, PAA <ref type="bibr" target="#b12">[13]</ref> and OTA <ref type="bibr" target="#b5">[6]</ref>, employ a learning-tomatch optimization strategy <ref type="bibr" target="#b49">[50]</ref> to choose the high-quality samples. In aerial scene, it is essential to select the highquality samples for learning the oriented detector due to the diversity of the orientation and dense distribution. Ming et al. <ref type="bibr" target="#b22">[23]</ref> introduce a matching degree measure to evaluate the spatial alignment based on the oriented anchors, which use the matching sensitive loss to enhance the correlation between classification and oriented localization. In this work, we suggest an effective quality assessment and sample assignment scheme to select the positive points samples.   <ref type="figure">Figure 2</ref>. The framework of Oriented RepPoints. The proposed method is an anchor-free approach with the adaptive points as the representation, where a backbone with FPN network is employed for feature encoding. The structure of the shared head is same as RepPoints <ref type="bibr" target="#b45">[46]</ref> for each scale of FPN, except of the proposed APAA and the oriented conversion function. Based on learning points from the initialization stage, the APAA scheme is performed only during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Oriented RepPoints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Instead of directly regressing the orientations like the conventional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">45]</ref>, we take advantage of the adaptive point set <ref type="bibr" target="#b45">[46]</ref> as a fine-grained representation, which is able to capture the geometric structure of the aerial objects with sharp variety on orientation in the cluttered environments. To this end, we introduce the differentiable conversion functions, where the representative points are driven to adaptively move toward the appropriate positions over an oriented object. In order to effectively learn the high-quality adaptive points without direct pointsto-points supervision, we suggest a quality measure scheme that selects the high-quality oriented reppoints at the training stage. To facilitate the robust adaptive point learning, the spatial constraints are employed to penalize the vulnerable outliers and find their instance owner from the complicated aerial context. <ref type="figure">Fig. 2</ref> shows the overview of our proposed Oriented Reppoints approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Points Learning with Orientation</head><p>To facilitate the oriented detector with point set representation, the conversion function is introduced to transform the adaptive points into the oriented bounding box. Let G denote the oriented conversion function as below:</p><formula xml:id="formula_0">OB = G(R)<label>(1)</label></formula><p>where OB represents an oriented box converted from the learned point set R. In this paper, we examine three oriented conversion functions:</p><p>? MinAeraRect intends to find the rotated rectangle with the minimum area from the learned point set over an oriented object.</p><p>? NearestGTCorner makes use of the ground-truth annotations. For each corner, we find the closest point from the learned point set as a predicted corner, where the selected corner points are used to build a quadrilateral as the oriented bounding box.</p><p>? ConvexHull. An oriented instance polygon can be defined as a convex hull of a set of points drived by the Jarvis March algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, which is used by many contourbased methods.</p><p>Note that the NearestGTCorner and ConvexHull are differentiable functions while MinAeraRect is not. Thus, we employ MinAeraRect in the post-processing to get the standard rotated rectangle prediction, and the other two differentiable functions are used to optimize adaptive points learning during the training. Under the supervision of the oriented ground-truth annotations, the points move towards the semantic key and geometric features adaptively for each aerial object, which are driven by the classification and localization loss simultaneously.</p><p>The proposed framework consists of two stages. The initialization stage generates the adaptive point sets by refining from the object center point (feature map bins). The refinement stage further gains the accurate adjustment by minimizing the loss function as below:</p><formula xml:id="formula_1">L = L cls + ? 1 L s1 + ? 2 L s2<label>(2)</label></formula><p>where ? 1 and ? 2 are balanced weighting. L cls denotes the object classification loss:</p><formula xml:id="formula_2">L cls = 1 N cls i F cls (R cls i (?), b cls j )<label>(3)</label></formula><p>where R cls i (?) represents the predicted class confidence based on the learned points, and b cls j is the assigned groundtruth class. F cls is the focal loss <ref type="bibr" target="#b18">[19]</ref>. N cls denotes the total number of point sets. L s1 and L s2 represent the spatial localization loss at the initialization and refinement stage, respectively. For each stage, L s can be denoted as below:</p><formula xml:id="formula_3">L s = L loc + L s.c.<label>(4)</label></formula><p>where L loc is localization loss based on converted oriented boxes, and L s.c., denotes the spatial constraint loss. Let N loc denote the total number of positive point set samples. b loc j indicates the location of ground-truth box. Thus, L loc is defined as follows:</p><formula xml:id="formula_4">L loc = 1 N loc i [b cls j ? 1]F loc (OB loc i (?), b loc j )<label>(5)</label></formula><p>where F loc is the GIoU loss <ref type="bibr" target="#b28">[29]</ref> for the oriented polygon. Due to the diversity of different categories and the cluttered background in aerial images, a portion of learned points are susceptible to the background or adjacent objects with strong key features, which may move outside the ground-truth bounding box. To facilitate the vulnerable points to capture the geometric features on its instance owner, we introduce an effective spatial constraint to penalize the adaptive points outside the bounding box. Let ? ij denote the penalty function. The spatial loss L s.c. for each oriented object is defined as below:</p><formula xml:id="formula_5">L s.c. = 1 N a 1 N o i=1 j=1 ? ij<label>(6)</label></formula><p>where N a indicates the number of assigned positive point set samples for each object. N o is the number of points outside the GT box in each point set.</p><p>Let p c denote the geometric center of the ground-truth bounding box. Given a sampled point p o outside the bounding box, the penalty term is defined as below:</p><formula xml:id="formula_6">? = p o ? p c , p o is outside GT 0, otherwise.<label>(7)</label></formula><p>where GT denotes the ground-truth box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Points Assessment and Assignment</head><p>Due to the lack of direct supervision, learning highquality points is essential to capture the geometric features adaptively for densely-packed and arbitrarily-oriented objects in aerial images. To this end, we propose an effective assessment and assignment scheme to measure the quality of learned points, which towards assigning the representative samples of adaptive points as the positive samples at the training stage.</p><p>Quality Measure of Adaptive Points. Firstly, we define a quality measure Q to assess the learned adaptive points from four aspects, including classification and localization ability Q cls , Q loc , orientation alignment Q ori , and pointwise correlation Q poc for each oriented point set. Thus, Q is derived as below:</p><formula xml:id="formula_7">Q = Q cls + ? 1 Q loc + ? 2 Q ori + ? 3 Q poc<label>(8)</label></formula><p>The classification ability Q cls of a point set R i directly reflects its classification confidence R cls i (?), where the corresponding classification loss L cls measures the compatibility of the points feature with ground-truth class label b cls j . We define Q cls as follows:</p><formula xml:id="formula_8">Q cls (R i , b j ) = L cls (R cls i (?), b cls j )<label>(9)</label></formula><p>To evaluate the compatibility of the points position with the ground-truth b loc j , we employ the localization loss as the quality assessment measure, which is based on the IoU transformation. It indicates the spatial alignment when the center of a point set is near to the object's geometric center. Therefore, Q loc is defined as below:</p><formula xml:id="formula_9">Q loc (R i , b j ) = L loc (OB loc i (?), b loc j )<label>(10)</label></formula><p>Since Q loc can be regarded as a measure of spatial location distance, it is insensitive to the orientation variations, especially for the square-like objects in the aerial images. To account for the orientation alignment, we employ Chamfer distance <ref type="bibr" target="#b4">[5]</ref> to assess the difference in orientation between the predicted point set and ground-truth box contour points. We firstly adopt the MinAeaRect conversion function to obtain four spatial corner points {v 1 , v 2 , v 3 , v 4 } from the learned point set. Then, an ordered point set R v (40 points by default) is sampled with the equal interval from two adjacent corner points. Similarly, the points R g are generated for the ground-truth corner points {g 1 , g 2 , g 3 , g 4 }. Therefore, Q ori is defined as follows:</p><formula xml:id="formula_10">Q ori (R i , b j ) = CD(R v i (?), R g bj )<label>(11)</label></formula><p>where CD denotes Chamfer distance between the above two group of sampling points:  <ref type="figure">Figure 3</ref>. The correlation between the predicted classification confidence and localization score of the oriented reppoints with and without APAA scheme.</p><formula xml:id="formula_11">CD(R v , R g ) = 1 2n n i=1 min j (x v i , y v j ) ? (x g i , y g j ) 2 + 1 2n n j=1 min i (x v i , y v j ) ? (x g i , y g j ) 2<label>(12)</label></formula><formula xml:id="formula_12">(x v i , y v j ) ? R v</formula><p>denotes the sampled points of predicted spatial corner points, and (x g i , y g j ) ? R g denotes the sampled points generated from ground-truth corner points.</p><p>To measure the point-wise association upon a point set for an oriented object, we extract the point-wise features and employ the cosine similarity between the feature vectors as the correlation measure Q poc for the learned adaptive points. Let e i,k denote the k-th point-wise feature vector of i-th set of adaptive points. e * i,k and e * i represent the normalized embedding feature vector and their mean from the i-th point set:</p><formula xml:id="formula_13">e * i,k = e i,k e i,k 2<label>(13)</label></formula><formula xml:id="formula_14">e * i = 1 N p k=1 e * i,k<label>(14)</label></formula><p>where N p denotes the num of points in a point set. The default setting is 9. Based on the above notations, Q poc of i-th point sets can be formulated as the point-wise feature diversity as below:</p><formula xml:id="formula_15">Q poc = 1 ? 1 N p k cos &lt; e * i,k , e * i &gt; = 1? 1 N p k e * i,k ? e * i e * i,k ? e * i<label>(15)</label></formula><p>Dynamic k Label Assignment. Based on the quality measure Q, we assign the oriented reppoints samples through an efficient and dynamic top k item selection scheme at different iteration. For each object, we sort all the point set samples from the initialization stage according to their quality scores. To retrieve high-quality adaptive point set samples, we set a sampling ratio ? to assign the top k samples at each iteration as the positive samples for training, which is calculated by:</p><formula xml:id="formula_16">k = ? * N t<label>(16)</label></formula><p>where N t denotes the total number of point set samples at the initialization stage for each oriented object.</p><p>During the training, the points assigner <ref type="bibr" target="#b45">[46]</ref> is used to get the sample assignment of center points at the initialization stage. At the refinement stage, the proposed adaptive points assessment and assignment (APAA) scheme is used to select the high-quality points samples according to the quality measure Q. Only the selected positive point sets are assigned with the ground-truth bounding box of target. As shown in <ref type="figure">Fig. 3</ref>, APAA scheme enables the detector to predict the high-quality oriented reppoints for improving both classification confidence and localization scores. It is worthy of mentioning that the presented scheme is only used for training, which does not incur the computational load at the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Testbed</head><p>DOTA <ref type="bibr" target="#b34">[35]</ref> is a large-scale dataset to evaluate the detection performance of oriented objects in aerial images, which contains 2806 images, 188,282 instances and 15 categories with a variety of orientations, scales, and shapes. The training set has 1411 images while the validation set contains 458 images. Testing set is made of 937 images. The image sizes range from 800 ? 800 to 4000 ? 4000. In our experiments, both the training set and validation set are employed to train the proposed detector, and the testing set without annotations is used for evaluation. We crop the original images into the patches of 1024 ? 1024 with a stride of 824. At the training stage, we randomly resize and flip the images to avoid overfitting.</p><p>HRSC2016 <ref type="bibr" target="#b20">[21]</ref> contains a large number of strip-like oriented objects with diverse appearances collected from several famous harbors for ship recognition. The entire dataset has 1061 images ranging from 300 ? 300 to 1500 ? 900. For a fair comparison, the training set (436 images) and validation set (181 images) are employed for training, and the testing set (444 images) is used for evaluation. UCAS-AOD <ref type="bibr" target="#b51">[52]</ref> has 1510 images with 510 car images and 1000 airplane images. There are 14,596 instances in total. The entire dataset is randomly divided into 755 images for training, 302 images for validation and 453 images for testing with a ratio of 5:2:3. The size of all images is approximately 1280 ? 659. DIOR-R <ref type="bibr" target="#b1">[2]</ref> gives the oriented bounding box annotations based on the DIOR dataset <ref type="bibr" target="#b15">[16]</ref> for the oriented detection task. There are 23,463 images with the size of 800 ? 800 and 192,518 instances covering 20 object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our proposed approach based on both ResNet-50 <ref type="bibr" target="#b9">[10]</ref> and ResNet-101 backbone with FPN <ref type="bibr" target="#b17">[18]</ref>. The FPN consists of P 3 to P 7 pyramid levels in our work. The stochastic gradient descent (SGD) optimizer is used in training. The initial learning rate is set to 0.008 with the warming up for 500 iterations, and the learning rate is decreased by a factor of 0.1 at each decay step. The momentum is set to 0.9, and weight decay is 10 ?4 . We train the models with 40 epochs, 40 epochs, 120 epochs and 120 epochs for DOTA, DIOR-R, HRSC2016 and UCAS-AOD, respectively. The scale jitter is employed during the training phase. The hyperparameters of focal loss are set to ? = 0.25 and ? = 2.0. In Eq. (2), we set the balanced weights for each stage ? 1 = 0.3 and ? 2 =1.0, empirically. We set ? 1 =1.0, ? 2 = 0.3 and ? 3 = 0.1 for quality evaluation Q in Eq. <ref type="bibr" target="#b7">(8)</ref>. A couple of experiments are performed to choose the appropriate values of the sampling ratio ? in <ref type="table">Table 6</ref>.</p><p>We conduct the experiments on a server with 4 RTX 2080Ti GPUs using a total batch size of 8 (2 images per GPU) for training while a single RTX 2080Ti GPU is employed for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To examine the effectiveness of each component in our proposed framework, a series of ablation experiments are performed on DOTA dataset with ResNet-50-FPN.</p><p>Evaluation on oriented conversion functions. The conventional point set-based object detector RepPoints <ref type="bibr" target="#b45">[46]</ref> obtains the upright bounding boxes by square conversion function like min-max, which cannot deal with the aerial objects having arbitrary orientations. To build a reasonable baseline, we compare the different conversion functions that map the adaptive points into the oriented box during the training and post-processing. <ref type="table">Table 1</ref> shows the experimental results. Based on the original RepPoints with min-max function both in training and post-processing, it is able to achieve 49.69% mAP. With the proposed oriented MinAeraRect function for post-processing to get the rotated rectangular boxes, it achieves the 53.21% mAP. With the differentiable oriented NearestGTCorner and ConvexHull functions, our Oriented RepPoints obtains the 66.97% mAP and 68.89% mAP, which shows that the oriented conversion functions are essential to aerial object detection. Comparison with angle-based detectors. To examine the effectiveness of adaptive points representation, we compare our approach with the angle-based orientation regression on the anchor-based detector. As in S 2 A-Net <ref type="bibr" target="#b7">[8]</ref>, the angle-base detector presets one squared anchor for each feature map location at the initialization stage, where the predicted angle-based boxes are regarded as the refined anchors for the next stage to get the oriented bounding box. <ref type="table">Table 2</ref> shows the results of two detectors with different backbones. The Oriented RepPoints outperforms the anglebased orientation regression with +1.39% and +1.46% mAP improvement using ResNet-50-FPN and ResNet-101-FPN backbone, respectively. Evaluation on spatial constraint. To investigate the effectiveness of spatial constraint, we compare it against the baseline method without using it. <ref type="table">Table 3</ref> shows the experimental results. It can be observed that our proposed spatial constraint is very effective, especially for the aerial objects with weak feature representation such as HC (Helicopter), and the objects similar to the background, e.g. BD (Baseball Diamond), BR (Bridge) and RA (Roundabout). This is because the spatial constraint enforces the adaptive points on their owner instance object.</p><p>APAA scheme for adaptive points learning. To study the proposed APAA scheme for adaptive points learning, we first report the performance of the quality measure termby-term. <ref type="table">Table 5</ref> gives the results of different settings on quality assessment measure Q. The detection result is progressively improved, and the proposed approach achieves the best performance with 75.97% mAP and +5.86% gains using all four terms. It indicates that the quality assessment measure is effective to reflect the quality of adaptive points for aerial object detection. In APAA scheme, the number of assigned adaptive points samples is determined by the sampling ratio ?. As shown in <ref type="table">Table 6</ref>, the model achieves the best performance when ? = 0.4. Additionally, we compare our APAA scheme with other sample assignment schemes for training the proposed detector, including Max-IoU <ref type="bibr" target="#b27">[28]</ref>, ATSS <ref type="bibr" target="#b47">[48]</ref>, PAA <ref type="bibr" target="#b12">[13]</ref> and CFA <ref type="bibr" target="#b6">[7]</ref>. As illustrated in <ref type="table">Table 7,   Methods   Backbone  PL  BD  BR  GTF  SV  LV  SH  TC  BC  ST  SBF  RA  HA  SP  HC</ref>   our APAA scheme achieves the best performance without the complicated operations, which demonstrates that our proposed APAA is effective for adaptive points learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the State-of-the-art methods</head><p>Results on DOTA. We report full experimental results of single scale to make a fair comparison with the previous methods on DOTA dataset. With ResNet-50-FPN and ResNet-101-FPN as the backbones, our method obtains 75.97% and 76.52% mAP, respectively. It outperforms other methods with the corresponding backbones. Using the tiny version of Swin-Transformer <ref type="bibr" target="#b19">[20]</ref> (Swin-T-FPN) as backbone with the random rotation and HSV transformation, we achieve the best performance with 77.63% mAP. <ref type="figure">Fig. 4</ref> shows some visual results on DOTA test set.</p><p>Results on HRSC2016. To make a comprehensive comparison on HRSC2016, we report the results with both VOC2007 and VOC2012 metrics. <ref type="table" target="#tab_5">Table 8</ref> shows the experimental results. Our Oriented RepPoints achieves the best performance under VOC2012 metric and the second-best under VOC2007 metric with ResNet-50-FPN backbone.</p><p>Results on UCAS-AOD. UCAS-AOD datasets contains a large number of small objects with complex surrounding scenes. <ref type="table">Table 9</ref> shows the evaluation results with the recent methods on UCAS-AOD dataset. Our presented method achieves the best performance of 90.11% mAP.</p><p>Results on DIOR-R. DIOR-R datasets consists of 20 classes of aerial objects. Compared with the recent methods on this dataset, we achieve the best performance with 66.71% mAP and outperform other methods, as shown in <ref type="table">Table 10</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on Orientation Accuracy</head><p>We further conduct experiment to evaluate the orientation accuracy of an oriented detector on DOTA dataset with ResNet-50-FPN backbone. We employ the mean Average Orientation Error (mAOE ? ) on all categories as the evaluation metric. As shown in <ref type="table" target="#tab_7">Table 11</ref>, our proposed approach obtains the smallest orientation errors, which demonstrates that our point set-based approach is effective for precise oriented object detection, comparing to the conventional orientation regression-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposed an effective aerial object detector by taking advantage of the adaptive points as a fine-grained representation, which is able to capture the key geometric features for arbitrary-oriented, cluttered and non-axis aligned targets. To effectively learn the adaptive points, we introduced the quality assessment and sample assignment scheme to measure and select the high-quality points samples for training. Furthermore, a spatial constraint is used to penalize the points outside the oriented box for robust adaptive points learning. The extensive experiments have been performed on four testbeds, whose promising results demonstrate the efficacy of our proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>77.62 41.81 58.17 74.58 71.64 79.11 90.29 82.18 74.32 54.75 60.60 62.57 69.67 60.64 68.43 DAL [23] R-101-FPN 88.61 79.69 46.27 70.37 65.89 76.10 78.53 90.84 79.98 78.41 58.71 62.02 69.23 71.32 60.65 71.78 RSDet [25] R-152-FPN 90.10 82.00 53.80 68.50 70.20 78.70 73.60 91.20 87.10 84.70 64.30 68.20 66.10 69.30 63.70 74.10 R 3 Det [42] R-152-FPN 89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17 73.74 S 2 A-Net [8] R-50-FPN 89.11 82.84 48.37 71.11 78.11 78.39 87.25 90.83 84.90 85.64 60.36 62.60 65.26 69.13 57.94 74.12 R 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 .17 54.13 71.16 80.18 78.40 87.28 90.90 85.97 86.25 59.90 70.49 73.53 72.27 58.97 75.97 Oriented RepPoints R-101-FPN 89.53 84.07 59.86 71.76 79.95 80.03 87.33 90.84 87.54 85.23 59.15 66.37 75.23 73.75 57.23 76.52 Oriented RepPoints Swin-T-FPN 89.11 82.32 56.71 74.95 80.70 83.73 87.67 90.81 87.11 85.85 63.60 68.60 75.95 73.54 63.76 77.63</figDesc><table><row><cell></cell><cell></cell><cell>mAP</cell></row><row><cell>Single-stage Methods</cell><cell></cell><cell></cell></row><row><cell cols="3">RetinaNet-O [19] 88.67 3 Det-DCL [40] R-50-FPN R-152-FPN 89.78 83.95 52.63 69.70 76.84 81.26 87.30 90.81 84.67 85.27 63.50 64.16 68.96 68.79 65.45 75.54</cell></row><row><cell>Two-stage Methods</cell><cell></cell><cell></cell></row><row><cell>Faster RCNN-O [28]</cell><cell>R-50-FPN</cell><cell>88.44 73.06 44.86 59.09 73.25 71.49 77.11 90.84 78.94 83.90 48.59 62.95 62.18 64.91 56.18 69.05</cell></row><row><cell>CAD-Net [47]</cell><cell>R-101-FPN</cell><cell>87.80 82.40 49.40 73.50 71.10 63.50 76.60 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20 69.90</cell></row><row><cell>SCRDet [45]</cell><cell>R-101-FPN</cell><cell>89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</cell></row><row><cell>FAOD [14]</cell><cell>R-101-FPN</cell><cell>90.21 79.58 45.49 76.41 73.18 68.27 79.56 90.83 83.40 84.68 53.40 65.42 74.17 69.69 64.86 73.28</cell></row><row><cell>RoI-Trans. [3]</cell><cell>R-101-FPN</cell><cell>88.65 82.60 52.53 70.87 77.93 76.67 86.87 90.71 83.83 82.51 53.95 67.61 74.67 68.75 61.03 74.61</cell></row><row><cell cols="3">Gliding Vertex [37] 89.64 MaskOBB [32] R-101-FPN R-50-FPN 89.61 85.09 51.85 72.90 75.28 73.23 85.57 90.37 82.08 85.05 55.73 68.39 71.61 69.87 66.33 74.86</cell></row><row><cell>CenterMap [33]</cell><cell>R-50-FPN</cell><cell>88.88 81.24 53.15 60.65 78.62 66.55 78.10 88.83 77.80 83.61 49.36 66.19 72.10 72.36 58.70 71.74</cell></row><row><cell>ReDet [9]</cell><cell cols="2">ReR-50-ReFPN [9] 88.79 82.64 53.97 74.00 78.13 84.06 88.04 90.89 87.78 85.75 61.76 60.39 75.96 68.07 63.59 76.25</cell></row><row><cell>Oriented R-CNN [36]</cell><cell>R-101-FPN</cell><cell>88.86 83.48 55.27 76.92 74.27 82.10 87.52 90.90 85.56 85.33 65.51 66.82 74.36 70.15 57.28 76.28</cell></row><row><cell>Anchor-free Methods</cell><cell></cell><cell></cell></row><row><cell>CenterNet-O [51]</cell><cell>DLA-34 [51]</cell><cell>81.00 64.00 22.60 56.60 38.60 64.00 64.90 90.80 78.00 72.50 44.00 41.10 55.50 55.00 57.40 59.10</cell></row><row><cell>PIoU [1]</cell><cell>DLA-34</cell><cell>80.90 69.70 24.10 60.20 38.30 64.40 64.80 90.90 77.20 70.40 46.50 37.10 57.10 61.90 64.00 60.50</cell></row><row><cell>O 2 -DNet [34]</cell><cell>H-104 [39]</cell><cell>89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell>DRN [24]</cell><cell>H-104</cell><cell>89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23</cell></row><row><cell>CFA [7]</cell><cell>R-101-FPN</cell><cell>89.26 81.72 51.81 67.17 79.99 78.25 84.46 90.77 83.40 85.54 54.86 67.75 73.04 70.24 64.96 75.05</cell></row><row><cell>Oriented RepPoints</cell><cell>R-50-FPN</cell><cell>87.02 83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 6 .</head><label>46</label><figDesc>Comparison with state-of-the-art methods on DOTA dataset. All reported results are performed on the single-scale DOTA dataset. The results with red color denote the best results and with blue color present the second-best results in each column. '-O' means the detection results with oriented bounding box (the same below). Quality Measure Q for Adaptive Points. Evaluation on various ? in the dynamic top k assignment of APAA scheme.</figDesc><table><row><cell>Q cls</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q loc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Qori</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Qpoc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>70.11</cell><cell>72.34</cell><cell>74.46</cell><cell cols="2">75.32</cell><cell>75.97</cell></row><row><cell cols="7">Table 5. Performance evaluation on different settings of quality</cell></row><row><cell cols="3">measure Q in APAA scheme.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.2</cell><cell>0.3</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.5</cell></row><row><cell>mAP</cell><cell>75.45</cell><cell cols="2">75.34</cell><cell>75.97</cell><cell></cell><cell>75.67</cell></row><row><cell cols="7">Methods Max-IoU [28] ATSS [48] PAA [13] CFA [7] APAA</cell></row><row><cell>mAP</cell><cell>70.11</cell><cell>72.87</cell><cell cols="2">74.62</cell><cell cols="2">74.89 75.97</cell></row><row><cell cols="7">Table 7. Comparisons with different samples assignment methods</cell></row><row><cell cols="3">on Oriented RepPoints detector.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Results on HRSC2016 test set. mAP(07) and mAP<ref type="bibr" target="#b11">(12)</ref> represent the results under VOC2007 and VOC2012 mAP metrics.</figDesc><table><row><cell>Methods</cell><cell>Car</cell><cell>Airplane</cell><cell>mAP</cell></row><row><cell>YOLOv3-O [27]</cell><cell>74.63</cell><cell>89.52</cell><cell>82.08</cell></row><row><cell>RetinaNet-O [19]</cell><cell>84.64</cell><cell>90.51</cell><cell>87.57</cell></row><row><cell>Faster R-CNN-O [35]</cell><cell>86.87</cell><cell>89.86</cell><cell>88.36</cell></row><row><cell>RoI Trans. [3]</cell><cell>87.99</cell><cell>89.90</cell><cell>88.95</cell></row><row><cell>DAL [23]</cell><cell>89.25</cell><cell>90.49</cell><cell>89.87</cell></row><row><cell>Oriented RepPoints</cell><cell>89.51</cell><cell>90.70</cell><cell>90.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Performance comparisons on UCAS-AOD dataset. Detection accuracy on DIOR-R dataset. All experimental results are performed with ResNet-50-FPN backbone.</figDesc><table><row><cell cols="4">Methods RetinaNet-O [19] Faster RCNN-O [28] Gliding Vertex [37]</cell></row><row><cell>mAP</cell><cell>57.55</cell><cell>59.54</cell><cell>60.06</cell></row><row><cell cols="2">Methods RoI-Trans. [3]</cell><cell>AOPG [2]</cell><cell>Oriented RepPoints</cell></row><row><cell>mAP</cell><cell>63.87</cell><cell>64.41</cell><cell>66.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 .</head><label>11</label><figDesc>Comparison on orientation errors with DOTA. All experiments are performed with train set for training, val set for testing.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Natural Science Foundation of China under Grants (61831015).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Piou loss: Towards accurate oriented object detection in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kean</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Anchor-free oriented proposal generator for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01931</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection in aerial images: A large-scale benchmark and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond bounding-box: Convexhull feature adaptation for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8792" to="8801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Align deep features for oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Redet: A rotation-equivariant detector for aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2786" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the identification of the convex hull of a finite set of points in the plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><forename type="middle">A</forename><surname>Jarvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-attentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gui song Xia, and Xiang Bai. Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yiping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iqdet: Instance-wise quality distribution sampling for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic anchor learning for arbitraryoriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2355" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11207" to="11216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attentionbased mask oriented bounding box representation for multicategory object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Oriented r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3520" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Clustered object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8311" to="8320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15819" to="15829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3163" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11830" to="11841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13316</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haigang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
