<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Nazir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deutsches Forschungszentrum f?r K?nstliche Intelligenz (DFKI)</orgName>
								<address>
									<addrLine>Trippstadter Str. 122</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lule? University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deutsches Forschungszentrum f?r K?nstliche Intelligenz (DFKI)</orgName>
								<address>
									<addrLine>Trippstadter Str. 122</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Zeshan Afzal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deutsches Forschungszentrum f?r K?nstliche Intelligenz (DFKI)</orgName>
								<address>
									<addrLine>Trippstadter Str. 122</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2017.DOI</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth completion involves recovering a dense depth map from a sparse map and an RGB image. Recent approaches focus on utilizing color images as guidance images to recover depth at invalid pixels. However, color images alone are not enough to provide the necessary semantic understanding of the scene. Consequently, the depth completion task suffers from sudden illumination changes in RGB images (e.g., shadows). In this paper, we propose a novel three-branch backbone comprising color-guided, semanticguided, and depth-guided branches. Specifically, the color-guided branch takes a sparse depth map and RGB image as an input and generates color depth which includes color cues (e.g., object boundaries) of the scene. The predicted dense depth map of color-guided branch along-with semantic image and sparse depth map is passed as input to semantic-guided branch for estimating semantic depth. The depth-guided branch takes sparse, color, and semantic depths to generate the dense depth map. The color depth, semantic depth, and guided depth are adaptively fused to produce the output of our proposed three-branch backbone. In addition, we also propose to apply semantic-aware multi-modal attention-based fusion block (SAMMAFB) to fuse features between all three branches. We further use CSPN++ with Atrous convolutions to refine the dense depth map produced by our three-branch backbone. Extensive experiments show that our model achieves state-of-the-art performance in the KITTI depth completion benchmark at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDEX TERMS State-of-the-art Depth Completion</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Estimation of dense depth measurements is crucial in various 3D vision and robotics applications such as augmented and mixed reality <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, scene reconstruction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, autonomous driving <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and obstacle avoidance <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. For obtaining a reliable prediction of depth in outdoor scenes, measurements from various sensors are used. Most commonly used sensors include RGB cameras in a stereo setting, light detection and ranging (LiDAR), and time of flight (ToF) cameras <ref type="bibr" target="#b8">[9]</ref>. Among all the sensors, LiDAR is considered the most reliable and efficient, and it produces accurate depth measurements in an outdoor setting <ref type="bibr" target="#b7">[8]</ref>. However, the density of LiDAR depth measurements is sparse, with a considerable amount of missing depth data <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. For example, the LiDAR sensor for mobile applications <ref type="bibr" target="#b7">[8]</ref>, Velodyne HDL-64e, which is also used in the KITTI <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> dataset produces depth maps consisting of valid depth values only on 5.9% pixels. Such sparse depth maps cannot be utilized directly in the application areas mentioned above. Therefore, dense depth maps estimation from sparse measurements is of utmost importance. It is considered a challenging problem since the measured depth values only form 5.9% of the complete depth map.</p><p>Recent methods <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b19">[20]</ref> employ the deep learning-based approaches to dense depth completion. These methods utilize convolutional neural networks and combine sparse Li-DAR data with different modalities, e.g. RGB images <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, semantic maps <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and surface normal's <ref type="bibr" target="#b11">[12]</ref>. These modalities act as guidance and significantly help in the recovery of missing depth values in the sparse maps. The idea is to actively fuse features between different modalities. Most of the current methods adopt twobranch networks architectures for feature fusion. For instance, DeepLiDAR <ref type="bibr" target="#b11">[12]</ref>, FusionNet <ref type="bibr" target="#b22">[23]</ref> and PENet <ref type="bibr" target="#b12">[13]</ref> utilize an encoder-decoder architecture to perform both early and late fusion between color images and LiDAR sparse depth maps. The combination of early and late fusion between the modalities enhances the depth completion perfor-VOLUME 4, 2016 1 arXiv:2204.13635v1 [cs.CV] 28 Apr 2022 FIGURE 1: Block diagram of SemAttNet. In the first stage, the multi-modal data consisting of RGB, semantic, and LiDAR sparse depth maps are passed to our novel threebranch backbone, which outputs a fused dense depth map. Our proposed backbone utilize Semantic-aware multi-model attention-based fusion (SAMMAFB) to adaptively fuse features between RGB, semantic and sparse depth branches. In the second stage, the output of the three-branch backbone is further refined through CSPN++ with Atrous convolutions.</p><p>mance. Recently, RigNet <ref type="bibr" target="#b13">[14]</ref> proposed a repetitive architecture, which repeats Hourglass network architecture for color images at multiple levels to extract features and fuse them with depth generation branch to create structure-detailed dense depth maps. These methods rely heavily only on color images to extract color dominant information such as object boundaries to complete sparse maps. However, color images truly do not provide this information. It is because color images suffer from shadows and reflections, which causes irregularities in pixel values <ref type="bibr" target="#b21">[22]</ref>.</p><p>To solve the problems in the methods mentioned earlier, we propose a novel three-branch network inspired by PENet <ref type="bibr" target="#b12">[13]</ref>, FusionNet <ref type="bibr" target="#b22">[23]</ref> and DeepLiDAR <ref type="bibr" target="#b11">[12]</ref>. It consists of color-guided (CG), semantic-guided (SG), and dense depthguided (DG) branches. In contrast to earlier methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we add SG branch to decrease the variance of depth values around object boundaries. The CG branch produces a noisy depth estimate through the color image and LiDAR sparse depth map. The output of the CG branch, LiDAR sparse depth and semantic map of the color image is passed to the SG branch, which actively refines the color depth and produces a depth based on semantic information. For ease of understanding, we name it semantic depth in this paper. The semantic depth is much more reliable around object boundaries than color depth. It is because the pixel values of semantic maps are uniform and have fewer irregularities. Furthermore, we also use the DG branch to refine further the depth produced by the SG branch. It takes LiDAR sparse depth map, CG and SG depth as input and produces a dense depth maps. Similar to earlier methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we also combine depth information between CG, SG and DG depths with learned confidence maps. The confidence map weights enables us to compare the reliability of the predictions at each branch. In the end, we perform multi-modal depth fusion between CG, SG and DG predicted dense depths to generate the final dense depth map. The whole backbone model is trained in an end-to-end manner. Furthermore, we apply CSPN++ with Atrous convolutions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref> on the final dense depth map predicted by the three-branch backbone for further refinement.</p><p>Since we have branches dealing with multi-modal data, we perform both early and late fusion between them for effective guidance of dense depth map generation. Early fusion is performed by the depth-wise concatenation of color, semantic or LiDAR sparse depth maps. The concatenation of the input is fed to the respective branch. Late fusion is performed at the feature level and involves the fusion of multi-modal data. The naive strategy of performing late fusion is to apply simply concatenation or addition of features maps of different modalities and it is used in many earlier works such as e.g. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>. However, this is not an optimal way because each branch contains different information. Therefore, we propose to apply semantic-aware multi-modal attention-based fusion block (SAMMAFB) inspired by CBAM <ref type="bibr" target="#b25">[26]</ref> and AFB <ref type="bibr" target="#b26">[27]</ref> to depth completion problem. SAMMAFB applies both channel and spatial wise attention maps of the input feature maps and produces refined feature maps. As demonstrated in <ref type="figure">Figure 1</ref>, we actively fuse features at multiple stages through SAMMAFB in all of our branches.</p><p>The main contributions of our work is summarized as follows:</p><p>? We propose a novel three-branch backbone for sparse depth completion, which counters the sensitivity of image-guided methods to optical changes (e.g., shadows and reflections). ? We present a novel SAMMAFB block to actively fuse the color, semantic, and depth modalities at multiple stages in our three-branch backbone. ? Extensive experimental results show that our model achieves state-of-the-art results on the outdoor KITTI depth completion dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. SPARSE DEPTH BASED APPROACHES</head><p>Earlier approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref> based on convolutional neural networks (CNN) utilized only sparse depth maps to generate dense depth maps. To counter the sparsity of data in sparse depth maps, Depth-Net <ref type="bibr" target="#b28">[29]</ref> performed nearest neighbor interpolation in the sparse maps to fill out the holes (points with no depth values). Later on, uncertainty aware CNN's <ref type="bibr" target="#b29">[30]</ref> proposed probabilistic normalized convolutions to model the uncertainty in the sparse depth maps. However, the obvious drawback of these approaches is that without color or semantic image guidance, the predicted depth maps lack clear object boundaries and also these methods are not suitable for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IMAGE-GUIDED METHODS</head><p>Image-Guided methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b33">[34]</ref> utilize multi-modal information to facilitate dense depth completion. The multi-modal data includes RGB images, semantic images, and surface normal's, which act as reference images for generating dense depth maps.</p><p>Methods such as SPN <ref type="bibr" target="#b33">[34]</ref>, CSPN <ref type="bibr" target="#b32">[33]</ref>, CSPN++ <ref type="bibr" target="#b18">[19]</ref>, DSPN <ref type="bibr" target="#b30">[31]</ref> and NLSPN <ref type="bibr" target="#b3">[4]</ref> focus on learning affinity matrix for high-level vision tasks including semantic segmentation and depth completion. SPN <ref type="bibr" target="#b33">[34]</ref> serially propagates the affinity matrix, which is inefficient for real-time systems. CSPN <ref type="bibr" target="#b32">[33]</ref> improved SPN <ref type="bibr" target="#b33">[34]</ref> by predicting affinity values of local neighbors and updating pixel values simultaneously. Both of the methods suffer with the problem of fixed local neighborhoods, which often have irrelevant information. To solve this problem, CSPN++ <ref type="bibr" target="#b18">[19]</ref>, DSPN <ref type="bibr" target="#b30">[31]</ref> and NLSPN <ref type="bibr" target="#b3">[4]</ref> methods are introduced. CSPN++ <ref type="bibr" target="#b18">[19]</ref> proposed adaptive learning of kernel sizes and the number of iterations of propagation, which helped in reducing the computation time of CSPN <ref type="bibr" target="#b32">[33]</ref>. DSPN <ref type="bibr" target="#b30">[31]</ref> and NLSPN <ref type="bibr" target="#b3">[4]</ref> learn deformable convolutional kernels to relax the fixed local neighborhood of pixels, which enabled them to focus only on relevant pixel neighbors for depth completion. All of the methods mentioned above utilizes a single branch AutoEncoder (AE) <ref type="bibr" target="#b34">[35]</ref> network architecture. The input of the AE is the concatenation of RGB image and sparse depth map, which outputs a dense depth map.</p><p>The two branch network architectures <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> consist of RGB and sparse depth map branches. The RGB branch extracts color dominant information, e.g., object boundaries, which is actively fused with a sparse depth map branch at multiple stages. ACMNet <ref type="bibr" target="#b16">[17]</ref> introduced a symmetric gated fusion strategy for performing the fusion between two branches. FCFR-Net proposed a channel shuffling and energy-based fusion between the two modalities. PENET <ref type="bibr" target="#b12">[13]</ref>, utilized concatenation and addition operations to perform fusion at multiple levels. Recently, RigNet <ref type="bibr" target="#b13">[14]</ref> proposed an efficient guidance algorithm to fuse and guide the sparse depth branch. In addition to RGB image and sparse depth map, DeepLiDAR <ref type="bibr" target="#b11">[12]</ref> introduced learning pixel-wise surface normal's of the scene, which is fused through addition with RGB branch to generate dense depth maps. Similarly, Multi-Task GAN's <ref type="bibr" target="#b15">[16]</ref> generated semantic maps of the RGB images and concatenated them with sparse and RGB images to guide dense depth map completion. Compared to earlier approaches, the performance of two branch methods are much better.</p><p>Unlike existing Image-guided methods, our approach consists of a semantic-guided branch, which given the color and sparse dense depths, learns to understand the semantic meaning of the scene. Consequently, it enables our approach to perform consistently under different lighting conditions, which is a challenging aspect of outdoor depth completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ATTENTION-BASED MULTI-MODAL DATA FUSION</head><p>Attention-based multi-modal information fusion has been studied in various computer vision applications, such as video description <ref type="bibr" target="#b35">[36]</ref>, human motion estimation <ref type="bibr" target="#b36">[37]</ref>, emotion and sentiment classification <ref type="bibr" target="#b37">[38]</ref> and many more. In the context of dense depth completion, the attention mechanism enables the model to focus on meaningful regions of multimodal information, generating depths with clear and sharp structural details. Moreover, given the sparsity of the Li-DAR depth data, the attention module is important for depth completion task. Recently, ACM Net <ref type="bibr" target="#b16">[17]</ref> proposed a Coattention Guided Graph Propagation (CGPM), which exploits the attention mechanism in the spatial domain to propagate graphs at multiple scales. However, spatial attention only capture local contexts within a fixed neighborhood. Therefore, to capture both local and global attention, we propose to apply SAMMAFB between RGB, semantic, and LiDAR sparse depth modalities. It produces both channel-wise and spatial-wise attention weights, which are applied to multimodal information to generate refined fused feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We formulate an image-guided depth completion problem as a two-stage task. <ref type="figure">Figure 2</ref> shows the pipeline of our approach. In the first stage, we design a three-branch backbone network to produce a dense depth map, whereas, in the second stage, we utilize CSPN++ <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref> with Atrous convolutions for further refinement. The three-branch backbone consists of CG, SG and DG branches. The purpose of the CG branch is to focus on color-dominant information, whereas the SG branch aims to learn the semantics information of the scene. The predicted dense depths of CG and SG branches are noisy depth estimates but they contain important color and semantic cues of the scene. The DG branch takes predicted dense depth maps of SG and CG branch along with aligned sparse map as input to produce a dense depth map. DG branch focuses on learning depth dominant features. All of the branches are complimentary to each other. Therefore, we adaptively fuse them with learned confidence weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SEMANTIC-AWARE MULTI-MODAL ATTENTION-BASED FUSION BLOCK</head><p>In order to perform fusion between RGB, semantic, and depth features, we propose to apply Semantic-aware Multi-Modal Attention-based Fusion Block (SAMMAFB) on RGB, semantic, and depth features in our three-branch backbone. SAMMAFB helps us refine the concatenated feature maps of modalities by capturing salient feature maps while suppressing the unnecessary ones <ref type="bibr" target="#b26">[27]</ref>. We apply SAM-MAFB in two different settings, first we apply it to fuse the intermediate feature maps of RGB and semantic guided branches. Then, for depth-guided branch, we perform the fusion between the intermediate feature maps of all branches. <ref type="figure" target="#fig_0">Figure 3</ref> depicts the visual representation of SAM-MAFB. It consists of a channel and spatial attention submodules, which enables it to capture channel-wise and FIGURE 2: The overview of proposed SemAttNet. It consists of a novel three-branch backbone and a CSPN++ module with Atrous convolutions. Unlike earlier image-guided methods, we design a separate branch for learning the semantic information of the scene. Furthermore, we propose to apply attention based fusion block (ABF) to perform semantic-aware fusion between RGB, depth, and semantic modalities. Each branch outputs a depth map and a confidence map, which are adaptively fused to produce a fused depth map. In the end, the fused depth map are sent to CSPN++ module with Atrous convolutions for refinement. Note, due to shortage of space, we use AFB to represent SAMMAFB block.  </p><formula xml:id="formula_0">= ? W 1 W 0 F c avg + W 1 (W 0 (F c max )) (1) Where A c (F) ? R C?1?1</formula><p>represents channel-wise at-tention weights of F, ? denotes sigmoid function, W 0 ? R C/r?C , W 1 ? R C?C/r represents the weights matrices for multi-layer perceptron (MLP) layers. The parameter r controls the number of learnable parameters in MLP layer. F c max and F c avg denotes the max-pooled and average-pooled features respectively.</p><p>The channel-wise attentions weights are applied to F to obtain F ? R 3C?H?W such that F = A c (F) ? F. The product F is then sent to spatial-wise attention module, which is given in <ref type="figure">Equation 2</ref>.</p><formula xml:id="formula_1">A s (F ) = ? Conv F c avg ; F c max<label>(2)</label></formula><p>Where A s (F) ? R 1?H?W represents spatial-wise attention weights of F, ? denotes sigmoid function. F c max and F c avg denotes the max-pooled and average-pooled features respectively.</p><p>The spatial-wise attentions weights are applied to F to obtain final output of SAMMAFB i.e. refined fused feature maps F ? R 3C?H?W such that F = A s (F ) ? F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. THE THREE-BRANCH BACKBONE</head><p>The earlier two-branch methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> relied heavily on the color image to extract both semantic and color-dominant information. However, color images alone might not be able to provide this information. Therefore, to overcome this limitation, we propose a three-branch backbone consisting of CG, SG and DG branches. The addition of the SG branch enables better learning of semantic cues of the scene, which help predict depth maps with accurate and sharp structure in the DG branch. The dataset comprises of color and semantic images that are represented by C = R N ?3?H?W and S = R N ?3?H?W , whereas D = R N ?1?H?W and D gt = R N ?1?H?W denotes sparse and ground truth depth maps, and N denotes the number of samples. Note, that we don't have any supervision of confidence maps due the unavailability of ground truth. Therefore, confidence maps are trained indirectly based on the 2 loss of each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Color-guided Branch</head><p>The CG branch aims to learn important color cues for dense depth completion. It takes a color image concatenated with an aligned sparse depth map as input and outputs a dense depth map. The concatenation of an aligned sparse depth map with a color image helps in the prediction of a dense depth map <ref type="bibr" target="#b12">[13]</ref>. The CG branch follows an encoder-decoder network architecture with skip connections. The encoder consists of ten ResNet <ref type="bibr" target="#b38">[39]</ref> blocks, whereas the decoder consists of one convolution and five transpose convolution layers for upsampling. The output of this branch consist of dense depth map and a confidence map. The resultant dense depth map of this branch is a noisy depth estimate, but it provides a baseline for learning structural information of the scene in other branches.</p><p>Let X cg = [C B ; D B ] ? R B?4?H?W represent the concatenation of batch of size B of color images C B ? C and sparse depth maps D B ? C. The CG branch takes X cg as input, and in the first step, it is encoded to a hidden representation ? cg ? R B?1024?H?W based on Equation 3. Then, ? cg is decoded into a confidence map C cg ? R B?1?H?W and a color depth map D cg ? R B?1?H?W based on Equation <ref type="bibr" target="#b3">4</ref>.</p><formula xml:id="formula_2">? cg = f (W cg X cg + b cg ) (3) D cg , C cg = g(V cg ? cg + c cg )<label>(4)</label></formula><p>Where W cg and V cg represents encoder and decoder weight matrices. The variables b cg and c cg denote encoding and decoding bias values. f (.) and g(.) represent activation functions.</p><formula xml:id="formula_3">L cg = argmin Dcg ||D gt ? D cg || 2<label>(5)</label></formula><p>Equation 5 defines the 2 loss for CG branch. Since the ground truth contains invalid depth values, we only consider pixels with valid depth values for computing loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Semantic-guided Branch</head><p>Semantic cues help to understand the scene and are essential for depth completion task <ref type="bibr" target="#b39">[40]</ref>. However, CG branch alone is not enough to learn semantic information. Therefore, we propose an SG branch to encourage learning effective semantic cues and to complement RGB images for depth completion. The SG branch takes the concatenation of color depth, semantic image and sparse depth map as an input and outputs a dense depth map, which consist of both color and semantic cues. The KITTI depth completion dataset does not provide aligned semantic maps of RGB images. Therefore, we transform the RGB images to semantic maps through a pre-trained WideResNet38 [41] model on KITTI semantic segmentation benchmark <ref type="bibr" target="#b41">[42]</ref>. Furthermore, Inspired by earlier works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we fuse the decoder features from the CG branch into the corresponding encoder features of the SG branch. The features from CG and SG are sent to SAMMAFB, which outputs refined fused depth feature maps. Similar to CG branch, we define X sg = D B cg ; S B ; D B ? R B?4?H?W to represent the concatenation of batch size B of color depths D B cg ? D cg , color images C B ? C and sparse depth maps D B ? D. The color depth consists of important color cues e.g. object boundaries and its concatenation will complement in learning semantic cues. The matrix X sg is passed as an input to SG branch. In the first step, the input is encoded to a hidden representation ? sg ? R B?1024?H?W based on Equation <ref type="bibr" target="#b5">6</ref>. In the second step, the encoded representation ? sg is decoded into a confidence map C sg ? R B?1?H?W and a semantic depth map D sg ? R B?1?H?W based on <ref type="bibr">Equation 7</ref>.</p><formula xml:id="formula_4">? sg = f (W T sg .X sg + b sg ) (6) D sg , C sg = g(V T sg .? sg + c sg )<label>(7)</label></formula><p>Where W sg and V sg represents encoder and decoder weight matrices. The variables b sg and c sg represents encoding and decoding bias values. f (.) and g(.) denotes activation functions.</p><formula xml:id="formula_5">L sg = argmin Dsg || D gt ? D sg || 2 (8) Equation 8</formula><p>defines the 2 loss for SG branch. we only consider pixels with valid depth values for loss calculation because the ground truth contains invalid depth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Depth-guided Branch</head><p>The DG branch focuses on learning depth-dominant features, which help generate accurate dense depth maps. It takes the concatenation of CG, SG, and sparse depth as input and outputs a dense depth map. Similar to the feature fusion strategy in the CG and SG branch, we fuse decoder features from CG and SG branches into the corresponding encoder features of the DG branch. However, in DG branch, SAM-MAFB take feature maps from three modalities as input and outputs refined fused feature maps. The refined fused feature maps consists of useful information from CG and SG branches, which guides the DG branch in learning effective depth feature representations. The network architecture is similar to CG and SG branches, but we add two more layers in encoder and decoder architectures to accommodate CG and SG branch decoder features.</p><p>We define X dg = D B cg ; D B sg ; D B ? R B?3?H?W to represent the concatenation of batch of size B of color depths D B cg , semantic depths D B sg and sparse depth maps D B . The DD branch takes X dg as an input and encodes it to a hidden representation ? dg ? R B?1984?H?W based on Equation <ref type="bibr" target="#b8">9</ref>. Afterwards, the encoded representation ? dg is decoded into a confidence map C dg ? R B?1?H?W and a dense depth map D dg ? R B?1?H?W based on Equation <ref type="bibr" target="#b9">10</ref>.</p><formula xml:id="formula_6">? dg = f (W T dg .X dg + b dg )<label>(9)</label></formula><formula xml:id="formula_7">D dg , C dg = g(V T dg .? dg + c dg ) (10) VOLUME 4, 2016</formula><p>Where W dg and V dg represents encoder and decoder weight matrices. The variables b sg and c sg represents encoding and decoding bias values. f (.) and g(.) denotes activation functions.</p><formula xml:id="formula_8">L dg = argmin D dg || D gt ? D dg || 2<label>(11)</label></formula><p>Equation 11 defines the 2 loss for DD branch. Similar to SG and CG branches, we only consider pixels with valid depth values in ground truth for loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Multi-modal Depth Fusion</head><p>Similar to earlier approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we fuse the predicted depth maps of each branch by using learned confidence weights. <ref type="figure">Equation 12</ref> shows the mathematical representation of our depth map fusion strategy.</p><formula xml:id="formula_9">D f = e Ccg ? D cg + e Csg ? D sg + e C dg ? D dg e Ccg + e Csg + e C dg<label>(12)</label></formula><p>Where D f denotes the fused depth map, D cg , D sg and D dg represents the depth maps predicted by CG, SG and DG branches and e Ccg , e Csg , e C dg denotes the learned confidence maps of CG, SG and DG predicted depth maps.</p><p>Following the CG, SG and DG branches, we also calculate the loss on fused depth map by taking 2 norm between the fused depth map and ground truth as depicted in Equation <ref type="bibr" target="#b12">13</ref>. We only consider pixels with valid depth values for loss calculation of fused depth map.</p><formula xml:id="formula_10">L fused = argmin D f || D gt ? D f || 2<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Loss Function</head><p>The training loss of our three-branch backbone consists of the sum of CD, SD, DD and fused depth losses. Equation <ref type="bibr" target="#b13">14</ref> defines the training loss of our three-branch backbone.</p><formula xml:id="formula_11">L total = ? cg L cg + ? sg L sg + ? dg L dg + L f used<label>(14)</label></formula><p>Where ? cg , ? sg , ? dg denotes the weights for color-guided, semantic-guided and depth-guided branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CSPN++ WITH ATROUS CONVOLUTIONS</head><p>The reason for applying CSPN++ for refining the dense depth maps produced by our three-branch backbone is two folds. First, it recovers depth values at valid pixels. Second, enabling a smooth transition between the neighbours of our completed dense depth map. Furthermore, inspired by PENet <ref type="bibr" target="#b12">[13]</ref>, we use Atrous convolutions <ref type="bibr" target="#b42">[43]</ref> to enlarge the propagation neighborhood of pixels <ref type="bibr" target="#b43">[44]</ref>.</p><p>Given a fused depth map D f ? R h?w?1 , it is embedded into a hidden representation H ? R h?w?c , the one step propagation of CSPN++ with Atrous convolution is given in <ref type="bibr">Equation 15</ref>.</p><formula xml:id="formula_12">H t+1 (x i ) = ? xi (x i ) H t (x i ) + xj ?N k (xi) ? xi (x j ) H t (x j ) ? xi (x j ) =? xi (x j ) xj ?N |? xi (x j )| , ? xi (x i ) = 1 ? xj ?N ? xi (x j )<label>(15)</label></formula><p>Where N k (x) = {x n + l | l ? R} denotes the neighborhood pixels of x i ,? xi ? R k?k?c represents the affinity values produced by the network. During propagation, pixel x i receives information from enlarged neighbourhood N k (x i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DATASET</head><p>We evaluate the performance of our method against different state-of-the-art (SoTA) methods on KITTI depth completion benchmark <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The KITTI dataset is a large outdoor dataset for autonomous vehicles. It provides 85K sparse depth maps with corresponding RGB images for training, 7K for validation, and 1K for testing. The sparse depth map in the KITTI dataset is generated by using LiDAR HDL-64, which provides valid depth values on only 5.9% of all pixels <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, the ground-truth contains valid depth values on 16% of all the pixels. The ground-truth is generated by accumulating LiDAR and stereo estimation of the scenes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. For all of our experiments, we use the official validation set, which consists of 1K frames <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. During training, we bottom crop the semantic images, RGB images, and sparse depth map from 375 ? 1275 to 352 ? 1252.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EVALUATION METRICS</head><p>We use the official evaluation metrics of the KITTI depth completion benchmark <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> to evaluate the performance of our approach. The official evaluation measures consist of root mean squared error (RMSE), mean absolute error (MAE), root mean squared error of inverse depth (iRMSE), and mean absolute error of the inverse depth (iMAE). Among the evaluation metrics, RMSE is chosen to rank the submissions on the KITTI leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. IMPLEMENTATION DETAILS</head><p>We used PyTorch <ref type="bibr" target="#b45">[46]</ref> framework to implement the threebranch backbone and CSPN++ with Atrous convolutions. The three-branch backbone is trained on two NVIDIA A100 GPUs, whereas the full model is trained on three NVIDIA RTXA6000 GPUs. Both models use same setting of ADAM optimizer <ref type="bibr" target="#b46">[47]</ref> with ? 1 of 0.9, ? 1 of 0.99 and weight decay of 10 ?6 . For efficient learning, we performed data prepossessing methods including random crop, flip and color jitter <ref type="bibr" target="#b47">[48]</ref> on the dataset. The random crops are performed at 320 ? 1216 resolution.</p><p>The three-branch backbone is trained with a batch size of 8 and an initial learning rate of 0.00128. The initial learning rate is decayed using ReduceLROnPlateu scheduler. Furthermore, we assign an initial weight of 0.2 to ? cg , ? sg and ? dg coefficients, but we decay the weight to 0 after few epochs. We train the backbone for 60 epochs. To train the CSPN++ with Atrous convolutions, similar to PENet <ref type="bibr" target="#b12">[13]</ref>, we first perform the initialization step and then fully train the models for 95 epochs.    <ref type="table" target="#tab_1">Table 1</ref> shows the quantitative comparison of our method with other state-of-the-art methods. Our proposed method, SemAttNet, outperforms the previous state-of-the-art methods under the primary evaluation metric RMSE at the time of submission, and presents comparable performance on other evaluation metrics. Specifically, in contrast to previous state-of-the-arts, i.e., RigNet and PENet, we observe a difference of 4.03mm and 21.03mm to the performance of SemAttNet. Moreover from the qualitative comparison of our method given in <ref type="figure">Figure 4</ref>, it's evident that our method produces much more precise results than other methods in the areas affected by optical changes in RGB images. Please refer to supplementary section for additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EVALUATION ON KITTI DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ABLATION STUDIES</head><p>In this section, we investigate the impact of each component proposed by our approach on final performance. All of the ablation studies are performed on selected KITTI validation dataset. <ref type="table" target="#tab_3">Table 2</ref> depicts the results of our experiments with different settings. Note, that we consider RMSE as our primary metric to rank the performance of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effect of exploiting Semantic information.</head><p>From methods (b) and (d) listed in <ref type="table" target="#tab_3">Table 2</ref>, we can observe that by using the proposed semantic-guided branch even with naive concatenation method for fusion, there is a significant improvement of 26.5mm in the RMSE metric. This shows the promise and importance of our proposed SG branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effect of SAMMAFB</head><p>The methods (d) and (e) in <ref type="table" target="#tab_3">Table 2</ref> list the performance of the naive fusion method, and SAMMAFB. It is quite evident that SAMMAFB, i.e., method (d), outperforms the naive fusion method by achieving 2.14mm less error than method (f). Moreover, if we consider methods (f) and (g) which employs CSPN++ refinement along with naive and SAMMAFB fusion methods, we see a significant improvement of 11.93mm of RMSE in the method utilizing the SAMMAFB for fusion. These improvement backs our claim that SAMMAFB is a better fusion strategy than naive fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Effect of CSPN++ refinement</head><p>Inspired by original CSPN++ <ref type="bibr" target="#b18">[19]</ref>, we also use 12 iterations for neighborhood propagation. However, similar to PENet <ref type="bibr" target="#b12">[13]</ref>, we apply a dilation strategy to the original CSPN++ in which for the first 6 iterations, we use a dilation rate of 2, and then we decrease it to 1 for remaining iterations to propagate the neighborhood of each pixel. From <ref type="table" target="#tab_3">Table 2</ref>, the method (g) that employs CSPN++ refinement with the dilation strategy, outperforms method (f) with a significant  margin of 14.89mm in RMSE metric. This proves that CSPN++ is an effective method for dense depth refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we presented a novel three-branch backbone consisting of color-guided, semantic-guided, and depthguided branches for dense depth completion. Existing imageguided methods focus only on RGB images to extract both color and semantic cues of the scene. However, RGB images suffer from artifacts such as varying contrast and shadows.</p><p>To tackle this problem, in our three-branch backbone, we introduced a novel semantic-guided branch, which, irrespective of any optical changes in RGB images, learns to understand the semantics of the scene and reduces the reliance on RGB images. Moreover, instead of fusing features naively through addition or concatenation, we also utilized semanticaware multi-model fusion block (SAMMAFB) for feature fusion between all branches. In addition, we also implement CSPN++ with Atrous convolutions to further refine the dense depth maps. Extensive experiments show that our results are superior both quantitatively and qualitatively compared to previous state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 3 :</head><label>3</label><figDesc>Architecture of SAMMAFB. The input to SAM-MAFB is the concatenation of RGB, semantic and sparse depth modalities. It applies channel-wise and spatial-wise attention to the concatenated features to produce the refined fused feature maps. spatial-wise attention. Specifically, channel-wise attention aims to learn the important channels by assigning a weight to each channel relative to their contribution towards increasing the overall performance. Similarly, the spatial-wise attention module focuses on learning the location of the important channels produced by the channel-wise attention sub-module. Both of the modules complement each other and vital to producing refined fused feature maps.Let F RGB ? R C?H?W , F Sem ? R C?H?W and F D ? R C?H?W represent the intermediate feature maps of RGB, semantic and depth guided branches at same level, respectively and F = [F RGB ; F Sem ; F D ] ? R 3C?H?W denotes their depth-wise concatenation. Channel-wise attention weights are computed with Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>c (F) = ?(M LP (AvgP ool(F)) + M LP (M axP ool(F)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art results on KITTI depth completion benchmark, ranked by RMSE. The results of other methods are obtained from KITTI leaderboard. Our method i.e. SemAttNet achieves state-of-the-art performance at the time of submission.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Ablation study on KITTI depth completion selected validation dataset. CG, SG and DG stands for color-guided, semantic-guided and depth-guided branches and concat, add refers to simple concatenation and addition of multi-modal features.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast depth densification for occlusion-aware augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A real-time interactive augmented reality depth estimation technique for surgical robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salcudean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8291" to="8297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d reconstruction with time-of-flight depth camera and multiple mirrors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="38106" to="38114" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="120" to="136" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII 16</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depthnet: Real-time lidar point cloud depth completion for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhousni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227825" to="227833" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime dense mapping for self-driving vehicles using fisheye cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6087" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An indoor obstacle detection system using depth information and region growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="27116" to="27141" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1116" to="1129" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Three-dimensional imaging in the studio and elsewhere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Iddan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Three-Dimensional Image Capture and Applications IV</title>
		<editor>B. D. Corner, J. H. Nurre, and R. P. Pargas</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">4298</biblScope>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards precise and efficient image guided depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rignet: Repetitive image guided network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth completion using planeresidual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13916" to="13925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitask gans for semantic segmentation and depth completion with cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive context-aware multimodal network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fcfrnet: Feature fusion based coarse-to-fine residual learning for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2136" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10615" to="10622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10023" to="10032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
	<note>in German conference on pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semanticaware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-modal attention-based fusion model for semantic segmentation of rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depthnet: Real-time lidar point cloud depth completion for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhousni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227825" to="227833" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uncertaintyaware cnns for depth completion: Uncertainty from beginning to end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holmquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12014" to="12023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="913" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Denselidar: A real-time pseudo dense depth guided depth completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1808" to="1815" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01020</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autoencoder for words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-R</forename><surname>Liou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="84" to="96" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4193" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for estimating human emotion in real-world hri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="340" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Huddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sannakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Rajpurohit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="13059" to="13076" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From depth what can you see? depth completion via auxiliary image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11303" to="11312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deformable spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="913" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Depth completion with twin surface extrapolation at occlusion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2583" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4796" to="4803" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
