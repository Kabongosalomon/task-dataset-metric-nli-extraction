<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianyu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It's common for current methods in skeleton-based action recognition to mainly consider capturing long-term temporal dependencies as skeleton sequences are typically long (&gt;128 frames), which forms a challenging problem for previous approaches. In such conditions, shortterm dependencies are few formally considered, which are critical for classifying similar actions. Most current approaches are consisted of interleaving spatial-only modules and temporal-only modules, where direct information flow among joints in adjacent frames are hindered, thus inferior to capture short-term motion and distinguish similar action pairs. To handle this limitation, we propose a general framework, coined as STGAT, to model cross-spacetime information flow. It equips the spatialonly modules with spatial-temporal modeling for regional perception. While STGAT is theoretically effective for spatial-temporal modeling, we propose three simple modules to reduce local spatial-temporal feature redundancy and further release the potential of STGAT, which (1) narrow the scope of self-attention mechanism, (2) dynamically weight joints along temporal dimension, and (3) separate subtle motion from static features, respectively. As a robust feature extractor, STGAT generalizes better upon classifying similar actions than previous methods, witnessed by both qualitative and quantitative results. STGAT achieves state-of-the-art performance on three large-scale datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400. Code is released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a special issue of action recognition, skeleton-based action recognition has attracted much attention recently due to the fast development of pose estimation algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> and wearable motion capture equipment. It involves predicting an action label from a sequence of 2D or 3D skeleton representations. Compared to action recognition in videos <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3]</ref>, skeleton data is more compact in 2D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref> or 3D <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> format and suffers less from environmental factors (e.g. camera motion, light transformations and viewpoint changing), which makes it a more robust representation of human action sequences.</p><p>It's usual for current methods to mainly tackle long-term temporal dependencies as skeleton sequences are rather long (&gt;128 frames), which forms a serious obstacle for previous methods to model human dynamics through a whole video.  However, in such conditions, critical short-term temporal dependencies are few formally considered. In contrast with long-term dependencies which recognize action sequences from an overall perspective, short-term dependencies distinguish similar sequences on small temporal scales with important details. For example, 'Eating' and 'Tooth brushing' have quite similar movements in both upper and lower body where hands are raised next to mouth for a long while, holding food or brushing teeth, which is quite similar in long sequences. To classify them relies on whether there exists subtle movement of the hand to brush horizontally, which is short-term and rather critical. Few methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43]</ref> have realized the importance of short-term motion and proposed dilated temporal convolutions to capture short-term temporal movements. We propose a general framework to perform spatial-temporal modeling, achieving superior performance than them under similar computational budgets as shown in Sec. 4.3.</p><p>Most existing methods employ interleaving spatial-only and temporal-only modules to extract spatial features and capture long-term temporal dependencies, respectively. A typical approach is to first use a graph module to extract spatial representations at each time step, and then deploy recurrent or temporal convolution layers to build temporal relationships ( <ref type="figure" target="#fig_1">Fig.1(a)</ref>) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>. Such design are efficient and effective for long-term modeling, but hinders direct information flow in local spatial-temporal neighborhoods. For example, messages have to pass through a spatial module and a temporal module to reach nodes in adjacent frames, which are inevitably weakened in multiple transitions. Especially, large temporal kernels adopted after spatial modules get more information mixed and may cause subtle motion impaired, where short-term temporal dependencies may be ineffectively captured by such factorized modeling.</p><p>In this work, we propose STGAT with spatial-temporal modeling to specially capture short-term dependencies. We retain the temporal-only modules for longterm modeling and give spatial-only modules more freedom to perform local spatial-temporal modeling. It builds local spatial-temporal graphs by connecting nodes in local spatial-temporal neighborhoods and dynamically constructing their relationships ( <ref type="figure" target="#fig_1">Fig.1(c)</ref>). Thus messages can directly reach other joints in lo-cal neighborhoods without multiple transmission costs. The connection strength of edges is adaptively determined to better fit local movement patterns. For further releasing the potential of STGAT, we propose several light-weight modules to reduce local feature redundancy. These modules make effect in narrowing the scope of self-attention mechanism, dynamically weighting joints along temporal dimension and separating subtle motion from static features. Equipped with the ability in spatial-temporal modeling, STGAT exhibits great progress on classifying similar action pairs, witnessed by both qualitative and quantitative results. As a result, STGAT achieves state-of-the-art performance on three large-scale datasets: NTU RGB+D 60 <ref type="bibr" target="#b26">[27]</ref>, NTU RGB+D 120 <ref type="bibr" target="#b22">[23]</ref>, and Kinetics Skeleton 400 <ref type="bibr" target="#b13">[14]</ref>. Code is releasedhttps://github.com/hulianyuyy/STGAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skeleton-Based Action Recognition</head><p>Earlier methods always make use of hand-crafted features <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> to depict human dynamic representations. However, they fail to model dependencies across spacetime and lack enough ability in extracting high-dimensional features. Later, convolutional neural network(CNNs) and recurrent neural networks(RNNs) become mainstream by modeling human dynamics as a pseudo-image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> or a series of coordinates along time <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32]</ref>. Nevertheless, they overlook the internal relationships between joints which are better captured by graph networks due to their natural advantage over handling non-euclidean data. Graph networks in skeleton-based action recognition fall into two streams: spatial perspective and spectral perspective. ST-GCN <ref type="bibr" target="#b40">[41]</ref> belonging to the spatial perspective firstly models human structure as a spatial graph to construct node relationships. Later methods mainly make efforts in enlarging the receptive field <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>, combining another stream <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, dividing graphs into structural ones <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>, design network structure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>, combining adaptive learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Attention Mechanism with Skeleton Data</head><p>One of the main benefits of self-attention mechanism is to adaptively adjust relationships with neighbors according to their responses. A self-attention module flexibly computes the response of a position with others' in an embedding space and then aggregates their weighted features. Self-attention mechanism has proven its power in many fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref> and is recently introduced to model relationships <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b30">31]</ref> in skeleton-based action recognition. 2s-AGCN adds an adaptively computed graph with the physically predefined graph to flexibly model joint relationships, while MS-G3D <ref type="bibr" target="#b24">[25]</ref>, CA-GCN <ref type="bibr" target="#b45">[46]</ref> and dynamic-GCN <ref type="bibr" target="#b41">[42]</ref> inherit it. DSTA-Net <ref type="bibr" target="#b30">[31]</ref> and SGN <ref type="bibr" target="#b44">[45]</ref> proposes to alternatively employ fully attention-based spatial modeling and temporal modeling for capturing movement patterns. In contrast with previous approaches, our method doesn't rely on a certain self-attention strategy, but proposes a spatial-temporal modeling framework for capturing long-term and especially short-term dependencies.</p><p>3 Learning STGAT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Notations. Formally, a graph is always represented as G=(V, E ), where V = {v 1 ,...,v N } is a set of N graph nodes representing joints and E is a series of graph edges representing bones between joints. An adjacent matrix A with size N ?N is adopted to depict bones where A i,j represents the connection strength between node i and j, which equals zero if they have no connections. As for input, action sequences have a node features set represented as a feature tensor X C?T ?N where each node v n ? N has a C dimensional feature vector over total T frames.</p><p>Graph Convolutional Networks. Given the input tensor X and graph structure A, layer-wise graph convolution at each time step can be implemented as:</p><formula xml:id="formula_0">X out = ?(W X in ? ? 1 2 (A + I)? ? 1 2 )<label>(1)</label></formula><p>where X out and X in separately corresponds to the output and input features and W is a trainable weight matrix. ? is the diagonal degree matrix of A to employ normalization and I adds self-loops to keep identity features. At last, ?(?) acts as an activation function for output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial-Temporal Graph Attention Network</head><p>Following the statement in Sec. 1, short-term dependencies are critical cues for classifying similar action pairs. We propose a general framework to capture shortterm dependencies by performing spatial-temporal modeling, detailed as follows.</p><p>Building Spatial-Temporal Graphs. To better capture short-term dependencies, in this paper we give spatial-only modules more freedom for shortterm aggregation, while maintaining the role of temporal-only modules for longterm modeling. We extend spatial-only modules to build a local robust spatialtemporal feature aggregator which directly exchange messages between nodes in a local spatial-temporal neighborhood. We start by building a spatial-temporal graph consisting of nodes in a local cross-spacetime neighborhood, which have links with not only other nodes in their original spatial graph but also their neighbors in neighboring timestamps. In this way, each node can directly aggregate messages from other local spatial-temporal neighbors. Let us consider a sliding window of size ? and dilation d at each time step over input sequences, which generates a local action sequence at each time step t represented as X t ? = {x t?? /2:t+? /2 ? R C?? N |t ? Z, 0 ? t &lt; T }. Here, ? controls the local sequence length along time while d represents the dilation rate by picking a frame every d frames. Dilated window allows us to control the interval between sampled frames to fit local movement patterns. Then, for each frame x t , a unique regionally spatial-temporal adjacent matrix A t ? is organized by enumerating all possible neighbors in local spatial-temporal neighborhood X t ? as:</p><formula xml:id="formula_1">A t ? = [ A t (1) , ?, A t (? ) ] ? R ? N ?N<label>(2)</label></formula><p>where A t (? ) represents a cross-time graph with size N ? N by establishing relationships between nodes at current timestamp t with their neighbors at ? neighboring frames in a local spatial-temporal neighborhood. If we take one node of current timestamp t as an example (i.e. one column of A t ? ), it's connected to ? ? N neighbors consisting of ? frames with N nodes at each timestamp. By incorporating A t ? into graph operations, the output of STGAT for each frame x t can be obtained as:</p><formula xml:id="formula_2">[X out ] t = ?(W [X in ? ] t A t ? ).<label>(3)</label></formula><p>Adaptive Computation for Spatial-Temporal Graph. For well distinguishing similar action pairs, more attention should be paid to aggregate messages from important joints and less to other joints. We incorporate self-attention mechanism to adaptively compute edge weights in local spatial-temporal graphs to achieve this goal.</p><p>Following the widely used self-attention formulation, a generic self-attention operation in graph networks at each time step can be defined as:</p><formula xml:id="formula_3">A i = 1 C(X in i ) ?j f (X in i , X in j )g(X in j )<label>(4)</label></formula><p>where i is the index of an output position, j is the index that enumerates all possible neighbors and A i is the computed edge weights of node v i with others.</p><p>Here, the pairwise function f (?) computes the affinity of features between node v i and v j . g(?) offers an embedding feature for node v j . C(?) adds normalization for the result. Given the adjacent matrix A, a weighted output X out can be computed as:</p><formula xml:id="formula_4">X out = ?(W X in A).<label>(5)</label></formula><p>A more stable and flexible choice is employing multi-head attention modules to learn various kinds of edge weights. Specially, S independent self-attention modules are employed to learn different graph structures, as:</p><formula xml:id="formula_5">X out = ?( 1 S S s=0 W s X in A s ).<label>(6)</label></formula><p>Here A s and W s are the computed adjacent matrix and weight matrix separately of s th head. By incorporating our spatial-temporal graph A t ? and our sampled local action sequence X t ? in Eq. 5, we arrive at the final form to compute edge weights and aggregate messages from neighbors in spatial-temporal graphs:</p><formula xml:id="formula_6">X out = ?( 1 S S s=0 W s [X in ] t ? A t ?,s )<label>(7)</label></formula><p>where A t ?,s represents a unique computed spatial-temporal graph at timestamp t with temporal length ? of the s th head. X out is the accumulated output of all heads with different spatial-temporal graphs.</p><p>Based on above design, our STGAT is a general framework which doesn't rely on a certain self-attention strategy and can be built with self-attention operators or any attention-based approaches by inflating their spatial-only modules to construct cross-spacetime edges for short-term modeling.</p><p>Discussion. We give some in-depth analysis for STGAT as follows: (1) Compared with MS-G3D <ref type="bibr" target="#b24">[25]</ref> which builds a spatial-temporal graph with size ? N ? ? N for cross-spacetime modeling, ours owns size ? N ? N . Thus, MS-G3D introduces more redundant messages from neighbors and cost N times of computations than ours. More importantly, MS-G3D assigns same edge weights for joints in a local spatial-temporal neighborhood ( <ref type="figure" target="#fig_1">fig.1(b)</ref>), which fails to distinguish beneficial messages and allows too much information to flood into current node without filter. In contrast, we dynamically construct joint relationships and thus is able to aggregate representative features. In this condition, MS-G3D deploys two pathways and proposes multiscale temporal convolutions to capture various short-term motion but still performs worse than ours. 2. STGAT is analogous to 3DConv <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref> in modeling spatial-temporal dependencies by a weighted aggregation of regional cross-spacetime messages. While the parameters of 3D convolution kernels are structurally learned and fixed for samples, STGAT adaptively computes the edge weights for each sample and is less prone to overfit than 3DConv during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reducing local spatial-temporal redundancy</head><p>While STGAT is theoretically effective for spatial-temporal aggregation, we empirically found direct aggregation of messages from spatial-temporal neighbors brings redundant static information, resulted from similar contents in adjacent frames. We propose several simple modules to reduce local spatial-temporal redundancy and further release the potential of STGAT, resulting in a robust feature extractor.</p><p>Separate Learning. Compared to the original N ?N spatial graph, STGAT builds a ? N ?N spatial-temporal graph to model local dependencies across spacetime. Obviously, the size of adjacent matrix increases ? times. That makes it harder for self-attention operators to adaptively construct joint relationships among neighbors with similar contexts. As we found in practice, as node number grows, attention-based modules tend to assign uniform weights around 1/(? ?N ) for each joint, which thus gradually loses the ability to adaptively construct relationships. We propose Separate Learning Strategy to limit node numbers and keep beneficial information lossless to relieve this problem. Inspired by groupwise convolution <ref type="bibr" target="#b15">[16]</ref>, we propose to divide joints in a local spatial-temporal region into several small groups, where STGAT only weights edges within groups.</p><formula xml:id="formula_7">1 ? 1 Conv ? ? ? ? ? ? Temporal split 1 ? 1 Conv ?1 +1 ? ? 1 ? 1 Conv 1 ? 1 Conv Concat ? ? /4 ? ? /4 ? ? /2 ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion features Motion features</head><p>Explicit motion modeling Normal <ref type="figure">Fig. 2</ref>. Comparison of our proposed Explicit Motion Modeling strategy with normal 1 ? 1 implementation. We try to separate short-term motion in local spatial-temporal region from static features for better short-term modeling.</p><p>Thus, the scope of self-attention operators is narrowed and the computation costs are proportionally lowered down as well. Features from different groups are concatenated as outputs which won't bring information loss. Practically, while there exist lots of methods to group joints, we choose to divide them according to their distances with target joint.</p><p>Dynamic Temporal Weighting. As defined in Eq. 2 and 3, STGAT builds spatial-temporal graphs to model cross-spacetime information. We notice that some keyframes play more important roles than others where considering frames equally may hurt modeling short-term motion. We propose two methods to dynamically weight frames regionally as follows and compare them in the experiments: <ref type="bibr" target="#b0">(1)</ref> We add a learnable parameter T DT W with size S ?? to weight frames for S heads with temporal duration ? , which is updated through network propagation. Note that T DT W structurally considers the weights of frames in each head. It's fixed for each sample during inference.</p><p>(2) Inspired by SENet <ref type="bibr" target="#b10">[11]</ref>, we design an adaptive temporal weighting module to weight frames by considering the short-term motion in a local spatial-temporal region. We first perform 1-hop graph convolutions for current frame X t to aggregate local spatial features. Then, a 1 ? 1 convolution kernel is utilized to enhance features of current frame X t . We subtract features of regional neighbors from those of current frame to obtain motion features, which reflect the movement degree of neighboring frames against current one. Finally, after pooling along channel and temporal dimension, they are passed through a sigmoid function to weight frames in local spatial-temporal region. These weights are computed adaptively in accordance with input for regional weighting. An Illustration can be found in the supplementary material.  Explicit Motion Modeling. Considering neighboring frames always contain similar contents, we infer only one frame can provide enough static information and short-term motion benefits recognition more, for reducing local feature redundancy. We propose Explicit Motion Modeling strategy to separate shortterm motion in local spatial-temporal neighborhood from static features. As illustrated in <ref type="figure">fig.2</ref>, compared to original 1 ? 1 transformations, we first split the features along temporal dimension. And then, we subtract features of adjacent frames from those of current frame to obtain motion features. In this condition, only static features of current timestamp is reserved and short-term motion is obtained. With appropriate padding, these motion features pass a 1 ? 1 convolution kernel for enhancement and are finally concatenated with current frame's features as output. Compared with normal 1 ? 1 convolution, this design introduces no additional parameters but enhances the ability for distinguishing short-term motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>???</head><p>Discussion. These three modules are lightweight and can be painlessly incorporated into STGAT, with quite little costs. Only Dynamic Temporal Weighting consumes S ?? parameters, which are negligible in deep-learning-based networks. They make powerful effect for reducing local redundancy, resulting in superior performance of STGAT, as witnessed in sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Architecture</head><p>Overall Architecture. Our method is a general framework which can be flexibly constructed upon self-attention operators or previous attention-based approaches. To demonstrate the effectiveness of STGAT, we choose DSTA-Net <ref type="bibr" target="#b30">[31]</ref> as our baseline, which employs fully attention-based spatial-only modules and temporal-only modules alternatively. It's already among the top-performing methods currently. As illustrated in <ref type="figure" target="#fig_2">fig.3</ref>, our model is consisted of r spatial-temporal graph attention blocks (STGABs) followed by a series of global average pooling, fully connected and softmax operations to perform prediction. Each STGAB contains a spatial-temporal graph attention module to capture regional crossspacetime dependencies and a temporal convolution module to perform longrange temporal information aggregation. For simplicity, BN and ReLU and neglected in the figure.</p><p>Multiple-Stream Fusion. Inspired by 2s-AGCN <ref type="bibr" target="#b29">[30]</ref>, beneficial bone information is modeled by another stream. Following our baseline, we also calculate motion for both joint and bone streams to obtain another two motion streams. The softmax scores of input streams are averaged to give the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NTU RGB+D 60 and NTU RGB+D 120. NTU RGB+D 60 <ref type="bibr" target="#b26">[27]</ref> and NTU RGB+D 120 <ref type="bibr" target="#b22">[23]</ref> are both large-scale widely used action recognition datasets which separately contain 56578 action sequences over 60 classes and 113945 action sequences from 120 classes. All samples are performed by 40 or 106 different volunteers modeled by their 3D locations with N =25. Two experiment setups are recommended by the author: (1) Cross-subject(X-Sub), where samples are split into training and testing groups with respect to subjects. (2) Cross-view(X-View) for NTU RGB+D 60 or Cross-Setup (X-Set) for NTU RGB+D 120, where action sequences are divided by their shooting cameras or camera setups. Kinetics Skeleton 400. The Kinetics Skeleton 400 Dataset <ref type="bibr" target="#b13">[14]</ref> is sampled from the Kinetics-400 dataset by Openpose pose estimation toolkit <ref type="bibr" target="#b1">[2]</ref>. It contains 240,436 training and 19,796 testing sequences. Every action sequence contains 18 joints wth their 2D locations and 1D confidence score at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We use 8 STGABs in our experiments with their channels in {64, 64, 128, 128, 256, 256, 256 and 256} respectively. We randomly/uniformly sample 150 frames and then randomly/centrally crop 128 frames for training/test splits. The learning rate is 0.1 and divided by 10 at epochs {60, 90}, {60, 90}, and {80, 110} for NTU RGB+D Datasets, and Kinetics Skeleton 400 with total 120/180 epochs. The temporal convolution following STGAB owns kernel size 7. We combine our Separate Learning Strategy with multi-head mechanism and set one head as a single group. Our model is trained on a two-2080ti machine with batch size 32 and weight decay 0.0005 in PyTorch framework. Note that, no data augmentation technique is adopted for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model configurations</head><p>Top  Effectiveness of proposed strategies. We incrementally build up the model with its individual components to verify their effectiveness and list the results in tab. 1. Here, SL denotes Separate Learning strategy and EMM means Explicit Motion Modeling. T DT W represents the learnable parameter to structurally weight edges while ATW denotes Adaptive Temporal Weighting. Compared to our baseline, directly building spatial-temporal graphs with STGAT causes 0.2% performance decrease. This demonstrates local feature redundancy severely hurts modeling short-term motion. We then testify the effectiveness of our proposed strategies for reducing local redundancy. Equipped with SL, STGAT obtains 0.6% performance boost. Through SL, STGAT deals with small groups of joints and is easier to distinguish vital connections. Combined with EMM, STGAT achieves 0.8% accuracy boost, which demonstrates EMM makes effect by capturing subtle motion instead of raw spare features. Next, we test the effect of Dynamic Temporal Weighting strategy. STGAT gains 0.8% performance boost from T DT W which structurally weights frames along temporal dimension. ATW also performs well and brings 0.7% performance boost. These results indicate our strategy corrects the equal weighting problem. We also try to combine T DT W with ATW to perform structural and adaptive weighting together, but don't witness further promotion. We speculate that they may overemphasize the weights. T DT W is adopted as default due to its high performance and low computation cost. In the following, we make a comparative test to verify the effectiveness of STGAT. We don't build spatial-temporal graphs but directly equip our baseline with proposed strategies. In this condition, SL and EMM will still make effect. As shown in tab. 1, it achieves 89.0% (+0.8%) but performs lower than STGAT (90.2%, +2.0%), which demonstrates STGAT can better capture local spatial-temporal dependencies for its local spatial-temporal modeling. Finally, we examine the effect of spatial-temporal modeling for STGAT. We compare it with spatial connections (baseline), dense connections among all joints in ? frames, and simply repeating spatial graph ? times along temporal dimension. Our STGAT outperforms all of them by a large margin (? 1.2%) which demonstrates the effectiveness of adaptive computation for edges in a local spatialtemporal neighborhood.</p><formula xml:id="formula_8">? =1 89.0 ? =3, d =1 90.2 ? =3, d =2 89.6 ? =5, d =1 89.2 ? =5, d =2 89.1 ? =7, d =1 89.1 2 Paths, ? ={3,5}, d ={1,1} 89.3 2 Paths, ? ={3,5}, d ={1,2} 89.1</formula><p>Study on model structure. We then test the choices of some hyperparameters in STGAT and give their results in tab. 1. We offer S, the number of heads, with four choices in {1, 4, 8 and 16}. We observe that larger S gives higher performance which indicates the effects of multiple built graphs, and the performance reaches top when S equals 8. For other STGAT configurations, like ? and d, we test different combinations of them. ? =1 means no temporal information is incorporated and only spatial modeling is performed. We first observe STGAT outperforms baseline when ? =1 and benefits from larger ? , which demonstrates the effect of local spatial-temporal dependencies. The gain of ? starts to decrease when it equals 5. We infer large ? makes similar effect with temporal-only modules and causes redundancy. Considering d, we observe accuracy drop when d increases, which shows closer frames are much more relevant in a local spatial-temporal neighborhood. We further construct two STGAT paths to make comparison with MS-G3D <ref type="bibr" target="#b24">[25]</ref>. MS-G3D obtains performance boost by incorporating more paths for multiscale temporal information. But STGAT already earns better performance through one path with fewer computations and parameters than MS-G3D for its superior spatial-temporal modeling ability.</p><p>Effect on classifying similar actions. We demonstrate the effect of STGAT on classifying similar patterns in tab. 3. We compare STGAT with baseline and another spatial-temporal modeling method, MS-G3D, upon several similar action pairs from NTU RGB+D 60 Dataset. Compared with MS-G3D, STGAT outperforms it in all action pairs with up to 9.2% performance boost. MS-G3D fails to distinguish short-term motion with equal weights for neighboring joints, which introduces too much redundancy, even with two paths. Compared with baseline, STGAT obtains notable improvements, up to 12.5%, upon similar action pairs. These results verify the effectiveness of our STGAT on classifying similar action pairs owing to its adaptive spatial-temporal modeling. Please refer to the supplementary material for more examples and the full  <ref type="table">Table 3</ref>. Class-wise Accuracy(%) Comparison for MS-G3D <ref type="bibr" target="#b24">[25]</ref>, baseline and STGAT on similar action pairs from NTU RGB+D 60 dataset with X-Sub Setting.</p><p>Comparison with previous methods with short-term modeling. We compare STGAT with MS-GCN <ref type="bibr" target="#b24">[25]</ref> which assigns equal weights for local neighbors for spatial-temporal modeling. <ref type="bibr" target="#b24">[25]</ref> also proposes MSTCN consisted of temporal convolutions with different dilations for multiscale temporal information. As shown in tab. 4, STGAT outperforms baseline as well as MSGCN by a large margin (+2.0%, +2.4%). When equipped with MSTCN, either baseline or MSGCN obtains satisfactory performance boost (+0.4%, +1.2%), accompanied by notable increase in parameters and GFLOPs. STGAT outperforms MSTCN hybrids by at least 1.2% with fewer parameters and GFLOPs and similar memory usage, which demonstrates the efficacy and efficiency of STGAT for capturing both short-term and long-term dependencies.  <ref type="table">Table 4</ref>. Comparison with previous methods with short-term modeling on NTU RGB+D 60 dataset with X-Sub Setting.</p><p>Visualizations. We visualize the generated edges by STGAT, dense connections among all joints, and spatial connections (Baseline), in <ref type="figure" target="#fig_4">fig. 4</ref> to give more analysis on the effect of STGAT. We select the action "hand waving" as an example. The left hand is the most important joint of action "hand waving" which forms the movement patterns of this action. Edges generated by STGAT   <ref type="table" target="#tab_5">Table 5</ref>, 6 and 7 compare our final model with other state-of-the-art methods on three large-scale datasets NTU RGB+D 60 <ref type="bibr" target="#b26">[27]</ref>, NTU RGB+D 120 <ref type="bibr" target="#b26">[27]</ref> and the Kinetics Skeleton 400 <ref type="bibr" target="#b13">[14]</ref>. These methods include CNN-based methods <ref type="bibr" target="#b18">[19]</ref>, RNN-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>, graph networks with predefined graph structure <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref> and graph networks with adaptive adjacent matrix <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42]</ref>. We first observe that most of graph-based methods gain better performance than non-graph methods by better modeling relationships. We then see graph methods with adaptive graph structure <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42]</ref> or multi-scale feature aggregation <ref type="bibr" target="#b24">[25]</ref> consistently perform better than previous methods, indicating the necessity to capture long-range dependencies. Our STGAT makes a step further to capture cross-spacetime information and outperforms all existing methods especially when MS-G3D aggregates spatial-temporal features with two paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU RGB+D 60</head><p>Methods X-Sub X-View AS-GCN <ref type="bibr" target="#b19">[20]</ref> 86.8 94.2 2s-AGCN <ref type="bibr" target="#b29">[30]</ref> 88. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a general framework, named STGAT, to model crossspacetime information. To reduce local spatial-temporal feature redundancy, we propose three simple modules on narrowing the scope of self-attention mechanism, dynamically weighting frames and separating subtle motion from static features. STGAT exceeds previous state-of-the-arts on three large-scale datasets and makes further progress over classifying similar action pairs, witnessed by both qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU RGB+D 120</head><p>Methods X-Sub X-Set SGN <ref type="bibr" target="#b44">[45]</ref> 79.2 81.5 2s-AGCN <ref type="bibr" target="#b29">[30]</ref> 82.9 84.9 Shift-GCN <ref type="bibr" target="#b6">[7]</ref> 85.9 87.6 4s DSTA-Net <ref type="bibr" target="#b30">[31]</ref>  where [?, ?] represents concatenation operation and W f is a weight vector projecting their concatenated result into a scalar. Besides, ? serves as an activation function that is usually ReLU or LeakyReLU in practice. Under the formulation defined above, a self-attention module can be implemented as shown in <ref type="figure" target="#fig_1">fig. 1</ref> in practice. Here, ?(?) and ?(?) are instantiated as 1?1 convolutions and C e is adopted to reduce computation costs. g(?) = 1. Empirically, C e is defined as C out /r where r is an adjustable factor to reduce computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation for Adaptive Temporal Weighting Module</head><p>We give the implementation for the adaptive temporal weighting module in the <ref type="figure">fig. 2</ref>. We first perform 1-hop graph convolutions for current frame X t to aggregate local spatial features. Then, a 1 ? 1 convolution kernel is utilized to enhance features of current frame X t . We subtract sampled features of regional neighbors from those of current frame to obtain motion features. Finally, after pooling along channel and temporal dimension, they are passed through a sigmoid function to weight frames in local spatial-temporal region.  <ref type="table">Table 1</ref>: Class-wise Accuracy Comparison for MS-G3D <ref type="bibr" target="#b24">[25]</ref>, our baseline (DSTA-Net <ref type="bibr" target="#b30">[31]</ref>) and STGAT on the Cross Subject setting of the NTU RGB+D 60 dataset, using the joint only. Values in the parentheses are accuracy differences compared to MS-G3D or baseline. Accuracy decrease is marked in red while accuracy increase is marked with green. STGAT outperforms MS-G3D and baseline in 38/60 and 46/60 action classes, respectively. Especially, on several similar action pairs like 'eat meal/snack' against 'brushing teeth', 'reading' against 'writing' and 'taking a selfie' against 'salute', STGAT makes great improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Class-wise Accuracy Comparison</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Factorized spatial-only and temporal-only modeling over input sequences. (b) Disentangled multi-scale feature aggregation in local spatial-temporal neighborhoods. Edges of the same color indicate equal weights in the predefined spatial-temporal graph. (c) Spatial-temporal feature aggregation with adaptively computed edges. Edges with different colors represent different degrees of connection strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture Overview of STGAT. After Explicit Motion Modeling, input data passes r consecutive STGABs and temporal convolutions, followed by global average pooling, FC and softmax to perform classification. In each STGAB, S heads are concatenated with two 1 ? 1 convolutions to transform channels. For simplicity, BN and ReLU are neglected in the figure. Best viewed with color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Visualizations for generated edges by STGAT, as well as dense connections among all joints, and spatial connections from adaptively computed spatial graph.all lie in the upper body and especially focuses on the left hand. In contrast, the spatial connections (Baseline) are more sparse and some land on the bottom body which has no movements. Dense connections have no focus and put same attention on all joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablations on NTU RGB+D 60 dataset. Top-1 accuracy(%) is reported. We compare various STGAT settings, including different values of S, ? , d. STGAT is configured with the best setting tested previously, i.e. STGAT w/ SL + EMM + TDT W .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell>Classes</cell><cell cols="2">MS-G3D Baseline</cell><cell>STGAT</cell></row><row><cell cols="2">Average of all classes 89.4</cell><cell>88.2</cell><cell>90.2</cell></row><row><cell>eat meal/snack</cell><cell>67.6</cell><cell>70.9</cell><cell>74.9 (+7.3,+4.0)</cell></row><row><cell>brushing teeth</cell><cell>87.5</cell><cell>82.1</cell><cell>89.0 (+1.5,+6.9)</cell></row><row><cell>reading</cell><cell>68.5</cell><cell cols="2">60.1 71.6 (+3.1,+11.5)</cell></row><row><cell>writing</cell><cell>54.4</cell><cell cols="2">51.1 63.6 (+9.2,+12.5)</cell></row><row><cell>taking a selfie</cell><cell>89.1</cell><cell>86.6</cell><cell>90.5 (+1.4,+3.9)</cell></row><row><cell>salute</cell><cell>89.9</cell><cell>88.0</cell><cell>93.1 (+3.2,+5.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison of STGAT against other state-of-the-art methods on the NTU RGB+D 60 dataset. Top-1 accuracy(%) is reported.</figDesc><table><row><cell></cell><cell>5</cell><cell>95.1</cell></row><row><cell cols="2">AGC-LSTM [32] 89.2</cell><cell>95.0</cell></row><row><cell>GCN-NAS [26]</cell><cell>89.4</cell><cell>95.7</cell></row><row><cell>DGNN [28]</cell><cell>89.9</cell><cell>96.1</cell></row><row><cell cols="2">MS-AAGCN [29] 90.0</cell><cell>96.2</cell></row><row><cell>Shift-GCN [7]</cell><cell>90.7</cell><cell>96.5</cell></row><row><cell cols="2">PA-ResGCN [33] 90.9</cell><cell>96.0</cell></row><row><cell cols="2">Dynamic GCN [42] 91.5</cell><cell>96.0</cell></row><row><cell cols="2">MS-G3D Net [25] 91.5</cell><cell>96.2</cell></row><row><cell cols="2">4s DSTA-Net [31] 91.5</cell><cell>96.4</cell></row><row><cell cols="2">4s MST-GCN [4] 91.5</cell><cell>96.6</cell></row><row><cell>GCsT [1]</cell><cell>91.6</cell><cell>96.2</cell></row><row><cell>STGAT(Joint)</cell><cell>90.2</cell><cell>95.4</cell></row><row><cell>STGAT(Bone)</cell><cell>90.6</cell><cell>95.8</cell></row><row><cell>2s-STGAT</cell><cell cols="2">92.2 96.9</cell></row><row><cell>4s-STGAT</cell><cell cols="2">92.8 97.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Fig. 1. A self-attention block with Embedded Gaussian version. ? and ? are implemented with 1?1 convolutions. g(?) = 1. Ce is adopted to reduce computation costs. The vanilla Gaussian version can be obtained by removing ? and ?, and the Dot-product version can be done by replacing softmax with a scaling factor 1/N. .</figDesc><table><row><cell>? ?</cell><cell>?</cell></row><row><cell></cell><cell>softmax</cell></row><row><cell>? ?</cell><cell>?</cell></row><row><cell></cell><cell>?</cell></row><row><cell>?</cell><cell></cell><cell>?</cell></row><row><cell cols="2">: 1 ? 1</cell><cell>: 1 ? 1</cell></row><row><cell></cell><cell>? ?</cell></row><row><cell></cell><cell cols="2">Kinetics Skeleton 400</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell>Top-1 Top-5</cell></row><row><cell></cell><cell cols="2">ST-GCN [41]</cell><cell>30.7 52.8</cell></row><row><cell></cell><cell cols="2">2s-AGCN [30]</cell><cell>36.1 58.7</cell></row><row><cell>86.6 89.0 MS-G3D Net [25] 86.9 88.4 PA-ResGCN [33] 87.3 88.3 Dynamic GCN [42] 87.3 88.6 4s MST-GCN [4] 87.5 88.8 GCsT [1] 87.7 89.3 4s-STGAT 88.7 90.4</cell><cell cols="2">DGNN [28] GCN-NAS [26] MS-AAGCN [29] 37.8 61.0 36.9 59.6 37.1 60.1 Dynamic GCN [42] 37.9 61.3 MS-G3D Net [25] 38.0 60.9 4s MST-GCN [4] 38.1 60.8 4s-STGAT 39.2 62.8</cell></row><row><cell>Table 6. Comparison of STGAT against other state-of-the-art methods on the NTU RGB+D 120 dataset. Top-1 accuracy(%) is reported.</cell><cell cols="2">Table 7. Comparison of STGAT against other state-of-the-art methods on the Kinetics Skeleton 400 dataset. Top-1 and Top-5 accuracies(%) are re-ported.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table . 1</head><label>.</label><figDesc>offers class-wise accuracy comparison for MS-G3D<ref type="bibr" target="#b24">[25]</ref>, our baseline and STGAT on the Cross Subject setting of the NTU RGB+D 60<ref type="bibr" target="#b26">[27]</ref> dataset. ConvFig. 2. Illustration of our proposed adaptively Dynamic Temporal Weighting strategy.The goal is to weight frames in spatial-temporal graph with an adaptively computed factor with size ? ? 1 ? 1.Specifically, accuracy decrease is marked in red while accuracy increase is marked with green. It can first be observed that MS-G3D and baseline have achieved great performance on most of the classes with 89.4% and 88.2% average, respectively. Our final model consistently outperforms MS-G3D and baseline in 38/60 and 46/60 classes. Especially, on several similar action pairs like 'eat meal/snack' against 'brushing teeth', 'reading' against 'writing' and 'taking a selfie' against 'salute', STGAT makes great improvements (+7.3%/+1.5%, +3.1%/+9.2%, +1.4%/+3.2%) over MS-G3D and (+4.0%/+6.9%, +11.5%/+12.5%, +3.9%/+5.1%) over baseline due to the special care of short-term dependencies.</figDesc><table><row><cell cols="5">? ? 1 ? hop graph convolution ? Input Dynamic temporal weighting Temporal sampling window Size= ? ? ? #ID MS-G3D Baseline Average of all classes -89.4 88.2 drink water 0 85.8 85.4 87.6 (+1.8, +2.2) STGAT 90.2 eat meal/snack 1 67.6 70.9 74.9 (+7.3, +4.0) touch head (headache) 43 83.8 81.5 74.3 (-9.5, -7.2) touch chest 44 90.2 90.2 91.3 (+1.1, +1.1) touch back (backache) 45 94.9 93.1 90.2 (-4.7, -2.9) touch neck (neckache) 46 86.2 80.4 86.5(+0.3, +6.1) nausea or vomiting condition 47 84.0 84.7 85.1 (+1.1, +0.4) 1 ? 1 Classes brushing teeth 2 87.5 82.1 89.0 (+1.5, +6.9) use a fan /feeling warm 48 91.6 86.5 90.9 (-0.7, +4.4) X t?1, t, t+1 : Repeat : brushing hair 3 90.8 88.3 92.7 (+1.9, +4.4) punching/slapping other person 49 89.8 90.9 91.2 (+1.4, +0.3) ? ? drop 4 89.8 91.3 93.5 (+3.7, +2.2) kicking other person 50 92.0 92.0 94.9 (+2.9, +2.9)</cell></row><row><cell>Reshape: ? pickup pushing other person ?</cell><cell>5 51</cell><cell>91.6 92.1</cell><cell cols="2">92.4 94.2 ( 97.1 (+5.0, +2.9) 92.7 (+1.1, +0.3)</cell></row><row><cell>Pooling pat on back of other person throw</cell><cell>6 52</cell><cell>94.2 92.4</cell><cell cols="2">92.7 80.1 96.7 (+4.3, +16.6) 88.4 (-5.8, -4.3)</cell></row><row><cell cols="2">? 1 ? 1 sitting down point finger at the other person 53 7</cell><cell>94.5 87.7</cell><cell>92.7 86.2</cell><cell>97.4 (+2.9, +4.7) 92.0 (+4.3, +5.8)</cell></row><row><cell>Sigmoid hugging other person standing up</cell><cell>8 54</cell><cell>98.2 98.5</cell><cell>97.1 96.4</cell><cell>97.4 (-0.8, +0.3) 98.5 (+0.0, +2.1)</cell></row><row><cell cols="2">? 1 ? 1 giving something to other person 55 Element-wise clapping 9</cell><cell>82.8 93.1</cell><cell>78.4 91.7</cell><cell>81 (-1.8, +2.6) 93.8 (+0.7, +2.1)</cell></row><row><cell cols="2">multiplication 10 touch other person's pocket reading 56 ? writing 11 handshaking 57 ?N?N tear up paper 12 walking towards each other 58</cell><cell cols="3">Computed spatial-temporal graph( ?N?N) 68.5 60.1 71.6 (+3.1, +13.1) 92.0 89.8 93.8 (+1.8, +4.0) 54.4 51.1 63.6 (+9.2, +12.5) 97.1 95.3 96.4 (-0.7, +1.1) 90.4 91.1 86.3 (-4.1, -4.8) 97.8 98.5 97.8 (+0.0, -0.7)</cell></row><row><cell cols="2">Spatial-temporal graph( ?N?N) wear jacket 13 walking apart from each other 59</cell><cell>98.9 96.0</cell><cell>96.4 95.7</cell><cell>98.5 (-0.4, +2.1) 94.6 (-1.4, -1.1)</cell></row><row><cell>take off jacket</cell><cell>14</cell><cell>96.7</cell><cell>96.4</cell><cell>97.1 (+0.4, +0.7)</cell></row><row><cell>wear a shoe</cell><cell>15</cell><cell>87.2</cell><cell>85.0</cell><cell>84.2 (-3.0, -0.8)</cell></row><row><cell>take off a shoe</cell><cell>16</cell><cell>74.5</cell><cell>82.8</cell><cell>78.5 (+4.0, -4.3)</cell></row><row><cell>wear on glasses</cell><cell>17</cell><cell>91.6</cell><cell>91.9</cell><cell>89.4 (-2.2, -2.5)</cell></row><row><cell>take off glasses</cell><cell>18</cell><cell>91.6</cell><cell>93.8</cell><cell>90.5 (-1.1, -3.3)</cell></row><row><cell>put on a hat/cap</cell><cell>19</cell><cell>95.2</cell><cell>94.1</cell><cell>94.1 (-1.1, +0.0)</cell></row><row><cell>take off a hat/cap</cell><cell>20</cell><cell>94.9</cell><cell>94.1</cell><cell>97.1 (+2.2, +3.0)</cell></row><row><cell>cheer up</cell><cell>21</cell><cell>95.3</cell><cell>91.6</cell><cell>92.0 (-2.3, +0.4)</cell></row><row><cell>hand waving</cell><cell>22</cell><cell>92.3</cell><cell cols="2">89.4 ( 93.0 (+0.7, +3.6)</cell></row><row><cell>kicking something</cell><cell>23</cell><cell>96.4</cell><cell>95.3</cell><cell>97.1 (+0.7, +1.8)</cell></row><row><cell>reach into pocket</cell><cell>24</cell><cell>83.9</cell><cell>81.8</cell><cell>80.3 (-3.6, -1.5)</cell></row><row><cell>hopping (one foot jumping)</cell><cell>25</cell><cell>97.5</cell><cell>96.4</cell><cell>98.9 (+1.4, +2.5)</cell></row><row><cell>jump up</cell><cell>26</cell><cell>99.3</cell><cell>97.8</cell><cell>100 (+0.7, +2.2)</cell></row><row><cell cols="2">make a phone call/answer phone 27</cell><cell>86.2</cell><cell>82.5</cell><cell>83.6 (-2.6, +1.1)</cell></row><row><cell>playing with phone/tablet</cell><cell>28</cell><cell>72.4</cell><cell cols="2">68.4 82.5 (+10.1, +14.1)</cell></row><row><cell>typing on a keyboard</cell><cell>29</cell><cell>67.3</cell><cell>75.6</cell><cell>69.8 (+2.5, -5.8)</cell></row><row><cell cols="2">pointing to something with finger 30</cell><cell>81.2</cell><cell>75.0</cell><cell>80.8 (-0.4, +5.8)</cell></row><row><cell>taking a selfie</cell><cell>31</cell><cell>89.1</cell><cell>86.6</cell><cell>90.5 (+1.4, +3.9)</cell></row><row><cell>check time (from watch)</cell><cell>32</cell><cell>88.4</cell><cell>84.4</cell><cell>91.7 (+3.3, +7.3)</cell></row><row><cell>rub two hands together</cell><cell>33</cell><cell>86.6</cell><cell>88.8</cell><cell>88.4 (+1.8, -0.4)</cell></row><row><cell>nod head/bow</cell><cell>34</cell><cell>94.6</cell><cell>93.1</cell><cell>96.0 (+1.4, +2.9)</cell></row><row><cell>shake head</cell><cell>35</cell><cell>94.5</cell><cell>93.8</cell><cell>93.8 (-0.7, +0.0)</cell></row><row><cell>wipe face</cell><cell>36</cell><cell>87.3</cell><cell>81.5</cell><cell>88.4 (+0.9, +6.9)</cell></row><row><cell>salute</cell><cell>37</cell><cell>89.9</cell><cell>88.0</cell><cell>93.1 (+3.2, +5.1)</cell></row><row><cell>put the palms together</cell><cell>38</cell><cell>92.4</cell><cell>92.8</cell><cell>93.1 (+0.7, +0.3)</cell></row><row><cell cols="2">cross hands in front (say stop) 39</cell><cell>94.9</cell><cell>89.5</cell><cell>94.6 (-0.3, +5.1)</cell></row><row><cell>sneeze/cough</cell><cell>40</cell><cell>75.7</cell><cell>73.2</cell><cell>75.0 (-0.7, +1.8)</cell></row><row><cell>staggering</cell><cell>41</cell><cell>99.6</cell><cell>96.4</cell><cell>97.8 (-1.8, +1.4)</cell></row><row><cell>falling</cell><cell>42</cell><cell>97.8</cell><cell>95.6</cell><cell>97.8 (+0.0, +2.2)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementations of Self-attention modules</head><p>Under the formulation defined in Eq.4 in our manuscript, we give four different flexible instantiations for the pairwise function f. Other alternative choices are also applicable and may give better results. We employ Embedded Gaussian function as default for f.</p><p>Gaussian. A natural choice for f is the widely used Gaussian function which can be expressed as:</p><p>Here x T i x j is the dot-product affinity of two position inputs for simplicity. Other distance measurements are also applicable. Correspondingly, the normalization factor C(x) is set as ?j f (x i , x j ).</p><p>Embedded Gaussian. A simple extension of the Gaussian function for f is to compute it in an embedding space, as:</p><p>where ?(x) and ?(x) are two embeddings. In practice, they are always implemented with 1?1 convolutions. C(x) is set as ?j f (x i , x j ). A reason for the popularity of the Embedded Gaussian function is that it can be easily implemented with the softmax function, which both serves as the activation function and adds normalization. As we can see, for a given position i, once we have set C(x) properly, 1 C(x) e ?(xi) T ?(xj ) is exactly the form of the softmax function along dimension j.</p><p>Dot Product. Except for the series of Gaussian functions, a dot-product affinity can also be adopted, as:</p><p>In this case, C(x) is set as the number of possible positions in x for convenience in gradient computation. Other activation functions can be followed.</p><p>Concatenation. Concatenation function is always implemented as:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02860</idno>
		<title level="m">Gcst: Graph convolutional skeleton transformer for action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-scale spatial temporal graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decoupling gcn with dropgraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention-guided multiscale dynamic aggregate graph convolutional networks for skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1589</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatio-temporal inception graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2122" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph routing for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8561" to="8568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2669" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with multistream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled spatial-temporal attention network for skeleton-based action-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dynamic gcn: Contextenriched topology learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14690</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A spatial attentive and temporal dilated (satd) gcn for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context aware graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14333" to="14342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
