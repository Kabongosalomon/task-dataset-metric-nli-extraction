<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-04">4 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
						</author>
						<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-04">4 Feb 2022</date>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Head pose estimation</term>
					<term>multi-task learning</term>
					<term>face align- ment</term>
					<term>occlusions detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Head pose greatly affects facial appearance. It is one of the parameters that influences to a largest extent the performance of many face analysis tasks. For this reason it is a fundamental step in computer vision algorithms estimating attention <ref type="bibr" target="#b0">[1]</ref>, identifying social interaction <ref type="bibr" target="#b1">[2]</ref>, recognizing faces <ref type="bibr" target="#b2">[3]</ref> or robustly estimating facial attributes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. It is a challenging problem in "in-the-wild" conditions, i.e., in presence of extreme orientations, partial occlusions and varying resolution, illumination, facial hair and makeup. Although it has been often considered as by-product or auxiliary task of facial landmark location <ref type="bibr" target="#b5">[6]</ref>, recent results prove that it is much more efficient than landmark estimation and it may achieve superior performance in subsequent face analysis tasks, such as recognition <ref type="bibr" target="#b2">[3]</ref>. In this paper we present a multi-task approach to head pose estimation in unrestricted images. We exploit the strong dependencies among head pose and landmark-related tasks within a multi-task Convolutional Neural Network (CNN) to produce a top performing model.</p><p>The multi-task learning (MTL) paradigm encompasses a set of learning techniques that provide effective mechanisms for sharing information among multiple tasks. It enables the use of larger and more diverse data sets, that improve the regularization during training and the generalization of the final model <ref type="bibr" target="#b6">[7]</ref>. MTL is intimately related to transfer learning (TFL). In TFL a model is trained for one or more auxiliary tasks and subsequently refined for a main target task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Traditionally MTL implies a simultaneous or parallel treatment of all tasks <ref type="bibr" target="#b6">[7]</ref>, whereas in TFL tasks are learned sequentially. In our approach we combine parallel and sequential learning, so it cannot be clearly cast into one of the above two schemes. We rather generalize the traditional concept of MTL to include both. Following other approaches in the literature <ref type="bibr" target="#b9">[10]</ref> we consider different degrees of MTL asymmetry. In this regard TFL is an extremely asymmetric MTL scheme in which auxiliary tasks are only used for pre-training. In our proposal we adopt an asymmetric approach where we seek to optimize head pose using visibility and alignment as auxiliary tasks. However, as we show in our experiments, the co-operation in our model among all three tasks is so high that all of them achieve state-of-the-art results in the most popular benchmarks and improve the performance they would otherwise achieve independently.</p><p>A key element in a multi-task CNN is the architecture of the model and the location of each task in the net. A natural approach is to share bottom layers among all tasks, since they model low-level features, whereas top layers, that capture high level features, are specific to each task <ref type="bibr" target="#b10">[11]</ref>. In the context of face processing, some approaches have completely separate networks to model each attribute <ref type="bibr" target="#b11">[12]</ref>, others share all features in a common backbone <ref type="bibr" target="#b12">[13]</ref>, and others combine feature maps from different parts of the encoder network <ref type="bibr" target="#b4">[5]</ref>. In our architecture, an encoder-decoder CNN, we carefully place each task to optimize the final performance. We locate head pose, a holistic task, at the end of the encoder. In this way the network bottleneck acts as embedding representing face pose. Visibility and alignment tasks are located at the decoder end, since they require information about the spatial location of landmarks in the image.</p><p>To train our model we leverage on the large face landmarks annotated data sets available. We first train the CNN for the landmarksbased face alignment task. Then we fine-tune it for head pose, face alignment and landmarks visibility. In the most asymmetric incarnation of our model, once trained, we may dispose of the decoder and the associated alignment and visibility tasks to produce a very efficient head pose estimation system. Alternatively, we may keep the full trained model and use the landmarks visibility and alignment outputs of the CNN as input to a novel face landmarks regression module based on an ensemble of regression trees. This model further improves the accuracy of landmarks location by imposing a valid face shape on the set of regressed landmarks. We evaluate our model for all three tasks using COFW, AFLW and AFLW2000-3D landmark-based data sets. In these experiments our model beats the top competing approaches in the head pose and visibility estimation tasks. It also achieves performance comparable with the state-of-the art for the landmark-based face alignment task. We also evaluate head pose estimation with Biwi. Although it was not acquired in-the-wild, this data set is a widely used marker-less benchmark for head pose estimation. Here our results also establish a new state-of-the-art.</p><p>In summary, we propose a multi-task approach for head pose estimation. The proposed solution combines a good model architecture, training strategy and a set of complementary tasks that boost final performance. The resulting model achieves top results for all three tasks, head pose, face alignment and visibility. In <ref type="figure">Fig. 1</ref> we display our predictions for some frames of a video from 300VW <ref type="bibr" target="#b13">[14]</ref>. It shows a remarkable tracker-like stability although each frame is processed independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The unique ability of neural networks to transfer and share knowledge among various tasks is one of the reasons for its present success. This is typically done using MTL techniques. In computer vision MTL has been widely used to simultaneously learn related tasks such as semantic segmentation and surface normal prediction <ref type="bibr" target="#b10">[11]</ref>. In the facial analysis field, head pose is often used as a pre-processing step to help estimate face landmarks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Other approaches simultaneously estimate head pose with facial landmarks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, Facial Action Units <ref type="bibr" target="#b17">[18]</ref>, gender <ref type="bibr" target="#b4">[5]</ref> and various other facial attributes <ref type="bibr" target="#b18">[19]</ref>. Alternatively, facial attributes estimation have also been combined with landmark detection <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In our approach we follow an asymmetric MTL scheme where the primary task is head pose ?2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. <ref type="figure">Fig. 1</ref>: Simultaneous head pose estimation, facial landmark location and their visibility predictions when processing a video from 300VW <ref type="bibr" target="#b13">[14]</ref>. Green and red points show visible and non-visible landmarks respectively. The co-ordinate system qualitatively represents head pose. estimation and use face landmarks as an auxiliary task that regularize and improve the performance of the primary task.</p><p>Pre-training a deep model with a large and general data set such as ImageNet has been a common practice for multiple vision tasks <ref type="bibr" target="#b7">[8]</ref>. In the context of face analysis, ImageNet <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and other large face-related data sets, such those for face recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, have also been extensively used for predicting various facial attributes. More recently, selfsupervised tasks have also emerged as powerful unsupervised pretraining mechanisms <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. For estimating head pose, pre-training with an unsupervised face alignment task yields better results than using a large supervised face recognition data set <ref type="bibr" target="#b28">[29]</ref>. This is possibly due to the geometrical cues learned in the alignment process. Following the same reasoning, we hypothesize that face landmark estimation is related to head pose. So, pre-training with the former task may improve the performance of the latter. Moreover, there is a lack of annotated "in-the-wild" head pose data sets. With our approach we leverage on the abundance of in-the-wild landmarkannotated data to train our model. As we show in the experiments, pretraining with a facial landmark estimation task improves head pose accuracy, beating other ImageNet pre-trained competing models <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In a MTL strategy the final results depend on the affinity or degree of co-operation among the tasks involved <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In extreme situations negative transfer may actually hinder the final performance <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Many approaches that simultaneously estimate head pose with other facial attributes, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, combine various competing tasks in the same network layer. In our experiments we show that head pose does not co-operate with landmark-related tasks when placed in the same layer. To address this issue we propose to use an encoderdecoder CNN and locate head pose, a holistic task, at the encoder end, that represents global face information. We place landmark-related tasks at the decoder end, where spatial information is represented at the finest detail (see <ref type="figure" target="#fig_0">Fig.2</ref>).</p><p>The best head pose estimation algorithms address the problem from a single task perspective. In the simplest case they fine-tune a backbone previously trained on ImageNet <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. QuatNet and GLDL are respectively the state-of-the-art in AFLW and AFLW2000-3D. They use standard CNN-based models pre-trained in ImageNet. QuatNet combines ordinal and L2 regression losses representing head pose angles with quaternions <ref type="bibr" target="#b22">[23]</ref>. GLDL learns a Gaussian distribution per co-ordinate using a Gaussian Labels Distribution Loss (GLDL) <ref type="bibr" target="#b23">[24]</ref>. FDN and FSA-Net are the top performers in the Biwi data set. Both approaches stand on specifically taylored network architectures. FDN uses a three-branch network with a feature decoupling module to explicitly learn discriminative features for each pose angle <ref type="bibr" target="#b24">[25]</ref>. FSA-Net combines spatially grouped pixel-level features of activation maps from different layers <ref type="bibr" target="#b34">[35]</ref>. A recent alternative achieves state-of-the-art results on Biwi training with synthetically generated data <ref type="bibr" target="#b35">[36]</ref>. To this end it introduces an adversarial domain adaptation approach for partially shared and continuous label spaces.</p><p>We leverage on the ideas discussed above to build a top performing head pose estimation algorithm. Our architecture is an standard encoder-decoder CNN with residual blocks and lateral skip connections. The key element of our proposal is a MTL scheme that combines a set of complementary tasks strategically located in the architecture. With our approach we improve not only the prediction accuracy, but also the computational and data efficiency, compared to training different models with data sets for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK HEAD POSE ESTIMATION</head><p>In this section we present our two-stage framework termed MNN+OR. First, we describe a novel Multi-task CNN (MNN) that estimates head pose, landmark heatmaps and their visibilities (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Second, we introduce an Occlusion-aware Regressor (OR) that we use to regress the location of facial landmarks (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task Neural Network (MNN)</head><p>The most successful CNN architectures for facial landmark detection use an encoder-decoder network with lateral connections such as U-Net <ref type="bibr" target="#b36">[37]</ref> and RCN <ref type="bibr" target="#b37">[38]</ref>. Both capture local and global features at different scales. The popular Hourglass architecture <ref type="bibr" target="#b38">[39]</ref> has a similar topology with extra convolutional layers in the lateral connections.</p><p>In this section, we introduce an architecture termed Multi-task Neural Network (MNN) based on a U-Net encoder-decoder with bottleneck residual blocks <ref type="bibr" target="#b39">[40]</ref> instead of its original convolutional layers. The residual block lets us reduce the number of operations and increase depth while preserving the gradient back propagation through. We also include lateral skip connections that link symmetric layers between the encoder and the decoder preserving the spatial information (see the Supplementary Material).</p><p>MNN is a symmetric encoder-decoder architecture each with 9 stages. The encoder reduces the spatial extent of the input face image from 256 ? 256 to 1 ? 1 pixels. In the depth dimension we increase the number of feature maps from 64 in the first layer up to 256 in the bottleneck. We also include BatchNormalization and ReLu after each convolutional layer.</p><p>We encourage the encoder to act as feature embedding that learns a holistic face representation, favouring the exchange of information among all tasks. We attach to this layer two losses related to head pose estimation. The decoder learns local features tailored to the estimation of non-rigid landmark locations and their visibilities. Henceforth, we describe these losses and tasks.</p><p>Holistic tasks. The location of the loss functions associated to our tasks is essential given that the feature maps in different layers of the CNN represent the image information at different levels of abstraction and aggregation.</p><p>Since the head pose is a global attribute, we compute it from the bottleneck layer at the encoder end. Our objective here is to estimate the six parameters of the rigid transformation, p ? R 6 , representing the relative pose between the head and the camera. To this end, we include two fully connected layers, p ED and p AE , with 6 outputs each at the end of the encoder (see <ref type="figure" target="#fig_0">Fig. 2</ref>). We optimize these layers with two loss functions,</p><formula xml:id="formula_0">LED(p ED ) = N i=1 ||p i ? p ED i ||2,<label>(1)</label></formula><formula xml:id="formula_1">LAE(p AE ) = N i=1 L l=1 w l i ||wi||1 ? ||x l i ? ?(p AE i , X l )||2 ,<label>(2)</label></formula><p>where N denotes the number of images, L the number of landmarks, p ED i and p AE i the predicted pose parameters for the i-th training image using each loss,p i the ground truth head pose parameters for the i-th training image,x l i ? R L?2 the l-th landmark ground truth coordinates for the i-th training image, X l ? R L?3 the 3D co-ordinates of the l-th landmark, and ? the camera projection.</p><p>Each loss plays an important role in our model. On the one hand, LED(p ED ) directly minimizes the euclidean error of pose parameters and provides an accurate and unambiguous pose estimation, p ED . On the other hand, LAE(p AE ) measures the alignment error produced by the rigid projection of the mean 3D face model, xi = ?(p AE i , X). The latter provides a better landmark initialization for the OR stage. However, the pose estimated, p AE , has projection ambiguity and estimation error caused by X not being the actual 3D landmark location, but that of the mean face. The combination of both losses provides unambiguous and accurate pose regression, as well as accurate rigid landmark localization.</p><p>Position-dependent tasks. Facial landmarks detection and their visibility estimation require both global and abstract features with a fine spatial resolution. Therefore, we use the feature maps at the end of the MNN decoder to estimate these attributes (see <ref type="figure" target="#fig_0">Fig. 2</ref>). For the landmark location task we introduce a convolutional layer producing [256?256?L] feature maps and a softmax activation layer to generate heatmaps, such that 256?256 p h(p) = 1. For the visibility task, we add a pooling layer with kernel size 256 ? 256 to generate the vector of L visibilities associated to our landmarks, v. To train this model we use the cross-entropy loss, where N is the number of images, L the number of landmarks,h l i , h l i the l-th ground truth and predicted heatmaps for the i-th training image, and? l i , v l i the l-th ground truth and predicted visibilities for the i-th training image.</p><formula xml:id="formula_2">LCE(h) = N i=1 L l=1 w l i ||wi||1 256?256 p=1 ?h l i (p) ? log(h l i (p)) ,<label>(3)</label></formula><formula xml:id="formula_3">LCE(v) = N i=1 L l=1 w l i ||wi||1 2 p=1 ?? l i (p) ? log(v l i (p)) ,<label>(4)</label></formula><p>To handle unlabelled landmarks we includew l , a landmark mask indicator variable (w l i = 1 when the l-th landmark is annotated, and w l i = 0 otherwise). This loss also enables data augmentation with large rotations, translations and scales, labelling landmarks falling outside of the bounding box as missing (w l i = 0). Multi-task loss. The loss function L(p ED , p AE , h, v) computes a global error obtained from the pose parameters p ED , p AE , the landmark heatmaps, h, and the visibilities, v, by combining them using a weighted sum of the losses,</p><formula xml:id="formula_4">L(p ED , p AE , h, v) = ?p 1 LED(p ED ) + ?p 2 LAE(p AE ) + ? h LCE(h) + ?vLCE(v).<label>(5)</label></formula><p>We empirically tune the weights ?p 1 , ?p 2 , ? h and ?v to balance the importance of all tasks. To this end, we train each task individually and determine the relative loss magnitudes when the learning process converges and ponder them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Occlusion-aware Regressor (OR)</head><p>To achieve top results in the facial landmarks detection task we use an Ensemble of Regression Trees (ERT) that regularizes the MNN result by enforcing it to be a valid face shape <ref type="bibr" target="#b40">[41]</ref>. To this end, we introduce an Occlusion-aware Regressor (OR). It is different from other landmark regressors in the literature <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> in that our approach leverages on the robust landmark location and visibility estimation available at the MNN decoder to regress the landmark coordinates with top accuracy.</p><p>OR initialization. We use the head pose estimated by the MNN (see Section 3.1) to project AFLW mean 3D face model onto the image using x 0 i = ?(p AE i , X), a L ? 2 matrix (see <ref type="figure" target="#fig_1">Fig. 3</ref>). This provides the OR with an initial estimation of the scale, and position of the target face shape. With this initialization we ensure that x 0 i is a valid face shape. This guarantees that the predictions in the next step of the algorithm, using an ERT, will also be valid face shapes <ref type="bibr" target="#b40">[41]</ref>. Here, we also initialize the visibilities according to the head pose (i.e., self-occlusions due to extreme head pose orientations) and the MNN prediction (i.e., occlusions), instead of regressing the visibility in the ERT cascade like <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>Non-rigid face shape deformation. Since the OR is initialized with the rigid face shape in the correct pose (see <ref type="figure" target="#fig_1">Fig. 3</ref>), to align the face it only needs to estimate the remaining non-rigid deformation of the face. To handle occlusions we incorporate the visibility labels for each i-th training image, {vi} N i=1 , estimated by the the MNN. The initial shape is progressively refined in the cascade in S stages by extracting shape indexed features on the heatmaps {?(hi, vi,</p><formula xml:id="formula_5">x s?1 i )} N i=1</formula><p>following a coarse-to-fine procedure like <ref type="bibr" target="#b44">[45]</ref>, where x s?1 i represents the shape of the i-th sample on the previous stage. The novelty of OR is that, the 2D displacements estimated by trees whose associated landmark is occluded are not added to the final estimation (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To evaluate our approach we perform experiments using four inthe-wild landmark-related data bases and one head pose data set acquired in laboratory conditions. COFW <ref type="bibr" target="#b41">[42]</ref> focuses on occlusions. It provides 1345 faces annotated with the positions and the binary occlusion labels for 29 landmarks. On average 28% of the landmarks are occluded. AFLW <ref type="bibr" target="#b45">[46]</ref> provides a collection of 25993 faces, with 21 facial landmarks annotated depending on their visibility. For our experiments we discard some images with reported annotation errors <ref type="bibr" target="#b46">[47]</ref>. We divide AFLW test subset into intervals of [0  <ref type="bibr" target="#b47">[48]</ref>. This last data set provides 61225 synthesized face images from 300W <ref type="bibr" target="#b48">[49]</ref>, also re-annotated with 68 3D landmarks using the same algorithm. The semi-automatic pipeline used to label 300W-LP and AFLW2000-3D has been criticised for not producing accurate annotations for extreme poses and occluded faces <ref type="bibr" target="#b49">[50]</ref>. For this reason we only use 300W-LP/AFLW2000-3D for comparing with the state-of-the-art that follows this protocol.</p><p>Although it was not acquired in-the-wild, we also evaluate our model with Biwi-Kinect <ref type="bibr" target="#b50">[51]</ref>. It contains 15677 images from 24 sequences of 20 subjects acquired in a controlled environment with a Kinect sensor. Since Biwi does not contain landmark annotations, we follow the protocol presented in <ref type="bibr" target="#b21">[22]</ref> using 300W-LP as train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>We use the Mean Absolute Error (MAE) metric to quantify the head pose estimation error,</p><formula xml:id="formula_6">MAE = 1 N N i=1 (|p i ? p i |) ,<label>(6)</label></formula><p>where N is the number of face images andp i , p i represent the ground truth and predicted pose parameters respectively. We also use the Normalized Mean Error (NME) as a metric to measure the shape estimation error</p><formula xml:id="formula_7">NME = 100 N N i=1 L l=1 w l i ||wi||1 ? ||x l i ?x l i ||2 di ,<label>(7)</label></formula><p>wherexi and xi are respectively the ground truth and estimated shape for the i-th training image and di is a normalization value. We use different values of di: the distance between eye pupils (pupils) and the bounding box height (height). Finally, we report recall percentage at 80% precision to compare landmarks visibility prediction with other published methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We train our models using Adam with an initial learning rate ? = 10 ?3 , which is halved whenever the loss plateaus for 15 epochs. We shuffle each training set and split it into 90% train and 10% validation. We also augment our training data by applying to each sample the following random operations: in plane rotation between ?45 ? , scaling by ?15%, translation by ?5% of the bounding box size, mirroring face image horizontally and colour change multiplying each HSV channel by a random value between [0.5, 1.5]. Additionally, we include synthetic rectangular occlusions to enforce the encoderdecoder to learn visibility. When provided we crop faces using the data set bounding box annotations. In 300W-LP/AFLW2000-3D and Biwi we use respectively the rectangle enclosing the annotated landmarks and the thresholded depth image. These detections are enlarged by 30% and resized to 256?256 pixels. In the landmark-annotated data sets we use POSIT <ref type="bibr" target="#b51">[52]</ref> with a set of 2D (image) and 3D (face model) landmark correspondences to compute the head pose. We use as model the mean 3D face shape provided with AFLW <ref type="bibr" target="#b45">[46]</ref>.</p><p>At runtime our implementation of MNN+OR processes test images on average at a rate of 12.8 FPS using a NVidia GeForce GTX 1080Ti (11GB) GPU and a dual Intel Xeon Silver 4114 CPU at 2.20GHz (2?10 cores/40 threads, 128 GB), where the MNN takes 66 ms and the OR 12 ms per face using C++, Tensorflow and OpenCV libraries. We may also dispose of the MNN decoder and the OR regressor to build a very efficient head pose estimation module. The resulting model infers head pose using the GPU at a rate of 62.5 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>In this section, we analyze the contribution of each component in our framework in the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Task location</head><p>In the first experiment we evaluate the importance of locating the head pose losses at the MNN bottleneck. To this end we adopt a MTL strategy pre-training the model with the landmark location task. The green and blue curves in <ref type="figure" target="#fig_2">Fig. 4</ref> show respectively the loss achieved when locating both rigid pose losses, LED and LAE, at the end of the decoder (L = 18.6) and at the end of the encoder (L = 7.9). In the second case we achieve a reduction of 57.5% in the final loss. We infer that this gain is caused by two reasons. First, the superiority of the holistic features extracted from the embedding in the encoder-decoder bottleneck. Second, because head pose and landmark-related tasks do not co-operate when located in the same layer. Hard parameter sharing among these tasks decreases the final performance. From now on, we attach the rigid head estimation losses, LED and LAE, at the end of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Training strategy</head><p>For these experiments, we incorporate two 2D landmark-based inthe-wild data sets. 300W <ref type="bibr" target="#b48">[49]</ref> provides 68 manually annotated facial landmarks. We followed the most established approach and divide the annotations into 3148 training and 689 testing images (public competition). Thereafter, we also perform experiments on the 300W private benchmark, using previous 3837 images for training and 600 newly updated images as testing set. WFLW <ref type="bibr" target="#b52">[53]</ref> consists of 7500 extremely challenging training and 2500 testing images divided into six subgroups, pose, expression, illumination, make-up, occlusion and blur, with 98 fully manual annotated landmarks. Since these data sets do not provide any head pose label, we compute it using POSIT <ref type="bibr" target="#b51">[52]</ref> with AFLW <ref type="bibr" target="#b45">[46]</ref> mean 3D face shape.</p><p>Here we evaluate our model under different training strategies. In the simplest case we follow a single task approach and minimize LED(p ED ) in Eq. (1) (Pose row in <ref type="table" target="#tab_2">Table 1</ref>). We also consider several symmetric and asymmetric MTL schemes. In the symmetric case we train our model from scratch with all three tasks, minimizing L(p ED , p AE , h, v) in (5) (Sym row in <ref type="table" target="#tab_2">Table 1</ref>, orange stroke in <ref type="figure" target="#fig_2">Fig. 4</ref>). We also look at an asymmetric MTL scheme in which we pre-train with the image alignment task, optimizing LCE(h) in Eq. <ref type="bibr" target="#b2">(3)</ref>. Once this training converges, we include the head pose and visibility tasks and optimize L(p ED , p AE , h, v) (Pre+Sym row in <ref type="table" target="#tab_2">Table 1</ref>, blue stroke in <ref type="figure" target="#fig_2">Fig. 4</ref>). Finally, in the most asymmetric MTL situation, we pre-train with the image alignment task, optimizing LCE(h). Upon convergence, we then only optimize the head pose task, LED(p ED ) (Pre+Pose row in <ref type="table" target="#tab_2">Table 1</ref>).</p><p>The orange and blue curves in <ref type="figure" target="#fig_2">Fig. 4</ref> respectively display the difference between using the symmetric MTL training scheme (L = 8.7) against the asymmetric MTL that pre-trains with the landmarks task followed by a symmetric MTL with all three tasks (L = 7.9). In our problem pre-training regularizes the learning process and achieves a 9% reduction in the final loss, L.</p><p>Further, in <ref type="table" target="#tab_2">Table 1</ref> we show head pose estimation results for different landmark-based data sets and training strategies. On average we achieve the largest improvement in mean MAE when changing from single task learning (first row) to MTL (three bottom rows). In the worst case, when moving from single task to the symmetric MTL case, we achieve a 7.5% reduction in mean MAE. The asymmetric approaches, that involve a pre-training step with the landmark face alignment task, achieve the best results, with a reduction of 11.9% in the average mean MAE with respect to the single task approach. There is no difference whether after pre-training we refine the model only for the pose task or for all three tasks. Hence, the second model will be the selected configuration and training strategy in our experiments.  We also evaluate the importance of the MTL scheme for visibility estimation using the COFW data set. Fist, we train MNN only for the visibility task. In this case, we achieve a recall of 21.75% at 80% precision for occlusion detection. This is a poor result, far worse than most published results (see <ref type="table" target="#tab_5">Table 3</ref>), possibly caused by the small size of the training data set. However, using our selected MTL strategy, we get a recall of 72.12%, a large improvement in recall at the typical 80% precision point. So, with a small training data set, such as COFW, the combination of multiple related tasks within a MTL scheme boosts the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Occlusion-aware regressor</head><p>Here we evaluate the contribution of the OR stage to the performance of the landmark location task. We report the NME of, 1) MNN alone, locating each landmark by the maximum response on its heatmap; 2) the full MNN+OR framework. We show in Tables 4, 5 and 6 the performances in COFW, AFLW and AFLW2000-3D data sets. These results prove the importance of the OR stage to regularize the MNN landmark predictions. Model MNN+OR reduces the NME of model MNN in COFW by 10.8%, in AFLW by 3% and in AFLW2000-3D by 6.9%. The improvement grows proportionally with the presence of self-occluded parts (i.e., AFLW2000-3D) and non-visible landmarks (i.e., COFW) in the data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>In this section we evaluate our model in the most challenging benchmarks for all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Head pose</head><p>In <ref type="table" target="#tab_4">Table 2</ref> we compare our head pose estimation proposal with the best published results in the literature. We train two MNN models. For AFLW there is no standard protocol to determine the training and testing partitions. We use the benchmark proposed in <ref type="bibr" target="#b20">[21]</ref>. For testing in AFLW2000-3D and Biwi we train our model with 300W-LP, like <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref>. However, we use the pose estimated from the correct 300W-LP landmarks from <ref type="bibr" target="#b49">[50]</ref>.</p><p>We outperform the state-of-the-art in AFLW (3.22 MAE), which represents an 11% mean MAE reduction over QuatNet <ref type="bibr" target="#b22">[23]</ref>, the best reported result in the literature. Moreover, in our MTL strategy we only use AFLW annotations, whereas QuatNet and many competing approaches use additional training data <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In the experiment evaluated on AFLW2000-3D and Biwi our approach establishes two new top results, again, with no extra training data. While in Biwi we reduce in 6.9% FDN's <ref type="bibr" target="#b24">[25]</ref> MAE, in AFLW2000-3D our result only improves by 2.3% GLDL's <ref type="bibr" target="#b23">[24]</ref>. This is caused by the inaccurate AFLW2000-3D annotations in extreme head poses <ref type="bibr" target="#b49">[50]</ref>. While our approach was trained with poses estimated from the corrected landmarks, our competitors were trained on the original 300W-LP annotations, poisoned with the same errors. We re-annotated AFLW2000-3D with the poses estimated from the correct landmarks. We denote this data set AFLW2000-3D-POSIT. In this case the mean MAE of our approach goes down to 1.71.</p><p>The results in <ref type="table" target="#tab_4">Table 2</ref> must be considered with caution. First, it seems obvious that landmark detection and head pose estimation tasks are clearly more connected if the pose is calculated from the landmarks. Moreover, the MAEs of AFLW and AFLW2000-3D-POSIT have a negative (optimist) bias. This is because in them the head pose in the train and test sets is computed with the same semi-automatic estimation procedure. The same argument applies to all our competitors in AFLW2000-3D. However, our results would be positively (pessimistically) biased in this data set, since some of its annotations are not correct. Hence, our unbiased MAE for AFLW2000-3D would be between the 3.83 and 1.71 bounds. In contrast, the experiment with Biwi involves different train/test data sets and annotation procedures. Hence, it provides the most accurate MAE estimations. Although, in a data set taken in laboratory conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Facial landmarks visibility</head><p>In <ref type="table" target="#tab_5">Table 3</ref> we compare the landmarks visibility estimation that we obtain with MNN, against the best published results in the literature.</p><p>To evaluate this task we use COFW <ref type="bibr" target="#b41">[42]</ref>, as far as we known, the only data set with annotated occlusions.</p><p>With our approach, we get a 72.12% recall at 80% precision, a new state-of-the-art for this data set. The notable improvement with respect to the closest competitor, 3DDE <ref type="bibr" target="#b44">[45]</ref>, is caused by two key differences. First, the MTL strategy boost the performance of the landmark visibility task. Second, in 3DDE the visibility is estimated by the landmark ERT regressor, like in <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, whereas in the Method AFLW AFLW2000-3D AFLW2000-3D-POSIT Biwi yaw pitch roll mean yaw pitch roll mean yaw pitch roll mean yaw pitch roll mean FAb-Net <ref type="bibr" target="#b28">[29]</ref> 10.70 7.13 5.   proposed approach the visibility is estimated in the MNN model. This result proves again the relevance of our MTL approch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Facial landmark location</head><p>We compare the MNN+OR framework with the state-of-the-art in face landmark regression. To this end we use results reported for 2D and 3D face alignment data sets. We use COFW and AFLW to provide a reference comparison with data sets involving 2D landmarks and AFLW2000-3D for 3D landmarks. We analyze in <ref type="table" target="#tab_7">Table 4</ref> the MNN+OR landmark location performance in COFW, the common benchmark to evaluate occlusions. Here, we achieve a performance comparable to the best reported result for this data set, CHR2C <ref type="bibr" target="#b56">[57]</ref>, based on two stacked U-Netlike models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Full pupils RCPR <ref type="bibr" target="#b41">[42]</ref> 8.50 TCDCN <ref type="bibr" target="#b19">[20]</ref> 8.05 Wu et al. <ref type="bibr" target="#b42">[43]</ref> 6.40 Wu et al. <ref type="bibr" target="#b54">[55]</ref> 5.93 ECT <ref type="bibr" target="#b55">[56]</ref> 5.98 PCD-CNN <ref type="bibr" target="#b16">[17]</ref> 5.77 SHN <ref type="bibr" target="#b38">[39]</ref> 5.  In <ref type="table" target="#tab_9">Table 5</ref> we compare MNN+OR with previous literature using AFLW images. This is a challenging database due to the large number of faces with extreme poses and occluded landmarks, which are not annotated. In this case, again, we achieve a performance comparable to the best reported result in the literature.</p><p>Finally, in <ref type="table" target="#tab_10">Table 6</ref>, we also evaluate our model using a 3D data set. To this end we train our model with 300W-LP and test in AFLW2000-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper we have presented a supervised multi-task approach to head pose, facial landmark location and visibility estimation. It is based on a heatmap encoder-decoder CNN, MNN, followed by an ensemble of regression trees to estimate the landmark co-ordinates. Rather than using head pose as a by-product or auxiliary task for landmark estimation, in our approach landmark-related tasks are used to boost head pose estimation. However, they are only required at training time. During testing we may dispose of the decoder and landmark regression modules to produce an extremely efficient head pose regressor with the best reported accuracy in the literature. In our head pose estimation experiments with landmark-based data sets we improve the best reported result in AFLW, QuatNet <ref type="bibr" target="#b22">[23]</ref>, and GLDL <ref type="bibr" target="#b23">[24]</ref>, the top performer in AFLW2000-3D. We also establish a new state-of-the-art in Biwi, a data set acquired in laboratory conditions and accurately annotated from depth images. The MNN model and the MTL training strategy are fundamental to achieve top performance with our framework. In the ablation analysis we show that we get the largest improvement when switching from single task to a multi-task approach. We can further improve the performance if we adopt an asymmetric MTL scheme and pre-train the MNN with the face landmark estimation task. This confirms previous results showing that pre-training a model with a hard problem significantly improves the performance of other related tasks <ref type="bibr" target="#b63">[64]</ref>. Also, our ablation shows that hard parameter sharing between head pose and face landmark estimation is detrimental of the final performance. This also confirms that multi-task and transfer (pre-training) relationships are different <ref type="bibr" target="#b32">[33]</ref>. To address this issue and to provide each task with the appearance information aggregated at the best level of abstraction, we hook up the head pose loss to the encoder end, whereas the losses of spatially related tasks, such as landmark location and visibility, are attached to the decoder end.</p><p>Our model also reaches top performance in the two landmark related tasks. In visibility estimation it achieves 72.12% recall at 80% precision in COFW. A 13% improvement over the previous reported state-of-the-art, 3DDE <ref type="bibr" target="#b44">[45]</ref>. We also compute the location of face landmarks using a novel occlusion-aware regressor (OR), that estimates face deformation from the heatmaps of visible landmarks. The full MNN+OR achieves results comparable to the state-of-the-art, 3DDE and CHR2C <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b56">[57]</ref>, when evaluated in AFLW and COFW. In AFLW2000-3D, where self-occlusions play a key role, it sets a reduction of 16% over the previous state-of-the-art, MHM <ref type="bibr" target="#b60">[61]</ref>.</p><p>A fundamental problem to build a head pose estimation algorithm is the lack of training data. We propose a MTL strategy that takes advantage of the data bases available with manually labelled face landmarks. Pre-training with large object or face recognition data sets are alternative popular means to address this issue. We have proved that in the context of head pose estimation our proposal beats this strategy. This is due to the better co-operation between head pose and face landmark tasks. To further increase the robustness and accuracy of head pose estimation, our approach may be combined with selfsupervised training <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and the use of synthetically generated data sets <ref type="bibr" target="#b35">[36]</ref>.</p><p>It is difficult to establish an state-of-the-art for head pose estimation in-the-wild, due to the lack of accurately annotated data sets. Present in-the-wild head pose evaluation methodologies are based on landmark data bases, such as AFLW and 300W-LP/AFLW2000-3D. The semi-automatic pipeline used to process them introduces errors in the train and test set annotations that bias the evaluation. Using reannotated 300W-LP/AFLW2000-3D head poses we were able to upper and lower bound the performance of our approach in this situation. Biwi's evaluation methodology, based on train/test data sets acquired with different technologies and annotation algorithms, provides a more realistic performance estimation in laboratory conditions. The proposed approach not only provides a satisfactory prediction accuracy but also a good computational efficiency. Instead of evaluating three different models, one for each task, we use a single encoderdecoder CNN, with an extremely efficient ERT, to simultaneously solve all three tasks at a rate of 12.8 FPS. However, if we are only interested in estimating head pose as a preliminary face processing step <ref type="bibr" target="#b2">[3]</ref>, our encoder-only model achieves 62.5 FPS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Multi-task encoder-decoder for the estimation of head pose, LED(p ED ), rigid and deformable facial landmarks location, LAE(p AE ) and LCE(h), and their visibilities, LCE(v). We locate the head pose and rigid landmarks estimation tasks at the bottleneck layer, and the non-rigid face deformation and visibilities at the decoder end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The OR is initialized with the 3D face model projected landmarks, x 0 , and their visibilities, v. It incrementally updates the landmark location discarding the predictions of those regression trees whose features are extracted around occluded landmarks, shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Blue, orange and green colored learning curves compare the overall validation loss, L, obtained with MNN by fine-tuning from landmarks, training from scratch, and locating the rigid pose losses at the end of the decoder respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, 30 ? ], [30 ? , 60 ? ] and [60 ? , 90 ? ] according to head absolute yaw angle. AFLW2000-3D [48] consists of 2000 faces from AFLW semiautomatically re-annotated with 68 3D facial landmarks. We divide it into intervals of [0 ? , 30 ? ], [30 ? , 60 ? ] and [60 ? , 90 ? ]. Each interval consists of 1306, 462 and 232 faces respectively. It has been typically used for testing head pose and facial landmark location algorithms using 300W-LP as train set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Head pose mean MAEs for different training strategies.</figDesc><table><row><cell>First row (Pose) single task encoder. Second row (Sym) symmetric</cell></row><row><cell>MTL for all three tasks. Third row (Pre+Sym) MTL learning scheme</cell></row><row><cell>pre-training with face landmarks. Fourth row (Pre+Pose) asymmetric</cell></row><row><cell>MTL scheme pre-training with landmarks fine-tuned with pose.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Head pose MAEs for AFLW, AFLW2000-3D and Biwi. AFLW200-3D-POSIT is the outcome of re-annotating AFLW2000-3D with the corrected landmarks annotations from<ref type="bibr" target="#b49">[50]</ref>.</figDesc><table><row><cell>Method</cell><cell>Full occlusion</cell></row><row><cell>RCPR [42]</cell><cell>40</cell></row><row><cell>Wu et al. [43]</cell><cell>44.43</cell></row><row><cell>Wu et al. [55]</cell><cell>49.11</cell></row><row><cell>ECT [56]</cell><cell>63.4</cell></row><row><cell>3DDE [45]</cell><cell>63.89</cell></row><row><cell>MNN</cell><cell>72.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Recall of landmarks visibility estimation methods at 80% precision using COFW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Face alignment NME using COFW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc>Face alignment NME using AFLW.3D. In this case, we achieve 2.58 NME in the Full set. This result sets the new state-of-the-art for this data set, with a 16.2% reduction in NME with respect to the best published result in the literature, MHM<ref type="bibr" target="#b60">[61]</ref> (3.08 NME), based on a two-stage cascade of heatmap regressors. Even without the final OR regressor, the MNN model alone already improves in 10% the previous best result. Our two-stage hybrid strategy is specially effective in 3D face alignment, where the OR stage is initialized using the extremely accurate head pose estimated by the MNN (seeFig. 3).</figDesc><table><row><cell>Method</cell><cell cols="3">[0 ? , 30 ? ] [30 ? , 60 ? ] [60 ? , 90 ? ] height height height</cell><cell>Full height</cell></row><row><cell>RCPR [42]</cell><cell>4.26</cell><cell>5.96</cell><cell>13.18</cell><cell>7.80</cell></row><row><cell>3DSTN [62]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell></row><row><cell>3DDFA [48]</cell><cell>2.84</cell><cell>3.57</cell><cell>4.96</cell><cell>3.79</cell></row><row><cell>PRN [63]</cell><cell>2.75</cell><cell>3.51</cell><cell>4.61</cell><cell>3.62</cell></row><row><cell>Binary-CNN [60]</cell><cell>2.47</cell><cell>3.01</cell><cell>4.31</cell><cell>3.26</cell></row><row><cell>MHM [61]</cell><cell>2.36</cell><cell>2.80</cell><cell>4.08</cell><cell>3.08</cell></row><row><cell>MNN</cell><cell>2.71</cell><cell>2.53</cell><cell>3.48</cell><cell>2.77</cell></row><row><cell>MNN+OR</cell><cell>2.54</cell><cell>2.24</cell><cell>3.34</cell><cell>2.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 :</head><label>6</label><figDesc>Face alignment NME using AFLW2000-3D.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors acknowledge funding from the Spanish Ministry of Economy and Competitiveness, project TIN2016-75982-C2-2-R. Jos? M. Buenaposada was also partially funded by the Comunidad de Madrid project RoboCity2030-DIH-CM (S2018/NMT-4331). They also thank the anonymous reviewers for their comments and Felix Kuhnke for his help in interpreting Biwi annotations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysing driver&apos;s attention level using computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nuevo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International IEEE Conference on Intelligent Transportation Systems, ITSC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiperson visual focus of attention from head pose and meeting contextual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faceposenet: Making a case for landmark-free face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision Workshops</title>
		<meeting>International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1599" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">KEPLER: simultaneous estimation of keypoints and 3D pose of unconstrained faces in a unified framework by learning efficient H-CNN regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial landmark detection: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Asymmetric multi-task learning based on task relatedness and loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5454" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep, landmark-free FAME: Face alignment, modeling, and expression estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="930" to="956" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision Workshops</title>
		<meeting>International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2578" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face alignment assisted by head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="130" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose-independent facial action unit intensity regression based on multi-task deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="872" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Benchmarking head pose estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Iberoamerican Congress on Pattern Recognition</title>
		<meeting>Iberoamerican Congress on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quatnet: Quaternionbased head pose estimation with multi-regression loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facial pose estimation by deep learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision Workshops</title>
		<meeting>International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FDN: Feature decoupling network for head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face attribute prediction using off-theshelf CNN features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partially shared multi-task convolutional neural network with local constraint for face attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4290" to="4299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">302</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of landmarks by descriptor vector exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6360" to="6370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint learning of semantic alignment and object landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7293" to="7302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1905.07553</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FSA-Net: Learning finegrained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep head pose estimation using synthetic images and partial adversarial domain adaption for continuous label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5719" to="5728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="609" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face alignment using a 3D deeply-initialized ensemble of regression trees</title>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">102846</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automatic detection of ground-truth labeling error for AFLW</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<ptr target="http://people.cs.umass.edu/?souyoungjin/AFLWgterrordetection.htm" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3D total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="92" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">300 faces inthe-wild challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Random forests for real time 3D face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="458" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Model-based object pose in 25 lines of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Crosscascading regression for simultaneous head pose estimation and facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Combining data-driven and model-driven methods for robust facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2409" to="2422" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cascade of encoderdecoder CNNs with learned coordinates regressor for robust facial landmarks detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="326" to="332" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusion-adaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3486" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3726" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cascade multi-view hourglass model for robust 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Faster than realtime facial alignment: A 3D spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4000" to="4009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="557" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Transferability and hardness of supervised classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
