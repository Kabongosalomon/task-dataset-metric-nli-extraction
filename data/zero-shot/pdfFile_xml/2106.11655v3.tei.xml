<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DARTS-PRIME: Regularization and Scheduling Improve Constrained Optimization in Differentiable NAS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitlin</forename><surname>Maile</surname></persName>
							<email>kaitlin.maile@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IRIT</orgName>
								<orgName type="institution" key="instit2">University of Toulouse</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwan</forename><surname>Lecarpentier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IRIT</orgName>
								<orgName type="institution" key="instit2">University of Toulouse</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IRT Saint-Exupery</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Luga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IRIT</orgName>
								<orgName type="institution" key="instit2">University of Toulouse</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ISAE-SUPAERO</orgName>
								<orgName type="institution">University of Toulouse</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DARTS-PRIME: Regularization and Scheduling Improve Constrained Optimization in Differentiable NAS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable Architecture Search (DARTS) is a recent neural architecture search (NAS) method based on a differentiable relaxation. Due to its success, numerous variants analyzing and improving parts of the DARTS framework have recently been proposed. By considering the problem as a constrained bilevel optimization, we present and analyze DARTS-PRIME, a variant including improvements to architectural weight update scheduling and regularization towards discretization. We propose a dynamic schedule based on per-minibatch network information to make architecture updates more informed, as well as proximity regularization to promote well-separated discretization. Our results in multiple domains show that DARTS-PRIME improves both performance and reliability, comparable to state-of-the-art in differentiable NAS. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since their inception, neural networks have progressed from being completely hand-designed to being more and more automated, allowing for larger size, more complex tasks, and better performance. The introduction of back-propagation to optimize parameters within an architecture was a major step towards automation <ref type="bibr" target="#b20">(Rumelhart, Hinton, and Williams 1986)</ref>, but until recently, network architectures were still handdesigned, limiting innovation. Neural architecture search has emerged as the path towards automating the structural design of networks. Initial methods used evolutionary algorithms <ref type="bibr" target="#b19">(Real et al. 2019)</ref> or reinforcement learning <ref type="bibr" target="#b28">(Zoph and Le 2017)</ref> to search for networks from building blocks of operators. By relaxing the search space over operators to be continuous, the original Differentiable Architecture Search (DARTS) algorithm was the first to use gradient descent for searching across network architectures <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref>. This one-shot search builds a supernetwork with every possible operator at every possible edge connecting activation states within the network, using trainable structural weights across operators within an edge to determine which edges and operators will be used in the final architecture.</p><p>While DARTS was able to drastically reduce search time while finding high-performing networks, it suffers from sev-Copyright ? 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr">1</ref> Our code is available anonymously here: https://anonymous.4open.science/r/DARTS-PRIME eral drawbacks. Among them, contributions pointed out the lack of early search decisions <ref type="bibr" target="#b10">(Li et al. 2020;</ref><ref type="bibr" target="#b2">Chen et al. 2019;</ref><ref type="bibr" target="#b23">Wang et al. 2020)</ref>, failure to approach the final target constraint <ref type="bibr" target="#b4">(Chu et al. 2020b)</ref>, performance collapse due to skip-connections <ref type="bibr" target="#b3">(Chu et al. 2020a;</ref><ref type="bibr" target="#b27">Zhou et al. 2020;</ref><ref type="bibr" target="#b0">Arber Zela et al. 2020)</ref>, and inefficiency due to unconstrained search <ref type="bibr" target="#b24">(Yao et al. 2020)</ref>.</p><p>In this work, we posit that difficulties arise in DARTS due to a lack of informed schedule and regularization befitting a constrained bilevel optimization problem. We formulate our contributions in a new augmentation called DARTS-PRIME (Differentiable Architecture Search with Proximity Regularization and Fisher Informed Schedule). Specifically, DARTS-PRIME is a combination of two additions to DARTS:</p><p>1. a dynamic update schedule, yielding less frequent architecture updates with respect to weight updates, using gradient information to guide the update schedule, and 2. a novel proximity regularization towards the desired discretized architecture, which promotes clearer distinctions between operator and edge choices at the time of discretization.</p><p>We analyze these two additions together as DARTS-PRIME, as well as each independently in ablation studies, over standard benchmarks and search spaces with CIFAR-10, CIFAR-100, and Penn TreeBank. We find that these additions independently improve DARTS and combine as DARTS-PRIME to make significant accuracy improvements over the original DARTS method and more recently published variants across all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this section we offer an explanation of the DARTS algorithm formulated as a constrained bilevel optimization problem, focusing on the issues which arise due to this complex search. We also cover related variants of DARTS which propose various solutions to the difficulties arising from this constrained bilevel problem.</p><p>DARTS sets up a supergraph neural network that contains all possible options in all possible connections within a framework architecture derived from that of other NAS works <ref type="bibr" target="#b28">(Zoph and Le 2017;</ref><ref type="bibr" target="#b19">Real et al. 2019)</ref>. For image classification tasks, this supernetwork consists of repeated cells of two different types, normal and reduce. Each cell of the same type shares the same architecture. This formulation allows for a shallower network with fewer normal cells to be searched over and a deeper one to be used for evaluation, such that both networks fit maximally on a standard GPU. The structure within each cell is a fully-connected directed acyclic graph, receiving input from the two previous cells and using a fixed number of intermediate activation states that are concatenated to produce that cell's output. Each of these edges contains all operators, each of which is a small stack of layers such as a skip-connection or a nonlinearity followed by convolution followed by batchnorm. In order to match the search space to that of the preceding NAS works, the search objective is to find exactly two operators from unique sources, either one of the two previous cells or one of the previous activation states within the cell, to go to each activation state. DARTS accomplishes this by keeping the two strongest operators, as measured by trained continuous architecture weights, from unique sources for each state within the cell.</p><p>The DARTS search can be formalized as follows. Let ? ijp be the architecture weight for operator p from node j to node i, so ? ij represents the set of architecture weights for all operators from node j to node i and ? i represents the set of architecture weights for all operators from all nodes to node i. The discretization for the DARTS search space may be stated as the intersection of the following sets:</p><formula xml:id="formula_0">S = i S 1 i ? S 2 i ? S 3 i (1) S 1 i ={? i |?j card(? ij ) ? 1} (2) S 2 i ={? i |card(? i ) = 2} (3) S 3 i ={? i |? ijp ? {0, 1}},<label>(4)</label></formula><p>where S 1 i states that there is at most one active operator coming in to each intermediate state from any given source node, S 2 i states that there are exactly two total active operators coming in to each intermediate state, and S 3 i states that all ? ijp values are binary, so all operators are either active or inactive. Thus, S contains the encoding for all possible discretized architectures ? in the search space of DARTS.</p><p>During the DARTS search, S is relaxed so that ? is not constrained. A softmax operation is applied over continuous ? across operators in an edge as an activation. During a forward pass, the output of each edge is computed as the weighted sum of each operator on that edge multiplied by its activated architecture weight; then, the output of each state is computed as the sum of all incoming edges; finally, the output of the entire cell is computed as the channel-wise concatenation of each intermediate state. After searching for a set number of epochs, the softmaxed ? is projected onto S to derive the final architecture, as shown in <ref type="figure">Figure 1</ref>.</p><p>In order to train the search supernetwork with architecture weights ? and network parameters w, DARTS optimizes the following objective: Even with the relaxed space of ?, the remaining bilevel optimization problem cannot be directly optimized with standard techniques due to the gradient of the outer optimization (5a) depending on the gradient of the inner optimization (5b), so it is not computationally feasible. Instead, DARTS alternates <ref type="figure">Figure 2</ref>: Extended trials of DARTS and DARTS-PRIME on CIFAR-10. A single extended trial for each algorithm was run to 150 epochs, and the architecture checkpointed every 10 generations was evaluated once. In DARTS, the architecture eventually has only skip-connections for the searchable part of the architecture, resulting in a high-error performance. DARTS-PRIME avoids this collapse. a single step of gradient descent for ?, the outer variable, using either first-order or second-order approximation of the gradient with a single step of gradient descent for w, the inner variable. This naive schedule of alternating steps may not be optimal, even in practice: if the parameterized operators are not trained enough at the current architecture encoding for their parameters to be a reasonable approximation of an optima, their architecture weights will be discounted unfairly.</p><formula xml:id="formula_1">min ? L val (w * (?), ?) (5a) s.t. w * (?) = argmin w L train (w, ?), ? ? S.<label>(5b)</label></formula><p>Architectures with more skip-connections converge faster even though they tend to have a worse performance at convergence , so this naive schedule may unfairly and undesirably favor skip-connections.</p><p>Since the publication of DARTS (Liu, Simonyan, and Yang 2019), many variations have been proposed to improve aspects of the original algorithm. We focus on those that are more relevant to the issues we tackle with DARTS-PRIME and study the same DARTS search space in the following.</p><p>RobustDARTS <ref type="bibr" target="#b0">(Arber Zela et al. 2020)</ref> investigates failure modes of DARTS, including avoiding the collapse towards excessive skip-connections. Skip-connection mode collapse is a known issue with DARTS, and is demonstrated in <ref type="figure">Figure</ref> 2. The authors propose several regularizations of the inner optimization (5b): in particular, their proposed DARTS-ADA utilizes Hessian information to impose an adaptively increasing regularization, which bares similarities to both our proposed dynamic informed scheduling and increasing proximity regularization. However, this specific variant is not analyzed in the same standard DARTS search space. DARTS+ <ref type="bibr" target="#b11">(Liang et al. 2019)</ref> uses early stopping to avoid skip-connection mode collapse. DARTS- <ref type="bibr" target="#b3">(Chu et al. 2020a)</ref> avoids skip-connection domination by adding an auxiliary skip-connection in parallel to every edge in addition to the skip-connection operation already being included for each edge in the search. The auxiliary skip-connection is linearly decayed over training, similar to our linear schedule of regularization.</p><p>NASP <ref type="bibr" target="#b24">(Yao et al. 2020</ref>) tackles the constraint problem in Differentiable NAS with a modified proximal algorithm <ref type="bibr" target="#b17">(Parikh and Boyd 2014)</ref>. A partial discretization is applied before each ? step and each w step, so forward and backward passes only use and update the current strongest edges of the supernetwork. While this does improve the efficiency of the search without decreasing benchmark performance, it may lead to unselected operators lagging in training compared to selected ones and thus having an unfair disadvantage during search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Towards Constrained Optimization</head><p>Neural Architecture Search, particularly when encoded as in DARTS, is a constrained bilevel optimization problem. Both being constrained and being bilevel separately make the optimization less straightforward, such that standard techniques are less applicable. We propose new approaches to the dimensions of bilevel scheduling and regularization towards the constraints to tackle this combined difficult problem in a more dynamic and informed fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dynamic FIMT Scheduling</head><p>Fisher Information of a neural network is closely related to the Hessian matrix of the loss of the network and can be used as a measure of information contained in the weights with respect to the training data as well as a proxy for the steepness of the gradient landscape. While true Fisher Information is expensive to calculate in neural networks, the empirical Fisher Information matrix diagonal can be computed using gradient information already calculated for each batch <ref type="bibr" target="#b16">(Martens 2016)</ref>. This form of the Fisher Information has been used for updating the momentum term in the Adam optimizer (Kingma and Ba 2014), overcoming catastrophic forgetting <ref type="bibr" target="#b8">(Kirkpatrick et al. 2017)</ref>, and pruning parameters <ref type="bibr" target="#b22">(Tu et al. 2016;</ref><ref type="bibr" target="#b21">Theis et al. 2018)</ref>.</p><p>We compute the trace of the empirical Fisher Information matrix with each minibatch:</p><formula xml:id="formula_2">F (w) = tr ? w L train (w, ?)? w L train (w, ?) T<label>(6)</label></formula><p>Because the network is changing at each weight update and this measure is subject to variability between minibatches, we use an exponentially weighted moving average of the Fisher Information Matrix Trace (FIMT), similar to the decay factor in the Adam optimizer (Kingma and Ba 2014):</p><formula xml:id="formula_3">F n (w) = ?F (w) + (1 ? ?)F n?1 (w),<label>(7)</label></formula><p>for the nth minibatch, where ? is a hyperparameter. We use the FIMT as a moving estimate of information contained within the weights of the network and the curvature of the gradient landscape. In particular, as noted in DARTS, when the gradient of w is 0, then w is a local optimum and thus w * = w, as desired. As a local optima is approached, the FIMT should decrease and ? updates can be triggered more frequently. To achieve this effect, we utilize an exponentially adaptive threshold denoted by h, parametrized by an initial threshold h 0 , threshold increasing factor h i ? 1, expected step ratio r, and moving average weighting factor ?. A threshold decreasing factor h i ? 1 is computed such that the expected ratio between w and ? updates, given con-stantF n (w), is equal to r. The protocol of this schedule is detailed in 1. The adaptive threshold causes the ? update frequency to increase when the FIMT is generally decreasing over batches and decrease when the FIMT is generally increasing over batches. In turn, this adaptive threshold yields a schedule of ? and w updates that is more informed about the convergence of the inner optimization given in Equation 5b while maintaining efficiency. This avoids unfair discount of relevant architectures. An example of our dynamic FIMT schedule is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regularization</head><p>In DARTS, the final ? values can be far from the resultant discretization, leading to disparity between the search process and its outcome. In order to reward proximity to discretization without losing training signal to inactive portions of the supernetwork, we utilize a proximity regularization that increases in strength over the course of training. The outer objective now becomes</p><formula xml:id="formula_4">min ? L val (w * (?), ?) + c? p 2 (? ? ? S (?)) 2 ,<label>(8)</label></formula><p>where c increases linearly from 0 to 1 over the course of search training , ? p is a regularization hyperparameter, and ? S is the projection onto set S, in this case equivalent to deriving a discretized architecture. This formulation allows for more exploration of ? and training of w in parameterized operations particularly at the beginning of the search training, progressively increasing the regularization pressure towards discretization as training continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DARTS-PRIME</head><p>We combine dynamic FIMT scheduling and proximity regularization with DARTS as DARTS-PRIME. The procedure for DARTS-PRIME is given in Algorithm 1.</p><p>Algorithm 1: DARTS-PRIME Input: Initial threshold h0, threshold increasing factor hi, expected step ratio r, moving average weighting factor ?, regularization parameter ?p, and learning rates ?w, ?? Output:</p><formula xml:id="formula_5">Discrete architecture ?S(?) 1: Initialize ?, w; let h ? h0, h d ? exp(?r ln hi), t ? 1 2: while not converged do 3: G ? ?wLtrain(w, ?) 4: w ? w ? ?wG 5:Fn(w) ? ? tr GG T + (1 ? ?)Fn?1(w) 6: ifFn(w) &lt; h then 7: ? ? ? ? ???? L val (w, ?) + c?p 2 (? ? ?S(?)) 2 8: h ? h ? h d 9:</formula><p>else 10:</p><p>h ? h ? hi 11: end if 12:</p><p>n ? n + 1 13: end while 14: return ?S(?)</p><p>In addition to these contributions, we also study a regularization based on the Alternating Direction Method of Multipliers (ADMM), an algorithm that uses dual ascent and the method of multipliers to solve constrained optimization problems <ref type="bibr" target="#b1">(Boyd et al. 2010)</ref>. We propose as well a noncompetitive activation function to naturally permit progressive discretization. Further explanation and the results of these studies are presented in Supplementary Section S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test the two presented modifications to DARTS together as DARTS-PRIME as well as each independently as ablation studies. Specifically, we test the dynamic ? update schedule using Fisher Information (FIMT), and the proximity regularization (PR), in addition to DARTS-PRIME. We compare these methods to DARTS and its variants on three benchmark datasets: the image classification datasets CIFAR-10 and CIFAR-100, and the text prediction dataset Penn TreeBank.</p><p>We also rerun DARTS using the code provided by (Liu, Simonyan, and Yang 2019) 2 . To better understand the impact of an informed schedule, we study a slight modification of DARTS with a constant schedule of exactly 10 w steps per ? step (CS10). The value of 10 was selected for all scheduled experiments as we hypothesize that more w steps should be taken per ? step to aid the bilevel optimization approach local minima in w before optimizing ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Tasks</head><p>CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b9">(Krizhevsky, Nair, and Hinton 2009</ref>) are image datasets with 10 and 100 classes, respectively, and each consisting of 50K training images and 10K testing images for image classification. Applying the operator and cell-based search space from <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> to CIFAR-10 has been heavily studied in related NAS works. We additionally perform both search and evaluation on CIFAR-100 using identical protocols and hyperparameters to explore their applicability to similar tasks.  <ref type="bibr" target="#b19">(Real et al. 2019)</ref> 2.55 ? 0.05 2.8 3150 PNAS ?  3.41 ? 0.09 3.2 225 ENAS ? <ref type="bibr" target="#b18">(Pham et al. 2018)</ref> 2.89 4.6 0.5</p><p>Random Search <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> 3.29 ? 0.15 3.2 4 DARTS 1st* <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> 3.00 ? 0.14 3.3 1.5 DARTS 2nd* <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> 2.76 ? 0.09 3.3 4 P-DARTS <ref type="bibr" target="#b2">(Chen et al. 2019)</ref> 2.5 -0.3 PD-DARTS <ref type="bibr" target="#b10">(Li et al. 2020)</ref> 2.57 ? 0.12 3.2 0.3 FairDARTS <ref type="bibr" target="#b4">(Chu et al. 2020b)</ref> 2.54 ? 0.05 3.32 ? 0.46 0.4 R-DARTS (L2) <ref type="bibr" target="#b0">(Arber Zela et al. 2020)</ref> 2.95 ? 0.21 -1.6 DARTS- <ref type="bibr" target="#b3">(Chu et al. 2020a)</ref> 2.59 ? 0.08 3.5 ? 0.13 0.4 NASP <ref type="bibr" target="#b24">(Yao et al. 2020)</ref> 2.83 ? 0.09 3. Previously reported results where the single best search architecture was evaluated multiple times rather than reporting results across multiple searches. Selecting the best architecture across searches is reflected in the search time. ? : Architectures searched in a slightly different search space in operator types and network meta-structure.</p><p>Penn TreeBank (Marcus, Marcinkiewicz, and Santorini 1993) (PTB) is a text corpus consisting of over 4.5M American English words for the task of word prediction. We use the standard pre-processed version of the dataset <ref type="bibr" target="#b18">(Pham et al. 2018;</ref><ref type="bibr" target="#b12">Liu, Simonyan, and Yang 2019)</ref>. The DARTS search space for RNNs for text prediction searches over activation functions of dense layers within an RNN cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DARTS-PRIME Parameters</head><p>We use all protocols and hyperparameters from <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> for each domain of image classification and text prediction and for both search and evaluation unless otherwise stated. We use the first order approximation of the gradient of ?. Hyperparameters for CIFAR-100 are maintained directly from CIFAR-10.</p><p>When the dynamic FIMT schedule is used, the expected ratio of w steps per ? step, r, is set to 10, as in CS10. The initial threshold, h 0 , is set to 1.0. The threshold increasing factor, h i , is set to 1.05. The moving average weighting factor, ?, is set to 0.2. These values are all set to intuitive values without formal tuning.</p><p>For proximity regularization (PR), the regularization parameter, ? p , is set to 0.1 for image classification and 1.0 for text prediction. This was exponentially searched over 0.001 to 1.0 on CIFAR-10 and PTB, respectively, during early development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data and Training Configuration</head><p>After search, we follow the same evaluation procedure as DARTS for image classification and text prediction. After the search training completes and the final discretization selects the cell architecture(s), the evaluation network is reinitialized at the full size, trained on the training set, and tested on the heldout test set.</p><p>For trials with the original DARTS schedule of alternating w and ? steps, the training and validation split was maintained at 50/50 during search, as done in <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref>. For trials with either the constant or dynamic schedule with more w steps per ? step, the dataset was split according to the expected schedule of steps. For the ratio of 10 w steps per ? step, this results in a dataset split of 90.91% in the training set and 9.09% in the validation set.  <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> 60.2 57.6 8000 23 0.5 DARTS 2nd <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> 58.1 55.7 8000 23 1 GDAS <ref type="bibr" target="#b5">(Dong and Yang 2019)</ref> 59  <ref type="table">Table 2</ref>: Results for RNN architecture search on PTB. Performance is measured by perplexity (lower is better). All previous works report the single best result from multiple runs rather than statistics across multiple trials. For our trials, convergence at 1200 epochs was sufficient to differentiate between different methods. ? : Architectures searched in a slightly different search space in operator types and network meta-structure. ? : Reevaluated and reported by <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref> using the current standard evaluation pipeline.</p><p>A difference from the experiment protocol in DARTS is that we do not select the best architecture from multiple searches based on validation performance. Instead, we evaluate each architecture fully once and report the mean test error across search trials with different random seeds, conducting each trial 4 times with random seeds listed in the Supplementary Section S2, in addition to more configuration details. 4 was chosen as the number of trials due to the prohibitive cost of the evaluation phase which can take up to 3 GPU days per trial on a V100 GPU. While this is a small number of independent trials, we believe that showing the evaluation results from multiple search trials, instead of the best search architecture, is a better measure of search algorithm performance and reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We conduct experiments for each of the contributions independently as well as combined with each other in various configurations in the standard DARTS search space for CIFAR-10. Our experiments also include the original DARTS algorithm. We also apply DARTS, DARTS-PRIME, and a subset of ablation configurations to CIFAR-100 and PTB. The mean and standard deviation across searches of the resulting test error or perplexity, final architecture size, and search time for each are reported in <ref type="table">Table 1</ref> for CIFAR-10 and CIFAR-100 and in <ref type="table">Table 2</ref> for PTB, compared to reported results from related works. The ? progressions over training and final architectures for the best trial on CIFAR-10 are shown in <ref type="figure">Figure 1</ref> for DARTS and <ref type="figure">Figure 4</ref> for DARTS-PRIME. The best architectures for all other experiments are shown in <ref type="figure" target="#fig_0">Supplementary Figures.</ref> The DARTS-PRIME trials show significant improvements over DARTS and match state-of-the-art results within the standard CIFAR-10 search space and task while also yielding one of the lowest variances in performance across our ablation studies. The training progress and final cell architectures for the best trial of DARTS-PRIME are shown in <ref type="figure">Figure 4</ref>. This architecture has an evaluation size of 3.76M parameters and a test error of 2.53.</p><p>DARTS-PRIME avoids skip-connection mode collapse, as shown in <ref type="figure">Figure 2</ref>. This is important in domains with variance in parameterization across operators. Avoiding skipconnection mode collapse in the CNN search space decreases the need for tuning the length of search, which improves generalizability across tasks. We see a clear correlation between network size and performance for this search space in image classification tasks, further visualized in <ref type="figure" target="#fig_0">Supplementary Figure S5</ref>. This is one of the reasons why the skip-connection mode collapse is catastrophic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benefits of Scheduling</head><p>Using either CS10 or dynamic FIMT scheduling increases the network size in all paired experiments and tends to improve performance across datasets. This is tied to avoiding the skip-connection mode collapse. Even with many fewer ? updates overall, trials using the less frequent schedules have comparable or larger ranges of ? values at discretization. Thus, using more informed architecture steps helps prevent unfair suppression of parameterized operations.</p><p>Both the constant and dynamic schedules show similar improvement over the original schedule. One characterization of the dynamic schedule is that in the first epoch, the ? updates are delayed significantly beyond the expected schedule, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. This appears beneficial, as this gives parameterized operations more time to train before ? updates start to occur. Characterizing further benefits of dynamic scheduling over constant scheduling is left as future work. Additionally, other information metrics and dynamic scheduling techniques beyond Fisher Information may be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Regularization Improves Reliability</head><p>Proximity regularization yields better and more consistent test error and perplexity results, whether applied alone or with FIMT as in our proposed DARTS-PRIME, especially for CIFAR-10. This is very beneficial for one-shot tasks to improve reliability.</p><p>This implementation of proximity regularization is not the only choice for regularization, particularly towards discretization. In parallel to PR, we developed and conducted a study about another regularization technique for discretization called ADMM regularization. However, our results show that proximity regularization consistently outperforms ADMM regularization. We include these experimental results in the Supplementary <ref type="table">Table S1</ref> for the sake of completeness.</p><p>The linear scheduling of the increase of proximity regularization strength is a simple implementation choice that yields sufficient performance improvement. It may conflict with applying early stopping as done in other DARTS variants, but only minorly, in a similar way as standard training protocols such as the cosine annealing of architecture weights used in DARTS. This leaves other informed and dynamic schedules, such as similar to our FIMT schedule, as an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DARTS-PRIME Performance Beyond CIFAR-10</head><p>Our findings in CIFAR-10 are also generally supported by the results on CIFAR-100 and PTB: DARTS-PRIME consistently and significantly outperforms DARTS. Specifically on PTB, DARTS-PRIME also has higher reliability in performance. The architecture from our best trial of DARTS-PRIME on CIFAR-100, shown in <ref type="figure" target="#fig_0">Supplementary Figure S2</ref>, has an evaluation size of 3.56M parameters and a test error of 16.82. The architecture from our best trial of DARTS-PRIME on PTB, shown in <ref type="figure" target="#fig_0">Supplementary Figure S4</ref>, has an validation perplexity of 60.4 and a test perplexity of 58.2 in the shortened evaluation time compared to previous works. We note that no further hyperparameter tuning was completed for CIFAR-100, but we anticipate that tuning directly on CIFAR-100 may have allowed DARTS-PRIME to get similar low-variance performance results as the other tasks. Only minor tuning was completed for PTB, which differs from CIFAR in application domain and search space, as the operators of the RNN search space are activation functions, which are not parameterized and do not affect the overall size of the network. The performance of DARTS-PRIME on CIFAR-100 and PTB shows the applicability of the methods and hyperparameters within DARTS-PRIME without extensive searching on target datasets and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose DARTS-PRIME with alternative approaches to two dimensions of differentiable NAS algorithms: informed dynamic scheduling of gradient descent steps and regularization towards discretization. Each of these are analyzed with ablation studies to isolate the effects of each approach across domains and tasks. For practitioners, we recommend dynamic FIMT scheduling for improved network performance and proximity regulation for both improved and more reliable network performance.</p><p>The benefits of these approaches are apparent in results on the CIFAR-10, CIFAR-100, and PTB benchmarks yet also offer insight for different applications of differentiable NAS. The majority of DARTS variants, including this work, focus on narrow and static benchmark problems, achieving similar values of minimal error or perplexity through extensive weight training and hyperparameter tuning. However, more dynamic tasks, such as shifting data distributions or semi-supervised learning as in reinforcement learning, could highlight the benefits of using dynamic optimization schemes, such as our proposed improvements of scheduling and regularization.</p><p>The two improvements of DARTS-PRIME, proximity regularization and a FIMT informed schedule, are modular and could be added to other differentiable NAS algorithms beyond DARTS <ref type="bibr" target="#b12">(Liu, Simonyan, and Yang 2019)</ref>. In particular, proximity regularization may be applied to any algorithm utilizing a continuous relaxation of the architecture, and dynamic FIMT scheduling may be applied to any NAS method posed as a multilevel optimization. While DARTS-PRIME shows a significant improvement over DARTS, we believe that the contributions of a dynamic FIMT scheduling and proximity regularization can be beneficial for one-shot NAS methods in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Supplementary Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1 CRB Activation</head><p>We propose a new activation of ? that improves fairness towards discretization and allows for progressive discretization, called Clipped ReLU Batchnorm (CRB) activation. For CRB activation of ?, the softmax activation is replaced by unit-clipped ReLU.</p><p>This activation function does not enforce all operator weights within an edge to sum to 1. So, the distribution of the output of an edge is not as regulated. To account for this in activation distributions, a non-affine batchnorm layer is placed after summing across edges for each state within the cell. This permits the ? values across edges to still be comparable, as necessary for discretization. Additionally, the "none" operator can be removed, since the activated ? values are no longer dependent on each other.</p><p>CRB activation provides bounds within [0, 1] and a natural progressive pruning heuristic. Over training, when any architecture weight becomes non-positive, the corresponding operator is pruned from that edge. This progressive pruning allows the network training to focus on the remaining operators and saves computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.2 ADMM Regularization</head><p>The Alternating Direction Method of Multipliers (ADMM) is an algorithm that uses dual ascent and the method of multipliers to solve constrained optimization problems <ref type="bibr" target="#b1">(Boyd et al. 2010</ref>). ADMM has previously been applied to training sparse networks <ref type="bibr" target="#b6">(Kiaee, Gagn?, and Abbasi 2016)</ref>, pruning weights <ref type="bibr" target="#b25">Ye et al. 2018)</ref>, and pruning channels <ref type="bibr">(Ma et al. 2019)</ref>, using the version of the algorithm designed for nonconvex constraints such as maximal cardinality of weight matrices. When applied in practice the non-convex objective of neural network objective functions, the optimization variable of the network parameters is usually not evaluated to convergence within every iteration. The implementation of ADMM functions similarly to a dynamic regularization, so we apply it to differentiable NAS in comparison to our proposed proximity regularization.</p><p>Directly applying ADMM for nonconvex constraints <ref type="bibr" target="#b1">(Boyd et al. 2010)</ref> to the optimization given in Problem 5 using ? ? S as the constraint, the iterative solution is</p><formula xml:id="formula_6">? k+1 := argmin ? L val (w * (?), ?) + ? a 2 ? ? z k + u k 2 2 (S1) s.t. w * (?) = argmin w L train (w, ?) (S2) z k+1 :=? S (? k+1 + u k ) (S3) u k+1 :=u k + ? k+1 ? z k+1 ,<label>(S4)</label></formula><p>where ? S is the projection onto S, z and u are introduced variables, and ? a is a regularization hyperparameter. Equation S1 is solved by using the DARTS 1 st order approximation for n minibatches before updating z and u. The number of w steps (Equation S2) for each ? step (Equation S1) is determined by the ? schedule.</p><p>Because neither level of the bilevel optimization is evaluated to convergence within each iteration, we discount past u values with each update using a discount factor, ? u . Thus, Equation S4 becomes</p><formula xml:id="formula_7">u k+1 :=? u u k + ? k+1 ? z k+1 . (S5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Supplementary Configurations</head><p>All experiments were run using the code here: https://anonymous.4open.science/r/DARTS-PRIME. All Python packages used in this repository are open-source, and current versions of each were used. Each trial was run with a single V100 GPU on a computing node running Linux. The random seeds used were 101, 102, 103, and 104.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Supplementary Results</head><p>In our CIFAR-10 experiments for ADMM regularization, z and u are updated after every 10 ? steps. The decay factor, ? u , is set to 0.8. The regularization parameter, ? a , is set to 0.1. When CRB activation is used, the cosine annealing of the architecture learning rate and the weight decay of ? are both turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.1 CRB Activation Produces Smaller Networks</head><p>For all combinations, the CRB activation finds smaller networks than the corresponding experiment with softmax activation, albeit with a small degradation in performance. This pattern is shown in <ref type="figure" target="#fig_0">Figure S5a</ref>. We noted that only trials using CRB activation selected any pooling operators in the normal cell architecture, where other methods use convolutional operators. This could cause performance degradation due to the difference in depth between the search and evaluation networks, particularly since only normal cells are added to reach the final depth. We also note that CRB activation has high variance without regulation or informed scheduling. Particularly in the DARTS+CRB experiment, one trial had a very high error of 4.95, which greatly increased the mean and variance. This trial fell into skip-connection mode collapse. As discussed in the following, scheduling and especially regularization help prevent this collapse from occurring, which is beneficial if the search is applied in a truly one-shot task. On the other hand, if multiple searches can be completed and small network size is beneficial to the application, this protocol may actually be the best approach to apply.</p><p>An example of progressive pruning provided by CRB activation is shown in <ref type="figure">Figure S6</ref>(a-b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.2 ADMM Regularization Improves Performance</head><p>Using ADMM regularization generally improves performance in each paired experiment. However, proximity regularization was more performant in all cases. ADMM is a very powerful algorithm but with many hyperparameters that require careful tuning when applied in practice. This leaves further development as open work.  <ref type="table">Table S1</ref>: Results for our variants using ADMM regularization on CIFAR-10.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Supplementary Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Normal cell ? over search training (b) Reduce cell ? over search training (c) Final normal cell (d) Final reduce cell Figure 1: Training progression and final results of our highest performing trial of DARTS on CIFAR-10. Each subplot in (a) and (b) represents an edge in the supernetwork cell: each column is the source of the edge, and each row is the destination of the edge. The softmaxed ? for each operator is plotted over the search training. To derive the architecture shown in (c) and (d) respectively, the two largest activated ? values from unique columns are selected in each row at the end of search. The "none" operator is not shown, as it cannot be selected at discretization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example of dynamic FIMT schedule, visualized over the first epoch. ? updates occur at an interval of roughly every 10 w updates, adjustable by the expected ratio scheduling hyperparameter, but do not occur at the beginning of training when the FIMT is constantly decreasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Normal cell ? over search training (b) Reduce cell ? over search training (c) Final normal cell (d) Final reduce cell Figure 4: Best trial of DARTS-PRIME on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Final normal cell (b) Final reduce cell Figure S1: Best final architecture of DARTS on CIFAR-100. (a) Final normal cell (b) Final reduce cell Figure S2: Best final architecture of DARTS-PRIME on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S3 :</head><label>S3</label><figDesc>Best final architecture of DARTS on PTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S4 :Figure S5 :</head><label>S4S5</label><figDesc>Best final architecture of DARTS-PRIME on PTB. Comparison across CIFAR-10 trials of test error vs. network size, recolored by (a) activation, (b) schedule, and (c) regularization. (a) Normal cell ? over search training (b) Reduce cell ? over search training (c) Final normal cell (d) Final reduce cell Figure S6: Best trial of DARTS-PRIME +CRB.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/quark0/darts</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Arber Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01027</idno>
		<title level="m">DARTS-: robustly stepping out of performance collapse without indicators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Alternating direction method of multipliers for sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kiaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gagn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01590</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cifar-10 and cifar-100 datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PD-DARTS: Progressive Discretization Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="306" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
	</analytic>
	<monogr>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the European conference on computer vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ResNet Can Be Pruned 60?: Introducing Network Purification and Unused Path Removal (P-RM) after Weight Pruning</title>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Second-order optimization for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto (Canada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Proximal algorithms. Foundations and Trends in optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05787</idno>
		<title level="m">Faster gaze prediction with dense networks and fisher pruning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reducing the model order of deep neural networks using information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Annual Symposium on VLSI</title>
		<imprint>
			<biblScope unit="page" from="93" to="98" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking architecture selection in differentiable nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6664" to="6671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07378</idno>
		<title level="m">Progressive weight pruning of deep neural networks using ADMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fardad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11091</idno>
		<title level="m">StructADMM: A systematic, high-efficiency framework of structured weight pruning for dnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16537</idno>
		<title level="m">Theoryinspired path-regularized differential network architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural Architecture Search with Reinforcement Learning. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
