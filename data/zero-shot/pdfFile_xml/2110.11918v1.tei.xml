<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIGS: META IMAGE GENERATION FROM SCENE GRAPHS MIGS: Meta Image Generation from Scene Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
							<email>azade.farshad@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><surname>Musatian</surname></persName>
							<email>sabrina.musatian@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
							<email>helisa.dhamo@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<email>nassir.navab@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIGS: META IMAGE GENERATION FROM SCENE GRAPHS MIGS: Meta Image Generation from Scene Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Sample of images generated by MIGS over a variety of scene attributes from BDD dataset <ref type="bibr" target="#b30">[31]</ref>. The model is trained on 10-shot data for generating the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generation of images from scene graphs is a promising direction towards explicit scene generation and manipulation. However, the images generated from the scene graphs lack quality, which in part comes due to high difficulty and diversity in the data. We propose MIGS (Meta Image Generation from Scene Graphs), a meta-learning based approach for few-shot image generation from graphs that enables adapting the model to different scenes and increases the image quality by training on diverse sets of tasks. By sampling the data in a task-driven fashion, we train the generator using meta-learning on different sets of tasks that are categorized based on the scene attributes. Our results show that using this meta-learning approach for the generation of images from scene graphs achieves state-of-the-art performance in terms of image quality and capturing the semantic relationships in the scene. Project Website: https://migs2021.github.io/ * The first two authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of high-quality image generation and manipulation has been capturing the attention of researchers for many years. Recent advances in deep learning for unconditional image synthesis <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> has led to producing high-quality images that are often indistingu-ishable from the real ones by a human. Some impressive results have been also achieved for the class-conditional image generation <ref type="bibr" target="#b2">[3]</ref> and generation from semantic segmentation masks <ref type="bibr" target="#b27">[28]</ref>. While the latter method allows for pixel-level control on the image, it requires a segmentation map which may be practically hard to obtain. An easier alternative to represent image semantics is a scene graph <ref type="bibr" target="#b15">[16]</ref>, i.e. a graph where nodes correspond to the objects and edges define the relationships between them. Image generation from such graphs is attractive as they allow full control over the semantics and it is easy to modify them in case of scene editing. Image generation from scene graphs has been introduced in <ref type="bibr" target="#b17">[18]</ref>. While the results are encouraging, the images generated by this model lack quality when trained on datasets with diverse scenes, as the network struggles to learn meaningful representations to accommodate such discrepancies. To counterpart this problem we suggest using metalearning in order to help the network focus its attention on specific tasks during training. Yet such a model is able to quickly adjust to a wide set of tasks during testing with only a few training samples available. Additionally, our approach allows us to introduce the task of fewshot learning for scene graph to image generation problem, which is an attractive scenario as it can help to generate semantically meaningful images in applications with a scarce amount of data. The contributions of our work can be summarized as follows:</p><p>? A novel meta-learning approach for the generation of images from scene graphs, which achieves state-of-the-art results compared to previous work</p><p>? Introduction of the few-shot learning problem for scene graph to image generation ? A novel task sampling method for scenes in the wild that can benefit other image generation with meta-learning scenarios.</p><p>We evaluate our proposed method on automatically generated scene graphs for Berkeley Deep Drive <ref type="bibr" target="#b30">[31]</ref>, Action Genome <ref type="bibr" target="#b14">[15]</ref> and Visual Genome <ref type="bibr" target="#b22">[23]</ref> datasets, showing superior results compared to the baselines, qualitatively and quantitatively. This is also verified by performing a user study on the quality of images. The source code of this work is provided in the supplement and will be publicly released upon its acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image generation Recent advances in generative models, in particular, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> have boosted the quality of image generation. A line of works explore generative models for unconditional image generation <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Image generation models have also been explored conditionally, with a diverse set of priors such as semantic segmentation maps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>, natural language descriptions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> or translating from one image domain to another using paired <ref type="bibr" target="#b13">[14]</ref> or unpaired data <ref type="bibr" target="#b41">[42]</ref>. Most related to our approach are methods that generate images from scene graphs <ref type="bibr" target="#b17">[18]</ref>.</p><p>Image generation from scene graphs Scene graphs <ref type="bibr" target="#b15">[16]</ref> refer to representations that describes images, where nodes are objects, and the edges represent relationships between them. With the recent rise of large-scale scene graph datasets, such as Visual Genome <ref type="bibr" target="#b22">[23]</ref> a diverse set of scene graph related tasks were explored. A line of works propose strategies for scene graph generation from images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>. Johnson et al. <ref type="bibr" target="#b17">[18]</ref> introduced the reverse task of image generation from scene graphs, using a 2D layout as an intermediate representation between graphs and images, where layouts are decoded to images using a Cascade Refinement Network (CRN) <ref type="bibr" target="#b3">[4]</ref> architecture. Later, a similar architecture was explored for image generation in an interactive form <ref type="bibr" target="#b0">[1]</ref> as well as for semantic image manipulation <ref type="bibr" target="#b5">[6]</ref>. Herzig et al. <ref type="bibr" target="#b11">[12]</ref> proposed a model that uses canonical scene graphs, to improve robustness in terms of graph size and noise. Recently, Garg et al. <ref type="bibr" target="#b8">[9]</ref> generate scene graphs unconditionally and later synthesize images from the resulting graphs. Other related works explore image generation directly from layouts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> or investigate 3D scene graphs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Meta-learning Meta-learning or learning to learn was initially introduced for the few-shot classification problem. Model-agnostic Meta-Learning <ref type="bibr" target="#b7">[8]</ref> (MAML) is one of the most wellknown works for few-shot image classification. MAML aims to optimize a model on a set of tasks using second-order gradient computation to obtain a model that adapts fast to newly seen tasks. Due to the extensive computation demands of MAML, a first-order approximation of it was proposed in Reptile <ref type="bibr" target="#b26">[27]</ref> with similar performance. These approaches have been mainly adopted in image classification and segmentation problems. A combination of meta-learning with GANs was introduced in <ref type="bibr" target="#b38">[39]</ref> which employs adversarial training for few-shot image classification. However, the employment of meta-learning for the few-shot image generation task has been rarely explored. FIGR <ref type="bibr" target="#b4">[5]</ref> uses meta-learning for few-shot image generation for small-scale datasets of black and white images, while few-shot image to image translation <ref type="bibr" target="#b24">[25]</ref> generates images by adapting input from a source domain to a target domain. Despite the previous use of meta-learning for image generation, it has been only used on data with limited diversity and low resolution. To our knowledge, this is the first work on the few-shot generation of high-resolution scenes in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>A task in meta-learning for few-shot image classification is defined as a set of image, label pairs. In our work, we define the task as scene graph and image pairs. Given a set D of image I and scene graph G pairs, we define our initial dataset D = {I, G}. The dataset is divided into different tasks based on the predefined task definition. In each iteration of the training phase, a task is randomly sampled and the scene graph to image model parameters are optimized on the selected task. In the test phase, the trained parameters are then used to fine-tune the model on specific target tasks. The main components of our method which are the image generation and meta-learning are described over the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image generation</head><p>To tackle the task of scene graph to image generation we build upon SG2Im <ref type="bibr" target="#b17">[18]</ref> architecture as a foundation. SG2Im takes as input a scene graph, where the nodes correspond to the objects and the edges define relationships between them. The scene graph is processed by graph convolutional network (GCN), which operates on triplets subject-predicate-object, to propagate information along the edges. This results in processed per-node features, where each object has its own embedding vector that encodes information about itself as well as relationships with connecting objects. These embeddings are then used to predict a set of bounding boxes and segmentation masks for each object. The predicted boxes and masks are combined to project the GCN features to image space and obtain a scene layout.  <ref type="figure">Figure 2</ref>: Method Overview. Our method consists of two phases: Meta-training and Testing. In the meta-training phase, the model parameters ? are updated on a sampled task l in each iteration. One task is a set of image and scene graph pairs that are unified in one group by some criteria. We pass a scene graph through a GCN to produce features from the nodes embeddings for creating a scene layout. The layout is passed to the generator, which synthesises the final image. In the testing phase, we fine-tune the model ? for a defined number of shots on each specific task which results in the generation of our final images. step in this pipeline is an image generator that receives a scene layout and produces an image corresponding to the given semantic definition. In order to force the network to produce both realistically looking and semantically correct images, two image discriminators are used on top of the generated image. The first one discriminates individual objects in a local context, while the second one classifies a picture as a whole. We use different loss terms for training the model. Most of the losses are adopted from <ref type="bibr" target="#b15">[16]</ref>, but some extra loss terms are also used which we mention below. To prevent the model from generating trivial solutions, we employ the perceptual loss <ref type="bibr" target="#b16">[17]</ref> ? p L p using the VGG network. There are two GAN losses defined, one for the whole image and one to make individual objects look realistic. These are defined as L GAN,global and L GAN,obj respectively. To ensure the quality of generated objects, the auxiliary classification loss L aux,obj is used. The loss for predicting the bounding boxes is defined by L box which is calculated using the L 1 loss between the predicted and ground truth bounding boxes. Finally, the image loss L im which is the L 1 distance between the predicted image and the ground truth image is used. Equation 1 shows the task loss L ? definition.</p><formula xml:id="formula_0">L ? = ? b L box + ? g min G max D L GAN,global + ? o min G max D L GAN,obj + ? a L aux,obj + ? p L p + ? im L im ,<label>(1)</label></formula><p>where ? b ,? g , ? o , ? a , ? p , ? im are weighting values and</p><formula xml:id="formula_1">L GAN = E q?p real log D(q) + E q?p fake log(1 ? D(q)),<label>(2)</label></formula><p>where p real refers to the real data distribution from the ground truth and p f ake is the distribution of generated fake images or objects. The input to the discriminator is defined by q.</p><p>In order to improve the image quality of the images generated by SG2Im and tackle a few-shot learning scenario, we adapt Reptile <ref type="bibr" target="#b26">[27]</ref> algorithm for GANs, similarly to how it is done in FIGR <ref type="bibr" target="#b4">[5]</ref>. However, FIGR is focused on the problem of unconditional image generation and experiments with images that have a single object drawn on them. We, on the other hand, have a setting conditioned on a scene graph with multiple objects, which is a more challenging problem compared to FIGR. We meta-train all components of the SG2Im pipeline on a diverse set of scenes and prove that such procedure enables high-quality image generation from the scene graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MIGS</head><p>In this section, we introduce our method of Meta image generation from scene graphs (MIGS) and its components. Our primary goal is to be able to quickly and effectively adapt a model trained on a variety of images to a specific task with just a few training shots. To tackle this problem we refer to the meta-learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> and their application in a few-shot learning setting. To the best of our knowledge, we introduce a few-shot learning scenario for the scene graph to image generation for the first time.</p><p>Task Definition Meta-learning models assume access to a set of tasks T , that consist of different learning problems ?. Each of these tasks ? represent an image generation from scene graph problem and has its own set of image-graph pairs D ? = {I, G} ? , that have been grouped together in one task based on certain criteria. Such criteria include the type of objects on the scene, specific background surroundings or any other attributes of either graphs, or images that unite them together. However, the splitting bases should be homogeneous across the whole set of tasks T . The task splitting criteria is dependent on the dataset characteristics. It can be defined based on the scene attributes such as time of the day, the context or by simply clustering the images into a set of clusters based on their visual attributes using an unsupervised clustering method such as <ref type="bibr" target="#b33">[34]</ref>.</p><p>Meta-learning A loss function on a task ? is denoted as L ? . For simplicity we define L ? as a combination of all generator L ?,G and discriminator L ?,D losses in SG2Im model. Then our meta-learning goal is to find such initial model parameters ? that for a randomly selected task ? the loss L ? will be low after k iterations with only a few data points available. In short, such objective is defined as</p><formula xml:id="formula_2">min ? [L ? U k ? (? ) ],<label>(3)</label></formula><p>where U k ? (? ) denotes an operator that updates weights ? k times using image-graph pairs from D ? .</p><p>In order to find such parameters ? , we train the models with the Reptile algorithm <ref type="bibr" target="#b26">[27]</ref>. It comprises of inner and outer loops. In the inner loop k iterations of operator U are performed on the locally copied weights ? l for a randomly sampled training task l. In an outer loop the weights vector of the meta-model ? are updated leveraging the difference between ? and ? l computed in the inner loop. This updated can be summarized as:</p><formula xml:id="formula_3">? ? ? + ? 1 L L ? l=1 (? l ? ? ),<label>(4)</label></formula><p>where L is number of tasks and ? is the meta learning rate. We perform such updates separately for the image generator model and two discriminators.  <ref type="figure">Figure 3</ref>: Examples of images generated from BDD dataset for a task with the following attributes: day time: daytime; weather: rainy; driving scenario: highway. All images were generated from the scene graph defined on top. MIGS not only produces more realistic images but also has a higher accuracy when cross-referenced with the provided scene graph, i.e. MIGS + SPADE is the only model for which the truck is clearly seen in all three scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIGS (Ours) + CRN SG2Im + CRN</head><p>Testing For testing the trained meta-model, we first fine-tune the trained weights ? on the training split of each specific test task l to obtain the final weights ? l for this task. Then, we generate images from the (unseen) test set scene graphs of each task.</p><p>To fairly compare our meta-models to baseline methods, we use transfer learning on each of the non-meta models' weights ? and fine-tune them on each task to obtain the corresponding ? l . Then for each task l we evaluate the images generated by ? l and ? l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method on Berkeley Deep Drive <ref type="bibr" target="#b30">[31]</ref> (BDD), Action Genome <ref type="bibr" target="#b14">[15]</ref> (AG) and Visual Genome <ref type="bibr" target="#b22">[23]</ref> (VG) datasets on different baselines. We show that our proposed method is independent of the generator architecture as the performance gain happens in two different generator architectures, namely CRN <ref type="bibr" target="#b3">[4]</ref> and SPADE <ref type="bibr" target="#b27">[28]</ref>. To measure the quality and realism of the images generated by our proposed method quantitatively, Fr?chet Inception Distance <ref type="bibr" target="#b12">[13]</ref> (FID) and Kernel Inception Distance <ref type="bibr" target="#b1">[2]</ref> (KID) are reported. Moreover, generated image samples are compared to related work in diverse scenarios. We refer the reader to the supplement for the results with more metrics such as precision and recall <ref type="bibr" target="#b29">[30]</ref> and the architecture details of the CRN, SPADE and GCN networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>As there are no established datasets for conditional image generation with meta-learning, we investigated and selected datasets that can be naturally categorized in a set of tasks. <ref type="figure">Figure 4</ref>: Examples of images generated from the Action Genome dataset. Each column correspond to one task (i.e. a video sequence) that each model was fine-tuned on. The MIGS results on each video notably contain more details as opposed to the baseline counterpart.</p><p>Berkeley Deep Drive <ref type="bibr" target="#b30">[31]</ref> (BDD) This dataset consists of images from city streets, residential areas, and highways in different scene conditions. As BDD does not contain scene graphs associated with images, we construct spatial scene graphs automatically, leveraging the ground truth bounding boxes, leading to six mutually exclusive spatial relationships: left of, right of, above, below, inside, and surrounding. As we are mostly interested in the objects and their relationships, we pre-process images and crop out the area of interest, which contains all the objects and as little background as possible.</p><p>For our meta-learning purpose, we split BDD into tasks using provided attributes for all images, such as time of day, weather conditions, and driving scenarios. We then filter out tasks that have less than 500 images available, which results in a total of 23 different meta-learning tasks. We use 20 tasks for training and validation and 3 for testing.</p><p>Action Genome <ref type="bibr" target="#b14">[15]</ref> (AG) The Action Genome dataset was originally designed for the action recognition problem. It consists of video frames of humans interacting with objects in a scene. We use human-object relationships labels from Action Genome directly to construct semantically meaningful scene graphs.</p><p>As the AG dataset consists of a large number of videos with different actions, we use those videos as task splitting criteria. We remove the frames of all videos that do not have persons or only persons and no other objects, as it is impossible to construct meaningful scene graphs from them. Furthermore, we extract as tasks only the horizontal videos with at least 30 frames annotated by a human. This procedure provides us with 735 tasks which we split into 662 train and validation and 73 test tasks accordingly.</p><p>Visual Genome <ref type="bibr" target="#b22">[23]</ref> (VG) The Visual Genome dataset provides images with their corresponding scene graphs and bounding box annotations which makes it suitable for the image generation from scene graphs problem. Since the VG dataset does not have specific annotations for scene attributes, we construct the tasks by splitting all the images into 100 classes using SCAN <ref type="bibr" target="#b33">[34]</ref> pre-trained on the imagenet dataset in an unsupervised manner. We use the first 60 clusters for the pre-training step and the last 40 clusters for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>We train our models to generate images of size 128 ? 256 for BDD and AG, and 64 ? 64 for VG. For all meta-learning models, we use an inner learning rate of 0.0001 and train for 10 inner iterations with Adam optimizer. The outer loop has a learning rate of 1 and uses SGD. The number of training iterations is dependant on the model and dataset, e.g. AG is more diverse than BDD and takes longer to converge. Meta-learning models on AG are trained for 40000 outer loop iterations and models on BDD are trained for 30000.</p><p>The baseline model, SG2Im <ref type="bibr" target="#b17">[18]</ref> is trained on the same data as MIGS (all training tasks) in the pre-training step. During the testing phase, the pre-trained model is used as an initialization and is fine-tuned and tested in the few-shot setting similarly to MIGS. In all datasets, both models are trained until convergence. The SG2Im model is trained for 250k iterations on AG, 200k iterations on BDD, and 40k iterations on VG. The MIGS model is trained for 40k and 30k iterations on AG and BDD respectively, and 8k iterations on VG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The quantitative results of our experiments are shown in <ref type="table">Table 1</ref>, <ref type="table" target="#tab_3">Table 2</ref>. We compare our method to two baselines which are SG2Im with the original CRN decoder and another version with the SPADE network as the decoder to improve the generation quality. More qualitative results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BDD Results</head><p>We show the performance of the mentioned models on BDD with different shot values ranging from 5 to 160 both quantitatively in <ref type="table">Table 1</ref> and qualitatively in <ref type="figure">Figure 3</ref>. Among all three experiments, we observe that FID and KID improve almost twice compared to the corresponding model with no meta-learning. We also perform a user study on BDD images, asking users to rank the quality of images from the same scene graph generated by different methods, and determine whether the scene represents the specified attribute. The results of the user study show that MIGS + SPADE is generally chosen as the most realistic method, MIGS + CRN as second and SG2Im + SPADE, SG2Im + CRN as third and fourth rank respectively. The exact percentages of rankings are shown in <ref type="table">Table 3</ref>. The values show the percentages of users choosing the method as the specified rank based on the image quality. For the attribute representation, MIGS + SPADE and MIGS + CRN stand as 1st and 2nd rank, while SG2Im + SPADE and SG2Im + CRN are ranked 3rd and 4th. <ref type="figure">Figure 3</ref> shows example scene graph and images generated with baselines and our method on a single testing task from BDD. It is clear from these images, that our method outperforms all baselines and is able to generate realistic-looking images with a high level of detail even in an extremely challenging scenario where only 5 frames are available for training. Additionally, example images generated for a diverse set of training tasks may be seen on <ref type="figure">Figure 1</ref>. Our method successfully captures the differences in the scenes associated with daytime and driving scenario change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Decoder  <ref type="table">Table 1</ref>: Quantitative results on BDD100k fine-tuned on 5,10 and 160 shots.</p><formula xml:id="formula_4">FID ? KID ?10 3 ? FID ? KID ?10 3 ? FID ? KID ?10 3 ? 160-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AG Results</head><p>We train the Action Genome model on all training images for each testing task (approximately 30 frames) and evaluate the model on the frames extracted from the full videos that were not used for training. The testing set has approximately 65000 images. AG dataset is extremely challenging for the scene graph to image generation, as it contains labeling only for a few chosen objects on the image and most commonly those objects are quite small, e.g. phone or book. <ref type="figure">Figure 4</ref> shows the example images generated by our model as well as baselines on AG, while the quantitative performance on AG is shown in Table 2. Due to the difficulty of the used dataset, it is quite expected that the results look different from the ones on BDD. However, even in this scenario, the number of details increases with our method compared to a corresponding baseline. This improvement can be also verified quantitatively from the substantial decrease in both FID and KID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Decoder FID ? KID ?10 3 ?   <ref type="table">Table 3</ref>: User study ranking results on randomly sampled images from BDD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VG Results</head><p>The performance of MIGS compared to SG2Im <ref type="bibr" target="#b17">[18]</ref> on the VG dataset is presented in <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_1">Figure 5</ref>. As shown in the results, MIGS outperforms the baseline in all metrics for different shot values, even with less number of training epochs. Despite the higher diversity of the images in the VG dataset and their wild nature, MIGS is able to generate images which look more realistic compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Our experiments show that using meta-learning for the task of image generation from scene graphs, outperforms the respective baselines almost twice, in terms of the employed metrics. The proposed method is shown to be advantageous in all scenarios even when using only 5 training samples. The results obtained on BDD qualitatively differ, which can be attributed  <ref type="table">Table 4</ref>: Quantitative results on VG fine-tuned on 5, 10 and 160 shots. not only to a more challenging setting of the AG but also to the unsuitability of this dataset for the image generation task. As it is intended for action recognition on video, only the objects that the person interacts with are annotated. It is thus common for some instances of the objects to be ignored in certain frames, until the action takes place, or be unannotated through the whole video if not used by the main actor. Such discrepancies may confuse the model and result in poor quality. Additionally, this dataset contains a lot of very small objects compared to the frame size. We believe that with a better dataset for semantically meaningful scene graphs the model should demonstrate results similarly to BDD. The results on the VG dataset show that using a simple yet effective task construction scheme, such as clustering, which could be applied to any other dataset, combined with the meta-learning approach can improve the performance of the image generation. The effect of task construction on the performance of meta-learning is an interesting topic and leaves room for research for future.</p><formula xml:id="formula_5">Method Decoder FID ? KID ?10 3 ? FID ? KID ?10 3 ? FID ? KID<label>?10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose MIGS, a meta-learning approach for the generation of images from scene graphs. To our knowledge, this is the first work for few-shot image generation of scenes in the wild. The proposed method could be applied to a different range of generator architectures and different datasets. We plan to replace the current SG2Im framework with <ref type="bibr" target="#b11">[12]</ref> as part of the future work. The results of the evaluation on three datasets show that our proposed metalearning-based image generation scheme proves to improve the quality of generated images significantly in all scenarios. We show that it is possible to generate high-quality images only given 5 shots of data. The performance gain compared to previous work is shown both quantitatively and qualitatively in the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Example images generated with MIGS + SPADE and SG2Im + SPADE on the Visual Genome dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on Action Genome dataset compared to related work. 25.57 25.16 24.81 SG2Im SPADE 25.36 24.60 28.29 21.74 MIGS(Ours) SPADE 34.72 26.48 20.0 17.79</figDesc><table><row><cell>Method</cell><cell>Decoder</cell><cell>Rank 1 (%) 2 (%) 3 (%) 4 (%)</cell></row><row><cell>SG2Im [18]</cell><cell>CRN</cell><cell>15.79 23.34 26.55 35.58</cell></row><row><cell>MIGS(Ours)</cell><cell>CRN</cell><cell>24.46</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We gratefully acknowledge the Munich Center for Machine Learning (MCML) with funding from the Bundesministerium f?r Bildung und Forschung (BMBF) under the project 01IS18036B. We are also thankful to Deutsche Forschungsgemeinschaft (DFG) for supporting this work, under the project 381855581.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<title level="m">Demystifying mmd gans</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Clou?tre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Demers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02199</idno>
		<title level="m">Figr: Few-shot image generation with reptile</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image manipulation using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-to-3d: End-to-end generation and manipulation of 3d scenes using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unconditional scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><surname>Musatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning canonical representations for scene graph to image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action genome: Actions as compositions of spatio-temporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalanditis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Storygan: A sequential conditional gan for story visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02784</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Assessing Generative Models via Precision and Recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bdd100k: A large-scale diverse driving video database. The Berkeley Artificial Intelligence Research Blog. Version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">511</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object-centric image generation from layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning 3d semantic scene graphs from 3d indoor reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
