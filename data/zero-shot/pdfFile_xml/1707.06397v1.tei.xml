<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Object Discovery and Co-Localization by Deep Descriptor Transforming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
							<email>zhangcl@lamda.nju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
							<email>zhouzh@lamda.nju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Object Discovery and Co-Localization by Deep Descriptor Transforming</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) The first two authors contributed equally to this work. This work was done when X.-S. Wei was visiting the University of Adelaide. Corresponding authors:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Unsupervised object discovery ? Image co- localization ? Deep descriptor transforming ? Pre-trained CNN models</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reusable model design becomes desirable with the rapid expansion of computer vision and machine learning applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the image colocalization problem. We propose a simple yet effective method, termed Deep Descriptor Transforming (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of unlabeled images, i.e., unsupervised object discovery. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark image co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data. Beyond those, DDT can be also employed for harvesting web images into valid external data sources for improving performance of both image recognition and object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Model reuse <ref type="bibr" target="#b41">(Zhou, 2016)</ref> attempts to construct a model by utilizing existing available models, mostly trained for other tasks, rather than building a model from scratch. Particularly in deep learning, since deep convolutional neural networks have achieved great success in various tasks involving images, videos, texts and more, there are several studies have the flavor of reusing deep models pre-trained on ImageNet <ref type="bibr" target="#b24">(Russakovsky et al, 2015)</ref>.</p><p>In computer vision, pre-trained models on ImageNet have been successfully adopted to various usages, e.g., as universal feature extractors <ref type="bibr" target="#b2">(Cimpoi et al, 2016;</ref><ref type="bibr" target="#b33">Wang et al, 2015;</ref><ref type="bibr" target="#b16">Li et al, 2016)</ref>, object proposal generators <ref type="bibr" target="#b8">(Ghodrati et al, 2015)</ref>, etc. In particular, <ref type="bibr" target="#b35">Wei et al (2017a)</ref> proposed the SCDA (Selective Convolutional Descriptor Aggregation) method to utilize pre-trained models for both localizing a single fine-grained object (e.g., birds of different species) in each image and retrieving fine-grained images of the same classes/species in an unsupervised fashion.</p><p>In this paper, we reveal that the convolutional activations can be used as a detector for the common object in image co-localization. Image co-localization (a.k.a. unsupervised object discovery) is a fundamental computer vision problem, which simultaneously localizes objects of the same category across a set of distinct images. Specifically, we propose a simple but effective method termed Deep Descriptor Transforming (DDT) for image co-localization. In DDT, the deep convolutional descriptors extracted from pre-trained deep convolu-CNN pre-trained models Deep Descriptor Transforming <ref type="figure">Figure 1</ref>: Pipeline of the proposed DDT method for image co-localization. In this instance, the goal is to localize the airplane within each image. Note that, there might be few noisy images in the image set. (Best viewed in color.) tional models are transformed into a new space, where it can evaluate the correlations between these descriptors. By leveraging the correlations among images in the image set, the common object inside these images can be located automatically without additional supervision signals. The pipeline of DDT is shown in <ref type="figure">Fig. 1</ref>. To our knowledge, this is the first work to demonstrate the possibility of convolutional activations/descriptors in pre-trained models being able to act as a detector for the common object.</p><p>Experimental results show that DDT significantly outperforms existing state-of-the-art methods, including image co-localization and weakly supervised object localization, in both the deep learning and hand-crafted feature scenarios. Besides, we empirically show that DDT has a good generalization ability for unseen images apart from ImageNet. More importantly, the proposed method is robust, because DDT can also detect the noisy images which do not contain the common object. Thanks to the advantages of DDT, our method could be used as a tool to harvest easy-to-obtain but noisy web images. We can employ DDT to remove noisy images from webly datasets for improving image recognition accuracy. Moreover, it can be also utilized to supply object bounding boxes of web images. Then, we use these images with automatically labeled object boxes as valid external data sources to enhance object detection performance.</p><p>Our main contributions are as follows:</p><p>1. We propose a simple yet effective method, i.e., Deep Descriptor Transforming, for unsupervised object discovery and co-localization. Besides, DDT reveals another probability of deep pre-trained network reusing, i.e., convolutional activations/descriptors can play a role as a common object detector. 2. The co-localization process of DDT is both effective and efficient, which does not require image labels, negative images or redundant object proposals. DDT consistently outperforms state-of-the-arts of image co-localization methods and weakly supervised object localization methods. With the ensemble of multiple CNN layers, DDT could further improve its co-localization performance. 3. DDT has a good generalization ability for unseen categories and robustness for dealing with noisy data. Thanks to these advantages, DDT can be employed beyond the narrow co-localization task. Specifically, it can be used as a generalized tool for exploiting noisy but free web images. By removing noisy images and automatically supplying object bounding boxes, these web images processed by DDT could become valid external data sources for improving both recognition and detection performance. We thus provide a very useful tool for automatically annotating images. The effectiveness of DDT augmentation on recognition and detection is validated in Sec. 4.6. 4. Based on the previous point, we also collect an object detection dataset from web images, named WebVOC. It shares the same 20 categories as the PASCAL VOC dataset <ref type="bibr" target="#b5">(Everingham et al, 2015)</ref>, and has a similar dataset scale (10k images) comparing with PASCAL VOC. We also release the WebVOC dataset with the automatically generated bounding boxes by DDT for further study.</p><p>This paper is extended based on our preliminary work <ref type="bibr" target="#b36">(Wei et al, 2017b)</ref>. Comparing with it, we now further introduce the multiple layer ensemble strategy for improving co-localization performance, provide DDT augmentation for handling web images, apply the proposed method on webly-supervised learning tasks (i.e., both recognition and detection), and supply our DDT based webly object detection dataset.</p><p>The remainder of the paper is organized as follows. In Sec. 2, we briefly review related literature of CNN model reuse, image co-localization and webly-supervised learning. In Sec. 3, we introduce our proposed method (DDT and its variant DDT + ). Sec. 4 reports the image co-localization results and the results of webly-supervised learning tasks. We conclude the paper in Sec. 5 finally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review three lines of related work: model reuse of CNNs, research on image co-localization and webly-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN Model Reuse</head><p>Reusability has been emphasized by <ref type="bibr" target="#b41">(Zhou, 2016)</ref> as a crucial characteristic of the new concept of learnware. It would be ideal if models can be reused in scenarios that are very different from their original training scenarios. Particularly, with the breakthrough in image classification using Convolutional Neural Networks (CNN), pre-trained CNN models trained for one task (e.g., recognition) have also been applied to domains different from their original purposes (e.g., for describing texture <ref type="bibr" target="#b2">(Cimpoi et al, 2016)</ref> or finding object proposals <ref type="bibr" target="#b8">(Ghodrati et al, 2015)</ref>). However, for such adaptations of pre-trained models, they still require further annotations in the new domain (e.g., image labels). While, DDT deals with the image co-localization problem in an unsupervised setting.</p><p>Coincidentally, several recent works also shed lights on CNN pre-trained model reuse in the unsupervised setting, e.g., SCDA (Selective Convolutional Descriptor Aggregation) <ref type="bibr" target="#b35">(Wei et al, 2017a)</ref>. SCDA is proposed for handling the fine-grained image retrieval task, where it uses pre-trained models (from ImageNet) to locate main objects in fine-grained images. It is the most related work to ours, even though SCDA is not for image colocalization. Different from our DDT, SCDA assumes only an object of interest in each image, and meanwhile objects from other categories does not exist. Thus, SCDA locates the object using cues from this single image assumption. Clearly, it can not work well for images containing diverse objects (cf. <ref type="table" target="#tab_1">Table 2 and Table 3)</ref>, and also can not handle data noise (cf. Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Co-Localization</head><p>Image co-localization, a.k.a. unsupervised object discovery <ref type="bibr" target="#b1">(Cho et al, 2015;</ref><ref type="bibr" target="#b33">Wang et al, 2015)</ref>, is a fundamental problem in computer vision, where it needs to discover the common object emerging in only positive sets of example images (without any negative examples or further supervisions). Image co-localization shares some similarities with image co-segmentation <ref type="bibr" target="#b40">(Zhao and Fu, 2015;</ref><ref type="bibr" target="#b15">Kim et al, 2011;</ref><ref type="bibr" target="#b13">Joulin et al, 2012)</ref>. Instead of generating a precise segmentation of the related objects in each image, co-localization methods aim to return a bounding box around the object. Moreover, it also allows us to extract rich features from within the boxes to compare across images, which has shown to be very helpful for detection .</p><p>Additionally, co-localization is also related to weakly supervised object localization (WSOL) <ref type="bibr" target="#b0">Bilen et al, 2015;</ref><ref type="bibr" target="#b32">Wang et al, 2014;</ref><ref type="bibr" target="#b28">Siva and Xiang, 2011)</ref>. But the key difference between them is that WSOL requires manually-labeled negative images whereas co-localization does not. Thus, WSOL methods could achieve better localization performance than co-localization methods. However, our proposed methods perform comparably with state-of-the-art WSOL methods and even outperform them (cf. <ref type="table" target="#tab_3">Table 4</ref>).</p><p>In the literature, some representative co-localization methods are based on low-level visual cues and optimization algorithms. <ref type="bibr" target="#b30">Tang et al (2014)</ref> formulated colocalization as a boolean constrained quadratic program which can be relaxed to a convex problem. Then, it was further accelerated by the Frank-Wolfe algorithm . After that, <ref type="bibr" target="#b1">Cho et al (2015)</ref> proposed a Probabilistic Hough Matching algorithm to match object proposals across images and then dominant objects are localized by selecting proposals based on matching scores.</p><p>Recently, there also emerge several co-localization methods based on pre-trained deep convolutional models, e.g., <ref type="bibr" target="#b16">Li et al (2016)</ref>; <ref type="bibr" target="#b32">Wang et al (2014)</ref>. Unfortunately, these methods just treated pre-trained models as simple feature extractors to extract the fully connected representations, which did not sufficiently mine the treasures beneath the convolutional layers (i.e., leveraging the original correlations between deep descriptors among convolutional layers). Moreover, these methods also require object proposals as a part of their object discovery, which not only made them highly depend on the quality of object proposals, but may lead to huge computational costs. In addition, almost all the previous co-localization methods can not handle noisy data, except for .</p><p>Comparing with previous works, our DDT is unsupervised, without utilizing bounding boxes, additional image labels or redundant object proposals. Images only need one forward run through a pre-trained model. Then, efficient deep descriptor transforming is employed for obtaining the category-consistent image regions. DDT is very easy to implement, and surprisingly has good generalization ability and robustness. Furthermore, DDT can be used a valid data augmentation tool for handling noisy but free web images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Webly-Supervised Learning</head><p>Recent development of deep CNNs has led to great success in a variety of computer vision tasks. This success is largely driven by the availability of large scale wellannotated image datasets, e.g., ImageNet <ref type="bibr" target="#b24">(Russakovsky et al, 2015)</ref>, MS COCO <ref type="bibr" target="#b17">(Lin et al, 2014)</ref> and PASCAL VOC <ref type="bibr" target="#b5">(Everingham et al, 2015)</ref>. However, annotating a massive number of images is extremely labor-intensive and costly. To reduce the annotation labor costs, an alternative approach is to obtain the image annotations directly from the image search engine from the Internet, e.g., Google or Bing.</p><p>However, the annotations of web images returned by a search engine will inevitably be noisy since the query keywords may not be consistent with the visual content of target images. Thus, webly-supervised learning methods are proposed for overcoming this issue.</p><p>There are two main branches of webly-supervised learning. The first branch attempts to boost existing object recognition task performance using web resources <ref type="bibr" target="#b42">(Zhuang et al, 2017;</ref><ref type="bibr" target="#b19">Papandreou et al, 2015;</ref><ref type="bibr" target="#b36">Xiao et al, 2015)</ref>. Some work was implemented as semi-supervised frameworks by first generating a small group of labeled seed images and then enlarging the dataset from these seeds via web data, e.g., <ref type="bibr" target="#b19">Papandreou et al (2015)</ref>; <ref type="bibr" target="#b36">Xiao et al (2015)</ref>. In very recently, <ref type="bibr" target="#b42">Zhuang et al (2017)</ref> proposed a two-level attention framework for dealing with webly-supervised classification, which achieves a new state-of-the-art. Specifically, they not only used a highlevel attention focusing on a group of images for filtering out noisy images, but also employed a low-level attention for capturing the discriminative image regions on the single image level</p><p>The second branch is learning visual concepts directly from the web, e.g., <ref type="bibr" target="#b6">Fergus et al (2014)</ref>; <ref type="bibr" target="#b34">Wang et al (2008)</ref>. Methods belonging to this category usually collected a large image pool from image search engines and then performed a filtering operation to remove noise and discover visual concepts. Our strategy for handling web data based on DDT naturally falls into the second category. In practice, since DDT could (1) recognize noisy images and also (2) supply bounding boxes of objects, we leverage the first usage of DDT to handle webly-supervised classification (cf. <ref type="table" target="#tab_5">Table 6 and  Table 7</ref>), and leverage both two usages to deal with webly-supervised detection (cf. <ref type="table" target="#tab_7">Table 8</ref> and <ref type="table">Table 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>In this section, we propose the Deep Descriptor Transforming (DDT) method. Firstly, we introduce notations used in this paper. Then, we present the DDT process followed by discussions and analyses. Finally, in order to further improve the image co-localization performance, the multiple layer ensemble strategy is utilized in DDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>The following notation is used in the rest of this paper. The term "feature map" indicates the convolution results of one channel; the term "activations" indicates feature maps of all channels in a convolution layer; and the term "descriptor" indicates the d-dimensional component vector of activations.</p><p>Given an input image I of size H ?W , the activations of a convolution layer are formulated as an order-3 tensor T with h ? w ? d elements. T can be considered as having h ? w cells and each cell contains one ddimensional deep descriptor. For the n-th image in the image set, we denote its corresponding deep descriptors as</p><formula xml:id="formula_0">X n = x n (i,j) ? R d , where (i, j) is a particular cell (i ? {1, . . . , h} , j ? {1, . . . , w}) and n ? {1, . . . , N }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCDA Recap</head><p>Since SCDA (Selective Convolutional Descriptor Aggregation) <ref type="bibr" target="#b35">(Wei et al, 2017a)</ref> is the most related work to ours, we hereby present a recap of this method. SCDA is proposed for dealing with the fine-grained image retrieval problem. It employs pre-trained CNN models to select the meaningful deep descriptors by localizing the main object in fine-grained images unsupervisedly. In SCDA, it assumes that each image contains only one main object of interest and without other categories' objects. Thus, the object localization strategy is based on the activation tensor of a single image.</p><p>Concretely, for an image, the activation tensor is added up through the depth direction. Thus, the h?w?d 3-D tensor becomes a h ? w 2-D matrix, which is called the "aggregation map" in SCDA. Then, the mean valu? a of the aggregation map is regarded as the threshold for localizing the object. If the activation response in the position (i, j) of the aggregation map is larger than a, it indicates the object might appear in that position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Descriptor Transforming (DDT)</head><p>What distinguishes DDT from SCDA is that we can leverage the correlations beneath the whole image set, instead of a single image. Additionally, different from weakly supervised object localization, we do not have either image labels or negative image sets in WSOL, so that the information we can use is only from the pretrained models. Here, we transform the deep descriptors in convolutional layers to mine the hidden cues for colocalizing common objects.</p><p>Principal component analysis (PCA) <ref type="bibr" target="#b20">(Pearson, 1901</ref>) is a statistical procedure, which uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables (i.e., the principal components). This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to all the preceding components.</p><p>PCA is widely used in computer vision and machine learning for image denoising <ref type="bibr" target="#b11">(Jiao et al, 2017)</ref>, 3D object retrieval <ref type="bibr" target="#b25">(Sfikas et al, 2011)</ref>, statistical shape modeling , subspace learning <ref type="bibr" target="#b7">(Garg et al, 2013;</ref><ref type="bibr" target="#b4">De la Torre and Black, 2003)</ref>, and so on. Specifically, in this paper, we utilize PCA as projection directions for transforming these deep descriptors {x ? (i,j) } to evaluate their correlations. Then, on each projection direction, the corresponding principal component's values are treated as the cues for image colocalization, especially the first principal component. Thanks to the property of this kind of transforming, DDT is also able to handle data noise.</p><p>In DDT, for a set of N images containing objects from the same category, we first collect the corresponding convolutional descriptors (X 1 , . . . , X N ) from the last convolutional layer by feeding the images into a pre-trained CNN model. Then, the mean vector of all the descriptors is calculated by:</p><formula xml:id="formula_1">x = 1 K n i,j x n (i,j) ,<label>(1)</label></formula><p>where K = h ? w ? N . Note that, here we assume each image has the same number of deep descriptors (i.e., h ? w) for presentation clarity. Our proposed method, however, can handle input images with arbitrary resolutions. Then, after obtaining the covariance matrix:</p><formula xml:id="formula_2">Cov(x) = 1 K n i,j (x n (i,j) ?x)(x n (i,j) ?x) ,<label>(2)</label></formula><p>we can get the eigenvectors ? 1 , . . . , ? d of Cov(x) which correspond to the sorted eigenvalues ? 1 ? ? ? ? ? ? d ? 0.</p><p>As aforementioned, since the first principal component has the largest variance, we take the eigenvector ? 1 corresponding to the largest eigenvalue as the main projection direction. For the deep descriptor at a particular</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Finding the largest connected component</head><p>Require: The resized indicator matrix P 1 corresponding to an image I; 1: Transform P 1 into a binary mapP 1 ,</p><formula xml:id="formula_3">wherep 1 (i,j) = 1 if p 1 (i,j) &gt; 0 0 otherwise</formula><p>; 2: Select one pixel p inP 1 as the starting point; 3: while True do 4: Use a flood-fill algorithm to label all the pixels in the connected component containing p; 5:</p><p>if All the pixels are labeled then 6:</p><p>Break; 7:</p><p>end if 8:</p><p>Search for the next unlabeled pixel as p; 9: end while 10: Obtain the connectivity of the connected components, and their corresponding size (pixel numbers); 11: Select the connected componentP 1 c with the largest pixel number; 12: return The largest connected componentP 1 c .</p><p>position (i, j) of an image, its first principal component p 1 is calculated as follows:</p><formula xml:id="formula_4">p 1 (i,j) = ? 1 x (i,j) ?x .<label>(3)</label></formula><p>According to their spatial locations, all p 1 (i,j) from an image are formed into a 2-D matrix whose dimensions are h ? w. We call that matrix as indicator matrix:</p><formula xml:id="formula_5">P 1 = ? ? ? ? ? ? p 1 (1,1) p 1 (1,2) . . . p 1 (1,w) p 1 (2,1) p 1 (2,2) . . . p 1 (2,w) . . . . . . . . . . . . p 1 (h,1) p 1 (h,2) . . . p 1 (h,w) ? ? ? ? ? ? ? R h?w .<label>(4)</label></formula><p>P 1 contains positive (negative) values which can reflect the positive (negative) correlations of these deep descriptors. The larger the absolute value is, the higher the positive (negative) correlation will be. Because ? 1 is obtained through all N images in that image set, the positive correlation could indicate the common characteristic through N images. Specifically, in the object co-localization scenario, the corresponding positive correlation indicates indeed the common object inside these images.</p><p>Therefore, the value zero could be used as a natural threshold for dividing P 1 of one image into two parts: one part has positive values indicating the common object, and the other part has negative values presenting background or objects that rarely appear. Additionally, if P 1 of an image has no positive value, it indicates that no common object exists in that image, which can be used for detecting noisy images.</p><p>In practice, for localizing objects, P 1 is resized by the nearest interpolation, such that its size is the same</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Deep Descriptor Transforming (DDT)</head><p>Require: A set of N images containing the common object, and a pre-trained CNN model F ; 1: Feed these images with their original resolutions into F ; 2: Collect the corresponding convolutional descriptors X 1 , . . . , X N from the last convolutional layer of F ; 3: Calculate the mean vectorx of all the descriptors using Eq. 1; 4: Compute the covariance matrix Cov(x) of these deep descriptors based on Eq. 2; 5: Compute the eigenvectors ? 1 , . . . , ? d of Cov(x); 6: Select ? 1 with the largest eigenvalue as the main transforming direction; 7: repeat 8:</p><p>Calculate the indicator matrix P 1 for image I based on Eq. 3 and Eq. 4; 9:</p><p>Resize P 1 into its image's resolution by nearest interpolation; 10:</p><p>Collect the largest connected componentP 1 c of these positive regions of the resized P 1 by Algo. 1; 11:</p><p>Obtain the minimum rectangle bounding box coverin? P 1 c as the prediction; 12: until All the N images are done; 13: return The minimum rectangle bounding boxes.</p><p>as that of the input image. Since the nearest interpolation is the zero-order interpolation method, it will not change the signs of the numbers in P 1 . Thus, the resized P 1 can be used for localizing the common object according to the aforementioned principle with the natural threshold (i.e., the value zero). Meanwhile, we employ the algorithm described in Algo. 1 to collect the largest connected component of the positive regions in the resized P 1 to remove several small noisy positive parts. Then, the minimum rectangle bounding box which contains the largest connected component of positive regions is returned as our object co-localization prediction for each image. The whole procedure of the proposed DDT method is shown in Algo. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions and Analyses</head><p>In this section, we investigate the effectiveness of DDT by comparing with SCDA.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the object localization regions of SCDA and DDT are highlighted in red. Because SCDA only considers the information from a single image, for example, in <ref type="figure" target="#fig_0">Fig. 2</ref> (2), "bike", "person" and even "guide-board" are all detected as main objects. Similar observations could be found in <ref type="figure" target="#fig_0">Fig. 2</ref> (5), (13), (17), (18), etc.</p><p>Furthermore, we normalize the values (all positive) of the aggregation map of SCDA into the scale of [0, 1], and calculate the mean value (which is taken as the object localization threshold in SCDA). The histogram of the normalized values in aggregation map is also shown in the corresponding sub-figure in <ref type="figure" target="#fig_0">Fig. 2</ref>. The red vertical line corresponds to the threshold. We can find that, beyond the threshold, there are still many values. It gives an explanation about why SCDA highlights more regions.</p><p>Whilst, for DDT, it leverages the whole image set to transform these deep descriptors into P 1 . Thus, for the bicycle class (cf. <ref type="figure" target="#fig_0">Fig. 2 (2)</ref>), DDT can accurately locate the "bicycle" object. The histogram of DDT is also drawn. But, P 1 has both positive and negative values. We normalize P 1 into the [?1, 1] scale this time. Apparently, few values are larger than the DDT threshold (i.e., the value zero). More importantly, many values are close to ?1 which indicates the strong negative correlation. This observation validates the effectiveness of DDT in image co-localization. As another example shown in <ref type="figure" target="#fig_0">Fig. 2 (11)</ref>, SCDA even wrongly locates "person" in the image belonging to the diningtable class. While, DDT can correctly and accurately locate the "diningtable" image region. More examples are presented in <ref type="figure" target="#fig_0">Fig. 2</ref>. In that figure, some failure cases can be also found, e.g., the chair class in <ref type="figure" target="#fig_0">Fig. 2 (9)</ref>.</p><p>In addition, the normalized P 1 can be also used as localization probability scores. Combining it with conditional random filed techniques might produce more accurate object boundaries. Thus, DDT can be modified slightly in that way, and then perform the cosegmentation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multiple Layer Ensemble</head><p>As is well known, CNNs are composed of multiple processing layers to learn representations of images with multiple levels of abstraction. Different layers will learn different level visual information <ref type="bibr" target="#b37">(Zeiler and Fergus, 2014)</ref>. Lower layers have more general representations (e.g., textures and shapes), and they can capture more detailed visual cues. By contrast, the learned representations of deeper layers contain more semantic information (i.e., high-level concepts). Thus, deeper layers are good at abstraction, but they lack visual details. Apparently, lower layer and deeper layer are complementary with each other. Based on this, several previous work, e.g., <ref type="bibr" target="#b9">Hariharan et al (2015)</ref>; <ref type="bibr" target="#b18">Long et al (2015)</ref>, aggregate the information of multiple layers to boost the final performance on their computer vision tasks.</p><p>Inspired by them, we also incorporate the lower convolutional layer in pre-trained CNNs to supply finer detailed information for object co-localization, which is named as DDT + .</p><p>Concretely, as aforementioned in Algo. 2, we can obtainP 1 c of the resize P 1 for each image from the last  <ref type="figure" target="#fig_1">Fig. 3</ref>. In DDT + , beyond that, those deep descriptors from the previous convolutional layer before the last one are also used for generating its corresponding resized P 1 , which is notated as P 1 prev . For P 1 prev , we directly transform it into a binary mapP 1 prev . In the middle column of <ref type="figure" target="#fig_1">Fig. 3</ref>, the red highlighted regions represent the co-localization results byP 1 prev . Since the activations from the previous convolutional layer are less related to the high-level semantic meaning than those from the last convolutional layer, other objects not belonging to the common object category are also being detected. However, the localization boundaries are much finer thanP 1 c . Therefore, we combineP 1 c andP 1 prev together to obtain the final co-localization prediction as follows:</p><formula xml:id="formula_6">P 1 c ?P 1 prev .<label>(5)</label></formula><p>As shown in the last column of <ref type="figure" target="#fig_1">Fig. 3</ref>, the co-localization visualization results of DDT + are better than the results of DDT, especially for the bottle class. In addition, from the quantitative perspective, DDT + will bring on average 1.5% improvements on image co-localization (cf. <ref type="table" target="#tab_1">Table 2</ref>, <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_4">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the evaluation metric and datasets used in image co-localization. Then, we compare the empirical results of our DDT and DDT + with other state-of-the-arts on these datasets. The computational cost is reported too. Moreover, the results in Sec. 4.4 and Sec. 4.5 illustrate the generalization ability and robustness of the proposed method. Furthermore, we will discuss the ability of DDT to utilize web data as valid augmentation for improving the accuracy of traditional image recognition and object detection tasks. Finally, the further study in Sec. 4.7 reveals DDT might deal with part-based image co-localization, which is a novel and challenging problem.</p><p>In our experiments, the images keep the original image resolutions. For the pre-trained deep model, the publicly available VGG-19 model <ref type="bibr" target="#b27">(Simonyan and Zisserman, 2015)</ref> is employed to perform DDT by extracting deep convolution descriptors from the last convolution layer (i.e., the relu 5 4 layer) and employed to perform DDT + by using both the last convolution layer (i.e., the relu 5 4 layer) and its previous layer (i.e., the relu 5 3 layer). We use the open-source library MatConvNet <ref type="bibr" target="#b31">(Vedaldi and Lenc, 2015)</ref> for conducting experiments. All the experiments are run on a computer with Intel Xeon E5-2660 v3, 500G main memory, and a K80 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metric and Datasets</head><p>Following previous image co-localization works <ref type="bibr" target="#b16">(Li et al, 2016;</ref><ref type="bibr" target="#b1">Cho et al, 2015;</ref><ref type="bibr" target="#b30">Tang et al, 2014)</ref>, we take the correct localization (CorLoc) metric for evaluating the proposed method. CorLoc is defined as the percentage of images correctly localized according to the PASCALcriterion <ref type="bibr" target="#b5">(Everingham et al, 2015)</ref>:</p><formula xml:id="formula_7">area(B p ? B gt ) area(B p ? B gt ) &gt; 0.5 ,<label>(6)</label></formula><p>where B p is the predicted bounding box and B gt is the ground-truth bounding box. All CorLoc results are reported in percentages. Our experiments are conducted on four challenging datasets commonly used in image co-localization, i.e., the Object Discovery dataset <ref type="bibr" target="#b23">(Rubinstein et al, 2013)</ref>, the PASCAL VOC 2007 /VOC 2012 dataset <ref type="bibr" target="#b5">(Everingham et al, 2015)</ref> and the ImageNet Subsets dataset <ref type="bibr" target="#b16">(Li et al, 2016)</ref>.</p><p>For experiments on the PASCAL VOC datasets, we follow <ref type="bibr" target="#b1">Cho et al (2015)</ref>; <ref type="bibr" target="#b16">Li et al (2016)</ref>; <ref type="bibr" target="#b14">Joulin et al (2014)</ref> to use all images in the trainval set (excluding images that only contain object instances annotated as difficult or truncated ). For Object Discovery, we use the 100-image subset following <ref type="bibr" target="#b23">Rubinstein et al (2013)</ref>; <ref type="bibr" target="#b1">Cho et al (2015)</ref> in order to make an appropriate comparison with other methods.</p><p>In addition, Object Discovery has 18%, 11% and 7% noisy images in the Airplane, Car and Horse categories, respectively. These noisy images contain no object belonging to their category, as the third image shown in <ref type="figure">Fig. 1</ref>. Particularly, in Sec. 4.5, we quantitatively measure the ability of our proposed DDT to identify these noisy images.</p><p>To further investigate the generalization ability of DDT, ImageNet Subsets <ref type="bibr" target="#b16">(Li et al, 2016)</ref> are used, which contain six subsets/categories. These subsets are heldout categories from the 1000-label ILSVRC classification <ref type="bibr" target="#b24">(Russakovsky et al, 2015)</ref>. That is to say, these subsets are "unseen" by pre-trained CNN models. Experimental results in Sec. 4.4 show that our proposed methods is insensitive to the object category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-Arts</head><p>In this section, we compare the image co-localization performance of our methods with state-of-the-art methods including both image co-localization and weakly supervised object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparisons to Image Co-Localization Methods</head><p>We first compare the results of DDT to state-of-the-arts (including SCDA) on Object Discovery in <ref type="table" target="#tab_0">Table 1</ref>. For SCDA, we also use VGG-19 to extract the convolution descriptors and perform experiments. As shown in that table, DDT outperforms other methods by about 4% in the mean CorLoc metric. Especially for the airplane class, it is about 10% higher than that of <ref type="bibr" target="#b1">Cho et al (2015)</ref>.</p><p>In addition, note that the images of each category in this dataset contain only one object, thus, SCDA can perform well. But, our DDT + gets a slightly lower CorLoc score than DDT, which is an exception in all the image colocalization datasets. In fact, for car and horse of the Object Discovery dataset, DDT + only returns one more wrong prediction than DDT for each category. For PASCAL VOC 2007 and 2012, these datasets contain diverse objects per image, which is more challenging than Object Discovery. The comparisons of the CorLoc metric on these two datasets are reported in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>, respectively. It is clear that on average our DDT and DDT + outperform the previous state-ofthe-arts (based on deep learning) by a large margin on both datasets. Moreover, our methods work well on localizing small common objects, e.g., "bottle" and "chair". In addition, because most images of these datasets have multiple objects, which do not obey SCDA's assumption, SCDA performs poorly in the complicated environment. For fair comparisons, we also use VGG-19 to extract the fully connected representations of the object proposals in <ref type="bibr" target="#b16">(Li et al, 2016)</ref>, and then perform the remaining processes of their method (the source codes are provided by the authors). As aforementioned, due to the high dependence on the quality of object proposals, their mean CorLoc metric of VGG-19 is 41.9% and 45.6% on VOC 2007 and 2012, respectively. The improvements are limited, and the performance is still significantly worse than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparisons to Weakly Supervised Localization Methods</head><p>To further verify the effectiveness of our methods, we also compare DDT and DDT + with some state-of-theart methods for weakly supervised object localization. <ref type="table" target="#tab_3">Table 4</ref> illustrates these empirical results on VOC 2007. Particularly, DDT achieves 46.9% on average which is higher than most WSOL methods in the literature. DDT + achieves 48.5% on average, and it even performs better than the state-of-the-art in WSOL (i.e., <ref type="bibr" target="#b32">Wang et al (2014)</ref>) which is also a deep learning based approach. Meanwhile, note that our methods do not use any negative data for co-localization. Moreover, our methods could handle noisy data (cf. Sec. 4.5). But, existing WSOL methods are not designed to deal with noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Costs of DDT/DDT +</head><p>Here, we take the total 171 images in the aeroplane category of VOC 2007 as examples to report the computational costs. The average image resolution of the 171 images is 350?498. The computational time of DDT has two main components: one is for feature extraction, the other is for deep descriptor transforming (cf. Algo. 2). Because we just need the first principal component, the transforming time on all the 120,941 descriptors of 512-d is only 5.7 seconds. The average descriptor extraction time is 0.18 second/image on GPU and 0.86 second/image on CPU, respectively. For DDT + , it has the same deep descriptor extraction time. Although it needs descriptors from two convolutional layers, it only requires one time feed-forward processing. The deep descriptor transforming time of DDT + is only 11.9 seconds for these 171 images. These numbers above could ensure the efficiency of the proposed methods in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unseen Classes Apart from ImageNet</head><p>In order to justify the generalization ability of the proposed methods, we also conduct experiments on some images (of six subsets) disjoint with the images from Im-ageNet. Note that, the six categories (i.e., "chipmunk", "rhino", "stoat", "racoon", "rake" and "wheelchair") of these images are unseen by pre-trained models. The six subsets were provided in <ref type="bibr" target="#b16">(Li et al, 2016)</ref>. <ref type="table" target="#tab_4">Table 5</ref> presents the CorLoc metric on these subsets. Our DDT  <ref type="formula" target="#formula_1">(2015)</ref>     (69.1% on average) and DDT + (70.4% on average) still significantly outperform other methods on all categories, especially for some difficult objects categories, e.g., rake and wheelchair. In addition, the mean CorLoc metric of <ref type="bibr" target="#b16">(Li et al, 2016</ref>) based on VGG-19 is only 51.6% on this dataset.</p><p>Furthermore, in <ref type="figure" target="#fig_3">Fig. 4</ref>, several successful predictions by DDT and also some failure cases on this dataset are provided. In particular, for "rake" ("wheelchair"), even though a large portion of images in these two categories contain both people and rakes (wheelchairs), our DDT could still accurately locate the common object in all the images, i.e., rakes (wheelchairs), and ignore people. This observation validates the effectiveness (especially for the high CorLoc metric on rake and wheelchair ) of our method from the qualitative perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detecting Noisy Images</head><p>In this section, we quantitatively present the ability of the proposed DDT method to identify noisy images. As aforementioned, in Object Discovery, there are 18%, 11% and 7% noisy images in the corresponding categories. In our DDT, the number of positive values in P 1 can be interpreted as a detection score. The lower the number is, the higher the probability of noisy images will be. In particular, no positive value at all in P 1 presents the image as definitely a noisy image. For each category in that dataset, the ROC curve is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, which measures how the methods correctly detect noisy images. In the literature, only the method in  (i.e., the Image-Box model in that paper) could solve image co-localization with noisy data. From these figures, it is apparent to see that, in image co-localization, our DDT has significantly better performance in detecting noisy images than Image-Box (whose noisy detection results are obtained by re-running the publicly available code released by the authors). Meanwhile, our mean  CorLoc metric without noise is about 12% higher than theirs on Object Discovery, cf. <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">DDT Augmentation based on Web Images</head><p>As validated by previous experiments, DDT can accurately detect noisy images and meanwhile supply object bounding boxes of images (except for noisy images). Therefore, we can use DDT to process web images. In this section, we report the results of both image classification and object detection when using DDT as a tool for generating valid external data sources from free but noisy web data. This DDT based strategy is denoted as DDT augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Webly-Supervised Classification</head><p>For web based image classification, we compare DDT augmentation with the current state-of-the-art weblysupervised classification method proposed by <ref type="bibr" target="#b42">Zhuang et al (2017)</ref>. As discussed in the related work, <ref type="bibr" target="#b42">Zhuang et al (2017)</ref> proposed a group attention framework for handling web data. In their method, it employed two level attentions: the first level is designed as the group attention for filtering out noise, and the second level attention is based on the single image for capturing discriminative regions of each image.</p><p>In the experiments, we test the methods on the WebCars and WebImageNet datasets which are also proposed by <ref type="bibr" target="#b42">Zhuang et al (2017)</ref>. In WebCars, there are 213,072 car images of totally 431 car model categories collected from web. In WebImageNet, <ref type="bibr" target="#b42">Zhuang et al (2017)</ref> used 100 sub-categories of the original ImageNet as the categories of their WebImageNet dataset. There are 61,639 images belonging to the 100 sub-categories from web in total.</p><p>In our DDT augmentation, as what we do in Sec. 4.5, we first use DDT to obtain the number of positive values in P 1 as the detection score for each image in every category. Here, we divide the detection score by the total number of values in P 1 as the noise rate which is in the range of [0, 1]. The more the noise rate is close to zero, the higher the probability of noisy images will be. In the following, we conduct experiments with two thresholds (i.e., 0 or 0.1) with respect to the noise rate. If the noise rate of an image equals to or is smaller than the threshold, that image will be regarded as a noisy image. Then, we remove it from the original webly dataset. After doing the above processing for every category, we can obtain a relatively clean training dataset. Finally, we train deep CNN networks on that clean dataset. The other specific experimental settings of these two webly datasets follow <ref type="bibr" target="#b42">Zhuang et al (2017)</ref>.</p><p>Two kinds of deep CNN networks are conducted as the test bed for evaluating the classification performance on both two webly datasets:</p><p>-"GAP" represents the CNN model with Global Average</p><p>Pooling as its last layer before the classification layer (i.e., fc+sigmoid), which is commonly used for the image classification task, e.g., <ref type="bibr" target="#b29">Szegedy et al (2015)</ref> and <ref type="bibr" target="#b10">He et al (2016)</ref>. -"Attention" represents the CNN model with the attention mechanism on the single image level. Because the method proposed in <ref type="bibr" target="#b42">Zhuang et al (2017)</ref> is equipped with the single image attention strategy, we also compare our method based on this baseline model for fair comparisons.</p><p>The quantitative comparisons of our DDT augmentation with <ref type="bibr" target="#b42">Zhuang et al (2017)</ref> are shown in <ref type="table" target="#tab_5">Table 6</ref> and <ref type="table" target="#tab_6">Table 7</ref>. In these tables, for example, "DDT ? GAP" denotes that we first deploy DDT augmentation and then use the GAP model to conduct classification. As shown in these two tables, for both two base models (i.e., "GAP" and "Attention"), our DDT augmentation with 0.1 threshold performs better than DDT augmentation with 0 threshold, which is reasonable. Because in many cases, the noisy images still contains several related concept regions, these (small) regions might be detected as a part of common objects. Therefore, if we set the threshold as 0.1, this kind of noisy images will be omitted. It will bring more satisfactory classification accuracy. Several detected noisy images by DDT of WebCars are listed in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>Comparing with the state-of-the-art (i.e., Zhuang et al <ref type="formula" target="#formula_1">(2017)</ref>), our DDT augmentation with 0.1 threshold outperforms it and the GAP baseline apparently, which validate the generalization ability and the effectiveness of the proposed DDT in real-life computer vision tasks, i.e., DDT augmentation in webly-supervised classification. Meanwhile, our DDT method is easy to implement and has low computational cost, which ensures its scalability and usability in the real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Webly-Supervised Detection</head><p>For web based object detection, we first collect an external dataset from the Internet by Google image search engine, named WebVOC, using the categories of the   PASCAL VOC dataset <ref type="bibr" target="#b5">(Everingham et al, 2015)</ref>. In total, we collect 12,776 noisy web images, which has a similar scale as the original PASCAL VOC dataset.</p><p>As the results shown in webly-supervised classification, DDT with 0.1 threshold could be the optimal option for webly noisy images. Firstly, we also use DDT with 0.1 threshold to remove the noisy images for the images belonging to 20 categories in WebVOC. Then, 10,081 images are remaining as valid images. Furthermore, DDT are used to automatically generate the corresponding object bounding box for each image. The generated bounding boxes by DDT are regarded as the object "ground truth" bounding boxes for our WebVOC detection dataset. Several random samples of our WebVOC dataset with the corresponding DDT generating bounding boxes are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. After that, a state-of-the-art object detection method, i.e., Faster RCNN <ref type="bibr" target="#b21">(Ren et al, 2017)</ref>, is trained as the   <ref type="formula" target="#formula_1">(2017)</ref>, Faster RCNN is trained on "07+12" and "COCO+07+12". "07+12" presents the training data is the union set of VOC 2007 trainval and VOC 2012 trainval. "COCO+07+12" denotes that except for VOC 2007 and VOC 2012, the COCO trainval set is also used for training. "DDT+07+12" is our proposal, which uses DDT to process the web images as aforementioned and then combines the processed web data with "07+12" as the final training data. <ref type="table" target="#tab_7">Table 8</ref>, our proposal outperforms "07+12" by 2.8% on VOC 2007, which is a large margin on the object detection task. In addition, the detection mAP of DDT augmentation is 4% better than "07++12" on the VOC 2012 test set, cf. <ref type="table">Table 9</ref>. Note that, our DDT augmentation only depends on 10k images of 20 object categories, in especial, these images are automatically labeled by the proposed DDT method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>On the other hand, our mAP is comparable with the mAP training on "COCO+07+12" in <ref type="table" target="#tab_7">Table 8</ref> (or "COCO+07++12" in <ref type="table">Table 9</ref>). Here, we would like to point out that the COCO trainval set contains 120k human labeled images involving 80 object categories, which requires much more human labors, capital and time costs than our DDT augmentation. Therefore, these detection results could validate the effectiveness of DDT augmentation on the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Further Study</head><p>In the above, DDT only utilizes the information of the first principal components, i.e., P 1 . How about others, e.g., the second principal components P 2 ? In <ref type="figure" target="#fig_7">Fig. 8</ref>, we show four images from each of three categories (i.e., dogs, airplanes and trains) in PASCAL VOC with the visualization of their P 1 and P 2 . Through these figures, it is apparently to find P 1 can locate the whole common object. However, P 2 interestingly separates a part region from the main object region, e.g., the head region from the torso region for dogs, the wheel and engine regions from the fuselage region for airplanes, and the wheel region from the train body region for trains. Meanwhile, these two meaningful regions can be easily distinguished from the background. These observations inspire us to <ref type="table">Table 9</ref>: Comparisons of detection results on the VOC 2012 test set. Note that, "07++12" presents the training data is the union set of VOC 2007 trainval+test and VOC 2012 trainval. "COCO" denotes that the COCO trainval set is used for training. "DDT" denotes that the webly data processed by DDT augmentation is used for training.  use DDT for the more challenging part-based image colocalization task in the future, which is never touched before in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Pre-trained models are widely used in diverse applications in computer vision. However, the treasures beneath pre-trained models are not exploited sufficiently.</p><p>In this paper, we proposed Deep Descriptor Transforming (DDT) for image co-localization. DDT indeed re-vealed another reusability of deep pre-trained networks, i.e., convolutional activations/descriptors can play a role as a common object detector. It offered further understanding and insights about CNNs. Besides, our proposed DDT method is easy to implement, and it achieved great image co-localization performance. Moreover, the generalization ability and robustness of DDT ensure its effectiveness and powerful reusability in realworld applications. Thus, DDT can be used to handle free but noisy web images and further generate valid data sources for improving both recognition and detection accuracy. Additionally, DDT also has the potential ability in the applications of video-based unsupervised object discovery. Furthermore, interesting observations in Sec. 4.7 make the more challenging but intriguing part-based image co-localization problem be a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples of twenty categories from the PASCAL VOC 2007 dataset (Everingham et al, 2015). The first column of each sub-figure is produced by SCDA, the second column is by our DDT. The red vertical lines in the histogram plots indicate the corresponding thresholds for localizing objects. The selected regions in images are highlighted in red. (Best viewed in color and zoomed in.) convolutional layer by our DDT. Several visualization examples ofP 1 c are shown in the first column of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples from five randomly sampled categories of PASCAL VOC 2007<ref type="bibr" target="#b5">(Everingham et al, 2015)</ref>. The red highlighted regions in images are detected as containing common objects by our proposed methods. In each sub-figure, the first column presents the prediction by our DDT (cf. Algo. 2). The middle column shows the DDT's result based on the lower convolutional layer. The last column are the predicted results by our DDT + . (Best viewed in color and zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>50.3 42.8 30.0 18.5 4.0 62.3 64.5 42.5 8.6 49.0 12.2 44.0 64.1 57.2 15.3 9.4 30.9 34.0 61.6 31.5 36.6 Li et al (2016) 73.1 45.0 43.4 27.7 6.8 53.3 58.3 45.0 6.2 48.0 14.3 47.3 69.4 66.8 24.3 12.8 51.5 25.5 65.2 16.8 40.0 Our DDT 67.3 63.3 61.3 22.7 8.5 64.8 57.0 80.5 9.4 49.0 22.5 72.6 73.8 69.0 7.2 15.0 35.3 54.7 75.0 29.4 46.9 Our DDT + 71.4 65.664.6 25.5 8.5 64.8 61.3 80.5 10.3 49.0 26.5 72.6 75.2 69.0 9.9 12.2 39.7 55.775.032.5 48.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Random samples of predicted object co-localization bounding box on ImageNet Subsets. Each sub-figure contains three successful predictions and one failure case. In these images, the red rectangle is the prediction by DDT, and the yellow dashed rectangle is the ground truth bounding box. In the successful predictions, the yellow rectangles are omitted since they are exactly the same as the red predictions. (Best viewed in color and zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>ROC curves illustrating the effectiveness of our DDT at identifying noisy images on the Object Discovery dataset. The curves in red line are the ROC curves of DDT. The curves in blue dashed line present the method in<ref type="bibr" target="#b30">Tang et al (2014)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Examples of noisy images in the WebCars dataset recognized by DDT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Examples of our WebVOC detection dataset. The red bounding boxes in these figures are automatically labeled by the proposed DDT method. (Best viewed in color and zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Four images belonging to each of three categories of VOC 2007 with visualization of their indicator matrices P 1 and P 2 . In visualization figures, warm colors indicate positive values, and cool colors present negative. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of CorLoc on Object Discovery.</figDesc><table><row><cell>Methods</cell><cell cols="2">airplane car</cell><cell>horse Mean</cell></row><row><cell>Joulin et al (2010)</cell><cell>32.93</cell><cell cols="2">66.29 54.84 51.35</cell></row><row><cell>Joulin et al (2012)</cell><cell>57.32</cell><cell cols="2">64.04 52.69 58.02</cell></row><row><cell>Rubinstein et al (2013)</cell><cell>74.39</cell><cell cols="2">87.64 63.44 75.16</cell></row><row><cell>Tang et al (2014)</cell><cell>71.95</cell><cell cols="2">93.26 64.52 76.58</cell></row><row><cell>SCDA</cell><cell>87.80</cell><cell cols="2">86.52 75.37 83.20</cell></row><row><cell>Cho et al (2015)</cell><cell>82.93</cell><cell cols="2">94.38 75.27 84.19</cell></row><row><cell>Our DDT</cell><cell cols="3">91.46 95.51 77.42 88.13</cell></row><row><cell>Our DDT +</cell><cell cols="3">91.46 94.38 76.34 87.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of the CorLoc metric with state-of-the-art co-localization methods on VOC 2007. aero bike bird boat bottle bus car cat chair cow table dog horsembikepersonplantsheep sofa train tv Mean Joulin et al (2014) 32.8 17.3 20.9 18.2 4.5 26.9 32.7 41.0 5.8 29.1 34.5 31.6 26.1 40.4 17.9 11.8 25.0 27.5 35.6 12.1 24.6 SCDA 54.4 27.2 43.4 13.5 2.8 39.3 44.5 48.0 6.2 32.0 16.3 49.8 51.5 49.7 7.7 6.1 22.1 22.6 46.4 6.1 29.5 Cho et al</figDesc><table><row><cell>Methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of the CorLoc metric with state-of-the-art co-localization methods on VOC 2012.</figDesc><table><row><cell>Methods</cell><cell>aero bike bird boat bottle bus car cat chair cow table dog horsembikepersonplantsheep sofa train tv Mean</cell></row><row><cell>SCDA</cell><cell>60.8 41.7 38.6 21.8 7.4 67.6 38.8 57.4 16.0 34.0 23.9 53.8 47.3 54.8 7.9 9.9 25.3 23.2 50.2 10.1 34.5</cell></row><row><cell cols="2">Cho et al (2015) 57.0 41.2 36.0 26.9 5.0 81.1 54.6 50.9 18.2 54.0 31.2 44.9 61.8 48.0 13.0 11.7 51.4 45.3 64.6 39.2 41.8</cell></row><row><cell cols="2">Li et al (2016) 65.7 57.8 47.9 28.9 6.0 74.9 48.4 48.4 14.6 54.4 23.9 50.2 69.9 68.4 24.0 14.2 52.7 30.9 72.4 21.6 43.8</cell></row><row><cell>Our DDT</cell><cell>76.7 67.1 57.9 30.5 13.0 81.9 48.3 75.7 18.4 48.8 27.5 71.8 66.8 73.7 6.1 18.5 38.0 54.7 78.6 34.6 49.4</cell></row><row><cell>Our DDT +</cell><cell>77.967.761.833.8 14.2 82.5 53.0 75.2 18.9 53.5 28.3 73.8 68.7 77.5 8.4 17.6 40.8 55.378.6 35.0 51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of the CorLoc metric with weakly supervised object localization methods on VOC 2007. Note that, the " " in the "Neg." column indicates that these WSOL methods require access to a negative image set, whereas our DDT does not.</figDesc><table><row><cell>Methods</cell><cell>Neg. aero bike bird boat bottle bus car cat chair cow table dog horsembikepersonplantsheep sofa train tv Mean</cell></row><row><cell>Siva and Xiang (2011)</cell><cell>42.4 46.5 18.2 8.8 2.9 40.9 73.2 44.8 5.4 30.5 19.0 34.0 48.8 65.3 8.2 9.4 16.7 32.3 54.8 5.5 30.4</cell></row><row><cell>Shi et al (2013)</cell><cell>67.3 54.4 34.3 17.8 1.3 46.6 60.7 68.9 2.5 32.4 16.2 58.9 51.5 64.6 18.2 3.1 20.9 34.7 63.4 5.9 36.2</cell></row><row><cell>Cinbis et al (2015)</cell><cell>56.6 58.3 28.4 20.7 6.8 54.9 69.1 20.8 9.2 50.5 10.2 29.0 58.0 64.9 36.7 18.7 56.5 13.2 54.9 59.4 38.8</cell></row><row><cell>Wang et al (2015)</cell><cell>37.7 58.8 39.0 4.7 4.0 48.4 70.0 63.7 9.0 54.2 33.3 37.4 61.6 57.6 30.1 31.7 32.4 52.8 49.0 27.8 40.2</cell></row><row><cell>Bilen et al (2015)</cell><cell>66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5 60.0 19.8 39.2 41.7 30.1 50.2 44.1 43.7</cell></row><row><cell>Ren et al (2016)</cell><cell>79.2 56.9 46.0 12.2 15.7 58.4 71.4 48.6 7.2 69.9 16.7 47.4 44.2 75.5 41.2 39.6 47.4 32.2 49.8 18.6 43.9</cell></row><row><cell>Wang et al (2014)</cell><cell>80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 47.7</cell></row><row><cell>Our DDT</cell><cell>67.3 63.3 61.3 22.7 8.5 64.8 57.0 80.5 9.4 49.0 22.5 72.6 73.8 69.0 7.2 15.0 35.3 54.7 75.0 29.4 46.9</cell></row><row><cell>Our DDT +</cell><cell>71.4 65.664.625.5 8.5 64.8 61.3 80.5 10.3 49.0 26.5 72.6 75.2 69.0 9.9 12.2 39.7 55.775.0 32.5 48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of the CorLoc metric with stateof-the-arts on image sets disjoint with ImageNet.</figDesc><table><row><cell>Methods</cell><cell>chipm.rhinostoatracoon rake wheelc. Mean</cell></row><row><cell cols="2">Cho et al (2015) 26.6 81.8 44.2 30.1 8.3 35.3 37.7</cell></row><row><cell>SCDA</cell><cell>32.3 71.6 52.9 34.0 7.6 28.3 37.8</cell></row><row><cell>Li et al (2016)</cell><cell>44.9 81.8 67.3 41.8 14.5 39.3 48.3</cell></row><row><cell>Our DDT</cell><cell>70.3 93.2 80.8 71.8 30.3 68.2 69.1</cell></row><row><cell>Our DDT +</cell><cell>72.8 93.2 80.8 75.7 28.3 71.7 70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of webly-supervised classification on WebCars (Zhuang et al, 2017).</figDesc><table><row><cell>Methods</cell><cell>Strategy</cell><cell>Accuracy</cell></row><row><cell>Simple-CNN</cell><cell>GAP</cell><cell>66.86</cell></row><row><cell>Zhuang et al (2017)</cell><cell>Attention</cell><cell>76.58</cell></row><row><cell>Ours (thr=0)</cell><cell>DDT ? GAP</cell><cell>69.79</cell></row><row><cell>Ours (thr=0)</cell><cell>DDT ? Attention</cell><cell>76.18</cell></row><row><cell>Ours (thr=0.1)</cell><cell>DDT ? GAP</cell><cell>71.66</cell></row><row><cell>Ours (thr=0.1)</cell><cell>DDT ? Attention</cell><cell>78.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of webly-supervised classification on WebImageNet (Zhuang et al, 2017).</figDesc><table><row><cell>Methods</cell><cell>Strategy</cell><cell>Accuracy</cell></row><row><cell>Simple-CNN</cell><cell>GAP</cell><cell>58.81</cell></row><row><cell>Zhuang et al (2017)</cell><cell>Attention+Neg 1</cell><cell>71.24</cell></row><row><cell>Ours (thr=0)</cell><cell>DDT ? GAP</cell><cell>62.31</cell></row><row><cell>Ours (thr=0)</cell><cell>DDT ? Attention</cell><cell>69.50</cell></row><row><cell>Ours (thr=0.1)</cell><cell>DDT ? GAP</cell><cell>65.59</cell></row><row><cell>Ours (thr=0.1)</cell><cell>DDT ? Attention</cell><cell>73.06</cell></row><row><cell cols="3">1 In the experiments on WebImageNet of Zhuang et al (2017),</cell></row><row><cell cols="3">beyond attention, they also incorporated 5,000 negative class web</cell></row><row><cell cols="3">images for reducing noise. However, we do not require any negative</cell></row><row><cell></cell><cell>images.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of detection results on the VOC 2007 test set. Note that, "07+12" presents the training data is the union set of VOC 2007 trainval and VOC 2012 trainval. "COCO" denotes that the COCO trainval set is used for training. "DDT" denotes that the webly data processed by DDT augmentation is used for training. +07+12 84.3 82.077.768.9 65.7 88.188.488.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.Note that, the COCO trainval set contains 120k human labeled images involving 80 object categories. While, our DDT augmentation only depends on 10k images of 20 object categories, in especial, these images are automatically labeled by the proposed DDT method.base model on different training data to validate the effectiveness of DDT augmentation on the object detection task. For the test sets of detection, we employ the VOC 2007 and VOC 2012 test set and report the results inTable 8 and Table 9, respectively.</figDesc><table><row><cell>Data</cell><cell cols="2">aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP (%)</cell></row><row><cell>07+12</cell><cell>76.5 79.070.965.5 52.1 83.184.786.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6</cell><cell>73.2</cell></row><row><cell cols="2">COCO 1 6 78.9</cell><cell>78.8</cell></row><row><cell>DDT+07+12</cell><cell>77.6 82.277.264.9 61.2 85.487.288.6 58.2 82.6 69.7 85.9 87.0 78.9 78.5 46.3 76.6 73.5 82.5 75.1</cell><cell>76.0</cell></row></table><note>1For testing on VOC 2007, following Ren et al</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Data aerobikebirdboatbottle bus car cat chair cow table dog horsembikepersonplantsheepsofatrain tv mAP (%) 07++12 84.979.874.3 53.9 49.8 77.575.988.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4 COCO 1 +07++12 87.483.676.8 62.9 59.6 81.982.091.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9 DDT+07++12 86.581.976.2 63.4 55.4 80.880.189.7 51.6 78.6 56.2 88.8 84.8 85.5 82.6 50.6 78.1 64.1 85.6 68.1 74.4 1 Note that, the COCO trainval set contains 120k human labeled images involving 80 object categories. While, our DDT augmentation only depends on 10k images of 20 object categories, in especial, these images are automatically labeled by the proposed DDT method.</figDesc><table><row><cell>Image</cell></row><row><cell>P 1</cell></row><row><cell>P 2</cell></row><row><cell>(a) Dog</cell></row><row><cell>Image</cell></row><row><cell>P 1</cell></row><row><cell>P 2</cell></row><row><cell>(b) Airplane</cell></row><row><cell>Image</cell></row><row><cell>P 1</cell></row><row><cell>P 2</cell></row><row><cell>(c) Train</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors want to thank Yao Li and Bohan Zhuang for conducting part of experiments, and thank Chen-Wei Xie and Hong-Yu Zhou for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Partbased matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition, description, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename><forename type="middle">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sma</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ckl</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A visual category filter for Google images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer, Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepproposal: Hunting objects by cascading deep convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2578" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint image denoising and disparity estimation via stereo structure PCA and noise-tolerant cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rwh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision in press</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2984" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Fleet D, Pajdla T, Schiele B, Tuytelaars T</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed co-segmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image colocalization by mimicking a good detector&apos;s confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avd</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Leibe B, Matas J, Sebe N, Welling M</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Fleet D, Pajdla T, Schiele B, Tuytelaars T</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised large scale object localization with multiple instance learning and bag splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ROSy+: 3D object pose normalization based on PCA and reflective object symmetry with application in 3D object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="279" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian joint topic modelling for weakly supervised object localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2984" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1464" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Fleet D, Pajdla T, Schiele B, Tuytelaars T</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relaxed multipleinstance SVM with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1224" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Annotating images by mining image search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1919" to="1932" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zh ;</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
	<note>Proceedings of International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Fleet D, Pajdla T, Schiele B, Tuytelaars T</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PCA: Statistical shape modelling in shell space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wap</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1671" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging saliency detection to weakly supervised object detection based on self-paced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3538" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic single video segmentation with robust graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2219" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learnware: On the future of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="590" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attend in groups: A weakly-supervised deep learning framework for learning from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
