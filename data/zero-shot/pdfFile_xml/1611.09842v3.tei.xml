<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
							<email>rich.zhang@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">(BAIR) Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
							<email>isola@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">(BAIR) Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<email>efros@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">(BAIR) Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkeley</forename><forename type="middle">Ai</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">(BAIR) Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult taskpredicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve crosschannel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A goal of unsupervised learning is to model raw data without the use of labels, in a manner which produces a useful representation. By "useful" we mean a representation that should be easily adaptable for other tasks, unknown during training time. Unsupervised deep methods typically induce representations by training a network to solve an auxiliary or "pretext" task, such as the image reconstruction objective in a traditional autoencoder model, as shown on <ref type="figure" target="#fig_0">Figure 1</ref>(top). We instead force the network to solve complementary prediction tasks by adding a split in the architecture, shown in <ref type="figure" target="#fig_0">Figure 1</ref> (bottom), dramatically improving transfer performance.</p><p>Despite their popularity, autoencoders have actually not been shown to produce strong representations for transfer tasks in practice <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35]</ref>. Why is this? One reason might be the mechanism for forcing model abstraction. To prevent a trivial identity mapping from being learned, a bottleneck is typically built into the autoencoder representation. However, an inherent tension is at play: the smaller the bottleneck, the greater the forced abstraction, but the smaller the information content that can be expressed.</p><p>Instead of forcing abstraction through compression, via a bottleneck in the network architecture, recent work has explored withholding parts of the input during training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Autoencoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Data</head><p>Reconstructed Data The proposed split-brain autoencoder is composed of two disjoint sub-networks F 1 , F 2 , each trained to predict one data subset from another, changing the problem from reconstruction to prediction. The split-brain representation F is formed by concatenating the two sub-networks, and achieves strong transfer learning performance. The model is publicly available on https://richzhang.github. io/splitbrainauto. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref>. For example, Vincent et al. <ref type="bibr" target="#b43">[44]</ref> propose denoising autoencoders, trained to remove iid noise added to the input. Pathak et al. <ref type="bibr" target="#b34">[35]</ref> propose context encoders, which learn features by training to inpaint large, random contiguous blocks of pixels. Rather than dropping data in the spatial direction, several works have dropped data in the channel direction, e.g. predicting color channels from grayscale (the colorization task) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X " X</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split-Brain Autoencoder</head><p>Context encoders, while an improvement over autoencoders, demonstrate lower performance than competitors on large-scale semantic representation learning benchmarks <ref type="bibr" target="#b48">[49]</ref>. This may be due to several reasons. First, im-auxiliary domain input task type gap handicap Autoencoder <ref type="bibr" target="#b19">[20]</ref> reconstruction no no Denoising autoencoder <ref type="bibr" target="#b43">[44]</ref> reconstruction suffers no Context Encoder <ref type="bibr" target="#b34">[35]</ref> prediction no suffers Cross-Channel Encoder <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b27">28]</ref> prediction no suffers Split-Brain Autoencoder prediction no no <ref type="table">Table 1</ref>: Qualitative Comparison We summarize various qualitative aspects inherent in several representation learning techniques. Auxiliary task type: pretext task predicated on reconstruction or prediction. Domain gap: gap between the input data during unsupervised pre-training and testing time. Input handicap: input data is systematically dropped out during test time.</p><p>age synthesis tasks are known to be notoriously difficult to evaluate <ref type="bibr" target="#b35">[36]</ref> and the loss function used in <ref type="bibr" target="#b34">[35]</ref> may not properly capture inpainting quality. Second, the model is trained on images with missing chunks, but applied, at test time, to full images. This causes a "domain gap" between training and deployment. Third, it could simply be that the inpainting task in <ref type="bibr" target="#b34">[35]</ref> could be adequately solved without high-level reasoning, instead mostly just copying low and mid-level structure from the surround. On the other hand, colorization turns out to be a surprisingly effective pretext task for inducing strong feature representations <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b27">28]</ref>. Though colorization, like inpainting, is a synthesis task, the spatial correspondence between the input and output pairs may enable basic off-the-shelf loss functions to be effective. In addition, the systematic, rather than stochastic nature of the input corruption removes the pre-training and testing domain gap. Finally, while inpainting may admit reasoning mainly about textural structure, predicting accurate color, e.g., knowing to paint a schoolbus yellow, may more strictly require object-level reasoning and therefore induce stronger semantic representations. Colorization is an example of what we refer to as a crosschannel encoding objective, a task which directly predicts one subset of data channels from another.</p><p>In this work, we further explore the space of crosschannel encoders by systematically evaluating various channel translation problems and training objectives. Cross-channel encoders, however, face an inherent handicap: different channels of the input data are not treated equally, as part of the data is used for feature extraction and another as the prediction target. In the case of colorization, the network can only extract features from the grayscale image and is blind to color, leaving the color information unused. A qualitative comparison of the different methods, along with their inherent strengths and weaknesses, is summarized in <ref type="table">Table 1</ref>.</p><p>Might there be a way to take advantage of the underlying principle of cross-channel encoders, while being able to extract features from the entire input signal? We propose an architectural modification to the autoencoder paradigm: adding a single split in the network, resulting in two disjoint, concatenated, sub-networks. Each sub-network is trained as a cross-channel encoder, predicting one subset of channels of the input from the other. A variety of auxiliary cross-channel prediction tasks may be used, such as colorization and depth prediction. For example, on RGB images, one sub-network can solve the problem of colorization (predicting a and b channels from the L channel in Lab colorspace), and the other can perform the opposite (synthesizing L from a, b channels). In the RGB-D domain, one sub-network may predict depth from images, while the other predicts images from depth. The architectural change induces the same forced abstraction as observed in crosschannel encoders, but is able to extract features from the full input tensor, leaving nothing on the table.</p><p>Our contributions are as follows:</p><p>? We propose the split-brain autoencoder, which is composed of concatenated cross-channel encoders, trained using raw data as its own supervisory signal. ? We demonstrate state-of-the-art performance on several semantic representation learning benchmarks in the RGB and RGB-D domains. ? To gain a better understanding, we perform extensive ablation studies by (i) investigating cross-channel prediction problems and loss functions and (ii) researching alternative aggregation methods for combining cross-channel encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many unsupervised learning methods have focused on modeling raw data using a reconstruction objective. Autoencoders <ref type="bibr" target="#b19">[20]</ref> train a network to reconstruct an input image, using a representation bottleneck to force abstraction. Denoising autoencoders <ref type="bibr" target="#b43">[44]</ref> train a network to undo a random iid corruption. Techniques for modeling the probability distribution of images in deep frameworks have also been explored. For example, variational autoencoders (VAEs) <ref type="bibr" target="#b23">[24]</ref> employ a variational Bayesian approach to modeling the data distribution. Other probabilistic models include restricted Boltzmann machines (RBMs) <ref type="bibr" target="#b40">[41]</ref>, deep Boltzmann machines (DBMs) <ref type="bibr" target="#b37">[38]</ref>, generative adversarial networks (GANs) <ref type="bibr" target="#b15">[16]</ref>, autoregressive models (Pixel-RNN <ref type="bibr" target="#b42">[43]</ref> and Pixel-CNN <ref type="bibr" target="#b31">[32]</ref>), bidirectional GANs (BiGANs) <ref type="bibr" target="#b8">[9]</ref> and Adversarially Learned Inference (ALI) <ref type="bibr" target="#b9">[10]</ref>, and real NVP <ref type="bibr" target="#b6">[7]</ref>. Many of these methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref> have been evaluated for representation learning.</p><p>Another form of unsupervised learning, sometimes referred to as "self-supervised" learning <ref type="bibr" target="#b4">[5]</ref>, has recently grown in popularity. Rather than predicting labels annotated by humans, these methods predict pseudo-labels computed from the raw data itself. For example, image colorization <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27]</ref> has been shown to be an effective pretext task. Other methods generate pseudo-labels from egomotion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, video <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref>, inpainting <ref type="bibr" target="#b34">[35]</ref>, cooccurence <ref type="bibr" target="#b21">[22]</ref>, context <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, and sound <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. Concurrently, Pathak et al. <ref type="bibr" target="#b33">[34]</ref> use motion masks extracted from video data. Also in these proceedings, Larsson et al. <ref type="bibr" target="#b27">[28]</ref> provide an in-depth analysis of colorization for selfsupervision. These methods generally focus on a single supervisory signal and involve some engineering effort. In this work, we show that simply predicting raw data channels with standard loss functions is surprisingly effective, often outperforming previously proposed methods.</p><p>The idea of learning representations from multisensory signals also shows up in structure learning <ref type="bibr" target="#b1">[2]</ref>, cotraining <ref type="bibr" target="#b2">[3]</ref>, and multi-view learning <ref type="bibr" target="#b46">[47]</ref>. Our method is especially related to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref>, which use bidirectional data prediction to learn representations from two sensory modalities.</p><p>A large body of additional work in computer vision and graphics focuses on image channel prediction as an end in itself, such as colorization <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21</ref>], depth prediction <ref type="bibr" target="#b10">[11]</ref>, and surface normal prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45]</ref>. In contrast, rather than focusing on the graphics problem, we explore its utility for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In Section 3.1, we define the paradigm of cross-channel encoding. In Section 3.2, we propose the split-brain autoencoder and explore alternatives methods for aggregating multiple cross-channel encoders into a single network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-Channel Encoders</head><p>We would like to learn a deep representation on input data tensor X ? R H?W ?C , with C channels. We split the data into X 1 ? R H?W ?C1 and X 2 ? R H?W ?C2 , where C 1 , C 2 ? C, and then train a deep representation to solve the prediction problem X 2 = F(X 1 ). Function F is learned with a CNN, which produces a layered representation of input X 1 , and we refer to each layer l as F l . By performing this pretext task of predicting X 2 from X 1 , we hope to achieve a representation F(X 1 ) which contains high-level abstractions or semantics.</p><p>This prediction task can be trained with various loss functions, and we study whether the loss function affects the quality of the learned representation. To begin, we explore the use of 2 regression, as shown in Equation 1.</p><formula xml:id="formula_0">2 (F(X 1 ), X 2 ) = 1 2 h,w X 2h,w ? F(X 1 ) h,w 2 2</formula><p>(1)</p><p>We also study the use of a classification loss. Here, the target output X 2 ? R H?W ?C2 is encoded with function H into a distribution Y 2 ? ? H?W ?Q , where Q is the number of elements in the quantized output space. Network F is then trained to predict a distribution, Y 2 = F(X 1 ) ? ? H?W ?Q . A standard cross-entropy loss between the predicted and ground truth distributions is used, as shown Equation 2.</p><formula xml:id="formula_1">cl (F(X 1 ), X 2 ) = ? h,w q H(X 2 ) h,w,q log(F(X 1 ) h,w,q ) (2)</formula><p>In <ref type="bibr" target="#b48">[49]</ref>, the authors discover that classification loss is more effective for the graphics task of automatic colorization than regression. We hypothesize that for some tasks, especially those with inherent uncertainty in the prediction, the classification loss may lead to better representations as well, as the network will be incentivized to match the whole distribution, and not only predict the first moment.</p><p>Note that with input and output sets C 1 , C 2 = C, and an 2 regression loss, the objective becomes identical to the autoencoder objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Split-Brain Autoencoders as Aggregated Cross-Channel Encoders</head><p>We can train multiple cross-channel encoders, F 1 , F 2 , on opposite prediction problems, with loss functions L 1 , L 2 , respectively, described in Equation <ref type="bibr" target="#b2">3</ref>.</p><formula xml:id="formula_2">F * 1 = arg min F1 L 1 (F 1 (X 1 ), X 2 ) F * 2 = arg min F2 L 2 (F 2 (X 2 ), X 1 )<label>(3)</label></formula><p>By concatenating the representations layer-wise, F l = {F l 1 , F l 2 }, we achieve a representation F which is pretrained on full input tensor X. Example split-brain autoencoders in the image and RGB-D domains are shown in Figures 2(a) and (b), respectively. If F is a CNN of a desired fixed size, e.g., AlexNet <ref type="bibr" target="#b25">[26]</ref>, we can design the subnetworks F 1 , F 2 by splitting each layer of the network F in half, along the channel dimension. Concatenated representation F will then have the appropriate dimensionality, and can be simply implemented by setting the group parameter to 2 in most deep learning libraries. As each channel in the representation is only connected to half of the channels in the preceding layer, the number of parameters in the network is actually halved, relative to a full network.</p><p>Note that the input and the output to the network F is the full input X, the same as an autoencoder. However, due to the split nature of the architecture, the network F is trained to predict X = {X 1 , X 2 }, rather than simply reconstruct it from the input. In essence, an architectural change in the autoencoder framework induces the same forced abstraction achieved by cross-channel encoding.</p><p>Alternative Aggregation Technique We found the split-brain autoencoder, which aggregates cross-channel encoders through concatenation, to be more effective than several alternative strategies. As a baseline, we also explore an alternative: the same representation F can be trained to perform both mappings simultaneously. The loss function is described in <ref type="bibr">Equation 4</ref>, with a slight abuse of notation:</p><formula xml:id="formula_3">Input Image X Predicted Image X " L Grayscale Channel X # ab Color Channels X $ Predicted Grayscale Channel X # % Predicted Color Channels X $ % (a) Lab Images Input RGB-HHA image RGB Channels</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HHA Channels</head><p>Predicted HHA channels <ref type="figure">Figure 2</ref>: Split-Brain Autoencoders applied to various domains (a) Lab images Input images are divided into the L channel, which contains grayscale information, and the a and b channels, which contain color information. Network F 1 performs automatic colorization, whereas network F 2 performs grayscale prediction. (b) RGB-D images Input data X contains registered RGB and depth images. Depth images are encoded using the HHA encoding <ref type="bibr" target="#b17">[18]</ref>. Image representation F 1 is trained by predicting HHA channels. Representation F 2 on HHA images is learned by predicting images in Lab space. Note that the goal of performing these synthesis tasks is to induce representations F 1 , F 2 that transfer well to other tasks.</p><formula xml:id="formula_4">Predicted RGB Channels Predicted RGB-HHA image X X $ X # X # % X $ % X " (b) RGB-D Images</formula><p>here, we redefine X 1 to be the same shape as original input X ? R H?W ?C , with channels in set C\C 1 zeroed out (along with the analogous modification to X 2 ).</p><formula xml:id="formula_5">F * = arg min F L 1 (F(X 1 ), X 2 ) + L 2 (X 1 , F(X 2 )) (4)</formula><p>The network only sees data subsets but never full input X.</p><p>To alleviate this problem, we mix in the autoencoder objective, as shown in <ref type="bibr">Equation 5</ref>, with ? ? [0, <ref type="bibr">1 2</ref> ]. F * = arg min F ?L 1 (F(X 1 ), X 2 ) + ?L 2 (F(X 2 ), X 1 )</p><formula xml:id="formula_6">+ (1 ? 2?)L 3 (X, F(X))<label>(5)</label></formula><p>Note that unlike the split-brain architecture, in these objectives, there is a domain gap between the distribution of pretraining data and the full input tensor X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In Section 4.1, we apply our proposed split-brain autoencoder architecture to learn unsupervised representations on large-scale image data from ImageNet <ref type="bibr" target="#b36">[37]</ref>. We evaluate on established representation learning benchmarks and demonstrate state-of-the-art performance relative to previous unsupervised methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. In Section 4.2, we apply the proposed method on the NYU-D dataset <ref type="bibr" target="#b38">[39]</ref>, and show performance above baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Split-Brain Autoencoders on Images</head><p>We work with image data X in the Lab color space, and learn cross-channel encoders with X 1 representing the L, or lightness channel, and X 2 containing the ab channels, or color information. This is a natural choice as (i) networks such as Alexnet, trained with grouping in their architecture, naturally separate into grayscale and color <ref type="bibr" target="#b25">[26]</ref> even in a fully-supervised setting, and (ii) the individual crosschannel prediction problem of colorization, L to ab, has produced strong representations <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b26">27]</ref>. In preliminary experiments, we have also explored different cross-channel prediction problems in other color spaces, such as RGB and YUV. We found the L and ab to be most effective data split.</p><p>To enable comparisons to previous unsupervised techniques, all of our trained networks use AlexNet architectures <ref type="bibr" target="#b25">[26]</ref>. Concurrent work from Larsson et al. <ref type="bibr" target="#b27">[28]</ref> shows large performance improvements for the colorization task when using deeper networks, such as VGG-16 <ref type="bibr" target="#b39">[40]</ref> and ResNet <ref type="bibr" target="#b18">[19]</ref>. Because we are training for a pixel-prediction task, we run the network fully convolutionally <ref type="bibr" target="#b28">[29]</ref>. Using the 1.3M ImageNet dataset <ref type="bibr" target="#b36">[37]</ref> (without labels), we train the following aggregated cross-channel encoders:</p><p>? Split-Brain Autoencoder (cl,cl) (Our full method):</p><p>A split-brain autoencoder, with one half performing colorization, and the other half performing grayscale prediction. The top-level architecture is shown in <ref type="figure">Figure 2</ref>(a). Both sub-networks are trained for classification (cl), with a cross-entropy objective. (In <ref type="figure">Figure  2</ref>(a), the predicted output is a per-pixel probability distribution, but is visualized with a point estimate using the annealed-mean <ref type="bibr" target="#b48">[49]</ref>.) ? Split-Brain Autoencoder (reg,reg): Same as above, with both sub-networks trained with an 2 loss (reg). ? Ensembled L?ab: Two concatenated disjoint subnetworks, both performing colorization (predicting ab from L). One subnetwork is trained with a classification objective, and the other with regression.    <ref type="bibr" target="#b48">[49]</ref>. All weights are frozen and feature maps spatially resized to be ?9000 dimensions. All methods use AlexNet variants <ref type="bibr" target="#b25">[26]</ref>, and were pre-trained on ImageNet without labels, except for ImageNet-labels. Note that the proposed split-brain autoencoder achieves the best performance on all layers across unsupervised methods.</p><p>Single cross-channel encoders are ablations of our main method. We systematically study combinations of loss functions and cross-channel prediction problems.</p><p>? L?ab(reg): Automatic colorization using an 2 loss.</p><p>? L?ab(cl): Automatic colorization using a classification loss. We follow the quantization procedure proposed in <ref type="bibr" target="#b48">[49]</ref>: the output ab space is binned into grid size 10?10, with a classification loss over the 313 bins that are within the ab gamut. ? ab?L(reg): Grayscale prediction using an 2 loss.</p><p>? ab?L(cl): Grayscale prediction using a classification loss. The L channel, which has values between 0 and 100, is quantized into 50 bins of size 2 and encoded. ? Lab?Lab: Autoencoder objective, reconstructing</p><p>Lab from itself using an 2 regression loss, with the same architecture as the cross-channel encoders. ? Lab(drop50)?Lab: Same as above, with 50% of the input randomly dropped out during pre-training. This is similar to denoising autoencoders <ref type="bibr" target="#b43">[44]</ref>. We compare to the following methods, which all use variants of Alexnet <ref type="bibr" target="#b25">[26]</ref>. For additional details, refer to Table 3 in <ref type="bibr" target="#b48">[49]</ref>. Note that one of these modifications resulted in a large deviation in feature map size 1 . <ref type="bibr" target="#b0">1</ref> The method from <ref type="bibr" target="#b30">[31]</ref> uses stride 2 instead of 4 in the conv1 layer,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset &amp; Task Generalization on Places Classification [50]</head><p>Method conv1 conv2 conv3 conv4 conv5 Places-labels <ref type="bibr" target="#b49">[50]</ref> 22.  <ref type="table">Table 3</ref>: Dataset &amp; Task Generalization on Places Classification We train logistic regression classifiers on top of frozen pre-trained representations for 205-way Places classification. Note that our split-brain autoencoder achieves the best performance among unsupervised learning methods from conv2-5 layers.</p><p>? ImageNet-labels <ref type="bibr" target="#b25">[26]</ref>: Trained on ImageNet labels for the classification task in a fully supervised fashion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Transfer Learning Tests</head><p>How well does the pre-text task of cross-channel prediction generalize to unseen tasks and data? We run various established large-scale representation learning benchmarks. ImageNet <ref type="bibr" target="#b25">[26]</ref> As proposed in <ref type="bibr" target="#b48">[49]</ref>, we test the task generalization of the representation by freezing the weights and training multinomial logistic regression classifiers on top of each layer to perform 1000-way ImageNet classification. Note that each classifier is a single learned linear layer, followed by a softmax. To reduce the effect of differences in feature map sizes, we spatially resize feature maps through bilinear interpolation, so that the flattened feature maps have approximately equal dimensionality (9600 for resulting in 4? denser feature maps throughout all convolutional layers. While it is unclear how this change affects representational quality, experiments from Larsson et al. <ref type="bibr" target="#b27">[28]</ref> indicate that changes in architecture can result in large changes in transfer performance, even given the same training task. The network uses the same number of parameters, but 5.6? the memory and 7.4? the run-time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Ablation Studies</head><p>We conduct various ablation studies on our proposed method, using the ImageNet classification benchmark proposed in <ref type="bibr" target="#b48">[49]</ref>. Specifically, we compare (a) variations using an autoencoder objective (b) different crosschannel problems and loss functions (c) different methods for aggregating multiple cross-channel encoders.</p><p>conv1,3,4 and 9216 for conv2,5). The results are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure" target="#fig_3">Figures 3(a)</ref> and 4.</p><p>Places <ref type="bibr" target="#b49">[50]</ref> In the previous test, we evaluated the representation on the same input training data, the ImageNet dataset, with a different task than the pretraining tasks. To see how well the network generalizes to new input data as well, we run the same linear classification task on the largescale Places dataset <ref type="bibr" target="#b49">[50]</ref>. The dataset contains 2.4M images for training and 20.5k for validation from 205 scene categories. The results are shown in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 3(b)</ref>.</p><p>PASCAL <ref type="bibr" target="#b11">[12]</ref> To further test generalization, we finetune the learned representation on standard representation learning benchmarks on the PASCAL dataset, as shown in <ref type="table" target="#tab_5">Table 4</ref>, using established testing frameworks in classification <ref type="bibr" target="#b24">[25]</ref>, detection <ref type="bibr" target="#b14">[15]</ref>, and segmentation <ref type="bibr" target="#b28">[29]</ref>. Classification involves 20 binary classification decisions, regarding the presence or absence of 20 object classes. Detection involves drawing an accurately localized bounding box around any objects in the image, and is performed using the Fast R-CNN <ref type="bibr" target="#b14">[15]</ref> framework. Segmentation is pixel-Task and Data Generalization on PASCAL VOC <ref type="bibr" target="#b11">[12]</ref> Classification <ref type="bibr" target="#b24">[25]</ref> Detection <ref type="bibr" target="#b14">[15]</ref> Seg. <ref type="bibr">[</ref>   <ref type="bibr" target="#b12">[13]</ref> and segmentation on PASCAL VOC 2012 <ref type="bibr" target="#b13">[14]</ref>, using mean average precision (mAP) and mean intersection over union (mIU) metrics for each task, with publicly available testing frameworks from <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Column Ref documents the source for a value obtained from a previous paper. Character indicates that value originates from this paper. ? indicates that network weights have been rescaled with <ref type="bibr" target="#b24">[25]</ref> before fine-tuning, as is common practice. Character indicates concurrent work in these proceedings.</p><p>wise labeling of the object class, either one of the 20 objects of interest or background. Here, the representation is finetuned through multiple layers of the network, rather than frozen. Prior to fine-tuning, we follow common practice and use the rescaling method from <ref type="bibr" target="#b24">[25]</ref>, which rescales the weights so that the layers learn at the same "rate", using the ratio of expected gradient magnitude over feature activation magnitude as a heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Split-Brain Autoencoder Performance</head><p>Our primary result is that the proposed method, Split-Brain Auto (cl,cl), achieves state-of-the-art performance on almost all established self-supervision benchmarks, as seen in the last row on <ref type="table" target="#tab_1">Tables 2, 3</ref>, 4, over previously proposed self-supervision methods, as well as our ablation baselines. <ref type="figure" target="#fig_3">Figures 3(a)</ref> and (b) shows our split brain autoencoder method compared to previous self-supervised methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref> on the ImageNet and Places classification tests, respectively. We especially note the straightforward nature of our proposed method: the network simply predicts raw data channels from other raw data channels, using a classification loss with a basic 1-hot encoding scheme.</p><p>As seen in <ref type="figure">Figure 4</ref>(a) and <ref type="table" target="#tab_1">Table 2</ref>, the autoencoder objective by itself, Lab?Lab, does not lead to a strong representation. Performance is near Gaussian initialization through the initial layers, and actually falls below in the conv5 layer. Dropping 50% of the data from the input randomly during training, Lab(drop50)?Lab, in the style of denoising autoencoders, adds a small performance boost of approximately 1%. A large performance boost is observed by adding a split in the architecture, Split-Brain Auto (reg,reg), even with the same regression objective. This achieves 5% to 20% higher performance throughout the network, state-of-the-art compared to previous unsupervised methods. A further boost of approximately 1-2% throughout the network observed using a classification loss, Split-Brain Auto (cl,cl), instead of regression. <ref type="figure">Figure 4</ref>(b) compares the performance of the different cross-channel objectives we tested on the ImageNet classification benchmark. As shown in <ref type="bibr" target="#b48">[49]</ref> and further confirmed here, colorization, L?ab(cl), leads to a strong representation on classification transfer tasks, with higher performance than other unsupervised representations pre-trained on ImageNet, using inpainting <ref type="bibr" target="#b34">[35]</ref>, relative context <ref type="bibr" target="#b7">[8]</ref>, and adversarial feature networks <ref type="bibr" target="#b8">[9]</ref> from layers from conv2 to pool5. We found that the classification loss produced stronger representations than regression for colorization, consistent with the findings from concurrent work from Larsson et al. <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Cross-Channel Encoding Objectives</head><p>Interestingly, the task of predicting grayscale from color can also learn representations. Though colorization lends itself closely to a graphics problem, the application of grayscale prediction from color channels is less obvious. As seen in <ref type="table" target="#tab_1">Tables 2 and 3</ref> and <ref type="figure">Figure 4</ref>(b), grayscale prediction objectives ab?L(cl) and ab?L(reg) can learn representations above the Gaussian baseline. Though the learned representation by itself is weaker than other self-supervised methods, the representation is learned on a and b channels, which makes it complementary to the colorization network. For grayscale prediction, regression results in higher performance than classification. Choosing the appropriate loss function for a given channel prediction problem is an open problem. However, note that the performance difference is typically small, indicating that the cross-channel prediction problem is often times an effective method, even without careful engineering of the objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Split-Brain Autoencoders on RGB-D</head><p>We also test the split-brain autoencoder method on registered images and depth scans from NYU-D <ref type="bibr" target="#b38">[39]</ref>. Because RGB and depth images are registered spatially, RGB-D data can be readily applied in our proposed framework. We split the data by modality, predicting RGB from D and viceversa. Previous work in the video and audio domain <ref type="bibr" target="#b5">[6]</ref> Method Data</p><p>Label RGB D RGB-D Gupta et al. <ref type="bibr" target="#b17">[18]</ref> 1M ImNet <ref type="bibr" target="#b36">[37]</ref> 27.8 41.7 47.1 Gupta et al. <ref type="bibr" target="#b16">[17]</ref> 1M ImNet <ref type="bibr" target="#b36">[37]</ref> 27. <ref type="bibr" target="#b7">8</ref>   <ref type="table">Table 5</ref>: Split-Brain Autoencoder Results on RGB-D images We perform unsupervised training on 10k RGB-D keyframes from the NYU-D <ref type="bibr" target="#b38">[39]</ref> dataset, extracted by <ref type="bibr" target="#b17">[18]</ref>.</p><p>We pre-train representations on RGB images using 2 loss on depth images in HHA space. We pre-train HHA representations on L and ab channels using 2 and classification loss, respectively. We show performance gains above Gaussian and Kr?henb?hl et al. <ref type="bibr" target="#b24">[25]</ref> initialization baselines. The methods proposed by Gupta et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> use 1.3M labeled images for supervised pre-training. We use the test procedure from <ref type="bibr" target="#b17">[18]</ref>: Fast R-CNN <ref type="bibr" target="#b14">[15]</ref> networks are first trained individually in the RGB and D domains separately, and then ensembled together by averaging (RGB-D).</p><p>suggest that separating modalities, rather than mixing them, provides more effective splits. This choice also provides easy comparison to the test procedure introduced by <ref type="bibr" target="#b16">[17]</ref>.</p><p>Dataset &amp; Detection Testbed The NYUD dataset contains 1449 RGB-D labeled images and over 400k unlabeled RGB-D video frames. We use 10k of these unlabeled frames to perform unsupervised pre-training, as extracted from <ref type="bibr" target="#b17">[18]</ref>. We evaluate the representation on the 1449 labeled images for the detection task, using the framework proposed in <ref type="bibr" target="#b17">[18]</ref>. The method first trains individual detectors on the RGB and D domains, using the Fast R-CNN framework <ref type="bibr" target="#b14">[15]</ref> on an AlexNet architecture, and then latefuses them together through ensembling.</p><p>Unsupervised Pre-training We represent depth images using the HHA encoding, introduced in <ref type="bibr" target="#b16">[17]</ref>. To learn image representation F HHA , we train an Alexnet architecture to regress from RGB channels to HHA channels, using an 2 regression loss. To learn depth representations, we train an Alexnet on HHA encodings, using 2 loss on L and classification loss on ab color channels. We chose this combination, as these objectives performed best for training individual crosschannel encoders in the image domain. The network extracts features up to the conv5 layer, using an Alexnet architecture, and then splits off into specific branches for the L and ab channels. Each branch contains AlexNettype fc6-7 layers, but with 512 channels each, evaluated fully convolutionally for pixel prediction. The loss on the ab term was weighted 200? with respect to the L term, so the gradient magnitude on the pool5 representation from channel-specific branches were approximately equal throughout training.</p><p>Across all methods, weights up to the conv5 layer are copied over during fine-tuning time, and fc6-7 layers are randomly initialized, following <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are shown in <ref type="table">Table 5</ref> for detectors learned in RGB and D domains separately, as well as the ensembled result. For a Gaussian initialization, the RGB detector did not train using default settings, while the depth detector achieved performance of 28.1%. Using the stacked k-means initialization scheme from Kr?henb?hl et al. <ref type="bibr" target="#b24">[25]</ref>, individual detectors on RGB and D perform at 12.5% and 32.2%, while achieving 34.5% after ensembling. Pre-training with our method reaches 18.9% and 33.2% on the individual domains, above the baselines. Our RGB-D ensembled performance was 38.1%, well above the Gaussian and Kr?henb?hl et al. <ref type="bibr" target="#b24">[25]</ref> baselines. These results suggest that split-brain autoencoding is effective not just on Lab images, but also on RGB-D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We present split-brain autoencoders, a method for unsupervised pre-training on large-scale data. The split-brain autoencoder contains two disjoint sub-networks, which are trained as cross-channel encoders. Each sub-network is trained to predict one subset of raw data from another. We test the proposed method on Lab images, and achieve stateof-the-art performance relative to previous self-supervised methods. We also demonstrate promising performance on RGB-D images. The proposed method solves some of the weaknesses of previous self-supervised methods. Specifically, the method (i) does not require a representational bottleneck for training, (ii) uses input dropout to help force abstraction in the representation, and (iii) is pre-trained on the full input data.</p><p>An interesting future direction of is exploring the concatenation of more than 2 cross-channel sub-networks. Given a fixed architecture size, e.g. AlexNet, dividing the network into N disjoint sub-networks results in each subnetwork becoming smaller, less expressive, and worse at its original task. To enable fair comparisons to previous large-scale representation learning methods, we focused on learning weights for a fixed AlexNet architecture. It would also be interesting to explore the regime of fixing the subnetwork size and allowing the full network to grow with additional cross-channel encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional analysis</head><p>Cross-Channel Encoder Aggregation Analysis In <ref type="figure">Figure  4</ref>(c), we show variations on aggregated cross-channel encoders. To begin, we hypothesize that the performance improvement of split-brain autoencoders Split-Brain Auto (cl,cl) over single cross-channel encoders L?ab is due to the merging of complementary signals, as each sub-network in Split-Brain Auto has been trained on different portions of the input space. However, the improvement could be simply due to an ensembling effect. To test this, we train a split-brain autoencoder, comprising of two L?ab networks, Ensemble L?ab. As seen in <ref type="figure">Figure 4</ref>(c) and <ref type="table" target="#tab_1">Table  2</ref>, the ensembled colorization network achieves lower performance than the split-brain autoencoder, suggesting that concatenating signals learned on complementary information is beneficial for representation learning.</p><p>We find that combining cross-channel encoders through concatenation is effective. We also test alternative aggregation techniques. As seen in <ref type="figure">Figure 4(c)</ref>, training a single network to perform multiple cross-channel tasks (L,ab)?(ab,L) is not effective for representation learning on full Lab images. Adding in the autoencoder objective during training, (L,ab,Lab)?(ab,L,Lab), in fact lowers performance in higher layers.</p><p>Our proposed methods outperform these alternatives, which indicates that (i) our choice of aggregating complementary signals improves performance (ii) concatenation is an appropriate choice of combining cross-channel encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Here, we describe the pre-training and feature evaluation architectures. For pre-training, we use an AlexNet architecture <ref type="bibr" target="#b25">[26]</ref>, trained fully convolutionally <ref type="bibr" target="#b28">[29]</ref>. The network is trained with 180?180 images, cropped from 256 ? 256 resolution, and predicts values at a heavily downsampled 12?12 resolution. One can add upsampling-convolutional layers or use a trous <ref type="bibr" target="#b3">[4]</ref>/dilated <ref type="bibr" target="#b47">[48]</ref> convolutions to predict full resolution images at the expense of additional memory and run-time, but we found predicting at a lower resolution to be sufficient for representation learning. See <ref type="table">Table  6</ref> for feature map and parameter sizes during pre-training time. We remove LRN layers and add BatchNorm layers after every convolution layer. After pre-training, we remove BatchNorm layers by absorbing the parameters into the preceding conv layers. The pre-training network predicts a downsampled version of the desired output, which we found to be adequate for feature learning.</p><p>During feature evaluation time (such as the ImageNet <ref type="bibr" target="#b25">[26]</ref>, Places <ref type="bibr" target="#b49">[50]</ref>, and PASCAL <ref type="bibr" target="#b11">[12]</ref> tests), the parameters are copied into an AlexNet classification architecture, shown in <ref type="table">Table 7</ref>. During the linear classification tests, we downsample feature maps spatially, so that each layer has approximately the same number of features.</p><p>Quantization procedure Zhang et al. <ref type="bibr" target="#b48">[49]</ref> use a class- <ref type="table">Table 6</ref>: Fully Convolutional AlexNet architecture used for pre-training. X spatial resolution of layer, C number of channels in layer; K conv or pool kernel size; S computation stride; D kernel dilation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>; P padding; * first and last layer channel sizes are dependent on the pre-text task, last layer is removed during transfer evaluation.  <ref type="table">Table 7</ref>: AlexNet architecture used for feature evaluation. X spatial resolution of layer, X d downsampled spatial resolution for feature evaluation, C number of channels in layer; F d = X 2 d C downsampled feature map size for feature evaluation (kept approximately constant throughout), K conv or pool kernel size; S computation stride; D kernel dilation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>; P padding; * first layer channel size is dependent on the pre-text task e.g., 3 for the split-brain autoencoder or 1 for the L ? ab(cl) cross-channel encoder rebalancing term, to over-sample rare colors in the training set, and a soft-encoding scheme for H. These choices were made from a graphics perspective, to produce more vibrant colorizations. In our classification colorization network, L?ab(cl), our objective is more straightforward, as we do not use class-rebalancing. In addition, we use a 1-hot encoding representation of classes, rather than softencoding. The simplification in the objective function achieves higher performance on ImageNet and Places classification, as shown on <ref type="table" target="#tab_1">Tables 2 and 3.</ref> C. Change Log v1 Initial Release. v2 Paper accepted to CVPR 2017. Updated <ref type="table" target="#tab_5">Table 4</ref> with results for Misra et al. <ref type="bibr" target="#b29">[30]</ref> and Donahue et al. <ref type="bibr" target="#b8">[9]</ref> with 112 ? 112 resolution model. Updated <ref type="table" target="#tab_1">Table 2</ref>, rows L?ab (cl) and Zhang et al. <ref type="bibr" target="#b48">[49]</ref> with corrected values. Supplemental material added. v3 CVPR 2017 Camera Ready. Added references to concurrent work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28]</ref>. Various changes to text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Traditional vs Split-Brain Autoencoder architectures. (top) Autoencoders learn feature representation F by learning to reconstruct input data X. (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? (L,ab)?(ab,L): A single network for both colorization and grayscale prediction, with regression loss, as described in Equation 4. This explores an alternative method for combining cross-channel encoders. ? (L,ab,Lab)?(ab,L,Lab): ? = 1 3 using Equation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Auto(cl,cl) (b) Places Classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison to Previous Unsupervised Methods We compare our proposed Split-Brain Autoencoder on the tasks of (a) ImageNet classification and (b) Places Classification. Note that our method outperforms other large-scale unsupervised methods<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9]</ref> on all layers in ImageNet and from conv2-5 on Places.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c o n v 1</head><label>1</label><figDesc>p o o l1 c o n v 2 p o o l2 c o n v 3 c o n v 4 c o n v 5 p o o ?Lab Split-Brain Auto(reg,reg) Split-Brain Auto(cl,cl) (a) Autoencoder Objective c o n v 1 p o o l1 c o n v 2 p o o l2 c o n v 3 c o n v 4 c o n v 5 p o cl) L ?ab(reg) ab ?L(cl) ab ?L(reg) Split-Brain Auto(cl,cl) (b) Cross-Channel Encoders c o n v 1 p o o l1 c o n v 2 p o o l2 c o n v 3 c o n v 4 c o n v 5 p o o ab) ?(ab,L) (L,ab,Lab) ?(ab,L,Lab) Split-Brain Auto(cl,cl) (c) Aggregation Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Task Generalization on ImageNet Classification To test unsupervised feature representations, we train linear logistic regression classifiers on top of each layer to perform 1000-way ImageNet classification, as proposed in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>?</head><label></label><figDesc>Gaussian: Random Gaussian initialization of weights.</figDesc><table /><note>? Kr?henb?hl et al. [25]: A stacked k-means initializa- tion method.? Doersch et al. [8], Noroozi &amp; Favaro [31], Pathak et al. [35], Donahue et al. [9], and Zhang et al. [49] all pre-train on the 1.3M ImageNet dataset [37].? Wang &amp; Gupta [46] and Owens et al. [33] pre-train on other large-scale data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Task and Dataset Generalization on PASCAL VOC Classification and detection on PASCAL VOC 2007</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank members of the Berkeley Artificial Intelligence Research Lab (BAIR), in particular Andrew Owens, for helpful discussions, as well as Saurabh Gupta for help with RGB-D experiments. This research was supported, in part, by Berkeley Deep Drive (BDD) sponsors, hardware donations by NVIDIA Corp and Algorithmia, an Intel research grant, NGA NURI, and NSF SMA-1514512. Thanks Obama.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In Section A, we provide additional analysis. In Section B, we provide implementation details.</p><p>Fully Convolutional AlexNet <ref type="bibr" target="#b25">[26]</ref> Architecture </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning classification with unlabeled data. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="112" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
		<title level="m">Sensory modality segregation. NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2016)</title>
		<meeting>of SIGGRAPH 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations, Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational bayes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Data-dependent initializations of convolutional neural networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning representations for automatic colorization. European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles. European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Conditional image generation with pixelcnn decoders. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<title level="m">Hariharan. Learning features by watching objects move. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual equivalence: towards a new standard for image fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<title level="m">A survey on multi-view learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<title level="m">Colorful image colorization. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
