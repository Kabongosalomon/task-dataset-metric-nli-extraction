<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<email>michihiro.yasunaga@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific documents rely on both mathematics and text to communicate ideas. Inspired by the topical correspondence between mathematical equations and word contexts observed in scientific texts, we propose a novel topic model that jointly generates mathematical equations and their surrounding text (TopicEq). Using an extension of the correlated topic model, the context is generated from a mixture of latent topics, and the equation is generated by an RNN that depends on the latent topic activations. To experiment with this model, we create a corpus of 400K equation-context pairs extracted from a range of scientific articles from arXiv, and fit the model using a variational autoencoder approach. Experimental results show that this joint model significantly outperforms existing topic models and equation models for scientific texts. Moreover, we qualitatively show that the model effectively captures the relationship between topics and mathematics, enabling novel applications such as topic-aware equation generation, equation topic inference, and topic-aware alignment of mathematical symbols and words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Technical scientific articles, such as those from physics and computer science, rely on both mathematics and text to communicate ideas. Most existing work in natural language processing (NLP) and machine learning studies these two components separately. For instance, text-based topic models have been used widely on scientific articles to uncover their semantic structure <ref type="bibr" target="#b4">(Blei, Ng, and Jordan 2003;</ref><ref type="bibr" target="#b2">Blei and Lafferty 2006;</ref><ref type="bibr" target="#b14">Newman et al. 2010a)</ref>. For mathematics, recent work <ref type="bibr" target="#b9">(Lan et al. 2015;</ref><ref type="bibr" target="#b20">Zanibbi et al. 2016;</ref><ref type="bibr" target="#b6">Deng et al. 2017)</ref> has studied methods to model and generate mathematical equations, for example using RNNs. However, ultimately these two components should be processed together in a seamless manner. Algorithms for automated understanding of scientific documents should extract the information encoded by not only words but also mathematical equations. At the same time, equations should ideally be modeled with the help of the surrounding text, as the meaning of an equation depends not only on its constituent symbols and syntax, but also on the context in which it appears <ref type="bibr" target="#b19">(Wang et al. 2015;</ref><ref type="bibr">Krstovski and Blei 2018)</ref>.</p><p>To this end, this paper proposes a topic-equation model that jointly generates equations and their surrounding text Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Black holes in Einstein gravity. As a warm-up exercise, in this section, we will briefly review the observation made by <ref type="bibr">Padmanabhan [14]</ref> by generalizing his discussion to a more general spherically symmetric case. In Einstein's general relativity , the gravitational field equations are G ?? = R ?? ? 1 2 Rg ?? = 8?GT ?? where G ?? is Einstein tensor and T ?? is the energy-momentum tensor of matter field. On the other hand, for a general static, spherically symmetric spacetime , its metric can be written down as ......</p><p>(snippet from Cai and Ohta <ref type="formula">(2010))</ref> We give the derivation for the primal-dual subgradient update, as composite mirror-descent is entirely similar. We need to solve update (3), which amounts to min x ? ? t , x + 1 2t ? x 2 2 + 1 2t x, diag(s t )x + ?? x 1</p><p>Letx denote the optimal solution of the above optimization problem . Standard subgradient calculus implies that when |? t,i | ? ? the solution isx = 0. Similarly, when? t,i ? ??, thenx &gt; 0, the objective is differentiable, and the solution is obtained by setting the gradient to zero. ...... <ref type="bibr">(snippet from Duchi et al. (2011))</ref> Point to make: equations may look weird, but actually reflect the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Generated Equations</head><p>Quantum physics</p><p>? E = ? 2 S ?t 2 ( ?? ?c ) ? k 2 ?B ?t (t + ?t?). ? ?deg ? 11 eV [1.8V (10 ?32 ergs ?1 )/5; 1.13 neV].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relativity</head><p>? dF ?? ? lc ??(1?f ) (d??? ? ?? ?? ? ? ?? dx ? d 5 Q).</p><formula xml:id="formula_0">? g?? = T ? 1 2 h(0)? = h(g) (1+?) v ? ? .</formula><p>Optimization ? min p p(x) subject to p x ? y 2 ? mp.</p><p>? w + = wt + gt ut ? ?u * 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability</head><p>? P(r? &lt; t) = E? wist (N? ).</p><formula xml:id="formula_1">? T * (t) = limt?? E[|N (t ? t) + E[?t(x)] * |] = limsup t?? 1 t t 0 h(x)dt ? af (x).</formula><p>Table 1: Generated equations from given topics.</p><p>1 <ref type="figure">Figure 1</ref>: The words in a given technical context often characterize the distinctive types of equations used, and vice versa. Top topic: Relativity; bottom topic: Optimization.</p><p>in scientific documents <ref type="bibr">(TopicEq)</ref>, and demonstrates that the model can effectively achieve the aforementioned two goals.</p><p>The intuition behind the model is illustrated in the sample passages in <ref type="figure">Figure 1</ref>, which shows how the topic of the word context is often indicative of the distinctive types of equations used, and vice versa. For instance, equations appearing in the topic of relativity (with context words like "back hole", "Einstein") tend to involve a series of tensors like G ?? and T ?? , while equations used in the topic of optimization (context words "gradient", "optimal") may use norms, the min operator, and often their combinations. Ideally, the strings of mathematical symbols in the equations should aid the training of topic models, and the context words should aid the modeling and understanding of the equations. Our model formalizes this intuition for scientific texts by generating each equation and its context passage using a shared latent topic. Specifically, we apply a topic model to the context passage, and use the same latent topic proportion vector in a recurrent neural network (RNN) to generate the equation as a sequence of symbols. To develop and experiment with this model, we construct a large corpus of contextequation pairs, extracted from the L A T E X source of arXiv articles across a range of scientific domains (ContextEq-400K). We fit the model on this corpus using approximate inference based on a variational autoencoder approach.</p><p>Our evaluation shows that this joint model significantly outperforms alternative topic models and RNN equation models for scientific texts. We further show that the model enables novel applications that bridge topics and mathematical equations. Concretely, the paper makes the following contributions.</p><p>? The first study of jointly modeling topics and mathematics in scientific texts. We also present a variant of this model that learns topic-aware associations between mathematical symbols and words. ? The model is unsupervised, and enables the aforementioned tasks and applications without manual labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our work is connected to a wide range of recent research, from topic models to mathematical equation processing. Topic models. Topic models provide a powerful tool to extract the semantic structure of texts in the form of the latent topics-usually multinomial distributions over words.</p><p>Starting from LDA <ref type="bibr" target="#b4">(Blei, Ng, and Jordan 2003)</ref>, topic models have been studied extensively <ref type="bibr" target="#b19">(Teh et al. 2005;</ref><ref type="bibr" target="#b2">Blei and Lafferty 2006;</ref><ref type="bibr" target="#b3">Blei and Lafferty 2007;</ref><ref type="bibr" target="#b7">Hall, Jurafsky, and Manning 2008)</ref>, especially for scientific articles. However, while mathematical equations play an essential role in scientific documents, topic models capable of processing equations besides word texts are yet to be studied. This work shows that incorporating joint modeling of equations via an RNN boosts the performance of topic modeling for scientific texts. Recent work <ref type="bibr" target="#b4">(Cao et al. 2015</ref>; Larochelle and Lauly 2012) has proposed neural topic models, leveraging the flexibility and representation power of neural networks. In particular, <ref type="bibr" target="#b12">(Miao, Yu, and Blunsom 2016;</ref><ref type="bibr" target="#b12">Miao, Grefenstette, and Blunsom 2017;</ref><ref type="bibr" target="#b18">Srivastava and Sutton 2017)</ref> employ neural variational inference to train topic models; we will apply their technique to fit our model. Language models &amp; equation models. Language modeling aims to learn a probability distribution over a sequence of words. It is a fundamental task in NLP, with a plethora of applications including text generation. RNN-based language models are shown effective for sequences with long-term dependencies <ref type="bibr" target="#b14">(Mikolov et al. 2010;</ref><ref type="bibr" target="#b9">Jozefowicz et al. 2016</ref>).</p><p>Similar to language models, equation models are useful for various tasks involving equation generation, such as semantic parsing <ref type="bibr" target="#b16">(Roy, Upadhyay, and Roth 2016)</ref> and handwriting / optical character recognition <ref type="bibr" target="#b6">(Deng et al. 2017)</ref>. The use of RNNs to model L A T E X was illustrated by (Karpathy 2015) for an algebraic geometry text. This work also employs an RNN to model each equation as a sequence of L A T E X tokens (or "symbols," interchangeably). Neural topic-language models. Our model architecture is motivated by joint topic-language models. Such models typically extract latent topics of a given document via a topic model, and utilize the topic knowledge to improve an RNN language model. <ref type="bibr" target="#b13">Mikolov and Zweig (2012)</ref> incorporate the topic vector of a pre-trained LDA model into an RNN language model; recent work <ref type="bibr" target="#b6">(Dieng et al. 2017;</ref><ref type="bibr" target="#b10">Lau, Baldwin, and Cohn 2017;</ref><ref type="bibr" target="#b19">Wang et al. 2018</ref>) trains neural topic and language models jointly, as we will do here.</p><p>Key distinctions can be made between our work and these models. First, while previous work uses topic models to improve language modeling on the same word text, our task models two different modalities: word text and equations. In this sense, our work is related to , which extends LDA to model image-text pairs. Moreover, taking advantage of these two modalities, we also present a variant of the TopicEq model that learns topic-aware association between mathematical symbols and words.</p><p>The second difference lies in the RNN equation model we propose. While <ref type="bibr" target="#b6">(Dieng et al. 2017;</ref><ref type="bibr" target="#b0">Ahn et al. 2016;</ref><ref type="bibr" target="#b10">Lau, Baldwin, and Cohn 2017)</ref> integrate the topic knowledge into either the output layer of the LSTM or the word predictions of the language model, we embed the topic proportion vector inside the LSTM, to enable the topic knowledge to have deeper influence on equation generation. Experimental results show that this method of incorporating topic information is more effective than the existing methods for improving the quality of equation modeling. Mathematical equation processing. Some work has processed equations as bags of math symbols to extract their features for searching (Sojka and L??ka 2011) and clustering <ref type="bibr" target="#b9">(Lan et al. 2015)</ref>. <ref type="bibr" target="#b20">Zanibbi et al. (2016)</ref> introduce tree-based representations for equations for mathematical information retrieval tasks. Most recently, <ref type="bibr" target="#b6">Deng et al. (2017)</ref> propose RNN-based models to generate equations. We will show that RNN-based equation processing can capture syntactic features of equations, and provides more effective help for topic modeling than bag of token-based equation processing does.</p><p>Finally, our work of modeling equations with contexts is related to (Krstovski and Blei 2018), which fits equation embeddings using surrounding words. While they limit the equation domains (i.e., ML, AI), this work aims to uncover topics for texts and equations from a range of scientific domains. This work also models each equation itself as a sequence of symbols, which is not studied in their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The TopicEq Model</head><p>Our starting point is the correlated topic model <ref type="bibr" target="#b3">(Blei and Lafferty 2007)</ref>, which models the topic proportion vector through a latent Gaussian vector. We extend this model to n } N n=1 , which we call the equation's context. Our joint model assumes that each equation and its context are generated from the same latent topic vector ?; see <ref type="figure" target="#fig_1">Figure 2</ref>. Concretely, the generative process for a given D = (C, eq) is</p><formula xml:id="formula_2">? ? N (0, I), ? = g(?) (1) w (c) n | ? ? Mult(? T ?) (2) eq | ? ? LSTM(?)<label>(3)</label></formula><p>where g(?) = softmax(W g ? + b g ). Note that this is equivalent to placing a logistic normal distribution on ? where the latent Gaussian has mean b g and covariance W g W T g . The parameters W g , b g , the topics ?, and the weights in the LSTM are to be estimated from data. Expressing the model as shown in <ref type="figure" target="#fig_1">Figure 2</ref> emphasizes the connection with neural topic models such as <ref type="bibr" target="#b12">(Miao, Grefenstette, and Blunsom 2017)</ref>; we will apply their model training technique.</p><p>Both the words and the equation are generated in a way that depends on the topic proportion vector ?. The topics ? T = (? 1 , . . . , ? K ) are distributions over a word vocabulary with V words; the context words w (c) n are then drawn from the mixture ? T ?, similar to <ref type="bibr" target="#b19">(Wang et al. 2018)</ref>. We employ an RNN to generate eq as a sequence of mathematical tokens, where the vocabulary is extracted from the set of L A T E X tokens. Specifically, to generate an equation conditioned on the latent topic proportion vector ? (equivalently ?), we consider a Topic-Embedded LSTM (TE-LSTM), an extension of the LSTM (Hochreiter and Schmidhuber 1997) where the t-th update is</p><formula xml:id="formula_3">i t = ? (W i [x t ; h t?1 ; ?] + b i ) f t = ? (W f [x t ; h t?1 ; ?] + b f ) c t = tanh (W c [x t ; h t?1 ; ?] + b c ) o t = ? (W o [x t ; h t?1 ; ?] + b o ) c t = f t c t?1 + i t c t , h t = o t tanh (c t ) .</formula><p>Here [x t ; h t?1 ; ?] denotes the concatenation of the current input, previous state and topic proportion vector; ? is the sigmoid function and denotes the Hadamard product. The probability of the next token in the equation is p(y t | y 1:t?1 ) = softmax (W y h t + b y ). Thus, the TE-LSTM embeds ? inside the LSTM cell to reflect the topic knowledge for equation generation. As a joint topic-equation model, it is similar to the topic-language model of (Wang et al. 2018).</p><p>Writing the equation as a sequence of tokens eq = y 1:T , the training objective is the marginal likelihood of C and eq p(C, y 1:</p><formula xml:id="formula_4">T ) = ? p(?)p(C|?) T t=1 p(y t |y 1:t?1 , ?)d? (4)</formula><p>Since its direct optimization is intractable, we employ variational inference <ref type="bibr" target="#b8">(Jordan et al. 1999)</ref>. Denoting the variational distribution by q(?), we maximize the variational lower bound (ELBO) for the log-likelihood, log p(C, y 1:T ):</p><formula xml:id="formula_5">L = E q(?) log p(C|?) ? D KL q(?) p(?) + E q(?) T t=1 log p(y t | y 1:t?1 , ?)<label>(5)</label></formula><p>Following recent approaches to neural topic-language models <ref type="bibr" target="#b12">(Miao, Grefenstette, and Blunsom 2017;</ref><ref type="bibr" target="#b6">Dieng et al. 2017;</ref><ref type="bibr" target="#b19">Wang et al. 2018)</ref>, we compute q(?) as a function of the context C using the variational autoencoder technique (Kingma and Welling 2014). Specifically, we use a feedforward neural network (FFNN) as an inference network to parameterize the mean and variance vectors of the (diagonal) Gaussian variational distribution q(? | C). We then use samples from q to optimize Eq 5. The parameters of the inference network, the topic model, and the equation model are jointly trained by stochastic gradient descent. We also include a topic diversity regularization term to Eq 5, following <ref type="bibr" target="#b20">(Xie, Deng, and Xing 2015)</ref>. We observed that this technique prevents learning generic, redundant topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We study the performance of the proposed model on a corpus of context-equation pairs constructed from arXiv articles. We quantitatively show that our joint topic-equation model provides superior fits than alternative topic models and equation models. We further demonstrate its efficacy through qualitative analyses and novel applications, such as equation generation and equation topic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Construction (ContextEq-400K)</head><p>To obtain a dataset of context-equation pairs, we used scientific articles published on arXiv.org. We sampled 100k articles from all domains in the past 5 years, and split them into train, validation and test sets (80%, 10%, 10%). For each article, we parsed its L A T E X source and extracted singleline display equations that have five consecutive sentences both before and after the equation, which are used to define the word context. Following <ref type="bibr" target="#b6">(Deng et al. 2017)</ref>, we further tokenized each equation into a sequence of L A T E X tokens (e.g., \sigma,?, {, 2, }) and kept those of length 20-150, yielding the final corpus of 400K equation-context pairs. An equation has 63 tokens on average. The context size of 10 sentences is similar to the document size used in recent work of topic-language models <ref type="bibr" target="#b6">(Dieng et al. 2017;</ref><ref type="bibr" target="#b19">Wang et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We fit the TopicEq model end-to-end on the train set and evaluate its performance on the test set. Preprocessing. For the topic modeling of context passages, we first removed all the inline math expressions in the text. We then followed the preprocessing steps in <ref type="bibr" target="#b19">(Wang et al. 2018)</ref> to tokenize and lowercase all words, exclude stopwords and words appearing in fewer than 100 documents; this resulted in a vocabulary size of 8,660. For equations, we use the 1,000 most frequent L A T E X tokens as our vocabulary.    <ref type="table" target="#tab_7">Table 3</ref>: Performance of different equation models, evaluated on held-out arXiv data. We report the perplexity metric (for # topics 50, 100 if topic info is used), and the syntax error rate of generated L A T E X equations (for # topics 100).</p><p>show that a joint RNN equation model provides significant information to aid topic modeling of scientific texts.</p><p>Why is the RNN helpful? We hypothesize that one reason why the joint RNN equation model is more helpful than the bag-of-tokens equation model is that the RNN also captures syntax-level information in equations. But one might argue that the introduction of the RNN itself was useful for topic modeling (e.g. as a form of regularization). To study our hypothesis, we re-trained TopicEq with each equation's token order randomly shuffled in the training data-thus corrupting the syntactic information of each equation. The result is shown in <ref type="table" target="#tab_2">Table 1</ref> as "Ours (context + Eq LSTM shuffled)." This time, the topic model performance degrades severely and falls to the level of the baseline topic model, "Ours (context only)". This result supports the claim that the original TopicEq's joint RNN actually captured syntactic features of equations, providing more effective help for topic modeling than a bag-of-token equation model does. This idea also makes intuitive sense. Mathematical equations use a much smaller vocabulary (symbols / variables) than word texts, and thus often need phrase or syntax-level information to aid topic modeling. For example, in the equations in <ref type="figure">Figure 1</ref>, phrases like T ?? (use of super/sub-scripts for a tensor) and ? x 1 (regularization term) provide rich information to identify the topics (relativity and optimization), while the corresponding bags of tokens {?, ?, T } and {1, ?, x, |} themselves do not provide as much help. Learned topics. To visualize the topic modeling performance, we sampled 10 topics learned by TopicEq <ref type="table" target="#tab_3">(Table 2)</ref>. They intuitively reflect the scientific topics of arXiv articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equation Model Evaluation</head><p>Next, we evaluate the equation model component of Top-icEq by measuring the test set perplexity. Additionally, as the grammaticality of equations can be measured using the L A T E X compiler, we also evaluate the syntax error rate of generated equations. We compare our TE-LSTM with</p><p>? a generic LSTM (no topic knowledge)</p><p>? LSTM + LDA: the topic vector ? obtained from a pretrained LDA is concatenated to the output of LSTM and a recent topic-dependent LSTM applied to our task ? TD-LSTM <ref type="bibr" target="#b10">(Lau, Baldwin, and Cohn 2017)</ref>: ? is added to the output of LSTM via a dense layer. TD-LSTM and our TE-LSTM are jointly trained with our Topic Generated Equations Quantum physics</p><formula xml:id="formula_6">? E =~@ 2 S @t 2 ( @' @c ) k 2 @B @t (t + @ t ). ? pr = P l ( r+" ? r# ) + P r 0 ( ? r#,"<label>r# ?</label></formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Particle physics</head><formula xml:id="formula_7">? H = 2 4 (@ ? ) 2 + 2m ? ( ) + 1 2 m 2 ( )(1 2 ) 2 . ? m e? (M ) = 1.4 ? 10 13</formula><p>GeV.</p><p>Relativity   <ref type="table" target="#tab_3">Table 2</ref>). Right: equations generated by the model conditioned on the given topic (one-hot topic vector ?).</p><formula xml:id="formula_8">? M = 2 2 g ?? (f ??,? g ??,? + g ??,b f ?,? ) + 1 2 g ?? . ? T ?? = R 1 0 ds ?? ds 2 + a 2 ? dr 2 + r 2 d? 2 . Number theory ? (2 k ) k + (1 n + 1)(1 + p k ) = 1. Linear algebra ? tr(E " X ? ) = U &gt; (tr(V " X)). ? h (?, y) = n X 2 Span P c (T[x, x]) i . Optimization ? min p p(x) subject to kp x yk 2 ? m p . ? w + = w t + g t ku t ru ? k 2 2 . Probability ? P(r ? &lt; t) = E ?wist (N ? ). ? T ? (t) = lim t!1 E[|N(t) + E[' t (x)] ? |]</formula><p>topic model component. As <ref type="table" target="#tab_7">Table 3</ref> shows, all the topicdependent LSTMs are superior to the vanilla LSTM in both the perplexity metric and syntax error metric. Moreover, our TE-LSTM outperforms TD-LSTM, suggesting that the model better incorporates topic knowledge by embedding ? inside the LSTM. We also find that compared to (Wang et al. 2018)'s Mixture-of-Expert LSTM, our model achieves similar performance in this task while requiring fewer parameters and much less training time (40% reduction). In total, compared to the generic LSTM, our TE-LSTM equation model reduces test perplexity by 8% (relative) and syntax error rate by 3.5% (absolute). This result suggests that incorporating context/topic information can improve the quality and grammaticality of equation modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis &amp; Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic-aware Equation Generation</head><p>The TopicEq model can generate meaningful equations from specified topics, using Eq 3 (TE-LSTM). For example, given a topic k, we let ? be the one-hot vector representing the topic; conditioned on ?, and starting from &lt;START&gt; token, we keep sampling the next L A T E X token until the &lt;END&gt; token is generated. <ref type="table" target="#tab_5">Table 4</ref> shows several topics picked from Table 2 (left), and equations generated from each of these topics (right). We see that the artificial equations generated by the model clearly reflect the distinctive characteristics of the given topics. For instance, derivatives, and number + units are generally used for physics; electron configuration ?, ? for quantum physics; series of tensors like T ?? for relativity; prime number p for number theory; E, P clauses for probability. We also note that the equations generated by our TE-LSTM use not only topic-specific symbols but also topicspecific phrases and syntax (e.g., a set definition is used for linear algebra; "min subject to" clause for optimization). <ref type="table" target="#tab_7">Table 3</ref>: Generated equations from given topics via greedy decoding 3 <ref type="table">Table 5</ref>: We let the TopicEq model greedily generate equations while smoothly changing ? between two topics (via linear interpolation). Left: given topic pair and its interpolation. Right: generated equation (for the first topic pair, we let the model generate from G; for the second pair, from L =).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Gradation Generated Equation (Greedily decoded)</head><formula xml:id="formula_9">Astrophysics (100%) G e? = 1 2 Me? M 1 Me? M 1 . . . G e? = 1 2 Me? M 1 . . . G e? = 1 2 1 2 + 1 2 50% -50% G s = 1 2 (1 1 2 ) . . . G s (x) = 1 2 x T x + x T x . . . G s (C) = C(C) + C(C). Graph theory (100%) G i = {(x, y) 2 R n : x i = x i } Optimization (100%) L = 1 2 kx xk 2 2 + kxk 2 2 + kxk 2 2 + kxk 2 2 . . . L = 1 2 kx xk 2 2 + kxk 2 2 + kxk 2 2 . . . L = 1 2 P n i=1 P n i=1 (x i x i ) 2 + P n i=1 (x i x i ) 2 50% -50% L = 1 2 P n i=1 P n i=1 (x i x i ) 2 + P n i=1 x 2 i . . . L = 1 N P N i=1 P N i=1 (x i x i ) 2 . . . L = 1 N P N i=1 E[x T i x i ], Statistics (100%) L = 1 N P N i=1 E[x T i x i ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantum physics</head><p>? E =~@ 2 S @t 2 ( @' @c ) k 2 @B @t (t + @ t ). ? ? deg / 11 eV [1.8V (10 32 ergs 1 )/5; 1.13 neV]. Relativity    <ref type="table">Table 6</ref>: Given a set of context words picked from an article abstract (1st column), we let TopicEq infer its topic proportions (2nd col) and generate equations (3rd col).</p><formula xml:id="formula_10">? dF ?? ? lc ??(1 f ) (d' ?? ?' ?? ? ?? dx ? d 5 Q). ? g ?? = T 1 2 h(0) 00 = h(g) (1+ ) v p ? . Linear algebra ? tr(E " X ? ) = U &gt; (tr(V " X)). ? h (?, y) = n X 2 Span P c (T[x, x]) i . Optimization ? min p p(x) subject to kp x yk 2 ? m p . ? w + = w t + g t ku t ru ? k 2 2 . Probability ? P(r ? &lt; t) = E ?wist (N ? ). ? T ? (t) = lim t!1 E[|N(t) + E[' t (x)] ? |] = limsup t!1 1 t R t 0 h(x)dt af (x).</formula><formula xml:id="formula_11">? L = 1 N P N i=1 R Ri (r i ) + V r (r i ). ? argmax U E W ? log exp ? ?( f W ) H ? .</formula><formula xml:id="formula_12">? L = 1 N P N i=1 R Ri (r i ) + V r (r i ). ? argmax U E W ? log exp ? ?( f W ) H ? .</formula><p>These qualitative results support that TopicEq is capable of fully incorporating topic information for equation modeling.</p><p>Mixtures of topics. The model can also generate equations from a mixture of topics by setting ? accordingly. To qualitatively analyze the space of the topic vector ? in terms of equation generation, we let the model generate equations while smoothly changing ? between two topics (i.e., one-hot vectors ? 1 and ? 2 ) via linear interpolation: ?(t) = (1?t)? 1 + t? 2 for t ? [0, 1]. In <ref type="table">Table 5</ref>, for two examples we show the given topic pair and its interpolation (left), and the equation greedily decoded from each ?(t) (right). We let the model start all equations from G in the first example (astrophysics and graph theory), and from L = in the second example (optmization and statistics). In both cases we observe that the generated equations make a smooth transition from one topic to the other -e.g., for the first example, from using M eff /M (astrophysics) to using linear algebraic term x T x, and finally a set notation (graph theory). In the second example, where the two topics optimization and statistics are closely related, the generated equations make a very intuitive transition: from an optimization objective with norms and regularization terms (top), to using summation terms (middle) and finally expectations (bottom; statistics topic). These observations support that TopicEq learns smooth represen-    tations for the latent topic vector ? (especially for a mixture of closely related topics), regarding equation generation.</p><formula xml:id="formula_13">#4 f m = ?(W f h m?1 + U f x m + b f ) [[LSTM]</formula><formula xml:id="formula_14">#7 h(b) = h(a) + h (b) 1! (b?a) + h (b) 2! (b?a) 2 + ? ? ? [[</formula><p>Finally, we illustrate that the model can generate equations from a given set of context words. Specifically, we let the model infer the topic proportion ? of the context words via the inference network q(?|C), and then generate equations from ? via Eq 3 (TE-LSTM). As <ref type="table">Table 6</ref> shows, the model is able to infer the right topic mixture (2nd column) and generate equations that reflect those topics (e.g., solar mass M and radius r are used for the top example; loss function L, arg max, and E for the bottom example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equation Topic Inference</head><p>Identifying the topic of equations is an important task that allows readers to obtain semantic descriptions for equations unfamiliar to them. However, while some work <ref type="bibr" target="#b16">(Schubotz et al. 2016;</ref><ref type="bibr" target="#b18">Stathopoulos et al. 2018)</ref> has studied the task of identifying the meaning of individual mathematical symbols, no prior work has succeeded in providing descriptions to entire equations from various domains.</p><p>Our TopicEq model can be utilized to identify the topic of given equations. Specifically, with a trained TopicEq model, for a given equation eq, we find the topic k ? [K] (so ? is a one-hot vector) that maximizes the likelihood p(eq | ?) in Eq 3, which is parametrized by our topic-dependent LSTM. <ref type="table" target="#tab_12">Table 7</ref> shows examples of equations across different domains (1st column), and the most likely topic inferred by our model for each equation (2nd column). We used K = 100 topics in this task. We observe that the TopicEq model correctly identifies the domains or even finer topics (e.g., note the distinction between #5 and #6) for most of the given equations. Is an RNN necessary for this task? We repeated this experiment using a bag of tokens model for equations in Eq 3 (instead of LSTM), to analyze whether the RNN equation model provides an advantage over the bag of tokens-based approach in this task. As can be seen in <ref type="table" target="#tab_12">Table 7</ref>, 3rd column, this bag-of-tokens baseline performs as well in #1 and #2, which have topic-specific variables like , ?, v, but fails in #3 and #4, which consist of a relatively generic set of symbols {f, h, m, U, W, x} and require recognizing phrases like f ? dx (work) and ?(W h + b) (neural network layer) to identify the correct topic. Indeed, the topics predicted for #3 and #4 are very generic and similar. Similarly, the bag-oftokens baseline fails to distinguish #5 and #6, most likely because it does not recognize the phase and syntax-level differences between these two equations. Finally, for #7 (Taylor Expansion), we also experimented with #7', where we just changed some variable names without altering the equation's meaning and syntax. While our TopicEq still recognizes this to be the same topic as #7, the bag-of-tokens baseline is fooled by the changed variable names and predicts a wrong topic. These observations suggest that the RNN equation model can capture phrase and syntax-level information, and can consistently infer the correct topics for equations from various domains. The TopicEq model could be used to help readers interpret equations unfamiliar to them.</p><p>Extension: Topic-aware alignment between mathematical tokens and words   <ref type="table">Table 9</ref>: Test perplexity for phrase prediction.</p><p>Baseline alignment model. We use the equations and context texts from our ContextEq corpus. Similar to (Pagael and Schubotz 2014), we consider that the descriptions of math symbols often appear in the sentence immediately before or after the given equation (immediate context). We then consider a simple alignment model between symbols s in the equation and phrases w in the immediate context, such that w ? Mult(softmax(As))</p><p>Here vector s ? R L is the bag-of-tokens representation of the equation. A ? R M ?L is the alignment matrix we estimate from the data, by maximizing the likelihood p(w | s). L, M are the vocab sizes of symbols and word descriptions. For the vocabulary of word descriptions, we collect the titles of Wikipedia pages that contain mathematical equations. We then use the top 2,000 phrases that appear in our arXiv dataset. For math symbols, we use the top L = 200.</p><p>To predict w given a single symbol s, we set s to be the one-hot vector representing s, as a surrogate. Topic-aware alignment model. To model p(w | s, ?), we want the alignment matrix A to depend on ?. Motivated by the tensor factorization method in <ref type="bibr" target="#b17">(Song, Gan, and Carin 2016)</ref>, we let</p><formula xml:id="formula_16">A(?) = W a ? diag(W b ?) ? W c<label>(7)</label></formula><p>where W a ? R M ?F , W b ? R F ?K , W c ? R F ?L are parameters to estimate. F is the number of factors, which we set to be equal to the number of topics K. To jointly perform topic modeling and alignment learning, we consider a variant of TopicEq, where we just replace Eq 3 by this topic-dependent alignment model. We train it on the ContextEq corpus. <ref type="table">Table 9</ref> shows the perplexity of the baseline / topic-aware alignment models evaluated on the held-out test set. We observe that the topic information significantly improves the alignment between math symbols and word descriptions, reducing the perplexity by more than 33% (relative).  Qualitative results. <ref type="table" target="#tab_14">Table 8</ref> shows the actual top phrases predicted by the alignment models for several math symbols that are used in a wide range of domains. The proposed Top-icEq variant indeed learns the topic-dependent alignment between symbols and words. For instance, it associates E with "expectation" for the probability topic, "electric field" for quantum physics, and "edge" for graph theory, which makes intuitive sense. On the other hand, the baseline (no topic) model associates E with "energy", which is simply the description that appears most frequently across all articles. This is another example where the TopicEq framework can be used to capture the relation of topics and mathematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Utility. We also note that our topic-aware alignment model can be conditioned on a mixture of topics by setting ? accordingly. Given a context text and equation, this model can infer the topic proportion by the topic model component, and then use the topic-aware alignment component to infer the most probable meaning of each variable in the given equation. This could aid readers to comprehend scientific documents containing mathematics unfamiliar to them.</p><p>Effect on topic modeling. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Motivated by the topical correspondence between text and mathematical equations observed in scientific documents, we proposed TopicEq, a joint topic-equation model that generates the text by a topic model and the equations by a topic-dependent RNN. This joint model outperforms existing topic models and equation models for scientific texts. We also qualitatively analyzed TopicEq, and showed its applications and extensions, such as equation topic inference and topic-aware alignment of mathematical symbols and words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For paper. Context-eq pair examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Graphical structure underlying the TopicEq model. the setting where each "document" consists of a displayed equation eq and its surrounding text C = {w (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Topic coherence of different topic models, evaluated on the held-out arXiv data. Our full TopicEq model is shown as "Ours (context + Eq LSTM)."Quantum physics spin energy field electron magnetic state states hamiltonian</figDesc><table><row><cell>Particle physics</cell><cell>higgs neutrino coupling decay scale masses mixing quark</cell></row><row><cell>Astrophysics</cell><cell>mass gas star stellar galaxies disk halo radius luminosity</cell></row><row><cell>Relativity</cell><cell>black metric hole schwarzschild gravity holes einstein</cell></row><row><cell>Number theory</cell><cell>prime integer numbers conjecture integers degree modulo</cell></row><row><cell>Graph theory</cell><cell>graph vertex vertices edges node edge number set tree</cell></row><row><cell>Linear algebra</cell><cell>matrix matrices vector basis vectors diagonal rank linear</cell></row><row><cell>Optimization</cell><cell>problem optimization algorithm function solution gradient</cell></row></table><note>Probability random probability distribution process measure time Machine learning layer word image feature sentence model cnn lstm training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Eq 3) is replaced by a baseline bag-of-tokens model similar to that for context words. The evaluation results are summarized inTable 1. The full TopicEq model is shown as "Ours (context + Eq LSTM)" in the table. We observe that TopicEq's topic model component (2nd row) performs on a par with LDA (1st row), but it achieves a significant boost (+0.01) when trained together with the LSTM equation model (4th row). Adding equations as bag of tokens (3rd row) does improve topic models marginally (+0.002), but the improvement made by using joint LSTM equation model is 5 times greater. These results</figDesc><table><row><cell>Topic Model Evaluation</cell></row></table><note>Topics learned by the TopicEq model. Left: topic name (summarized by us). Right: top words in topic. Model setting. For the inference network q(?|C), we use a 2-layer FFNN with 300 units, similar to (Miao, Yu, and Blunsom 2016; Miao, Grefenstette, and Blunsom 2017). The equation TE-LSTM architecture has two layers and state size 500, with dropout rate 0.5 applied to each layer (Sri- vastava et al. 2014). The parameters of the TopicEq model are jointly optimized by Adam (Kingma and Ba 2015), with batch size 200, learning rate 0.002, and gradient clipping 1.0 (Pascanu, Mikolov, and Bengio 2012).We first study the topic modeling performance of TopicEq, by evaluating the coherence of the learned topics ? (Chang et al. 2009; Newman et al. 2010b; Mimno et al. 2011). Specifically, following (Lau, Newman, and Baldwin 2014), we compute the normalized PMI metric on the held-out test set. As our TopicEq model incorporates joint, RNN-based equation model, to analyze its effect, we compare the full TopicEq model with the following baseline topic models: ? LDA (context only): we apply LDA to the word text ? Ours (context only): TopicEq without the equation model ? Ours (context + Eq BOW): TopicEq's joint LSTM equa- tion model (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Generated equations from given topics.</figDesc><table><row><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The TopicEq model generates equations that reflect the characteristics of given topics. Left: topic (picked from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Generated equations from given topics.</figDesc><table><row><cell>Context words star mass gravity galaxies einstein model data training likelihood gradient</cell><cell>Inferred Topics 58% Astrophysics 36% Relativity 3% Quantum physics 62% Machine learning 21% Statistics 15% Optimization</cell><cell>Generated Equations ? mb M b? ? g q? 0 ?e? . R r0 r0 dr p log(r)(r + r 0 (w)). ? G(r) =</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Generated equations from given topics.</figDesc><table><row><cell>Context words star gravity einstein mass galaxies</cell><cell>Inferred Topics 58% Astrophysics 36% Relativity 3% Quantum physics</cell><cell>Generated Equations ? mb M b? ? g q? 0 ?e? . ? G(r) = R r0 r0 dr p log(r)(r + r 0 (w)).</cell></row><row><cell>data training likelihood model gradient</cell><cell>62% Machine learning 21% Statistics 15% Optimization</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Generated equations from given topics.</figDesc><table><row><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The TopicEq model can infer the appropriate topic for equations from various domains, with better precision and consistency than bag-of-token baseline. Left: given equation. Right: topic inferred by our model and the baseline. indicates that the inferred topic is correct; not good. We verified that the exact same equations did not appear in the training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Top word phrases predicted by our topic-aware alignment model for each math symbol. We show the prediction results for three of the learned topics (3rd-5th column), as well as the non-topic baseline (2nd column).</figDesc><table><row><cell>Alignment Model</cell><cell>50 100 (# Topics)</cell></row><row><cell>Baseline (no topic)</cell><cell>602 602</cell></row><row><cell>Topic-Aware</cell><cell>406 387</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Topic coherence evaluation for each topic model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10</head><label>10</label><figDesc>, we compare our baseline topic model (top) and this TopicEq variant with the alignment component (bottom). The joint alignment model provides moderate improvements for topic modeling quality.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Matt Bonakdarpour, Paul Ginsparg, Samuel Helms, and Kriste Krstovski for their assistance, and Jungo Kasai as well as the anonymous reviewers for their feedback. This work was supported in part by a grant from the Alfred P. Sloan Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>and Jordan</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>and Lafferty</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A correlated topic model of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>and Lafferty</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">;</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Latent dirichlet allocation. JMLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topicrnn: A recurrent neural network with long-range semantic dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Studying the history of ideas using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mathematical language processing: Automatic grading and feedback for open response mathematical questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<idno>arXiv:1803.09123</idno>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Learning@ Scale</title>
		<editor>ICLR. [Krstovski and Blei 2018] Krstovski, K., and Blei, D. M</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topically driven neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldwin</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newman</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SLT</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="234" to="239" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing search results and document collections using topic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="169" to="175" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mathematical language processing project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
	</analytic>
	<monogr>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantification of identifiers in mathematics for better math information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upadhyay</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth ;</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schubotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Meuschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>L??ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">dexing and searching mathematics in digital libraries</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>CICM</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factored temporal sigmoid belief networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename><forename type="middle">;</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variable typing: Assigning meaning to variables in mathematical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikimirs 3.0: a hybrid mir system based on the context, structure and importance of formulae in a document</title>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<editor>JCDL. [Wang et al</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-stage math formula search: Using appearance-based similarity metrics at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xing ; Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
