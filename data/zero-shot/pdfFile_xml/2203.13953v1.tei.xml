<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
							<email>lzhang@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Cheng</surname></persName>
							<email>ydcheng@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction (RE) aims to identify relations between two entities in a given document.</p><p>Compared with its sentence-level counterpart, document-level RE requires complex reasoning. Previous research normally completed reasoning through information propagation on the mention-level or entity-level document-graph, but rarely considered reasoning at the entity-pair-level. In this paper, we propose a novel model, called Densely Connected Criss-Cross Attention Network (Dense-CCNet), for documentlevel RE, which can complete logical reasoning at the entity-pair-level. Specifically, the Dense-CCNet performs entity-pair-level logical reasoning through the Criss-Cross Attention (CCA), which can collect contextual information in horizontal and vertical directions on the entity-pair matrix to enhance the corresponding entity-pair representation. In addition, we densely connect multiple layers of the CCA to simultaneously capture the features of single-hop and multi-hop logical reasoning. We evaluate our Dense-CCNet model on three public document-level RE datasets, DocRED, CDR, and GDA. Experimental results demonstrate that our model achieves state-of-the-art performance on these three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we elaborate on our Dense-CCNet mode. Our entire model (as shown in <ref type="figure">Figure 2</ref>) is mainly composed of three parts: Encoder module (Sec. 2.1), Dense-CCNet module (Sec. 2.2), and Classifier module (Sec. 2.3).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to identify relationships between two entities from raw texts. It is of great importance to many real-world applications such as knowledge base construction, question answering, and biomedical text analysis <ref type="bibr" target="#b26">(Xu et al., 2021a)</ref>. Most of the existing work focuses on sentence-level RE, which predicts the relationship between entities in a single sentence <ref type="bibr" target="#b36">(Zhang et al., 2018;</ref><ref type="bibr" target="#b18">Soares et al., 2019)</ref>. However, large amounts of relationships are expressed by multiple sentences in real life <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>. According to the statistics of the DocRED <ref type="bibr" target="#b28">(Yao et al., 2019)</ref> dataset which is obtained from Wikipedia <ref type="figure">Figure 1</ref>: An example comes from the DocRED dataset, which shows that triples with overlapping entities provide important information for reasoning the complex inter-sentential relations. (a) is a document, in which different colors represent different entities. (b) lists some intra-sentential triplets, which can be easily identified. (c) shows a triple whose relationship requires logical reasoning techniques to be recognized. The arrows between (b) and (c) indicate the correlation among triples. documents, at least 40.7% of relations can only be extracted from multiple sentences. Therefore, researches on document-level RE models that can extract relational facts across sentences have gained increasing attention recently.</p><p>Compared with sentence-level RE, the main challenge is that many relations in document-level RE could only be extracted through the technique of reasoning. Since these relationship facts are not explicitly expressed in the document, the model must captures the correlation between the relationships to infer these relationships. Therefore, capturing the relevance of the relationships is essential to improve the reasoning ability of document-level RE models. <ref type="figure">Figure 1</ref> shows an example from the DocRED dataset. The <ref type="figure">Figure 1b</ref> lists the intrasentential triplets, such as (Altomonte, date of birth, 24 February 1694), (Altomonte, father, Martino Altomonte), and (Altomonte, country of citizen-ship, Austrian), which could be easily recognized since two related entities appear in the same sentence. However, it is non-trivial to predict the intersentential relations between Martino Altomonte and Austrian because the document does not explicitly express the relationship between them. In fact, the model needs to firstly capture the correlation among (Altomonte, father, Martino Altomonte), (Altomonte, country of citizenship, Austrian), and (Martino Altomonte, country of citizenship, Austrian) and use logical reasoning techniques to identify this complex relationship, as shown in <ref type="figure">Figure  1c</ref>.</p><p>To extract these complex relationships, most current approaches constructed a document-level graph based on heuristics, structured attention, or dependency structures <ref type="bibr" target="#b32">(Zeng et al., 2020;</ref><ref type="bibr" target="#b14">Nan et al., 2020;</ref><ref type="bibr" target="#b2">Christopoulou et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2020a)</ref>, and then perform inference with graph convolutional network (GCN) <ref type="bibr" target="#b6">(Guo et al., 2019;</ref><ref type="bibr" target="#b10">Kipf and Welling, 2016)</ref> on the graph. Meanwhile, considering the transformer architecture can implicitly model long-distance dependencies, some studies <ref type="bibr" target="#b19">(Tang et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021)</ref> directly applied pre-trained models rather than explicit graph reasoning . These methods captures the correlation between relationships through the information transfer between tokens, mentions or entities, which can be indirect and inefficient.</p><p>In this paper, we use the information transfer between the entity-pairs to capture the correlation between relationships more efficiently and directly. Moreover, as it can be seen in <ref type="figure">Figure  1</ref>, only (Altomonte, father, Martino Altomonte) and (Altomonte, country of citizenship, Austrian) triples, rather than the other triples, provide important information to infer the relations between Martino Altomonte and Austrian. Inspired by this phenomenon, we guess that the interaction between the triples with overlapping entities is a reasonable way of entity-pair-level reasoning.</p><p>Therefore, we propose a novel Dense-CCNet model by integrating the Criss-Cross Attention (CCA)  into the densely connected framework <ref type="bibr" target="#b7">(Huang et al., 2017)</ref>. The CCNet model  is an advanced semantic segmentation model recently proposed in the field of computer vision, which captures global context information from full-image through the CCA (as shown in <ref type="figure" target="#fig_0">Figure 2</ref>). The CCA applied to the entity-pair matrix can realize the interac-tion between entity-pairs with overlapping entities, which can complete the logical reasoning of the entity-pair-level. To fully capture the features of single-hop and multi-hop reasoning, we stack the multi-layer modules CCA modules by the densely connected framework. The lower layers in Dense-CCNet can capture local interdependence among entity-pairs and complete single-hop logical reasoning, while the upper layers can capture global interdependence among entity-pairs and complete multi-hop logical reasoning.</p><p>Since the CCA can only complete the reasoning mode of A? * ?B, we expand the field (which single-layer CCA can pay attention to) to cover a wider range of reasoning modes, such as A? * ?B, A? * ?B, and A? * ?B. In addition, we found that more than 90% of the entity pairs are irrelevant (that is, there is no relationship between two entities) in the document, and these entity pairs may limit the model's reasoning ability.</p><p>To reduce the influence of unrelated entity-pairs, we use two techniques: (1) Clustering loss: The clustering loss separates the related entity-pairs (that is, there is relationship between two entities) from the unrelated entity-pairs in the representation space.</p><p>(2) Attention bias: We add a bias term to a bias term to the attention score of the CCA, which makes the CCA pay more attention to related entity pairs.</p><p>In summary, our main contributions are as follows:</p><p>? We introduce the Dense-CCNet module that can more directly and effectively model the correlation between relationships through the entity-pair-level reasoning.</p><p>? We introduce four methods to further improve the reasoning ability of the CCA: Dense connection, Expanding the Field of Attention, Clustering loss, and Attention bias.</p><p>? Experimental results on three public document-level RE datasets shows that our Dense-CCNet model can achieve state-of-the-art performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder Module</head><p>We first treat the document D as a sequence of words, i.e. D = {w i } L i , where L is the total number of words in the document. Then, we insert special symbols E i and \E i to mark the start and end positions of the mention respectively, where E i is the entity id of the mention. It is an improved version of entity marker technology <ref type="bibr" target="#b40">(Zhou et al., 2021;</ref><ref type="bibr" target="#b17">Shi and Lin, 2019;</ref><ref type="bibr" target="#b18">Soares et al., 2019)</ref> by introducing entity id information which can help align the information of different mentions from the same entity. Finally, we leverage the pre-trained language model as an encoder to convert documents D into a sequence of contextual embeddings as follows:</p><formula xml:id="formula_0">H = [h 1 , ..., h L ] = BERT ([w 1 , ..., w L ]). (1)</formula><p>We take the embedding of E i at the start of mentions m j as the mention embedding h m j . Then, we leverage logsumexp pooling <ref type="bibr" target="#b9">(Jia et al., 2019)</ref>, a smooth version of max pooling, to obtain the embedding h e i of entity e i which contains p i mentions {m i j } p i j :</p><formula xml:id="formula_1">h e i = log p i j=1 exp(h m i j ).<label>(2)</label></formula><p>After obtaining the embedding of all entities in the document, we construct an Entity-Pair Matrix M ?R Ne?Ne?d through the Relation Feature Calculation module, where N e refers to the number of entities and d is the dimension of the relation feature vector. The M s,o item in M represents the relation feature vector between the entity e s and the entity e o , which is calculated as follows:</p><formula xml:id="formula_2">M s,o = F F N N ([u s , u o ]), u s = tanh(W s [h es , h doc , c s,o ]), u o = tanh(W o [h eo , h doc , c s,o ),<label>(3)</label></formula><p>where h es is subject entity embedding, h eo is object entity embedding, h doc is document embedding, and c s,o is entity-pair-aware context feature, F F N () refers to a feed-forward neural network, W o , W s is the learnable weight matrix.</p><p>We use the embedding of the document start token "[CLS]" as the document embedding h doc , which can help aggregate cross-sentence information and provide document-aware representation.</p><p>The entity-pair-aware context feature c s,o represents the contextual information in the document that the entity e s and the entity e o pay attention to together. The c s,o is formulated as follows:</p><formula xml:id="formula_3">c s,o = L i=1 A s,i ? A o,i ? h i ,<label>(4)</label></formula><p>where A s,i is the attention score of the entity e s paying attention to the i-th token h i in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dense-CCNet Module</head><p>In this part, we introduce the Dense-CCNet module in detail. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the Dense-CCNet module consists of densely connected N identical layers that are composed of two sub-modules: the Criss-Cross Attention (CCA) module and the Transition module.</p><p>We followed the CCNet model  for the CCA module. Each entity pair in the entity-pair matrix can pay attention to the relation feature of other entity pairs in horizontal and vertical directions through the CCA module. The CCA module can be formulated as follows:</p><formula xml:id="formula_4">M s,o = Ne i=1 A (s,o)?(s,i) M s,i + A (s,o)?(i,o) M i,o where A (s,o)?(s,i)</formula><p>is the attention score of the M s,o paying attention to the M s,i . Therefore, the CCA module can complete entity-pair-level one-hop reasoning on the entity-pair matrix, and it is possible to complete multi-hop reasoning by stacking multiple layers of the CCA module.</p><p>However, simply using Recurrent Criss-Cross Attention (RCCA)  to complete the logical reasoning of the entity-pair level may have several problems: (1) The RCCA only focuses on the high-level multi-hop inference feature and ignores the low-level single-hop inference feature which is also very important for documentlevel RE.</p><p>(2) The CCA module can only model the reasoning mode of A? * ?B, but cannot model the reasoning mode of A? * ?B, A? * ?B, and A? * ?B.</p><p>(3) Since most of the entity pairs are irrelevant in the document, the entity-pair matrix M contains a lot of noise which may affect the reasoning ability of the model. Therefore, distinguishing related entity-pairs from unrelated entity-pairs and strengthening the interaction of the relationship feature vectors of related entity-pairs is the key to improving The reasoning ability of the model. To solve these problems, we have introduced the following methods:</p><p>Dense Connection: Since dense connections can reuse the features of low-level networks, we stack multiple layers of the CCA modules through the densely connected framework to solve the problem (1) . In addition, the dense connection can also reduce noise propagation to a certain extent. Expanding the Field of Attention: To allow the CCA module to model more inference modes, we modify the CCA module as follows:</p><formula xml:id="formula_5">M s,o = Ne i=1 A (s,o)?(s,i) M s,i + A (s,o)?(i,o) M i,o + A (s,o)?(s,i) M s,i + A (s,o)?(i,o) M i,o</formula><p>The modified CCA module can cover a wider range of reasoning modes including: A? * ?B, A? * ?B, A? * ?B, and A? * ?B. Clustering Loss: We design a clustering loss function that separates the related entity pairs and the unrelated entity pairs in the feature space to reduce the influence of unrelated entity pairs on the inference process. Clustering Loss is formulated as follows:</p><formula xml:id="formula_6">L dist = (max{0, (? + cos(? 0 , ? 1 ))}) 2 , L var1 = i?Npos (max{0, (? ? cos(? 1 , f i ))}) 2 , L var0 = j?Nneg (max{0, (2? ? cos(? 0 , f j ))}) 2 , L C = ?L dist + ?L var0 + ?L var1 ,</formula><p>where N neg is the set of the irrelevant entity pairs, N pos is the set of the related entity pairs, ? 0 is the average vector of the feature vectors of the entity pairs in the N neg , ? 1 is the average vector of the feature vectors of the entity pairs in the N pos , f i is the feature vector of the i-th entity pair. Attention Bias: To make the CCA more focused on the related entity pairs, we added a bias to the attention score of the CCA:</p><formula xml:id="formula_7">A (s,o)? = sof tmax(s (s,o)? + bias ), (5)</formula><p>where, bias is a bias term of the entity pair , which reflects the confidence that the entity pair is a related entity pair. bias is predicted and trained through a feed-forward neural network:</p><formula xml:id="formula_8">bias * = F F N N (f ), L bias = BCE(bias , label 01 )<label>(6)</label></formula><p>Where BCE is a cross-entropy loss function, and label 01 is the 0-1 label of the entity pair. The Transition module controls the dimensions of the new features generated by each layer of the Dense-CCNet model, which reduces the computational complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification Module</head><p>We use the Dense-CCNet to convert the original entity-pair matrix M into a new context-enhanced entity-pair matrix M . Given an entity pair (e s , e o ), we first concatenate the two entity embedding (h es , h eo ) and new relation feature M so , then we obtain the distribution of relationship via a bilinear function. Formally, we have:</p><formula xml:id="formula_9">z s = tanh(W s [h es , M so ]), z o = tanh(W o [h eo , M so ]), P (r|e s , e o ) = ?(z T s W r Z o + b r ).<label>(7)</label></formula><p>For the loss function, we use adaptivethresholding loss <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref>, which learns an adaptive threshold for each entity pair. The loss function is broken down into two parts as shown below:</p><formula xml:id="formula_10">L 1 = ? r?P D log( exp(logit r ) r ?P D ?{T H} exp(logit r )</formula><p>),</p><formula xml:id="formula_11">L 2 = ? log( exp(logit T H ) r ?N D ?{T H} exp(logit r )</formula><p>),</p><formula xml:id="formula_12">L adap = L 1 + L 2 ,</formula><p>where T H is an introduced class to separate positive classes and negative classes: positive classes would have higher probabilities than T H, and negative classes would have lower probabilities than T H, P D and N D are the positive classes set and negative classes set in document D respectively. Finally, our total loss function is defined as follows:</p><formula xml:id="formula_13">L = L adap + L bias + L C ,<label>(8)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our Dense-CCNet model on three public document-level RE datasets. The statistics of the datasets could be found in Appendix A.</p><p>? DocRED <ref type="formula">(</ref> ? CDR <ref type="bibr" target="#b12">(Li et al., 2016)</ref>: The CDR is a biomedical dataset is constructed by using the PubMed abstracts, which aims to predict the binary interactions between Chemical and Disease concepts. The CDR contains only one relationship and consists of 1500 human-annotated documents in total. The CDR are equally split into training, development, and test sets.</p><p>? GDA <ref type="bibr" target="#b25">(Wu et al., 2019)</ref>: Similar to the CDR, the GDA is also a dataset in the biomedical domain, but is constructed by distant supervision from the MEDLINE abstracts. The GDA contains 29192 documents as the training set and 1000 as the test set. Since there is no development set, we follow <ref type="bibr" target="#b2">(Christopoulou et al., 2019)</ref> to divide the training set into two parts according to the ratio of 8:2 and use them as training set and development set respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>Our model was implemented based on PyTorch. We used cased BERT-base <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> as the encoder on DocRED and SciBERT-base (Beltagy et al., 2019) on CDR and GDA. We set the number of layers of Dense-CCNet to 3. Our model is optimized with AdamW (Loshchilov and Hutter, 2017) with a linear warmup <ref type="bibr">(Goyal et al., 2017)</ref> for the first 6% steps followed by a linear decay to 0. All hyper-parameters are tuned on the development set, some of which are listed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on the DocRED Dataset</head><p>The experimental results of our model on DocRED are shown in <ref type="table">Table 1</ref>. We followed <ref type="bibr" target="#b28">(Yao et al., 2019)</ref> and used F 1 and IgnF 1 as the evaluation metrics to evaluate the overall performance of the model. IgnF 1 denotes the F 1 score excluding the relational facts that are shared by the training and dev/test sets. We compare the Dense-CCNet model with the following two types of models on the Do-cRED dataset :</p><p>? Graph-based Models: these models first construct the document-graph from the document,  <ref type="bibr" target="#b27">(Xu et al., 2021b)</ref> 58.13 60.18 57.12 59.45 GAIN-BERT base <ref type="bibr" target="#b32">(Zeng et al., 2020)</ref> 59  <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> and DocuNet  for the scores of all baseline models. The results on the test set are obtained by submitting to the official Codalab. and then perform inferences through GCN (Kipf and Welling, 2016) on the graph. We include GEDA , LSR <ref type="bibr" target="#b14">(Nan et al., 2020)</ref>, GLRE <ref type="bibr" target="#b21">(Wang et al., 2020a)</ref>, GAIN <ref type="bibr" target="#b32">(Zeng et al., 2020)</ref>, and HeterGSAN <ref type="bibr" target="#b27">(Xu et al., 2021b)</ref> for comparison.</p><p>? Transformer-based Models: These models directly use the pre-trained language models for document-level RE without graph structures. we compared BERT base , BERT-TS base , HIN-BERT base <ref type="bibr" target="#b19">(Tang et al., 2020)</ref>, CorefBERT base , CorefBERT base , and ATLOP-BERT base <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> with our model.</p><p>In addition, we also consider the DocuNet  model in the comparison, which formulates document-level RE as a semantic segmentation problem. As shown in <ref type="table">Table 1</ref>, our Dense-CCNet model achieved 62.74%F 1 and 62.55% F 1 in the training set and test set, which outperforms the state-of-theart model with 0.91% F 1 and 0.69% F 1 respectively. Compared with the GAIN model that is the state-of-the-art model of graph-based methods, our model exceeds it by 1.52% F 1 on the dev set and 1.31% F 1 on the test set. This proves that the logical reasoning on the entity-pairs level is more effective than previous methods on mentions or entities level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on the Biomedical Datasets</head><p>On the CDR and GDA data sets, we compared BRAN <ref type="bibr" target="#b20">(Verga et al., 2018)</ref>, EoG <ref type="bibr" target="#b2">(Christopoulou et al., 2019)</ref>, LSR <ref type="bibr" target="#b14">(Nan et al., 2020)</ref>, DHG <ref type="bibr" target="#b37">(Zhang et al., 2020c)</ref>, GLRE <ref type="bibr" target="#b21">(Wang et al., 2020a)</ref>, ATLOP <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref>, and DocuNet  with our model. The experimental results on two biomedical datasets are shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Our Dense-CCNet-SciBERT base model obtained 77.06(?0.71)% F 1 and 86.44(?0.25)% F 1 on two data sets respectively, which is also the new state-of-the-art result. The Dense-CCNet-SciBERT base improved the F 1 score by 0.76% and 1.14% on CDR and GDA compared with DocuNet-SciBERT base . These results demonstrate the strong applicability and generality of our approach in the biomedical field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We conducted an ablation experiment to validate the effectiveness of different components of our Dense-CCNet model on the development set of the DocRED dataset. The results are listed in <ref type="table">Ta</ref>  <ref type="bibr" target="#b20">(Verga et al., 2018)</ref> 62.1 -EoG <ref type="bibr" target="#b2">(Christopoulou et al., 2019)</ref> 63.6 81.5</p><p>LSR <ref type="bibr" target="#b14">(Nan et al., 2020)</ref> 64.8 82.2 DHG <ref type="bibr" target="#b37">(Zhang et al., 2020c)</ref> 65.9 83.1 GLRE <ref type="bibr" target="#b21">(Wang et al., 2020a)</ref> 68.5 -SciBERT base <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref> 65.1 82.5 ATLOP-SciBERT base <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> 69.4 83.9</p><p>DocuNet-SciBERT base  76.3 85.3</p><p>Dense-CCNet-SciBERT base 77.06 86.44 From <ref type="table" target="#tab_5">Table 3</ref>, we can observe that the w/o Dense leads to a drop of 1.62% F 1, which shows that the features of low-level inference are very helpful for relation extraction and the features of high-level inference may contain noises. The w/o Expanding Attention caused a performance drop of 0.83% F 1, which indicates that document-level RE may include multiple inference modes and our model can effectively expand the reasoning mode of the Criss-Cross Attention through the Expanding the Field of Attention technology.</p><p>The w/o Clustering Loss module and w/o Attention Bias module led to performance degradation of 0.72% F 1 and 1.14% F 1 points respectively, which reflects that reducing noise (irrelevant entitypairs) may be the key to further improving entitypair level inference. We guess that the most ideal entity-pair-level reasoning method may be to only propagate information between related entity pairs.</p><p>In addition, we also introduced the ablation study of the number of layers of the Dense-CCNet, and the experimental results are shown in <ref type="table" target="#tab_6">Table 4</ref>. When the number of layers increases from 2 to 3, our model can capture more multi-hop inference features, so the performance of the model is improved by 1.3% F 1. However, when the number of layers is increased to 4, the performance drops slightly by 0.37% F 1 points. The possible reasons is that the noise has a greater impact on the highlevel feature or the model falls into over-fitting.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study</head><p>We followed GAIN <ref type="bibr" target="#b32">(Zeng et al., 2020)</ref> to select the same example and conduct a case study to further illustrate that our Dense-CCNet model can effectively capture the interdependence among entitypairs and perform entity-pair-level logical reasoning compared with the baseline. The experimental results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. <ref type="figure" target="#fig_2">Figure 3c</ref> demonstrates that our model has better logical reasoning ability than the baseline. <ref type="figure" target="#fig_2">Figure  3a</ref> shows that the entity pair <ref type="bibr">("Without Me", May 26, 2002)</ref> has more attention to the entity pairs ("Without Me", The Eminem Show) and <ref type="bibr">(The Eminem Show, May 26,2002)</ref>, which indicates that our model could capture the correlation these among entity-pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Sentence-level RE: Early research on RE focused on sentence-level RE, which predicts the relationship between two entities in a single sentence. Many approaches <ref type="bibr" target="#b31">(Zeng et al., 2015;</ref><ref type="bibr" target="#b4">Feng et al., 2018;</ref><ref type="bibr">Zhang et al., 2020b,a;</ref><ref type="bibr">Wang et al., 2020b;</ref><ref type="bibr" target="#b30">Yu et al., 2020;</ref><ref type="bibr" target="#b24">Wu et al., 2021;</ref><ref type="bibr">Zheng et al., 2021)</ref> have been proven to effectively solve this problem. Since many relational facts in real applications can only be recognized across sentences, sentence-level RE face an inevitable restriction in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level RE:</head><p>To solve the limitations of sentence-level RE in reality, a lot of recent work gradually shift their attention to document-level RE. Since graph neural network(GNN) can effectively model long-distance dependence and complete logical reasoning, Many methods based on document-graphs are widely used for documentlevel RE. Specifically, they first constructed a graph structure from the document, and then applied the <ref type="bibr">GCN (Kipf and Welling, 2016;</ref><ref type="bibr" target="#b7">Huang et al., 2017)</ref> to the graph to complete logical reasoning. The graph-based method was first introduced by <ref type="bibr" target="#b15">(Quirk and Poon, 2016)</ref> and has recently been extended by many works <ref type="bibr" target="#b2">(Christopoulou et al., 2019;</ref><ref type="bibr" target="#b37">Zhang et al., 2020c;</ref><ref type="bibr" target="#b39">Zhou et al., 2020;</ref><ref type="bibr" target="#b21">Wang et al., 2020a;</ref><ref type="bibr" target="#b14">Nan et al., 2020;</ref><ref type="bibr" target="#b32">Zeng et al., 2020;</ref><ref type="bibr" target="#b27">Xu et al., 2021b)</ref>.  proposed the Graph Enhanced Dual Attention network (GEDA) model and used it to characterize the complex interaction between sentences and potential relation instances. <ref type="bibr" target="#b32">(Zeng et al., 2020)</ref> propose Graph Aggregation-and-Inference Network (GAIN) model. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document and then constructs an entity-level graph (EG), finally uses the path reasoning mechanism to infer relations between entities on EG. <ref type="bibr" target="#b14">(Nan et al., 2020)</ref> proposed a novel LSR model, which constructs a latent documentlevel graph and completes logical reasoning on the graph.</p><p>In addition, due to the pre-trained language model based on the transformer architecture can implicitly model long-distance dependence and complete logical reasoning, some studies <ref type="bibr" target="#b19">(Tang et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021;</ref> directly apply pre-trained model without introducing document graphs. <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> proposed an ATLOP model that consists of two parts: adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. Recently, the state-of-the-ar model, DocuNet , formulates document-level RE as semantic segmentation task and capture global information among relational triples through the Ushaped segmentation module <ref type="bibr" target="#b16">(Ronneberger et al., 2015)</ref>.</p><p>However, none of the models completes the logical reasoning for document-level RE through the information propagation between the entitypairs. Our Dense-CCNet model can capture the correlation among entity-pairs and complete the entity-pair-level reasoning by integrating the CCA  into the dense connection framework <ref type="bibr" target="#b7">(Huang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we propose a novel Dense-CCNet model by integrating the Criss-Cross Attention into the densely connected framework. Dense-CCNet model can complete entity-pairs-level logical reasoning and model the correlation between entity pairs. Experiments on three public document-level RE datasets demonstrate that our Dense-CCNet model achieved better results than the existing stateof-the-art model. In the future, we will try to use our model for other inter-sentence or documentlevel tasks, such as cross-sentence collective event detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DocRED</head><p>CDR <ref type="table" target="#tab_3">GDA   Train  3053  500  23353  Dev  1000  500  5839  Test  1000  500  1000  Relations  97  2</ref>    A Datasets <ref type="table" target="#tab_8">Table 5</ref> details the statistics of the three documentlevel relational extraction datasets, DocRED, CDR, and GDA. These statis-tics further demonstrate the complexity of entity structure in document-level relation extraction tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-parameters Setting</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture of our Dense-CCNet-based document-level RE model. Firstly, the BERT model encodes the input document to obtain the context embedding of each words, and then we obtains the representations of the entities (h es ,h eo ) through a pooling operation. Secondly, the relation features (M s,o /M o,s ) of all entity pairs are calculated through the Relation Feature Calculation module, which is used to construct the original entity-pair matrix (M ). Thirdly, the Dense-CCNet module transforms M into a context-enhanced entity-pair matrix (M ). Finally, the context-enhanced relation features (M s,o ) of the entity pairs (e s , e o ), the subject entity embedding (h es ), and object entity embedding (h eo ) are concatenated and inputted to the classifier to predict the relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ble 3, where w/o Dense Connection replaces the Densely connected Criss-Cross Attention with the Recurrent Criss-Cross Attention (RCCA), w/o Expanding Attention uses standard Criss-Cross Attention and does not extend the field of attention, w/o Clustering Loss and w/o Attention Bias respectively removes the Clustering Loss and the Attention Bias from our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Case study of our Dense-CCNet mode and the baseline model. (c) shows that our model has better logical reasoning ability than the baseline. (a) visualize the attention scores of entity pairs ("Without Me", May 26, 2002) paying attention to other entity pairs, which shows that our model can effectively capture the correlation among entity-pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>F1 scores (%) on test sets of the CDR and the GDA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Ablation study of the Dense-CCNet on the</cell></row><row><cell cols="3">development set of the DocRED. We turn off different</cell></row><row><cell cols="2">components of the model one at a time.</cell><cell></cell></row><row><cell>Layer-number</cell><cell>Dev</cell><cell></cell></row><row><cell></cell><cell>IgnF 1</cell><cell>F 1</cell></row><row><cell>2-Layer</cell><cell>59.41</cell><cell>61.44</cell></row><row><cell>3-Layer</cell><cell>60.72</cell><cell>62.74</cell></row><row><cell>4-Layer</cell><cell>60.30</cell><cell>62.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of the Dense-CCNet with the different numbers of layers on the development set of the DocRE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Summary of DocRED, CDR and GDA datasets.</figDesc><table><row><cell>Hyperparam</cell><cell>DocRED</cell><cell>CDR</cell><cell>GDA</cell></row><row><cell></cell><cell>BERT</cell><cell>SciBERT</cell><cell>SciBERT</cell></row><row><cell>Batch size</cell><cell>8</cell><cell>16</cell><cell>16</cell></row><row><cell>Epoch</cell><cell>100</cell><cell>20</cell><cell>5</cell></row><row><cell>lr for encoder</cell><cell>2e-5</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell>lr for other parts</cell><cell>1e-4</cell><cell>5e-5</cell><cell>5e-5</cell></row><row><cell>{?, ?}</cell><cell>{1, 0.5}</cell><cell>{1, 0.5}</cell><cell>{1, 0.5}</cell></row><row><cell>{?, ?, ?}</cell><cell>{1, 1, 1}</cell><cell>{1, 1, 1}</cell><cell>{1, 1, 1}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters Setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>details our hyper-parameters setting. All of our hyperparameters were tuned on the development set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07650</idno>
		<title level="m">Adaprompt: Adaptive promptbased finetuning for relation extraction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00228</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07510</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02347</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph enhanced dual attention network for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06312</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04873</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simple bert models for relation extraction and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05255</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03158</idno>
		<title level="m">Matching the blanks: Distributional similarity for relation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hin: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12084</biblScope>
			<biblScope unit="page">197</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10569</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10359</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Lun</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09841</idno>
		<title level="m">Ningyu Zhang, and Yefeng Zheng. 2020b. Finding influential instances for distantly supervised relation extraction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Curriculum-meta learning for order-robust continual relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuekai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/2101.01926</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research in Computational Molecular Biology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10249</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 35th AAAI Conference on Artificial Intelligence (AAAI-21)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06127</idno>
		<title level="m">Docred: A large-scale document-level relation extraction dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06870</idno>
		<title level="m">Coreferential reasoning learning for language representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bridging text and knowledge with multi-prototype embedding for few-shot relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16059</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Double graph based reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13752</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03618</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Openue: An open toolkit of universal extraction from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation adversarial network for low resource knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with dual-tier heterogeneous graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09895</idno>
		<title level="m">Bin Qin, Ming Xu, and Yefeng Zheng. 2021. Prgc: Potential relation and global correspondence based joint relational triple extraction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Global context-enhanced graph convolutional networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
