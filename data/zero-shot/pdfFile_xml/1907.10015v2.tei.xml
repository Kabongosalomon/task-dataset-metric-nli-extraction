<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>discrete cosine transform (DCT)</term>
					<term>JPEG</term>
					<term>compressed-domain analytics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typical convolutional networks are trained and conducted on RGB images. However, images are often compressed for memory savings and efficient transmission in real-world applications. In this paper, we explore methods for performing semantic segmentation on the discrete cosine transform (DCT) representation defined by the JPEG standard. We first rearrange the DCT coefficients to form a preferred input type, then we tailor an existing network to the DCT inputs. The proposed method has an accuracy close to the RGB model at about the same network complexity. Moreover, we investigate the impact of selecting different DCT components on segmentation performance. With a proper selection, one can achieve the same level accuracy using only 36% of the DCT coefficients. We further show the robustness of our method under the quantization errors. To our knowledge, this paper is the first to explore semantic segmentation on the DCT representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep convolutional neural networks (CNNs) have shown tremendous success in a variety of computer vision tasks, such as image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, and semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. These CNN models have been broadly adopted not only in academia but also in industry. In many realworld applications, images/videos are compressed into specific coding formats for storage savings and high-speed transmission. For example, JPEG <ref type="bibr" target="#b23">[24]</ref>, GIF, PNG are the widespread image compression standards, and H.264, H.265 (HEVC) are the recent video compression standards. However, most of the existing CNNs are trained to perform on the RGB format images, i.e., the input data are expressed as an array of RGB pixels. Hence, the compressed data have to undergo a decompression step before being fed to a CNN. However, this step is time-consuming and requires high computation and memory demands, so it is favored to skip it. Therefore, performing computer vision tasks in the compressed domain has become an emerging research topic.</p><p>The discrete cosine transform (DCT) representation encodes the original spatial-domain RGB images into components in the frequency domain. The DCT representation has been widely used in image compression, and it is the core processing of the JPEG compression standard (with a block size of 8?8). The JPEG standard was established in 1992, but it is still one of the most popular image file formats today. Except for JPEG 2000, the recent image/video coding standards (H.264 and H.265) also adopt DCT (with possibly different sizes). Several researchers started looking into the possibilities of using DCT coefficients to do the classification or detection tasks. In this study, we extend this direction to semantic segmentation, exploring the semantic segmentation task in the DCT domain.</p><p>In this paper, we propose a method for conducting semantic segmentation on the compressed DCT representation. <ref type="figure" target="#fig_0">Figure 1</ref> shows the pipeline. We first rearrange the order and format of the DCT coefficients before feeding them to CNNs. After the rearrangement, the spatial relationship of these coefficients are represented on the first and the second dimensions (spatial dimension), and their frequency representation is coded on the third dimension. We call this technique as Frequency Component Rearrangement (FCR). Next, we choose EDANet <ref type="bibr" target="#b15">[16]</ref> as our baseline network owing to its good balance between performance and complexity. Because the spatial resolution of the FCRed DCT representation is 1/8 to its RGB counterpart, the three downsampling operations of EDANet would make the feature size too small. Therefore, we modify EDANet by removing all the downsampling operations to facilitate the DCT input data size. We call the modified network as DCT-EDANet. The complexity reduction due to the smaller input spatial size enables DCT-EDANet to be deeper, increasing its capacity. Our method achieves an accuracy close to that of the RGB baseline at about the same network complexity. To the best of our knowledge, we are the first </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring Semantic Segmentation on the DCT Representation</head><p>Shao-Yuan Lo Hsueh-Ming Hang National Chiao Tung University sylo95.eecs02@g2.nctu.edu.tw, hmhang@nctu.edu.tw to explore this research topic, DCT-domain semantic segmentation.</p><p>One step further, we examine how different DCT components would affect performance. We select different numbers of lowfrequency sub-bands from each of the three Y, Cb, and Cr channels (YCbCr color space), and form various combinations of the coefficients as the system inputs for both training and testing. We demonstrate the impact of these components on performance, and identify the best input coefficient proportion of the Y, Cb, and Cr components, which can achieve a similar accuracy using only 36% of input coefficients compared to the model with 100% input coefficients. The effect of quantization using different quality factors is also investigated. The proposed system is robust against serious quantization errors.</p><p>The rest of this article is organized as follows. Section 2 covers several related work on the compressed-domain computer vision. Section 3 reviews the JPEG compression, DCT operation, and DCT representation. In Section 4, we describe the proposed method and its implementation results. Then, Section 5 and Section 6 report our experiments on the coefficient selection and quantization effect, respectively. Finally, Section 7 concludes this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several studies on compressed-domain analytics have been reported using the conventional image processing techniques. These studies aim at, image indexing <ref type="bibr" target="#b17">[18]</ref>, face recognition <ref type="bibr" target="#b7">[8]</ref>, image retrieval <ref type="bibr" target="#b8">[9]</ref>, etc. In the deep learning age, a few researchers started employing CNNs to do analytics in the compressed-domain. One example is multiple JPEG compression detection. Verma et al. <ref type="bibr" target="#b22">[23]</ref> computed histograms of different DCT sub-bands, then concatenated these histograms to form a 1-D vector. This vector would be sent to a 1-D CNN to do detection. The multi-branch CNN architecture <ref type="bibr" target="#b13">[14]</ref> processes different DCT sub-bands separately, then these features are concatenated for the final inference.</p><p>Another research trend is to develop tailored CNNs to DCT input for image classification or object detection. Ghosh and Chellappa <ref type="bibr" target="#b4">[5]</ref> applied the DCT operation to the feature maps generated by the first convolutional layer of their CNN, accelerating the convergence in training. Ulicny and Dahyot <ref type="bibr" target="#b19">[20]</ref> used a CNN to classify images in the DCT domain. Ulicny et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> designed the so-called harmonic blocks to replace the traditional convolutions. These blocks generate features by learning combinations of spectral filters defined by DCT. Gueguen et al. <ref type="bibr" target="#b6">[7]</ref> developed ResNet-50 <ref type="bibr" target="#b9">[10]</ref> variants, which can operate on JPEG data. It is shown to be faster and more accurate. Ehrlich and Davis <ref type="bibr" target="#b3">[4]</ref> created a DCT-domain ResNet, which is mathematically equivalent to the spatial-domain network, by including the transform into network weights. SSD_freq <ref type="bibr" target="#b2">[3]</ref> is a modified version of SSD <ref type="bibr" target="#b14">[15]</ref> so that it is able to process the DCT inputs. It is a pioneer in the JPEG-domain object detection.</p><p>Moving towards a different direction, Torfason et al. <ref type="bibr" target="#b18">[19]</ref> focused on the learned compressed representation other than JPEG. They jointly trained a compression network with an inference network and bring performance gain. On the video side, Wu et al.</p><p>[25] designed a compressed video action recognition system by using separate networks for I-frames and P-frames. Their approach is more efficient than the conventional 3D convolution structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JPEG COMPRESSION</head><p>This work is motivated by the JPEG standard <ref type="bibr" target="#b23">[24]</ref>, a dominant image file format. The JPEG compression algorithm is summarized in the following steps.</p><p>1. Convert the color space from RGB to YCbCr. 2. Perform block-wise (8?8 pixels) DCT. 3. Quantize the DCT coefficients by a quantization matrix. 4. Encode the coefficients by entropy encoding.</p><p>The YCbCr color space consists of a luma (luminance) component (Y), and two chroma (chrominance) components (Cb and Cr). The chroma channels are usually subsampled by a factor of 2, for human vision is less sensitive to subtle color changes than to subtle illuminance changes. In order to observe the influence of different DCT coefficients on performance, we bypass the chroma subsampling step for the DCT representation in our implementation. Each of the three channels is partitioned into blocks of 8?8 pixels, and 128 is subtracted from all the pixel values. Then, each block is transformed by 2-D DCT (type-II), which is defined as:</p><formula xml:id="formula_0">, = 1 4 ? ? , cos[ (2 + 1) 16 ] cos[ (2 + 1) 16 ] 7 =0 7 =0<label>(1)</label></formula><p>where ?u and ?v are the normalizing factors, gx,y is the pixel value at (x,y), Gu,v is the DCT coefficient at (u,v), and 0 ? u, v &lt; 8.</p><p>In the DCT domain, the pixel information is represented by spatial frequency spectrums. The upper-left of each 8?8 block comprises low-frequency sub-bands, while the high-frequency subbands are located on the bottom-right. If the compression is lossy, the frequency coefficients are quantized by a quantization matrix and rounded to integers. Because human eyes are less sensitive to high-frequency variations, the high-frequency sub-hands are recorded with a lower accuracy or even discarded. Finally, the quantized coefficients are coded by the run-length encoding (RLE) and Huffman coding. Since the structure of Huffman codes is incompatible with the input of CNNs, we follow the typical compressed-domain analytic setup; that is, use the outputs of Step 2 or Step 3 as our DCT representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>We develop a method for doing semantic segmentation on the DCT representation. In this section, we first describe the Frequency Component Rearrangement (FCR) technique, which is used to rearrange the DCT coefficients to make them be easily exploited by CNNs. Next, we introduce the proposed DCT-EDANet, a modification of EDANet <ref type="bibr" target="#b15">[16]</ref> to operate in the DCT domain. Finally, the benchmark, training procedure, and experimental results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Frequency Component Rearrangement</head><p>After performing block-wise DCT on each of the Y, Cb, and Cr channels, the spatial dimension (each channel) contains not only the spatial relationship but also the frequency relationship of the DCT coefficients. To be more specific, in an 8?8 block, the location of each coefficient in this block corresponds its frequency index, i.e., each coefficient represents a specific frequency component. However, when a CNN performs the convolution operation, such frequency relationship is treated as their spatial relationship. This makes CNNs misinterpret the input information and fail to extract essential features in the DCT domain.</p><p>To solve this problem, we use FCR to rearrange the DCT coefficients before feeding them into CNNs. FCR reshapes each block of dimension (8, 8, 1) into <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">64)</ref>. That is, each frequency component is placed at its corresponding channels on the third dimension. For an RGB image with a size of (h, w, 3), the size of its FCRed DCT representation is (h/8, w/ <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192)</ref>, where 192 comes from 3 ? (8 ? 8). <ref type="figure" target="#fig_1">Figure 2</ref> gives an example of h = w = 16. With FCR, the spatial dimension contains simply the spatial relationship of these coefficients, and their frequency relationship is purely represented on the third dimension. In this manner, the 2-D convolution is applied to the spatial neighbors of the same frequency component, so that CNNs are able to exploit the DCT representation properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Design</head><p>In general, a CNN includes several downsampling operations to integrate the input information and enlarge the receptive field. The selected baseline network, EDANet, contains three downsampling operations (see <ref type="figure">Figure 3a</ref>). In other words, the size of feature maps at the final stage (EDA Block 2) is 1/8 to the input images. As discussed in Section 4.1, the spatial resolution of the FCRed DCT representation is 1/8 of its RGB counterpart. Thus, the downsampling operations would results in a feature map size of 1/64 of the RGB image, which is too small. Particularly, the spatial information and boundary details are important to localize objects in semantic segmentation.</p><p>The proposed DCT-EDANet is modified from EDANet mainly on two aspects, removing downsampling and increasing depth. <ref type="figure">Figure 3b</ref> shows its structure. We remove all the downsampling operations to accommodate our target image representation. In this way, the output feature maps maintain the size of 1/8 to the original RGB image, and thus the necessary spatial information remains. The Initial Layer is a vanilla 3?3 convolutional layer with batch normalization <ref type="bibr" target="#b10">[11]</ref> and ReLU. It is similar to the Dowsampling Block of EDANet but has no downsampling operation. In addition, because the entire DCT-EDANet operates on a spatial resolution of 1/8, the computational complexity is significantly reduced. As a result, we can substantially deepen DCT-EDANet. At the same level of computational complexity of EDANet, it can include 22 EDA modules in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmark and Training Procedure</head><p>We use the Cityscapes dataset <ref type="bibr" target="#b1">[2]</ref> as the benchmark. It is a road scene dataset for the semantic segmentation task. The dataset has 19 object classes, and it consists of 2975, 500, and 1525 images for training, validation, and testing, respectively. Its original image resolution is 1024?2048. Since EDANet is our baseline, we conform to the setup of EDANet, which trains and tests networks on the downsampled 512?1024 images.</p><p>We follow the training setup similar to that in EDANet. Our networks are trained by using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with batch size 10 and weight decay 1e -4 . The poly learning rate policy is used, which multiplies the learning rate by (1 ? /max _ ) with power 0.9. We set the initial learning rate to 5e -4 . A class weighting scheme,</p><formula xml:id="formula_1">= 1 ( + ) ?</formula><p>, is included, where k is set to 1.12. We adopt random horizontal flip and the translation of 0~2 pixels on both axes for data augmentation. All models are  <ref type="figure" target="#fig_0">(16, 16, 1)</ref>. The left side is the outputs of DCT. The right side is the FCRed DCT representation (64 channels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: The structures of (a) the baseline network, EDANet [16] and (b) the proposed DCT-EDANet.</head><p>evaluated by the mean of intersection-over-union (mIoU) metric. We do not include any testing tricks, such as multi-crop and multiscale testing. In our experiments, we train models with one-stage and through just 2/3 number of iterations compared to that in EDANet <ref type="bibr" target="#b15">[16]</ref> since we compare the relative accuracy in our analysis. Our experiments are conducted on a single GTX 1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We evaluate the performance of the proposed DCT-EDANet on the Cityscapes dataset. We also compare it to the models of the original EDANet architecture with different types of input representation. These models are as follows. EDANet-RGB uses the RGB input, EDANet-DCT uses the original DCT input, and EDANet-FCRedDCT uses the FCRed DCT input. <ref type="table" target="#tab_1">Table 1</ref> reports the experimental results. EDANet-RGB serves as our baseline. Since the purpose of ablation experiments here is to verify our approach, training the models through 2/3 number of iterations compared to the implementation of the authors of EDANet is enough. Hence, the mIoU accuracy of EDANet-RGB is slightly lower than that reported in the EDANet paper <ref type="bibr" target="#b15">[16]</ref>. Next, EDANet-DCT is obviously less accurate than EDANet-RGB though they have identical computational cost and an identical amount of input information. This reflects the issue that EDANet is not tailored to the DCT input. When the FCR technique is adopted, the performance becomes even worse owing to the extremely small feature size. Compared to EDANet-FCRedDCT, the proposed DCT-EDANet obtains a dramatic improvement, showing the importance of larger feature size.</p><p>Moreover, DCT-EDANet surpasses EDANet-DCT at about the same network complexity. A similar result is observed in SSD_freq <ref type="bibr" target="#b2">[3]</ref>. SSD_freq does not employ FCR, which leads to lower accuracy than its RGB counterpart. By contrast, other studies that adopt ideas similar to FCR perform better <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. These results demonstrate the effectiveness of FCR. On the other hand, <ref type="table" target="#tab_2">Table 2</ref> views this issue from another perspective. EDANet-DCT-1/4coef and DCT-EDANet-1/4coef are EDANet-DCT and DCT-EDANet that use 1/4 number of total DCT coefficients as inputs, respectively. More precisely, they take the first 16 low-frequency coefficients from each 8?8 block. In this manner, EDANet-DCT-1/4coef has an input size of (h/2, w/2, 3), i.e., each 8?8 block is condensed to 4?4. DCT-EDANet-1/4coef, who adopts the FCR, has an input size of (h/8, w <ref type="bibr">/8, 48)</ref>. Under this situation, the accuracy gap between these two models is widened from 2.3% to 4.0%, which indicates DCT-EDANet are more favorable when the inputs are condensed. A detailed investigation in this issue is discussed in Section 5. In summary, when the spatial relationship and the frequency relationship are encoded on separate dimensions, CNNs achieve better performance. <ref type="figure" target="#fig_2">Figure 4</ref> shows some visual results.</p><p>Still, DCT-EDANet is narrowly defeated by EDANet-RGB. The reason is that spatial information is particularly paramount to semantic segmentation, but the DCT format reduces the spatial resolution in trading of the frequency decomposition. It is thus more challenging to do semantic segmentation on the DCT representation. Object detection is another task focusing on localization. Therefore, SSD_freq, the first method for detection in the DCT domain, still has a lower performance than its RGB counterpart by a huge margin. Compared to SSD_freq, our method is very close to its baseline.     <ref type="bibr">12:</ref> if Qi,j = 0 for i = 0, 1, ?, 7; j = 0, 1, ?, 7 <ref type="bibr">13:</ref> Qi,j = 1 <ref type="bibr">14:</ref> end <ref type="bibr">15:</ref> Vi,j = round ( Ui,j / Qi,j ) for i = 0, 1, ?, 7; j = 0, 1, ?, 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Quality factor mIoU (%) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FREQUENCY COMPONENT SELECTION</head><p>In this section, we investigate the impact of different combinations of DCT components on segmentation performance. We select different numbers of low-frequency coefficients from each of the Y, Cb, and Cr channels to form various combinations of these components. Then, each combination is used as the inputs to the networks for both training and testing stages. <ref type="table" target="#tab_5">Table 3</ref> reports our experimental results. From M-64-0-0 to M-25-25-25, each model has a near number of input components that ranges between 64 and 75. We can see that M-36-16-16 performs the best, M-49-9-9 is the runner-up, and M-25-25-25 is the worst. M-36-16-16 attains an accuracy close to that of DCT-EDANet (with full input components) by using barely 36% input components. This indicates that the best proportion of the Y, Cb and Cr coefficients is approximately 50:25:25. That is, the Y information plays the most important role, but a certain amount of the Cb and Cr is needed to support. This result is consistent with a principle of the JPEG compression algorithm, in which the chroma information is less critical and thus subsampled in the JPEG codec. M-49-9-9 has a similar proportion, so it gets a second. M-64-0-0 only has the Y components without the aid of the Cb and Cr, and M-25-25-25 uses an equal ratio without a sufficient amount of Y information. Therefore, they are the last.</p><p>From M-16-1-1 to M-0-0-16, they present a similar result. M-9-4-4 has the same ratio as M-36-16-16, and it is the winner within its group. In addition, we can also observe that M-16-4-4 outperforms DCT-EDANet-1/4coef, M-64-0-0, and M-25-25-25 by using significantly fewer input components, which also indicates that the proportion is critical. As a result, we find the best input component proportion is around 50:25:25, providing a guideline for future studies on DCT-domain analytics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">QUANTIZATION</head><p>In the JPEG codec, if the compression is lossy, the quantization step is included. Because human visual system is more sensitive to lowfrequency variations, the resolution of high-frequency components can be reduced more significantly. The quality factor, ranging from 0 to 100, determines the degree of quantization. A small quality factor specifies a rough quantization, which means lower image quality but a smaller file size. The JPEG quantization procedure is described in Algorithm 1.</p><p>In this section, we investigate the influence of different quality factors on segmentation performance. Models M-QF70, M-QF50, and M-QF30 are trained and tested on the DCT coefficients quantized by their specified quality factor. The experimental results are shown in <ref type="table" target="#tab_6">Table 4</ref>. We can see that even when the quality factor is as low as 30, the accuracy merely drops 1.6%. These results show that with heavy quantization, the compressed DCT components can still provide a comparable accuracy. It also demonstrates that the proposed method is able to tolerate serious quantization errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we propose a solution for performing semantic segmentation on the DCT representation. We rearrange the DCT coefficients by using FCR. Then, we modify EDANet by discarding all the downsampling operations to maintain the DCT inputs at 1/8 spatial resolution, and by deepening the network to maintain the network capacity. We obtain near accuracy to the RGB baseline. Furthermore, we investigate the best proportion of YCbCr information selections, which can attain the same level of accuracy using significantly fewer coefficients. We also demonstrate our method is highly resistant to quantization errors, even when the quality factor is as low as 30. This work shows the feasibility of performing the challenging semantic segmentation task in the JPEG compressed domain. The elaborated analysis of DCT coefficient selections provides a guideline for future studies on compresseddomain analytics. Further improvements in performance and inference speed can be anticipated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of the proposed method of semantic segmentation on the DCT representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of FCR. It takes an image with size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample results of the proposed DCT-EDANet on the Cityscapes validation set. From left to right: (a) RGB images, (b) Ground truths, (c) DCT-EDANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Evaluation results of the proposed method and other models. Multi-Adds: the number of multiply-add operations.</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Comparison between EDANet-DCT and DCT- EDANet using 1/4 DCT coefficients as input.</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 : Experimental results on frequency component selection. Each model is both trained and tested by its listed numbers of low-frequency components.</head><label>3</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 : Experimental results on quantization.</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKOWLEDGMENTS</head><p>We would like to thank Dr. Yen-Kuang Chen, Alibaba and Prof. Wen-Hsiao Peng, National Chiao Tung University, for their helpful comments and discussions.</p><p>This work is partially supported by the Ministry of Science and Technology, Taiwan under Grant MOST 108-2634-F-009-007 through Pervasive AI Research (PAIR) Labs, National Chiao Tung University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic imae segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast object detection in compressed jpeg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deguerre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08408</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning in the jpeg transform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep feature extraction in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster neural networks straight from jpeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face recognition using the discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Hafed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient image retrieval in the dct domain by hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multi-branch convolutional neural network for detecting double jpeg compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Diginal Crime and Forensics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient dense modules of asymmetric convolution for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1809.06323</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A critical evaluation of image and video indexing techniques in the compressed domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Idris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward image understanding from deep compression without decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Torfason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On using cnn with dct based image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Irish Machine Vision and Image Processing Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Harmonic networks: Integrating spectral information into cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03205</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Harmonic networks with limited training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00135</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dct-domain deep convolutional neural networks for multiple jpeg compression classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The jpeg still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
