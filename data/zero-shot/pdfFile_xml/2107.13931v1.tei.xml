<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinmin</forename><surname>Zhang</surname></persName>
							<email>zhwang@dlut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hou</surname></persName>
							<email>houjun@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>danxu@cse.ust.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a crucial task of autonomous driving, 3D object detection has made great progress in recent years. However, monocular 3D object detection remains a challenging problem due to the unsatisfactory performance in depth estimation. Most existing monocular methods typically directly regress the scene depth while ignoring important relationships between the depth and various geometric elements (e.g. bounding box sizes, 3D object dimensions, and object poses). In this paper, we propose to learn geometryguided depth estimation with projective modeling to advance monocular 3D object detection. Specifically, a principled geometry formula with projective modeling of 2D and 3D depth predictions in the monocular 3D object detection network is devised. We further implement and embed the proposed formula to enable geometry-aware deep representation learning, allowing effective 2D and 3D interactions for boosting the depth estimation. Moreover, we provide a strong baseline through addressing substantial misalignment between 2D annotation and projected boxes to ensure robust learning with the proposed geometric formula. Experiments on the KITTI dataset show that our method remarkably improves the detection performance of the state-of-the-art monocular-based method without extra data by 2.80% on the moderate test setting. The model and code will be released at https://github.com/ YinminZhang/MonoGeo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As an important and challenging problem, 3D object detection plays a fundamental role in various computer vision applications, such as autonomous driving, robotics, and augmented/virtual reality. In recent years monocular 3D object detection has received great attention, because it simply uses monocular camera instead of requiring extra sens-   ing devices as in LiDAR-based <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref> and stereobased <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref> methods. However, the performance gap between LiDAR-based and monocular image-based approaches remains significant mainly because of the lack of reliable depth information. A quantitative investigation is conducted by only replacing the depth predictions with the ground-truth depth values on a baseline model. The detection performance of the model can be remarkably improved from 11.84% to 70.91% in terms of the AP 40 metric <ref type="bibr" target="#b38">[39]</ref> under the moderate setting of car category on the KITTI val set (see <ref type="table" target="#tab_0">Table 1</ref>), which suggests that the depth estimation is a critical performance bottleneck in the monocular 3D object detection.</p><formula xml:id="formula_0">V R R a N O G J 6 o Z E A 2 c C W o Y Z D l 2 p g M Q h h 0 4 4 v p 3 5 n U d Q m i X i w U w k B D E Z C h Y x S o y V / N H N R c 3 H k j 0 B 1 / 1 y x a 2 6 c + B V 4 u W k g n I 0 + + U v f 5 D Q N A Z h K C d a 9 z x X m i A j y j D K Y V r y U w 2 S 0 D E Z Q s 9 S Q W L Q Q T a / e Y r P r D L A U a J s C Y P n 6 u + J j M R a T + L Q d s b E j P S y N x P / 8 3 q p i a 6 D j A m Z G h B 0 s S h K O T Y J n g W A B 0 w B N X x i C a G K 2 V s x H R F F q L E x l W w I 3 v L L q 6 R d</formula><formula xml:id="formula_1">v n B 2 + j f N i 3 j Q U / d n j j V / L k = " &gt; A A A B / H i c b V D L S s N A F J 3 4 r P U V 7 d L N Y B F c h a S 2 6 E Y o u n F Z w T 6 g D W U y m b R D Z y Z h Z i K E U H / F j Q t F 3 P o h 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i B h V G n X / b b W 1 j c 2 t 7 Z L O + X d v f 2 D Q / v o u K P i V G L S x j G L Z S 9 A i j A q S F t T z U g v k Q T x g J F u M L m d + d 1 H I h W N x Y P O E u J z N B I 0 o h h p I w 3 t S j 6 Q H I Y k 0 e M p v I Y X D a f G h 3 b V d d w 5 4 C r x C l I F B V p D + 2 s Q x j j l R G j M k F J 9 z 0 2 0 n y O p K W Z k W h 6 k i i Q I T 9 C I 9 A 0 V i B P l 5 / P j p / D M K C G M Y m l K a D h X f 0 / k i C u V 8 c B 0 c q T H a t m b i f 9 5 / V R H V 3 5 O R Z J q I v B i U Z Q y q G M 4 S w K G V B K s W W Y I w p K a W y E e I 4 m w N n m V T Q j e 8 s u r p F N z v L r T u K 9 X m z d F H C V w A k 7 B O f D A J W i C O 9 A C b Y B B B</formula><p>The depth information has also been successfully applied as an important 3D geometry element to facilitate the learning in other problems, such as 2D object detection <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref>, human pose estimation <ref type="bibr" target="#b32">[33]</ref>, and camera localization <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35]</ref>. However, how to jointly model the geometry relationships between the scene depth and differ-ent 2D/3D network predictions, such as 2D box sizes, 3D dimensions, and poses, and enable joint learning with the modeled geometry constraints for geometry-aware monocular 3D object detection is rarely explored in the literature. An intuitive way to introduce the geometric relationships is to leverage perspective projection between the 3D scene space and the 2D image plane. Prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> either weakly use the geometry considering the projection consistency between 2D and 3D for post-processing or employ perspective projection regardless of the object poses and 3D dimensions, which however can provide considerably stronger geometric constraints and are extremely important for accurate depth estimation. As can be observed in <ref type="figure" target="#fig_2">Fig. 1</ref>, the depth values differ by more than 5 meters due to the distinct poses and positions of the cars with the same height of 2D/3D boxes.</p><p>In this paper, we propose a novel geometric formula by principled modeling of the relationships between the scene depth and different geometry elements predicted from the deep network for the task of monocular 3D object detection, including 2D bounding boxes, 3D object dimensions, and object poses. We further implement the proposed formula to develop a geometry-based network module, which can be flexibly embedded into the deep learning framework, allowing effective geometry-aware learning on the representation level for guiding the depth estimation and advancing the monocular 3D object detection. Besides, the geometry module can be utilized during both the training and inference phases without additional complex post-processing. Moreover, we provide a simple yet strong baseline for ensuring robust learning with the proposed geometry module, which is achieved through addressing the severe misalignment between the annotated 2D bounding box and the projected 2D bounding box from the 3D annotations. This effective baseline achieves an AP of 13.37% under the moderate setting of car category on the KITTI val set.</p><p>To summarize, the contribution of this paper is threefold:</p><p>? We propose a novel geometric formula, which jointly models the perspective geometry relationships of multiple 2D/3D elements predicted from the deep monocular 3D objection network, providing strong geometric constraints for learning the 3D detection network. ? We implement the proposed geometric formula in neural network as a module, which can be leveraged to guide the representation learning for boosting the depth estimation to significantly advance the performance of the monocular 3D object detection. ? We provide a simple yet strong baseline through dealing with the misalignment between 2D projected boxes and 2D annotation boxes, which achieves 13.37% on the moderate of the KITTI val set. We expect our baseline will be beneficial for the community in future research on monocular 3D object detection.</p><p>Extensive experiments conducted on the challenging KITTI <ref type="bibr" target="#b10">[11]</ref> dataset clearly demonstrate the effectiveness of the proposed approach and show that our method achieves 13.81% in terms of the AP 40 metric, which is 2.80% absolute AP 40 improvement over the state-of-the-art of the monocular 3D object detection on the moderate setting of the KITTI test set for the car category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are two groups of works closely related to ours, i.e. monocular 3D object detection and geometry-guided 3D object detection. Monocular 3D Object Detection. Compared with the methods with LiDAR and stereo sensors, 3D object detection with monocular images is challenging due to the absence of reliable depth information. Existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> have considered using external pretrained networks, extra training data, and prior knowledge to improve the performance of the monocular 3D object detection. Particularly, DeepMANTA <ref type="bibr" target="#b5">[6]</ref> utilizes extra 3D shape and template datasets in learning 2D/3D vehicle models and then performs 2D/3D matching for the detection. Inspired by the importance of accurate depth for 3D object detection, many works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">47]</ref> develop monocular 3D object detection by introducing pretrained external network for depth estimation. In contrast to these methods, we only use the monocular image as input without any extra burden.</p><p>In recent years, some works also only use RGB data as the input for the task <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b21">22]</ref>. For instance, MonoDIS <ref type="bibr" target="#b38">[39]</ref> proposes to leverage a disentangling transformation between different 2D and 3D tasks to optimize the parameters at the loss level. M3D-RPN <ref type="bibr" target="#b2">[3]</ref> focuses on the design of depth-aware convolution layers to improve 3D parameter estimation and post-optimization of the orientation by exploring the consistency between projected and annotated bounding boxes. To address the common occlusion issue in monocular object detection, MonoPair <ref type="bibr" target="#b8">[9]</ref> proposes to model spatial relationships of objects in paired adjacent RGB images via introducing an uncertainty-based prediction for improving the detection. MoVi-3D <ref type="bibr" target="#b39">[40]</ref> builds virtual views where the object appearance is normalized depending on the distance to reduce the visual appearance variability. RAR-Net <ref type="bibr" target="#b21">[22]</ref> builds a post-processing method by introducing Reinforcement learning to improve the 3D object detection performance. Although these existing methods achieved very promising results, the beneficial geometry relationships between the different 2D and 3D predictions from the detection network are not explicitly modeled for boosting the learning of the detection network. Geometry-Guided 3D Object Detection. There are several recent methods considering utilizing the geometric information for monocular 3D object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. One research direction mainly focuses on using ge-    ometry information to improve the detection performance in the inference stage via post-processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref>. For instance, M3D-RPN <ref type="bibr" target="#b2">[3]</ref> employs the consistency between the 2D projected and the predicted 2D bounding boxes to optimize orientation parameters in a post-processing process. UR3D <ref type="bibr" target="#b37">[38]</ref> uses estimated key points to post-optimize the predictions of physical sizes and yaw angles by minimizing the objective function. Some other works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref> consider using a simplified perspective projection relationship in the training phase. In particular, MonoGRNet <ref type="bibr" target="#b29">[30]</ref> presents a geometric reasoning method based on instance depth estimation and 2D bounding box projection to obtain more accurate 3D localization. GS3D <ref type="bibr" target="#b17">[18]</ref> uses average object sizes based on the statistics on the training data to guide the location estimation. Decoupled-3D <ref type="bibr" target="#b4">[5]</ref> estimates the depth from the projected average height of each vertical edge and the 3D height of the objects. RTM3D <ref type="bibr" target="#b19">[20]</ref> predicts keypoints including eight vertexes and the center of 3D object in the image plane, and then minimize the energy function using geometric constraints of perspective projection. Ivan et al. <ref type="bibr" target="#b0">[1]</ref> relies on extra CAD models to process labels for keypoint detection and enforces the constrain between 2D keypoints and the CAD models using a consistency loss. However, these methods basically utilize the geometry at the prediction level and ignore several important geometry elements (e.g. object poses and locations) in their geometric modeling. In contrast to these methods, we jointly model the geometry relationships between the scene depth and 2D bounding boxes, 3D dimensions, and object poses, and the geometric model is implemented as a network module to be leveraged for geometry-aware representation learning to directly boost the depth estimation.</p><formula xml:id="formula_2">+ k O F R q E v q m M 8 R 6 p J Z r M / h f r Z v o 4 N p L m Y g T T Q V Z L A o S j n S E Z t G g A Z O U a D 4 x B h P J z F 8 R G W G T h j Y B F k w I 7 v L J q 6 Z V L r m V 0 u V N p V g t Z 3 H k 4 Q R O 4 Q J c u I I q 1 K E B T S D w A E / w A q / W o / V s v V n v i</formula><formula xml:id="formula_3">O i Z q x X v b n 4 n 9 d L T X j r Z 1 w m q U H J l o v C V B A T k / n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T d G G 4 K 2 + v E 7 a 1 Y p X q 1 w 3 a + V 6 N Y + j A O d w A V f g w Q 3 U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f</formula><formula xml:id="formula_4">q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U 9 A a l s l t x F y D r x M t J G X I 0 B q W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m x n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k 0 i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D W z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 6 I N w V t 9 e Z 2 0 q x W v V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>A framework overview is illustrated in <ref type="figure">Fig. 2</ref>. We model an object as a single point following <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b8">9]</ref>. Our framework consists of three key steps. First, we use deep layer aggregation <ref type="bibr" target="#b47">[48]</ref>, i.e. a fully-convolutional encoder-decoder network, to extract features from a monocular image. Second, the features are fed into several network branches to separately predict 2D bounding box, 3D object dimension, and orientation (Sec. 3.2). Third, the geometric module models the geometry relationships from these 2D/3D predictions to obtain a geometric formula, which is implemented as a network module for geometry-aware feature learning (Sec. 3.3). Finally, we utilize the geometric features for depth estimation (Sec. 3.3), which combines with other 3D predictions for obtaining the 3D object detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Base Detection Structure</head><p>Our base network structure for 2D detection, 3D dimension and orientation prediction is derived from the anchorfree 2D object detection <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b40">41]</ref> with six output branches. Each branch takes the backbone features as input and uses 3x3 convolution, ReLU, and 1x1 convolution for prediction. The heatmap branch is used to locate 2D object center. The 2D/3D offset branch is applied for estimating 2D/3D center in 2D image coordinate system. The 2D box size and the 3D dimension branch predicts the size of 2D bounding box and the 3D dimension of the 3D object, respectively. Similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">49]</ref>, the orientation branch predicts observation angle ? of the object via encoding it into scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometric Module for Learning Geometric Representations</head><p>In this section, we introduce the proposed geometric formula via modeling the relationships between the depth and 2D/3D predictions, and present how it can be implemented to learn geometric representations for depth estimation. Formulation and notation. We adopt the 3D object definition described by the KITTI dataset. The coordinate system is constructed in meters with the camera center as the origin of coordinate. A 3D bounding box is represented as a 7-tuple (W, H, L, x, y, z, r y ), where W, H and L are the dimensions of the 3D bounding box, i.e. width, height, and length, respectively, and (x, y, z) is the bottom center coordinate of the 3D bounding box. As shown in <ref type="figure">Fig. 3</ref>, r y denotes the rotation around the Y-axis in the camera coordinate system, in a range of [??, ?]. Moreover, to facilitate the introduction of the proposed geometric formula, we define the 2D bounding box with a 4-tuple (w, h, u, v) , where (w, h) and (u, v) represent the size and the center of 2D bounding box, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Projective Modeling of Depth and 2D/3D Network Predictions</head><p>We derive a geometric formula for modeling the geometric relationships between the scene depth and multiple 2D/3D network predictions, i.e. 2D bounding box, 3D dimension, and object orientation from the perspective projection. Geometric relationship of 2D and 3D corners. First, we represent an object in the object coordinate system, in which the origin is the bottom center of the object via the translation transformation from the camera coordinate system. As shown in <ref type="figure">Fig. 3</ref>, the coordinate of the c-th (c = 1, ..., 8) corner in the 3D object bounding box, denoted as P c cor , can be given as follows:</p><formula xml:id="formula_5">P c cor = ??x c i , +?y c i , ??z c i T s.t. ? ? ? ?x c i ?y c i ?z c i ? ? ? = ? ? ? 1 2 L cos(r y ) ? 1 2 W sin(r y ) 1 2 H ? 1 2 H 1 2 L sin(r y ) ? 1 2 W cos(r y ) ? ? ? ,<label>(1)</label></formula><p>where ?x c i , ?y c i , and ?z c i represent the coordinate difference between the corner and the center of the object in X, Y, and Z direction, respectively; i ? {1, 2} denotes the index of different ? values as shown in <ref type="figure">Fig. 3</ref>. With the position of the object in the camera coordinate system, we can represent the corner in the same coordinate system as:</p><formula xml:id="formula_6">P c cam = P obj + P c cor = ? ? x ? ?x c i y + ?y c i z ? ?z c i ? ? ,<label>(2)</label></formula><p>where P c obj and P c cam respectively represent the bottom center coordinate and the corner coordinate of the 3D object bounding box in the camera coordinate system; x, y, and z denote the coordinate value along the X, Y, and Z dimension in the camera plane. z also represents the distance from the bottom center of object to the camera plane, i.e. the depth of the object in the camera coordinate system; Given the intrinsic matrix of the camera provided by the official KITTI dataset, K inc , we can project the corner in the camera coordinate system to the pixel coordinate system as:    where P c pix denotes the projected corner coordinate in the pixel coordinate system; z c indicates the depth of the c-th corner; u c and v c respectively denote the horizontal and vertical coordinate of the corner in the pixel coordinate system. Relationship between 2D height and 3D corners. Given the eight corners of the 3D object box in the pixel plane, the height of the projected 2D bounding box h can be estimated from the difference between the vertical coordinate of the uppermost corner (i.e. max c {v c }) and that of the lowermost corner (i.e. min c {v c }) in the pixel coordinate system as:</p><formula xml:id="formula_7">P c pix = [u c , v c , 1] T = K inc ? P c cam z c ,<label>(3)</label></formula><formula xml:id="formula_8">v x 9 E T d M L 0 v w t W L b D i / 1 q k 4 = " &gt; A A A B 8 X i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F Q A v L C O Y D k y P s b T b J k r 2 9 Y 3 d O i E f + h Y 2 F I r b + G z v / j Z v k C</formula><formula xml:id="formula_9">= " &gt; A A A B 8 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I n c E o 4 U F i R a W m A g Y 4 U L 2 l j 3 Y s L d 3 2 Z 0 z E s K / s L H Q G F v / j Z 3 / x g W u U P A l k 7 y 8 N 5 O Z e U E i h U H X / X Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o G n i V D P e Y L G M 9 X 1 A D Z d C 8 Q Y K l P w + 0 Z x G g e S t Y H g 1 9 V u P X B s R q z s c J d y P a F + J U D C K V n r o X H O J l D x 1 K 9 1 i y S 2 7 M 5 B l 4 m W k B B n q 3 e J X p x e z N O I K m a T G t D 0 3 Q X 9 M N Q o m + a T Q S Q 1 P K B v S P m 9 b q m j E j T + e X T w h J 1 b p k T D W t h S S m f p 7 Y k w j Y 0 Z R Y D s j i g O z 6 E 3 F / 7 x 2 i u G F P x Y q S Z E r N l 8 U p p J g T K b v k 5 7 Q n K E c W U K Z F v Z W w g Z U U 4 Y 2 p I I N w V t 8 e Z k 0 K 2 W v W j 6 7 r Z Z q l 1 k c e T i C Y z g F D 8 6 h B j d Q h w Y w U P A M r / D m G O f F e X c + 5 q 0 5 J 5 s 5 h D 9 w P n 8 A w i O Q T w = = &lt; / l a t e x i t &gt; x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T z v S a P 6 G A R K b B j B 9 i y 7 z c j / J f G k = " &gt; A A A B 8 X i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F Q A v L C O Y D k y P s b S b J k r 2 9 Y 3 d P D E f + h Y 2 F I r b + G z v / j Z v k C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L L Y B V Q A U U 1 + v p X b x D j N C S R x A w J 0 b X M R H o Z 4 p J i R v J y L x U k Q X i M h q S r M E I h E V 4 2 W y m H p 0 o Z w C D m 6 k Q S z t T f E x k K h Z i E v u o M k R y J R W 8 q / u d 1 U x m 4 X k a j J J U k w v O H g p R B G c N p S H B A O c G S T R Q g z K n a F e I R 4 g h L F W V Z h W A t f n k Z W l X D s g 3 n z q 7 U r 4 s 4 S u A Y n I A z Y I E a q I N b 0 A B N g M E j e</head><formula xml:id="formula_10">h = max c {v c } ? min c {v c } = (y + ?y max )f v z ? ?z max ? (y + ?y min )f v z + ?z max ,<label>(4)</label></formula><p>where v c is derived from Eq. 3; ?z max = max c {?z c i } represents the maximum of ?z c i of the eight corners, analogically for ?y max ; f v denotes the focal length in the vertical direction of the pixel plane. Relationship between depth and other 2D/3D parameters. Similar to the definition of the bird's-eye view angle ? (see <ref type="figure">Fig. 3a</ref>), we define the angle between the bottom center of the object and the horizontal plane as ? (see <ref type="figure">Fig. 3b</ref>). Given the projected coordinate (u o , v o ) of the object bottom center in the pixel plane based on Eq. 3, we can obtain the following geometric relationship:</p><formula xml:id="formula_11">y = z * tan(?) = z * v o ? c v f v ,<label>(5)</label></formula><p>where c v is the location of the principal point relative to the origin in the pixel plane. Then, combining Eq. 4 and Eq. 5, the depth of the center of the object, z, can be written as:</p><formula xml:id="formula_12">z = 1 2 b + 1 2 b 2 + 4( ?Hf v h ?z max + ?z 2 max ),<label>(6)</label></formula><p>where b = fv h (2 tan ? * ?z max + H). It can be clearly observed that, the depth z is correlated to the camera intrinsic parameters (i.e. f v and c v ), the object position (when deriving ?), 3D dimension (when deriving ?z max and H), and orientation of the object (when deriving ?z max ). Relationship to existing works. Obviously, Eq. 6 obeys the perspective projection principle that further objects tend to be smaller than the nearer objects. It is also clearly different from prior works that, in our formula there is a nonlinear relationship between the scene depth z and H/h, due to the modeling via the introduction of the object pose and 3D dimensions. We can simplify the proposed formula in two different ways: (i) To reduce the computation complexity, we can consider only the first term in Eq. 6 to obtain a simplified geometric formula v1:</p><formula xml:id="formula_13">z = f v h (2 tan ? * ?z + H).<label>(7)</label></formula><p>(ii) If the variation of pose and position is not considered, then the formulation in Eq. 7 can be further derived as a simplified geometric formula v2:</p><formula xml:id="formula_14">z = k * H h ,<label>(8)</label></formula><p>where k represents the scale factor for the depth scale conversation. The formula in Eq. 8 is widely used in 3D object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref>. We report detailed comparison and analysis on our formulation in Eq. 6 and the two simplified versions in the experimental results (see Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Geometry-Guided Scene Depth Learning</head><p>Following the proposed geometric formula, we devise and implement a network module for the geometry-guided deep representation learning for accurate depth prediction, as shown in the red dashed box of <ref type="figure">Fig. 2</ref>. The module aims to learn geometric representations using the 2D/3D geometryrelated network predictions (i.e. 2D bounding box, 3D object dimension, and orientation) as input. Specifically, in the training stage, the module first produces a calculated onechannel depth map with the proposed geometric formula as described in Eq. 6. The depth map is then transformed into 3D maps of 3 channels with each spatial position representing a 3D data point [x, y, z] by introducing camera parameters as the initial geometric input. Then, the 3D map goes through three non-linear transformation blocks, with each block consisting of a convolution and taking the previous transformation block as input, a batch-norm and a ReLU layer, to learn a robust geometric representation map with C channels (typically C &gt; 3). We set C as 32 in our experiments. These learned geometric representations are further concatenated with the image representations produced from the backbone network to learn the depth estimation.</p><p>In the inference stage, we perform the same procedure as in the training, and the final depth output is further used to combine with other predictions, including 2D bounding boxes, 3D dimensions and orientations to produce 3D object bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Misalignment in 2D and 3D bounding Boxes</head><p>There is misalignment between the 2D projected box and 2D annotation box remains. Generally, due to the perspec-  <ref type="bibr" target="#b48">[49]</ref>, we replace the predicted depth and 3D dimensions with their corresponding ground-truth values. Using the ground-truth depth remarkably improves the AP from 11.84% to 70.91% on the moderate, suggesting that the depth is a significantly important factor that affects the accuracy the monocular 3D object detection. tive projection effect, i.e. further objects appear smaller than nearer objects, the misalignment is more serious for nearby objects, which makes the learning with the proposed formula inaccurate, especially for nearby objects. To handle this misalignment, we propose to use the 2D projected box instead of the 2D annotation box as the ground-truth to ensure the correctness of the depth estimation. According to Eq. 1 and 2, we compute the 3D corner coordinates of the object through the 3D poses and 3D dimensions of the object. We further obtain their coordinates on the pixel plane through the projection transformation according to Eq. 3. We also calculate the difference between vertices in the image plane as the height and width of the 2D projected box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>Backbone. We adopt a DLA-34 <ref type="bibr" target="#b47">[48]</ref> network architecture without deformable convolutions as our backbone. During training, we set the input resolution of the network as 380 ? 1280. The spatial size of the feature map from the backbone is 380 R ? 1280 R , where R = 4 represents the down-sampling factor of the backbone CNN. Optimization loss. The optimization objective of our deep detection framework follows a multi-task learning setting, and consist of classification and regression losses for both the 2D and 3D predictions. Specifically, we train the heatmap prediction with the focal loss <ref type="bibr" target="#b20">[21]</ref>. The branches for offsets and dimensions in both the 2D and 3D detection are trained with L1 losses. The branch for the orientation predictionn is trained with a MultiBin loss following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">49]</ref>. Based on <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, we use an L1 loss with heteroscedastic aleatoric uncertainty for the depth estimation (More details are illustrated in Appendix). Training: We use a batch size of 32 and train the overall deep network for 140 epochs on 6 NVIDIA 1080ti GPUs. To alleviate overfitting, we adopt data augmentation techniques including random scaling, random horizontal flipping, and random cropping for the 2D detection, and random horizontal flipping for the 3D detection, respectively. We use the Adam optimizer with 1e-5 weight decay to optimize the full training loss as described in <ref type="bibr" target="#b8">[9]</ref>. The initial learning rate is 1.25e-4, which is dropped by multiplying 0.1 after the 90-th and the 120-th epoch. To make train sta-  <ref type="table">Table 2</ref>. State-of-the-art comparison on the KITTI test set for the car category in terms of the metric of AP40. Extra data denotes the methods with extra data or external networks used in the training or inference or not. '-' denotes the methods have not been published yet without specific details. The bold black/blue color indicates the best/the second best performing method under the same 'No' setting. 'Improvement' denotes the increasing in performance compared to methods without extra data. Our approach achieves the best performance compared with the state-of-the-arts under both settings on moderate.</p><p>ble, we apply the linear warm-up strategy for learning with the geometric network module in the first 5 epochs. Inference: We first predict 2D bounding boxes, 3D dimensions, and orientations via a shared backbone and several separate task branches. Than, we use the proposed formula to predict coarse depth followed by several convolution layers for the depth estimation. Finally, similar to <ref type="bibr" target="#b48">[49]</ref>, we use a simple post-processing algorithm through 3 ? 3 maxpooling and back-projection to recover 3D bounding boxes from 2D boxes, 3D dimensions, orientations, and the depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Setup. The KITTI dataset <ref type="bibr" target="#b10">[11]</ref> provides widely used benchmarks for various visual tasks in the autonomous driving, including 2D Object detection, Average Orientation Similarity (AOS), Bird's Eye View (BEV), and 3D Object Detection. The official data set contains 7481 training and 7518 test images with 2D and 3D bounding box annotations for cars, pedestrians, and cyclists. We report the average accuracy (AP) for each task under three different settings: easy, moderate, and hard, as defined in <ref type="bibr" target="#b10">[11]</ref>. Moreover, we use 40 recall positions instead of 11 recall positions proposed in the original Pascal VOC benchmark, following <ref type="bibr" target="#b38">[39]</ref>. This results in a more fair comparison of the results. Each class uses different IoU standards for further evaluations. We report our results on the official settings of IoU ? 0.7 for cars. <ref type="table">Table 2</ref> and 3 show the overall performance of the proposed approach on the KITTI 3D test and val sets for cars from the official online leaderboard as of Mar. 12th, 2021. Existing state-of-the-art monocular 3D object detectors, including methods using extra data and only using monocular image are listed in the tables for comparison. The KITTI val results of MonoGRNet <ref type="bibr" target="#b29">[30]</ref>, M3D-RPN <ref type="bibr" target="#b2">[3]</ref> and MonoPair <ref type="bibr" target="#b8">[9]</ref> are quoted from <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overall Performance Comparison and Analysis</head><p>Build a simple yet strong baseline for monocular 3D object detection. We report the enhanced baseline results of 3D monocular object detection in <ref type="table">Table 4</ref>. Overall, the baseline significantly increases the AP 40 performance upon the original one by 3.76%, 3.54%, 2.88% on easy, moderate and hard difficulty levels, respectively. This is achieved by introducing three methods to the original baseline. First, we adopt the L1 loss with the aleatoric uncertainty in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, which makes training stage more robust to noise input. Second, we use the projected 3D center as the ground-truth for 2D heatmap prediction similar to SMOKE <ref type="bibr" target="#b22">[23]</ref>. Third, we address the misalignment between 2D ground-truth bounding boxes and the 2D projection bounding boxes by using 2D projected box as the ground-truth. This guarantees the consistency between 2D and 3D boxes from the projection relationships in the proposed geometric formula, and ensure the robust learning with the formula. The enhanced base-  <ref type="table">Table 3</ref>. Monocular 3D object detection results on the KITTI val set for the car category with the evaluation metric of AP40. The results of the previous works are from <ref type="bibr" target="#b8">[9]</ref>. Our approach significantly outperforms the previous state-of-the-arts on almost all the different evaluation protocols and settings. The bold black/blue color indicates the best/the second best performing method.  <ref type="table">Table 4</ref>. Results of the enhanced baseline on KITTI val set for the car category with the evalution metric of AP40. Each row adds an extra component to the above row. line achieves 16.54%, 13.37%, 11.15% on easy, moderate and hard difficulty levels, respectively. Comparison with monocular image based methods. Our approach achieves a notable improvement over the state-ofthe-art monocular image-based detectors <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> on both the val and test sets. As shown in <ref type="table">Table 2</ref>, the performance of our approach on the KITTI test set, for the detection on the car category, an indispensable part of the 3D object detection task for the autonomous driving scenario, our method achieves 18.85% (2.48% improvement) on the easy, 13.81% (2.80% improvement) on the moderate, and 11.52% (2.00% improvement) on the hard compared with the previous state-of-the-art image-only method. Besides, compared with unpublished <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref> our method still increases the AP 40 by 1.49 % on moderate. For the Bird's Eye View (BEV) on the car class, our method also achieves the best performance, increasing the AP 40 over the second best method by 3.10%, 1.96%, 1.34% on the easy, moderate, and hard level, respectively. For the KITTI val set, our method also establishes new state-of-the-art performance on both the 3D object detection and the BEV. <ref type="table">Table 2</ref> and 3 shows considerable improvement over the state-ofthe-art monocular detection methods with the great robustness, benefiting from the introduction of the proposed geometric formula for learning geometry-aware representations to advance the depth estimation. Comparison with methods using extra data or networks. The prior methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>   <ref type="table">Table 5</ref>. Quantitative comparison on different variants of the proposed approach. The experiments are conducted on the KITTI val set for the car category with the evaluation metric of AP40, to investigate the effect of the proposed geometry formula and geometry-guided representation learning. '3D-CAT', 'Geo-SV1' and 'Geo-SV2' represents transformation blocks combined with 3D dimension, simplified geometry formula v1, and v2. these kinds of information, as shown in <ref type="table">Table 2</ref>, it can still outperform these comparison methods in terms of the AP 40 metric by 0.40% on the moderate level. These significant improvements demonstrate the superior performance of our method with the proposed geometry-guided depth learning for the monocular 3D object detection. Latency. We test our model on Nvidia GTX 1080 Ti, Pytorch 1.1, CUDA 9.0, Intel @ 2.60GHz As shown in Table 2, the proposed method achieves 20 fps and runs similar to other real-time state-of-the-arts <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref>. This clearly demonstrates the efficiency of our method when compared with other competitive methods under the similar experimental environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We conduct extensive ablation studies on the KITTI val set, to demonstrate the effectiveness of the proposed approach for geometry-guided depth learning in advancing the monocular 3D object detection. For all the evaluation, the AP 40 metric is employed. We mainly investigate from two aspects, including the effect of the proposed geometric formula and module, and the effect of the geometry-guided representation learning for depth estimation. Baseline and variant models. To conduct an extensive evaluation, we consider the following baseline and variant models: (i) Baseline, which is a base model achieving a <ref type="figure">Figure 4</ref>. Qualitative results of our method for multi-class 3D object detection. We use orange box for cars, purple box for pedestrians, and green box for cyclists. All illustrated images are from the KITTI test set. Zoom in the image for more details. <ref type="figure">Figure 5</ref>. Qualitative results of our method for Bird's-Eye-View. We use black box for ground-truth, red box for baseline results, and blue box for our results. All the illustrated images are from the KITTI val set. Zoom in on the circles for more detailed comparison. strong 3D detection performance with an AP 40 of 11.8% on the moderate; (ii) 3D-CAT., which directly inputs the concatenation of the 3D network predictions to the non-linear transformation blocks while bypassing the depth calculation with geometric formula; (iii) Geo-SV1, which uses our simplified geometry formula v1 as in Eq. 7; (iv) Geo-SV2, which uses our simplified geometry formula v2 as in Eq. 8. Effects of the geometric formula and module. A detailed ablation study is shown in <ref type="table">Table 5</ref>. As we can observe, ours (full model) achieves a large gain (2.68% on the moderate level) over Baseline + 3D-CAT, meaning that directly using the 3D network predictions are not effective enough for learning the geometric representations, thus verifying the importance of the proposed geometric formula. By comparing Baseline + Geo-SV2, Baseline + Geo-SV1, and ours (full model), all these three with the geometric relationships, the performance gradually improves when more geometry elements are involved in modeling, confirming our motivation of modeling between depth and multiple 2D/3D geometry elements, instead of partial of them, e.g. only height typically considered in most existing works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref> similar to the Geo-SV2. Finally, Ours (full model) is 1.11% and 1.98% improvement on the moderate for the 3D detec- tion and BEV, respectively, which adequately demonstrate the effectiveness of our proposed approach. Effect of the geometry-guided representation learning for depth estimation. <ref type="figure" target="#fig_11">Fig. 6</ref> shows a performance comparison between baseline and our approach on the depth estimation. Specifically, we evaluate the predicted depth of all car samples in different depth ranges under two primary metrics (i.e. SILog and sqRel) widely used in depth estimation field. <ref type="figure" target="#fig_12">Fig. 7</ref> shows that 87% of the cars are within 40m, while only 5.0% of those are 45m away. <ref type="figure" target="#fig_11">Fig. 6</ref> shows that our approach outperforms the baseline consistently in all the depth ranges, especially in the 40m range with most samples, which further validates our idea of using geometryguided representation learning to boost depth estimation to advance the monocular 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel geometric formula principally modeled from multiple 2D/3D network predictions, to guide the depth estimation and advance the monocular 3D object detection. We design and implement this formula as a neural network module to have geometry-aware feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this Supplementary Material, we provide more elaboration on the implementation details, experiment results, and qualitative results. Specifically, we present the implementation details of the model training in Section A, additional quantitative results and analysis in Section B, and additional qualitative results in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>The overall network optimization loss of the proposed approach consists of three parts, i.e. a classification loss L c , a 2D regression loss L 2D , and a 3D regression loss L 3D . We present the details of these losses one by one: (i) Regarding to the classification loss, similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>, we employ a variant of focal loss L c which reduces the penalty for negative locations according to the distance from a positive location as:</p><formula xml:id="formula_15">L c = ?(1 ? p) ? log(p) if y = 1 ?(1 ? y) ? p ? log(1 ? p) otherwise,<label>(9)</label></formula><p>where y and p represent the ground-truth class probability given by an unnormalized 2D Gaussian and the model's predicted probability for the class, respectively. And ? and ? are hyperparameters that control the importance of each sample. We set ? to 2 and ? to 4 as a default setting in our experiments. (ii) For the 2D regression loss L 2D , it is defined upon a 6-tuple of ground-truth bounding-box targets and a predicted 6-tuple. Specifically, the 6-tuple consists of two 2D offsets, two 3D offsets, and two 2D box sizes. 2D/3D offsets are used to adjust the 2D/3D center locations before remapping them to the input resolution following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>. We use an L1 loss to optimize each 6-tuple parameters. (iii) For the 3D regression loss L 3D , it consists of an L1 loss for regressing the dimension of the 3D bounding box (i.e. width, height, and length), and an L1 loss with an uncertainty term for regressing the depth. Specifically, we follow <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and employ the heteroscedastic aleatoric uncertainty in the L1 depth estimation loss as:</p><formula xml:id="formula_16">[d * , ? * ] = f ? (x),<label>(10)</label></formula><formula xml:id="formula_17">L(?) = ? 2 ? d ? d * + log ? * .<label>(11)</label></formula><p>Where d * and d represent the predicted depth and the ground-truth depth, respectively. ? * is the noisy observation parameter of the model. Hence, the overall optimization loss is the sum of the three losses written as:</p><formula xml:id="formula_18">L = L c + ? 1 L 2D + ? 2 L 3D ,<label>(12)</label></formula><p>where ? 1 and ? 2 are loss weights controlling the balance between the different losses. We consider L 2D and L 3D equally important and use ? 1 = ? 2 = 1 in all experiments. As mentioned in the main paper, the KITTI <ref type="bibr" target="#b10">[11]</ref> official data set contains 7,481 training and 7,518 test images with 2D and 3D bounding box annotations for pedestrian and cyclist categories. We report our quantitative results in <ref type="table" target="#tab_5">Table 6</ref>, using the official settings with IoU ? 0.5 for pedestrians and cyclists on the KITTI test set. Our method establishes new state-of-the-art performance on all the three detection levels (i.e. easy, medium, and hard) for the cyclist category with only slight drop for the pedestrian category. We investigate the slight performance drop in the pedestrian category by comparing 2D detection results between car and pedestrian. In fact, the advantage of the proposed geometric formula is independent of different classes as 2D images conform with projective camera models, and every object meets the geometric reasoning. However, a performance gap between car detection and pedestrian/cyclist detection commonly exists in ours and many previous works on the KITTI dataset. This is mainly due to insufficient training samples of pedestrian and cyclist categories on KITTI, leading to unstable training, sensitivity to hyperparameters, and inaccurate prediction of 2D/3D information(e.g. 2D boxes, orientation, and the 3D dimensions) with high variance. This imbalance of the category data is however a common issue on the KITTI dataset for the 3D object detection task. <ref type="table">Table 7</ref> shows that the 2D detection results on the moderate level are only 50.48% and 44.63% for cyclist and pedestrian respectively, while up to 90.14% for car on the test set. Similarly for orientation estimation, the pedestrian (39.76%) has less than half of the car (89.44%) on the moderate. The two factors above in-troduce more noise into our geometry formula to affect the geometry-guided representation learning. However, our results for pedestrians and cyclists are highly competitive with other SOTA methods on the KITTI test set.  <ref type="table">Table 7</ref>. Monocular 2D object detection results on the KITTI test set for the All categories with the evaluation metric of AP40. The metric AP40 is used for detection evaluation and the IoU threshold is set to 0.5. The bold black/blue color indicates the best/the second best performing method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Further Analysis on Depth Estimation from Geometry Modeling</head><p>We conduct a further depth statistic analysis on the train+val set. <ref type="table" target="#tab_7">Table 8</ref> shows that for two cars with the same height in both the 2D bounding box and the 3D bounding box, the depth values of their centers may differ by more than 5 meters due to their distinct poses and locations. This confirms the critical importance of considering 3D pose and locations simultaneously in the geometric modeling for depth estimation, which is however not investigated by previous works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional Results at Different Distances</head><p>We provide additional results on depth estimation and monocular 3D object detection at different distances. <ref type="table" target="#tab_8">Table 9</ref> shows more depth estimation results on KITTI val set via comparing the enhanced baseline and our method. Specifically, we evaluate the depth estimation by computing Scale Invariant Logarithmic (SILog) error, squared Relative (sqRel) error, absolute Relative (absRel) error, and Root Mean Squared Error of the inverse depth (iRMSE). Our method outperforms the enhanced baseline by large margins on all these evaluation metrics. The depth estimation results clearly demonstrate the effectiveness of our proposed idea of using geometry-guided representation learning to boost depth estimation from monocular images for advancing the monocular 3D object detection. Moreover, we conduct experiments about the 3D monocular object detection improvement at different distances. <ref type="table" target="#tab_0">Table 10</ref> reports performance on AP 40 at different object distance ranges following <ref type="bibr" target="#b31">[32]</ref>. It is clear that our method consistently outperforms the baseline at different ranges.  <ref type="table" target="#tab_0">Table 10</ref>. Performance on KITTI val at different ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablation Study for Uncertainty and Equation</head><p>We investigate the effect of uncertainty with our geometric module as requested on the KITTI val set in <ref type="table" target="#tab_0">Table 12</ref>. It can be seen that the uncertainty is helpful for learning the geometry, but the main improvement is from the proposed principled geometric modeling. To further validate the effectiveness of Eq. (6), we compare all predictions followed by pointwise MLP as the reviewer described with our geometric module in <ref type="table" target="#tab_0">Table 11</ref>. Ours is significantly better than the pointwise MLP.  <ref type="table" target="#tab_0">Table 11</ref>. Results of different modules on KITTI val with AP40. <ref type="figure">Figure 8</ref>. Qualitative results of our method for Bird's-Eye-View. We use black box for ground-truth, red box for baseline results, and blue box for our results. All the illustrated images are from the KITTI val set. Zoom in on the circles for more detailed comparison.   <ref type="figure">Fig. 8</ref> also show the comparison results between the enhanced baseline and the proposed method from the Bird-Eye-View. <ref type="figure" target="#fig_4">Figure 9</ref> also present additional qualitative 3D detection results on the images with a comparison between those two on the KITTI val set. We could observe from the figures that the proposed geometry-guided learning approach can achieve significantly better 3D detection and localization performance than the enhanced baseline. <ref type="figure" target="#fig_2">Figure 10</ref> and 11 show additional visualization of the prediction results on KITTI 3D raw data in both the image plane and the LiDAR coordinate system, respectively. We use orange box, purple box, and green box for car, pedestrian, and cyclist, respectively. Our approach is able to accurately localize the different-depth 3D objects. <ref type="figure" target="#fig_4">Figure 9</ref>. Qualitative Results. The predictions on the KITTI val set. Results are from the enhanced baseline (left column) and ours (right column). <ref type="figure" target="#fig_2">Figure 10</ref>. Qualitative results of our method for multi-class 3D object detection. We use orange box for cars, purple box for pedestrians, and green box for cyclists. All illustrated images are from the KITTI test set. Zoom in on the images for more details. <ref type="figure" target="#fig_2">Figure 11</ref>. Qualitative results of our method for multi-class 3D object detection. We use orange box for cars, purple box for pedestrians, and green box for cyclists. All illustrated images are from the KITTI test set. Zoom in on the images for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " U 5 K Z 6 t c K T w / + G K m I 9 m e c Q K 5 5 s C s = " &gt; A A A B 7 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K R Y 8 F L x 4 r u G 2 h X U o 2 z b a h S X Z J s k J Z + h u 8 e F D E q z / I m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 K B X c W M / 7 R q W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m K Q t o I h L d j Y h h g i s W W G 4 F 6 6 a a E R k J 1 o k m d 3 O / 8 8 S 0 4 Y l 6 t N O U h Z K M F I 8 5 J d Z J Q V 9 L 3 B 1 U a 1 7 d W w C v E 7 8 g N S j Q G l S / + s O E Z p I p S w U x p u d 7 q Q 1 z o i 2 n g s 0 q / c y w l N A J G b G e o 4 p I Z s J 8 c e w M X z h l i O N E u 1 I W L 9 T f E z m R x k x l 5 D o l s W O z 6 s 3 F / 7 x e Z u P b M O c q z S x T d L k o z g S 2 C Z 5 / j o d c M 2 r F 1 B F C N X e 3 Y j o m m l D r 8 q m 4 E P z V l 9 d J + 6 r u N + r X D 4 1 a s 1 n E U Y Y z O I d L 8 O E G m n A P L Q i A A o d n e I U 3 p N A L e k c f y 9 Y S K m Z O 4 Q / Q 5 w 9 d L I 5 l &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g Q N A v 3 a V M i A 3 p 4 E v S 8 u Y F u 5 Q Q 7 E = " &gt; A A A B 7 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o s e C F 4 8 V 3 L b S L i W b Z t v Q J L s k W a E s / Q 1 e P C j i 1 R / k z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e l A p u r O d 9 o 9 L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q m S T T l A U 0 E Y n u R M Q w w R U L L L e C d V L N i I w E a 0 f j 2 5 n f f m L a 8 E Q 9 2 E n K Q k m G i s e c E u u k o K c l f u x X a 1 7 d m w O v E r 8 g N S j Q 7 F e / e o O E Z p I p S w U x p u t 7 q Q 1 z o i 2 n g k 0 r v c y w l N A x G b K u o 4 p I Z s J 8 f u w U n z l l g O N E u 1 I W z 9 X f E z m R x k x k 5 D o l s S O z 7 M 3 E / 7 x u Z u O b M O c q z S x T d L E o z g S 2 C Z 5 9 j g d c M 2 r F x B F C N X e 3 Y j o i m l D r 8 q m 4 E P z l l 1 d J 6 6 L u X 9 a v 7 i 9 r j U Y R R x l O 4 B T O w Y d r a M A d N C E A C h y e 4 R X e k E I v 6 B 1 9 L F p L q J g 5 h j 9 A n z 9 e s I 5 m &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O W m 2 m w Q i G J S E C t z U J W e K n U h / 1 M = " &gt; A A A B 7 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o s e C F 4 8 V 3 L b Y L i W b Z t v Q J L s k W a E s / Q 1 e P C j i 1 R / k z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e l A p u r O d 9 o 9 L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q m S T T l A U 0 E Y n u R M Q w w R U L L L e C d V L N i I w E a 0 f j 2 5 n f f m L a 8 E Q 9 2 E n K Q k m G i s e c E u u k o K c l f u x X a 1 7 d m w O v E r 8 g N S j Q 7 F e / e o O E Z p I p S w U x p u t 7 q Q 1 z o i 2 n g k 0 r v c y w l N A x G b K u o 4 p I Z s J 8 f u w U n z l l g O N E u 1 I W z 9 X f E z m R x k x k 5 D o l s S O z 7 M 3 E / 7 x u Z u O b M O c q z S x T d L E o z g S 2 C Z 5 9 j g d c M 2 r F x B F C N X e 3 Y j o i m l D r 8 q m 4 E P z l l 1 d J 6 6 L u X 9 a v 7 i 9 r j U Y R R x l O 4 B T O w Y d r a M A d N C E A C h y e 4 R X e k E I v 6 B 1 9 L F p L q J g 5 h j 9 A n z 9 g N I 5 n &lt; / l a t e x i t &gt; u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u j x d 0 l O F P R 8 O L u 2 t 8 7 y u 7 6 4 v 0 / c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p m Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 W 9 W v W 6 W a v U 6 3 k c R T i D c 7 g E D 2 6 g D v f Q g B Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B 4 6 u M / w = = &lt; / l a t e x i t &gt; v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 7 e m x a T d 2 O E C F o S G X I P e F 3 + p I 8 c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 W P B i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 4 U S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k p e N U M W y y W M S q E 1 C N g k t s G m 4 E d h K F N A o E t o P x / d x v T 1 B p H s t H M 0 3 Q j + h Q 8 p A z a q z U m P T L F b f q L k D W i Z e T C u S o 9 8 t f v U H M 0 g i l Y Y J q 3 f X c x P g Z V Y Y z g b N S L 9 W Y U D a m Q + x a K m m E 2 s 8 W h 8 7 I h V U G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m p F e 9 e b i f 1 4 3 N e G d n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k w F X y I y Y W k K Z 4 v Z W w k Z U U W Z s N i U b g r f 6 8 j p p X V W 9 6 + p N 4 7 p S q + V x F O E M z u E S P L i F G j x A H Z r A A O E Z X u H N e X J e n H f n Y 9 l a c P K Z U / g D 5 / M H 5 S + N A A = = &lt; / l a t e x i t &gt; c1 = (u1, v1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 2 S b S 9 0 L 8 U D O Y K G D J + S y j S a H 0 b M = " &gt; A A A B + X i c b V B N S w M x E J 3 1 s 9 a v V Y 9 e g k W o I G U j F b 0 I B S 8 e K 9 g P a J c l m 6 Z t a D a 7 J N l C W f p P v H h Q x K v / x J v / x r T d g 7 Y + G H i 8 N 8 P M v D A R X B v P + 3 b W 1 j c 2 t 7 Y L O 8 X d v f 2 D Q / f o u K n j V F H W o L G I V T s k m g k u W c N w I 1 g 7 U Y x E o W C t c H Q / 8 1 t j p j S P 5 Z O Z J M y P y E D y P q f E W C l w X R p g d I f K a Y A v 0 T j A F 4 F b 8 i r e H G i V 4 J y U I E c 9 c L + 6 v Z i m E Z O G C q J 1 B 3 u J 8 T O i D K e C T Y v d V L O E 0 B E Z s I 6 l k k R M + 9 n 8 8 i k 6 t 0 o P 9 W N l S x o 0 V 3 9 P Z C T S e h K F t j M i Z q i X v Z n 4 n 9 d J T f / W z 7 h M U s M k X S z q p w K Z G M 1 i Q D 2 u G D V i Y g m h i t t b E R 0 S R a i x Y R V t C H j 5 5 V X S v K r g a u X 6 s V q q 1 f I 4 C n A K Z 1 A G D D d Q g w e o Q w M o j O E Z X u H N y Z w X 5 9 3 5 W L S u O f n M C f y B 8 / k D K J e R a Q = = &lt; / l a t e x i t &gt; c2 = (u2, v2) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D P y 9 C u X Z 1 P R 9 H W l p / E C I + y b + l X c = " &gt; A A A B + X i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B a h g p Q k V P Q i F L x 4 r G A / o A 1 h s 9 2 2 S z e b s L s p l N B / 4 s W D I l 7 9 J 9 7 8 N 2 7 b H L T 1 w c D j v R l m 5 o U J Z 0 o 7 z r d V 2 N j c 2 t 4 p 7 p b 2 9 g 8 O j + z j k 5 a K U 0 l o k 8 Q 8 l p 0 Q K 8 q Z o E 3 N N K e d R F I c h Z y 2 w / H 9 3 G 9 P q F Q s F k 9 6 m l A / w k P B B o x g b a T A t k n g o T t U S Q P v C k 0 C 7 z K w y 0 7 V W Q C t E z c n Z c j R C O y v X j 8 m a U S F J h w r 1 X W d R P s Z l p o R T m e l X q p o g s k Y D 2 n X U I E j q v x s c f k M X R i l j w a x N C U 0 W q i / J z I c K T W N Q t M Z Y T 1 S q 9 5 c / M / r p n p w 6 2 d M J K m m g i w X D V K O d I z m M a A + k 5 R o P j U E E 8 n M r Y i M s M R E m 7 B K J g R 3 9 e V 1 0 v K q b q 1 6 / V g r 1 + t 5 H E U 4 g 3 O o g A s 3 U I c H a E A T C E z g G V 7 h z c q s F + v d + l i 2 F q x 8 5 h T + w P r 8 A S 0 3 k W w = &lt; / l a t e x i t &gt; h = 32 pixels &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h b Z R O x G h 3 d 4 G 8 6 N b z 2 0 h k t d L Y b M = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g q e z W i l 6 E g h e P F e w H d J e S T W f b 0 G w 2 J F m x L P 0 b X j w o 4 t U / 4 8 1 / Y 9 r u Q V s f D D z e m 2 F m X i g 5 0 8 Z 1 v 5 3 C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a u s k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>q 3 r 1 6 u V 9 v d J o 5 H E U 0 Q k 6 R e f I Q 1 e o g e 5 Q E 7 U Q R R I 9 o 1 f 0 5 q T O i / P u f C x a C 0 4 + c 4 z + w P n 8 A R X g k R M = &lt; / l a t e x i t &gt; H &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w 0 N s g R L J z 8 1 e I U e V F g x 9 q X t 5 9 j Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 W P B S 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J / d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I 7 P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 1 9 W b 5 n W l V s v j K M I Z n M M l e H A L N a h D A 1 r A A O E Z X u H N e X R e n H f n Y 9 l a c P K Z U / g D 5 / M H n 3 e M 0 g = = &lt; / l a t e x i t &gt; H &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w 0 N s g R L J z 8 1 e I U e V F g x 9 q X t 5 9 j Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 W P B S 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J / d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V uu e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I 7 P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 1 9 W b 5 n W l V s v j K M I Z n M M l e H A L N a h D A 1 r A A O E Z X u H N e X R e n H f n Y 9 l a c P K Z U / g D 5 / M H n 3 e M 0 g = = &lt; / l a t e x i t &gt; h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 8 C 2 H g K 4 u f y l s T 0 F D + i F n h x f 9 Z E = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R 6 U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s J b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 t e p 1 s 1 a p 1 / M 4 i n A G 5 3 A J H t x A H e 6 h A S 1 g g P A M r / D m P D o v z r v z s W w t O P n M K f y B 8 / k D z / e M 8 g = = &lt; / l a t e x i t &gt; C2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j z i 7 X C O I u u Y q N 7 M R O w i e U n G / t S 4 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K R Y + F X j x W t L X Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J R 8 e p Y t h m s Y h V N 6 A a B Z f Y N t w I 7 C Y K a R Q I f A w m z b n / + I R K 8 1 g + m G m C f k R H k o e c U W O l + + a g N i h X 3 K q 7 A F k n X k 4 q k K M 1 K H / 1 h z F L I 5 S G C a p 1 z 3 M T 4 2 d U G c 4 E z k r 9 V G N C 2 Y S O s G e p p B F q P 1 u c O i M X V h m S M F a 2 p C E L 9 f d E R i O t p 1 F g O y N q x n r V m 4 v / e b 3 U h D d + x m W S G p R s u S h M B T E x m f 9 N h l w h M 2 J q C W W K 2 1 s J G 1 N F m b H p l G w I 3 u r L 6 6 R T q 3 r 1 6 t V d v d J o 5 H E U 4 Q z O 4 R I 8 u I Y G 3 E I L 2 s B g B M / w C m + O c F 6 c d + d j 2 V p w 8 p l T + A P n 8 w e + Z Y 1 y &lt; / l a t e x i t &gt; C1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T Y 0 x Q B W x v W 0 m 8 w x 2 W D s 4 K N G X E w w = " &gt; A A A B 6 n i c d V D L S s N A F L 2 p r 1 p f V Z d u B o v g K i T S m n Z X 7 M Z l R f u A N p T J d N I O n T y Y m Q g l 9 B P c u F D E r V / k z r 9 x k l Z Q 0 Q M X D u f c y 7 3 3 e D F n U l n W h 1 F Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H X R k l g t A O i X g k + h 6 W l L O Q d h R T n P Z j Q X H g c d r z Z q 3 M 7 9 1 T I V k U 3 q l 5 T N 0 A T 0 L m M 4 K V l m 5 b I 3 t U r l h m o + b U q x b K S N 2 y G z l x n M Y l s k 0 r R w V W a I / K 7 8 N x R J K A h o p w L O X A t m L l p l g o R j h d l I a J p D E m M z y h A 0 1 D H F D p p v m p C 3 S m l T H y I 6 E r V C h X v 0 + k O J B y H n i 6 M 8 B q K n 9 7 m f i X N 0 i U X 3 d T F s a J o i F Z L v I T j l S E s r / R m A l K F J 9 r g o l g + l Z E p l h g o n Q 6 J R 3 C 1 6 f o f 9 K 9 M O 2 q W b u p V p p X q z i K c A K n c A 4 2 O N C E a 2 h D B w h M 4 A G e 4 N n g x q P x Y r w u W w v G a u Y Y f s B 4 + w R Z o Y 3 f &lt; / l a t e x i t &gt; depth = 35.2m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y W Z h M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>p 7 B K 3 i z n q w X 6 9 3 6 W L S u W c V M B f y B 9 f k D w h y T i w = = &lt; / l a t e x i t &gt; depth = 40.5m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 3 Y m Y S T K d b F e D g + S V D N f b N D y L N M = " &gt; A A A B / H i c b V B N S 8 N A E J 3 4 W e t X t E c v i 0 X w F B J p 0 Y t Q 9 O K x g v 2 A N p T N Z t s u 3 U 3 C 7 k Y I o f 4 V L x 4 U 8 e o P 8 e a / c d v m o K 0 P B h 7 v z T A z L 0 g 4 U 9 p 1 v 6 2 1 9 Y 3 N r e 3 S T n l 3 b / / g 0 D 4 6 b q s 4 l Y S 2 S M x j 2 Q 2 w o p x F t K W Z 5 r S b S I p F w G k n m N z O / M 4 j l Y r F 0 Y P O E u o L P I r Y k B G s j T S w K 3 l f C h T S R I + n 6 B r V X K c u B n b V d d w 5 0 C r x C l K F A s 2 B / d U P Y 5 I K G m n C s V I 9 z 0 2 0 n 2 O p G e F 0 W u 6 n i i a Y T P C I 9 g y N s K D K z + f H T 9 G Z U U I 0 j K W p S K O 5 + n s i x 0 K p T A S m U 2 A 9 V s v e T P z P 6 6 V 6 e O X n L E p S T S O y W D R M O d I x m i W B Q i Y p 0 T w z B B P J z K 2 I j L H E R J u 8 y i Y E b / n l V d K + c L y a U 7 + v V R s 3 R R w l O I F T O A c P L q E B d 9 C E F h D I 4 B l e 4 c 1 6 s l 6 s d + t j 0 b p m F T M V + A P r 8 w f A k J O K &lt; / l a t e x i t &gt; Visualization of the depth difference in geometry projection. For two cars with the same height in both the 2D bounding box (in blue) and the 3D bounding box (in orange), the depth values of their centers differ by more than 5 meters because of their distinct poses and locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " X p E F q Q T Q C 5 p V d B 8 i O Z O I 3 M 4 O q 4 w = " &gt; A A A C D X i c b Z D L S s N A F I Z P 6 q 3 W W 9 S l m 8 E q u C p J q e i y 0 E 2 X V e w F 2 l A m 0 0 k 7 d D I J M x O h h L 6 A G 1 / F j Q t F 3 L p 3 5 9 s 4 b Q N q 6 w 8 D P 9 8 5 h z P n 9 2 P O l H a c L y u 3 t r 6 x u Z X f L u z s 7 u 0 f 2 I d H L R U l k t A m i X g k O z 5 W l D N B m 5 p p T j u x p D j 0 O W 3 7 4 9 q s 3 r 6 n U r F I 3 O l J T L 0 Q D w U L G M H a o L 5 9 1 g s k J q i N b l F P s 5 A q t A D 1 H 1 B D f b v o l J y 5 0 K p x M 1 O E T I 2 + / d k b R C Q J q d C E Y 6 W 6 r h N r L 8 V S M 8 L p t N B L F I 0 x G e M h 7 R o r s N n j p f N r p u j c k A E K I m m e 0 G h O f 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9 a c l c 0 c w x 9 Z</head><label>9</label><figDesc>H 9 8 T X 5 m d &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " a P c 6 z q F C L N l j I b W q Y y u 7 C e h M R 3 c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k m p 6 L H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G q G L Z Y L G L V D a h G w S W 2 D D c C u 4 l C G g U C O 8 H k b u 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U Z I N S 2 a 2 4 C 5 B 1 4 u W k D D k a g 9 J X f x i z N E J p m K B a 9 z w 3 M X 5 G l e F M 4 K z Y T z U m l E 3 o C H u W S h q h 9 r P F o T N y a Z U h C W N l S x q y U H 9 P Z D T S e h o F t j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 &lt; 1 &lt;</head><label>31</label><figDesc>y 9 Y N J 5 8 5 g z 9 w P n 8 A x C 2 M 3 w = = &lt; / l a t e x i t &gt; 3D map l a t e x i t s h a 1 _ b a s e 6 4 = " t m V 3 n T y E A 9 M G 8 7 A C v v 0 + 0 f q j k 4 0 = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y R o 0 c S L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 p f L L l l d w G y T r y M l C B D v V / 8 6 g 1 i l k Y o D R N U 6 6 7 n J s a f U m U 4 E z g r 9 F K N C W V j O s S u p Z J G q P 3 p 4 t A Z u b D K g I S x s i U N W a i / J 6 Y 0 0 n o S B b Y z o m a k V 7 2 5 + J / X T U 1 4 6 0 + 5 T F K D k i 0 X h a k g J i b z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u C D c F b f X m d t C p l r 1 q + b l R L t U o W R x 7 O 4 B w u w Y M b q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A H t t j K 8 = &lt; / l a t e x i t &gt; Depth map l a t e x i t s h a 1 _ b a s e 6 4 = " v o n V k Z 8 K h N O i h + D d P 1 v s s 5 V c a O k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k m p 6 L H g x W M L 9 g P a U D b b S b t 2 s w m 7 G 6 G E / g I v H h T x 6 k / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a 1 n G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>r l u 1 s r 1 a h 5 HFigure 2 .</head><label>52</label><figDesc>A c 7 h A q 7 A g x u o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 b T j 5 z B n / g f P 4 A e G W M r Q = = &lt; / l a t e x i t &gt; An overview of our proposed approach. We leverage a 2D backbone network to extract features from the input monocular RGB image. Then several 2D/3D output branches are used for generating 2D/3D predictions through decoding, with depth excluded from prediction. The 2D/3D predictions are utilized by the geometric module to compute and generate geometric features via the proposed geometric formula implemented in a network module. The geometric features are concatenated with the image features of the backbone for depth estimation. Based on the depth and other 3D predictions from the output branches, the decoder outputs the 3D object detection results. Dash lines and solid lines represent normal flows and neural network forward flows, respectively. The symbol ? represents a tensor concatenation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>z 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " C g i c z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 &lt; 2 &lt;</head><label>22</label><figDesc>k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o G G i R D N e Z 5 G M d C u g h k u h e B 0 F S t 6 K N a d h I H k z G F 1 P / e Y j 1 0 Z E 6 h 7 H M f d D O l C i L x h F K z 1 0 b r h E S p 6 6 X r d Y c s v u D G S Z e B k p Q Y Z a t / j V 6 U U s C b l C J q k x b c + N 0 U + p R s E k n x Q 6 i e E x Z S M 6 4 G 1 L F Q 2 5 8 d P Z x R N y Y p U e 6 U f a l k I y U 3 9 P p D Q 0 Z h w G t j O k O D S L 3 l T 8 z 2 s n 2 L / 0 U 6 H i B L l i 8 0 X 9 R B K M y P R 9 0 h O a M 5 R j S y j T w t 5 K 2 J B q y t C G V L A h e I s v L 5 P G W d m r l M / v K q X q V R Z H H o 7 g G E 7 B g w u o w i 3 U o A 4 M F D z D K 7 w 5 x n l x 3 p 2 P e W v O y W Y O 4 Q + c z x / D q 5 B Q &lt; / l a t e x i t &gt; z l a t e x i t s h a 1 _ b a s e 6 4 = " R / C G 0 B W l w 2 d J x K 9 5 + y / M j G a D H / U = " &gt; A A A B 8 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I n c E o 4 U F i R a W m A g Y 4 U L 2 l j 3 Y s L d 3 2 Z 0 z Q c K / s L H Q G F v / j Z 3 / x g W u U P A l k 7 y 8 N 5 O Z e U E i h U H X / X Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o G n i V D P e Y L G M 9 X 1 A D Z d C 8 Q Y K l P w + 0 Z x G g e S t Y H g 1 9 V u P X B s R q z s c J d y P a F + J U D C K V n r o X H O J l D x 1 K 9 1 i y S 2 7 M 5 B l 4 m W k B B n q 3 e J X p x e z N O I K m a T G t D 0 3 Q X 9 M N Q o m + a T Q S Q 1 P K B v S P m 9 b q m j E j T + e X T w h J 1 b p k T D W t h S S m f p 7 Y k w j Y 0 Z R Y D s j i g O z 6 E 3 F / 7 x 2 i u G F P x Y q S Z E r N l 8 U p p J g T K b v k 5 7 Q n K E c W U K Z F v Z W w g Z U U 4 Y 2 p I I N w V t 8 e Z k 0 K 2 W v W j 6 7 r Z Z q l 1 k c e T i C Y z g F D 8 6 h B j d Q h w Y w U P A M r / D m G O f F e X c + 5 q 0 5 J 5 s 5 h D 9 w P n 8 A x S + Q U Q = = &lt; / l a t e x i t &gt; L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 r F 2 G B + W E 3 J c 5 V R w m L + O a 1 Q I r 2 M = " &gt; A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F w M b C I g H z A c k R 9 j Z z y Z q 9 v W N 3 T w g h v 8 D G Q h F b f 5 K d / 8 Z N c o U m P h h 4 v D f D z L w g E V w b 1 / 1 2 c m v r G 5 t b + e 3 C z u 7 e / k H x 8 K i p 4 1 Q x b L B Y x K o d U I 2 C S 2 w Y b g S 2 E 4 U 0 C g S 2 g t H t z G 8 9 o d I 8 l g 9 m n K A f 0 Y H k I W f U W K l + 3 y u W 3 L I 7 B 1 k l X k Z K k K H W K 3 5 1 + z F L I 5 S G C a p 1 x 3 M T 4 0 + o M p w J n B a 6 q c a E s h E d Y M d S S S P U / m R + 6 J S c W a V P w l j Z k o b M 1 d 8 T E x p p P Y 4 C 2 x l R M 9 T L 3 k z 8 z + u k J r z 2 J 1 w m q U H J F o v C V B A T k 9 n X p M 8 V M i P G l l C m u L 2 V s C F V l B m b T c G G 4 C 2 / v E q a F 2 W v U r 6 s V 0 r V m y y O P J z A K Z y D B 1 d Q h T u o Q Q M Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F p z T j Z z D H / g f P 4 A p F O M 0 g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " s 3 C 4 W f q 8 x h I W X o d 7 7 m f s B Y O d q c 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>k 1 8 M 1 &lt; l a t e x i t s h a 1 _</head><label>811</label><figDesc>P B 4 b 4 a Z e U E s u D a u + + 3 k V l b X 1 j f y m 4 W t 7 Z 3 d v e L + Q U N H i W J Y Z 5 G I V C u g G g W X W D f c C G z F C m k Y C G w G o + u p 3 3 x E p X k k 7 8 0 4 R j + k A 8 n 7 n F F j p Y f O D Q p D y V P X 6 x Z L b t m d g S w T L y M l y F D r F r 8 6 v Y g l I U r D B N W 6 7 b m x 8 V O q D G c C J 4 V O o j G m b E Q H 2 L Z U 0 h C 1 n 8 4 u n p A T q / R I P 1 K 2 p C E z 9 f d E S k O t x 2 F g O 0 N q h n r R m 4 r / e e 3 E 9 C / 9 l M s 4 M S j Z f F E / E c R E Z P o + 6 X G F z I i x J Z Q p b m 8 l b E g V Z c a G V L A h e I s v L 5 P G W d m r l M / v K q X q V R Z H H o 7 g G E 7 B g w u o w i 3 U o A 4 M J D z D K 7 w 5 2 n l x 3 p 2 P e W v O y W Y O 4 Q + c z x / A n 5 B O &lt; / l a t e x i t &gt; W &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 5 k g 4 b 6 Y I + p n A y f G M n N 4 s N 6 D f R A = " &gt; A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 E 0 c I i Y G O Z g P m A 5 A h 7 m 7 l k z d 7 e s b s n hC O / w M Z C E V t / k p 3 / x k 1 y h S Y + G H i 8 N 8 P M v C A R X B v X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V D F s s l j E q h N Q j Y J L b B p u B H Y S h T Q K B L a D 8 d 3 M b z + h 0 j y W D 2 a S o B / R o e Q h Z 9 R Y q d H u l y t u 1 Z 2 D r B I v J x X I U e + X v 3 q D m K U R S s M E 1 b r r u Y n x M 6 o M Z w K n p V 6 q M a F s T I f Y t V T S C L W f z Q + d k j O r D E g Y K 1 v S k L n 6 e y K j k d a T K L C d E T U j v e z N x P + 8 b m r C G z / j M k k N S r Z Y F K a C m J j M v i Y D r p A Z M b G E M s X t r Y S N q K L M 2 G x K N g R v + e V V 0 r q o e p f V q 8 Z l p X a b x 1 G E E z i F c / D g G m p w D 3 V o A g O E Z 3 i F N + f R e X He n Y 9 F a 8 H J Z 4 7 h D 5 z P H 7 T / j N 0 = &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d X l W o x + 6 k S g t M w + J k V R d i z c g g J Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o g c P B S 8 e K 5 i 2 0 o a y 2 W 7 a p b u b s L s R S u h v 8 O J B E a / + I G / + G 7 d p D t r 6 Y O D x 3 g w z 8 8 K E M 2 1 c 9 9 s p r a 1 v b G 6 V t y s 7 u 3 v 7 B 9 X D o 7 a O U 0 W o T 2 I e q 2 6 I N e V M U t 8 w w 2 k 3 U R S L k N N O O L m d + 5 0 n q j S L 5 Y O Z J j Q Q e C R Z x A g 2 V v L 7 S q D H Q b X m 1 t 0 c a J V 4 B a l B g d a g + t U f x i Q V V B r C s d Y 9 z 0 1 M k G F l G O F 0 V u m n m i a Y T P C I 9 i y V W F A d Z P m x M 3 R m l S G K Y m V L G p S r v y c y L L S e i t B 2 C m z G e t m b i / 9 5 v d R E 1 0 H G Z J I a K s l i U Z R y Z G I 0 / x w N m a L E 8 K k l m C h m b 0 V k j B U m x u Z T s S F 4 y y + v k v Z F 3 W v U L + 8 b t e Z N E U c Z T u A U z s G D K 2 j C H b T A B w I M n u E V 3 h z p v D j v z s e i t e Q U M 8 f w B 8 7 n D 1 1 8 j m I = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k G h r 7 p g w k A 0 B I 5 V Z p a j u Q Z / i S o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o g c P B S 8 e K 5 i 2 2 I a y 2 W 7 a p b u b s L s R S u h v 8 O J B E a / + I G / + G 7 d p D t r 6 Y O D x 3 g w z 8 8 K E M 2 1 c 9 9 s p r a 1 v b G 6 V t y s 7 u 3 v 7 B 9 X D o 7 a O U 0 W o T 2 I e q 2 6 I N e V M U t 8 w w 2 k 3 U R S L k N N O O L m d + 5 0 n q j S L 5 Y O Z J j Q Q e C R Z x A g 2 V v L 7 S q D H Q b X m 1 t 0 c a J V 4 B a l B g d a g + t U f x i Q V V B r C s d Y 9 z 0 1 M k G F l G O F 0 V u m n m i a Y T P C I 9 i y V W F A d Z P m x M 3 R m l S G K Y m V L G p S r v y c y L L S e i t B 2 C m z G e t m b i / 9 5 v d R E 1 0 H G Z J I a K s l i U Z R y Z G I 0 / x w N m a L E 8 K k l m C h m b 0 V k j B U m x u Z T s S F 4 y y + v k v Z F 3 W v U L + 8 b t e Z N E U c Z T u A U z s G D K 2 j C H b T A B w I M n u E V 3 h z p v D j v z s e i t e Q U M 8 f w B 8 7 n D 1 8 A j m M = &lt; / l a t e x i t &gt; (x, y, z) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P E A C d k T y b q a V a h 4 h R S x L E d q q 2 K Y = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h Q i m J V P T g o e D F Y w X 7 I W 0 o m + 2 m X b r Z h N 2 N G E N / h R c P i n j 1 5 3 j z 3 7 h t c 9 D W B w O P 9 2 a Y m e d F n C l t 2 9 9 W b m V 1 b X 0 j v 1 n Y 2 t 7 Z 3 S v u H 7 R U G E t C m y T k o e x 4 W F H O B G 1 q p j n t R J L i w O O 0 7 Y 2 v p 3 7 7 g U r F Q n G n k 4 i 6 A R 4 K 5 j O C t Z H u y 4 8 V l F T Q 0 2 m / W L K r 9 g x o m T g Z K U G G R r / 4 1 R u E J A 6 o 0 I R j p b q O H W k 3 x V I z w u m k 0 I s V j T A Z 4 y H t G i p w Q J W b z g 6 e o B O j D J A f S l N C o 5 n 6 e y L F g V J J 4 J n O A O u R W v S m 4 n 9 e N 9 b + p Z s y E c W a C j J f 5 M c c 6 R B N v 0 c D J i n R P D E E E 8 n M r Y i M s M R E m 4 w K J g R n 8 e V l 0 j q r O r X q + W 2 t V L / K 4 s j D E R x D G R y 4 g D r c Q A O a Q C C A Z 3 i F N 0 t a L 9 a 7 9 T F v z V n Z z C H 8 g f X 5 A + l Z j y o = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N u F n 6 w j + 0 u 3 2 u t 7 M Y p N F j + n y 7j A = " &gt; A A A C G H i c b Z C 7 T s M w F I Y d r q X c A o w s E R U S A w p J m y o d G C q x M B a J X q Q k q h z X a a 0 6 F 9 k O U h X l M V h 4 F R Y G E G L t x t v g p h m g 5 U i W P v 3 / O f b x 7 y e U c G E Y 3 8 r G 5 t b 2 z m 5 l r 7 p / c H h 0 r J 6 c 9 n i c M o S 7 K K Y x G / i Q Y 0 o i 3 B V E U D x I G I a h T 3 H f n 9 4 t / P 4 T Z p z E 0 a O Y J d g L 4 T g i A U F Q S G m o 3 m R u c Y n D x r 6 X G b r V a N m W e W 3 o p t 1 s 1 u s S D K t l N + z c h T S Z w H y o 1 q R U l L Y O Z g k 1 U F Z n q M 7 d U Y z S E E c C U c i 5 Y x q J 8 D L I B E E U 5 1 U 3 5 T i B a A r H 2 J E Y w R B z L y t 2 y r V L q Y y 0 I G b y R E I r 1 N 8 T G Q w 5 n 4 W + 7 A y h m P B V b y H + 5 z m p C F p e R q I k F T h C y 4 e C l G o i 1 h Y p a S P C M B J 0 J g E i R u S u G p p A B p G Q W V Z l C O b q l 9 e h V 9 d N S 2 8 + W L X 2 b R l H B Z y D C 3 A F T G C D N r g H H d A F C D y D V / A O P p Q X 5 U 3 5 V L 6 W r R t K O X M G / p Q y / w G q / p x j &lt; / l a t e x i t &gt;? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J E U 2 3 s s b c k J 9 c 3 m P z + I / C n K 3 J X M = " &gt; A A A C G H i c b Z D L S g M x F I Y z X m u 9 j b p 0 M 1 g E F 1 I z 0 5 Y q u C i 4 c V n B X q A d S i Z N 2 9 D M h e S M U I Y + h h t f x Y 0 L R d x 2 5 9 u Y T m e h r Q c C H / 9 / T n L y e 5 H g C j D + N t b W N z a 3 t n M 7 + d 2 9 / Y N D 8 + i 4 q c J Y U t a g o Q h l 2 y O K C R 6 w B n A Q r B 1 J R n x P s J Y 3 v p v 7 r S c m F Q + D R 5 h E z P X J M O A D T g l o q W d e J d 3 0 k o 4 c e m 6 C i 1 X H q e K b S 1 z E F a d S L W m w K x i X n G k X R g z I t G c W t J e W t Q p 2 B g W U V b 1 n z r r 9 k M Y + C 4 A K o l T H x h G 4 C Z H A q W D T f D d W L C J 0 T I a s o z E g P l N u k u 4 0 t c 6 1 0 r c G o d Q n A C t V f 0 8 k x F d q 4 n u 6 0 y c w U s v e X P z P 6 8 Q w u H Y T H k Q x s I A u H h r E w o L Q m q d k 9 b l k F M R E A 6 G S 6 1 0 t O i K S U N B Z 5 n U I 9 v K X V 6 H p F O 1 y s f J Q L t R u s z h y 6 B S d o Q t k o y q q o X t U R w 1 E 0 T N 6 R e / o w 3 g x 3 o x P 4 2 v R u m Z k M y f o T x m z H 6 d z n G E = &lt; / l a t e x i t &gt; r y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M Y T N d S e W U Z l Y / c m Y q X 7 C s M y J B i A = " &gt; A A A C F X i c d V B L S w M x G M z W V 6 2 v V Y 9 e g k X w U J b d d r u 2 t 6 I X j x X s A 9 p l y a Z p G 5 p 9 k G S F s u y f 8 O J f 8 e J B E a + C N / + N 6 Q t U d C A w z M y X f B k / Z l R I 0 / z U c m v r G 5 t b + e 3 C z u 7 e / o F + e N Q W U c I x a e G I R b z r I 0 E Y D U l L U s l I N + Y E B T 4 j H X 9 y N f M 7 d 4 Q L G o W 3 c h o T N 0 C j k A 4 p R l J J n l 5 K + / N L e n z k u 6 l p l B 2 7 V r d L p m H b 1 U q 9 o o j l m I 5 T y 7 g 3 z T y 9 u E r A V Q K u E t A y z D m K Y I m m p 3 / 0 B x F O A h J K z J A Q P c u M p Z s i L i l m J C v 0 E 0 F i h C d o R H q K h i g g w k 3 n C 2 X w T C k D O I y 4 O q G E c / X 7 R I o C I a a B r 5 I B k m P x 2 5 u J f 3 m 9 R A 5 r b k r D O J E k x I u H h g m D M o K z i u C A c o I l m y q C M K d q V 4 j H i C M s V Z E F V c L q p / B / 0 i 4 b l m 1 U b + x i 4 3 J Z R x 6 c g F N w D i x w A R r g G j R B C 2 B w D x 7 B M 3 j R H r Q n 7 V V 7 W 0 R z 2 n L m G P y A 9 v 4 F / O y b l Q = = &lt; / l a t e x i t &gt; (a) Bird's-eye view z b a s e 6 4 = " C g i c z v x 9 E T d M L 0 v w t W L b D i / 1 q k 4 = " &gt; A A A B 8 X i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F Q A v L C O Y D k y P s b T b J k r 2 9 Y 3 d O i E f + h Y 2 F I r b + G z v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o G G i R D N e Z 5 G M d C u g h k u h e B 0 F S t 6 K N a d h I H k z G F 1 P / e Y j 1 0 Z E 6 h 7 H M f d D O l C i L x h F K z 1 0 b r h E S p 6 6 X r d Y c s v u D G S Z e B k p Q Y Z a t / j V 6 U U s C b l C J q k x b c + N 0 U + p R s E k n x Q 6 i e E x Z S M 6 4 G 1 L F Q 2 5 8 d P Z x R N y Y p U e 6 U f a l k I y U 3 9 P p D Q 0 Z h w G t j O k O D S L 3 l T 8 z 2 s n 2 L / 0 U 6 H i B L l i 8 0 X 9 R B K M y P R 9 0 h O a M 5 R j S y j T w t 5 K 2 J B q y t C G V L A h e I s v L 5 P G W d m r l M / v K q X q V R Z H H o 7 g G E 7 B g w u o w i 3 U o A 4 M F D z D K 7 w 5 x n l x 3 p 2 P e W v O y W Y O 4 Q + c z x / D q 5 B Q &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M r H 2 5 p t D 6 H l m W W f G F o D 1 M 0 y y q 0 w = " &gt; A A A C F 3 i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S y C C w l J T W w E F w U 3 L i v Y C 7 S h T K a T d u j k w s x E K C F v 4 c Z X c e N C E b e 6 8 2 2 c t l l o 6 4 G B j / 8 / Z + b M 7 y e M C m m a 3 9 r K 6 t r 6 x m Z p q 7 y 9 s 7 u 3 r x 8 c t k S c c k y a O G Y x 7 / h I E E Y j 0 p R U M t J J O E G h z 0 j b H 9 9 M / f Y D 4 Y L G 0 b 2 c J M Q L 0 T C i A c V I K q m v G 1 l v d k m X D 3 0 v M 4 2 q e W W 7 9 r l p X D g 1 5 9 J V U L P c m u P m P Z 9 I l P f 1 i m m Y s 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 &lt; l a t e x i t s h a 1 _Figure 3 .</head><label>213</label><figDesc>A a v 4 E 1 7 0 l 6 0 d + 1 j 3 r q i F T N H 4 E 9 p n z / 1 R J w C &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 D e b u + 3 n I N 6 e 0 K 4 M A s 5 I F P 4 I B M Y = " &gt; A A A B 7 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o g c P B S 8 e K 7 h t o V 1 K N s 2 2 o U l 2 S b J C W f o b v H h Q x K s / y J v / x r T d g 7 Y + G H i 8 N 8 P M v C g V 3 F j P + 0 a l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D J J p i k L a C I S 3 Y m I Y Y I r F l h u B e u k m h E Z C d a O x n c z v / 3 E t O G J e r S T l I W S D B W P O S X W S U F P S 9 z p V 2 t e 3 Z s D r x K / I D U o 0 O x X v 3 q D h G a S K U s F M a b r e 6 k N c 6 I t p 4 J N K 7 3 M s J T Q M R m y r q O K S G b C f H 7 s F J 8 5 Z Y D j R L t S F s / V 3 x M 5 k c Z M Z O Q 6 J b E j s + z N x P + 8 b m b j m z D n K s 0 s U 3 S x K M 4 E t g m e f Y 4 H X D N q x c Q R Q j V 3 t 2 I 6 I p p Q 6 / K p u B D 8 5 Z d X S e u i 7 l / W r x 4 u a 4 3 b I o 4 y n M A p n I M P 1 9 C A e 2 h C A B Q 4 P M M r v C G F X t A 7 + l i 0 l l A x c w x / g D 5 / A F v 4 j m E = &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d X l W o x + 6 k S g t M w + J k V R d i z c g g J Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o g c P B S 8 e K 5 i 2 0 o a y 2 W 7 a p b u b s L s R S u h v 8 O J B E a / + I G / + G 7 d p D t r 6 Y O D x 3 g w z 8 8 K E M 2 1 c 9 9 s p r a 1 v b G 6 V t y s 7 u 3 v 7 B 9 X D o 7 a O U 0 W o T 2 I e q 2 6 I N e V M U t 8 w w 2 k 3 U R S L k N N O O L m d + 5 0 n q j S L 5 Y O Z J j Q Q e C R Z x A g 2 V v L 7 S q D H Q b X m 1 t 0 c a J V 4 B a l B g d a g + t U f x i Q V V B r C s d Y 9 z 0 1 M k G F l G O F 0 V u m n m i a Y T P C I 9 i y V W F A d Z P m x M 3 R m l S G K Y m V L G p S r v y c y L L S e i t B 2 C m z G e t m b i / 9 5 v d R E 1 0 H G Z J I a K s l i U Z R y Z G I 0 / x w N m a L E 8 K k l m C h m b 0 V k j B U m x u Z T s S F 4 y y + v k v Z F 3 W v U L + 8 b t e Z N E U c Z T u A U z s G D K 2 j C H b T A B w I M n u E V 3 h z p v D j v z s e i t e Q U M 8 f w B 8 7 n D 1 1 8 j m I = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k G h r 7 p g w k A 0 B I 5 V Z p a j u Q Z / i S o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o g c P B S 8 e K 5 i 2 2 I a y 2 W 7 a p b u b s L s R S u h v 8 O J B E a / + I G / + G 7 d p D t r 6 Y O D x 3 g w z 8 8 K E M 2 1 c 9 9 s p r a 1 v b G 6 V t y s 7 u 3 v 7 B 9 X D o 7 a O U 0 W o T 2 I e q 2 6 I N e V M U t 8 w w 2 k 3 U R S L k N N O O L m d + 5 0 n q jS L 5 Y O Z J j Q Q e C R Z x A g 2 V v L 7 S q D H Q b X m 1 t 0 c a J V 4 B a l B g d a g + t U f x i Q V V B r C s d Y 9 z 0 1 M k G F l G O F 0 V u m n m ia Y T P C I 9 i y V W F A d Z P m x M 3 R m l S G K Y m V L G p S r v y c y L L S e i t B 2 C m z G e t m b i / 9 5 v d R E 1 0 H G Z J I a K s l i U Z R y Z G I 0 / x w N m a L E 8 K k l m C h m b 0 V k j B U m x u Z T s S F 4 y y + v k v Z F 3 W v U L + 8 b t e Z N E U c Z T u A U z s G D K 2 j C H b T A B w I M n u E V 3 h z p v D j v z s e i t e QU M 8 f w B 8 7 n D 1 8 A j m M = &lt; / l a t e x i t &gt; z b a s e 6 4 = " R / C G 0 B W l w 2 d J x K 9 5 + y / M j G a D H / U = " &gt; A A A B 8 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I n c E o 4 U F i R a W m A g Y 4 U L 2 l j 3 Y s L d 3 2 Z 0 z Q c K / s L H Q G F v / j Z 3 / x g W u U P A l k 7 y 8 N 5 O Z e U E i h U H X / X Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o G n i V D P e Y L G M 9 X 1 A D Z d C 8 Q Y K l P w + 0 Z x G g e S t Y H g 1 9 V u P X B s R q z s c J d y P a F + J U D C K V n r o X H O J l D x 1 K 9 1 i y S 2 7 M 5 B l 4 m W k B B n q 3 e J X p x e z N O I K m a T G t D 0 3 Q X 9 M N Q o m + a T Q S Q 1 P K B v S P m 9 b q m j E j T + e X T w h J 1b p k T D W t h S S m f p 7 Y k w j Y 0 Z R Y D s j i g O z 6 E 3 F / 7 x 2 i u G F P x Y q S Z E r N l 8 U p p J g T K b v k 5 7 Q n K E c W U K Z F v Z W w g Z U U 4 Y 2 p I I N w V t 8 e Z k 0 K 2 W v W j 6 7 r Z Z q l 1 k c e T i C Y z g F D 8 6 h B j d Q h w Y w U P A M r / D m G O f F e Xc + 5 q 0 5 J 5 s 5 h D 9 w P n 8 A x S + Q U Q = = &lt; / l a t e x i t &gt; L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 r F 2 G B + W E 3 J c 5 V R w m L + O a 1 Q I r 2 M = " &gt; A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F w M b C I g H z A c k R 9 j Z z y Z q 9 v W N 3 T w g h v 8 D G Q h F b f 5 K d / 8 Z N c o U m P h h 4 v D f D z L w g E V w b 1 / 1 2 c m v r G 5 t b + e 3 C z u 7 e / k H x 8 K i p 4 1 Q x b L B Y x K o d U I 2 C S 2 w Y b g S 2 E 4 U 0 C g S 2 g t H t z G 8 9 o d I 8 l g 9 m n K A f 0 Y H k I W f U W K l + 3 y u W 3 L I 7 B 1 k l X k Z K k K H W K 3 5 1 + z F L I 5 S G C a p 1 x 3 M T 4 0 + o M p w J n B a 6 q c a E s h E d Y M d S S S P U / m R + 6 J S c W a V P w l j Z k o b M 1 d 8 T E x p p P Y 4 C 2 x l R M 9 T L 3 k z 8 z + u k J r z 2 J 1 w m q U H J F o v C V B A T k 9 n X p M 8 V M i P G l l C m u L 2 V s C F V l B m b T c G G 4 C 2 / v E q a F 2 W v U r 6 s V 0 r V m y y O P J z A K Z y D B 1 d Q h T u o Q Q M Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F p z T j Z z D H / g f P 4 A p F O M 0 g = = &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n E E U 1 q s 0 e x h Z V 8 q G A j j a V + E x 4 t w = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 E 0 c I i o I V l B P M B y R H 2 N n P J k r 2 9 c 3 d P O E L + h I 2 F I r b + H T v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E i u D a u + + 0 U V l b X 1 j e K m 6 W t 7 Z 3 d v f L + Q V P H q W L Y Y L G I V T u g G g W X 2 D D c C G w n C m k U C G w F o 5 u p 3 3 p C p X k s H 0 y W o B / R g e Q h Z 9 R Y q d 2 9 R W E o y X r l i l t 1 Z y D L x M t J B X L U e + W v b j 9 m a Y T S M E G 1 7 n h u Y v w x V Y Y z g Z N S N 9 W Y U D a i A + x Y K m m E 2 h / P 7 p 2 Q E 6 v 0 S R g r W 9 K Q m f p 7 Y k w j r b M o s J 0 R N U O 9 6 E 3 F / 7 x O a s I r f 8 x l k h q U b L 4 o T A U x M Z k + T / p c I T M i s 4 Q y x e 2 t h A 2 p o s z Y i E o 2 B G / x 5 W X S P K t 6 5 9 W L + / N K 7 T q P o w h H c A y n 4 M E l 1 O A O 6 t A A B g K e 4 R X e n E f n x X l 3 P u a t B S e f O Y Q / c D 5 / A J f T j 6 s = &lt; / l a t e x i t &gt; (x, y, z) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P E A C d k T y b q a V a h 4 h R S x L E d q q 2 K Y = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h Q i m J V P T g o e D F Y w X 7 I W 0 o m + 2 m X b r Z h N 2 N G E N / h R c P i n j 1 5 3 j z 3 7 h t c 9 D W B w O P 9 2 a Y m e d F n C l t 2 9 9 W b m V 1 b X 0 j v 1 n Y 2 t 7 Z 3 S v u H 7 R U G E t C m y T k o e x 4 W F H O B G 1 q p j n t R J L i w O O 0 7 Y 2 v p 3 7 7 g U r F Q n G n k 4 i 6 A R 4 K 5 j O C t Z H u y 4 8 V l F T Q 0 2 m / W L K r 9 g x o m T g Z K U G G R r / 4 1 R u E J A 6 o 0 I R j p b q O H W k 3 x V I z w u m k 0 I s V j T A Z 4 y H t G i p w Q J W b z g 6 e o B O j D J A f S l N C o 5 n 6 e y L F g V J J 4 J n O A O u R W v S m 4 n 9 e N 9 b + p Z s y E c W a C j J f 5 M c c 6 R B N v 0 c D J i n R P D E E E 8 n M r Y i M s M R E m 4 w K J g R n 8 e V l 0 j q r O r X q + W 2 t V L / K 4 s j D E R x D G R y 4 g D r c Q A O a Q C C A Z 3 i F N 0 t a L 9 a 7 9 T F v z V n Z z C H 8 g f X 5 A + l Z j y o = &lt; / l a t e x i t &gt; min &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N D B / 3 w 3 s i t y f z e H z F r W 9 8 E + e I N M = " &gt; A A A B 6 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y K o g c P B S 8 e K 9 g P a J e S T b N t a J J d k q x Q l v 4 F L x 4 U 8 e o f 8 u a / M d v u Q V s f D D z e m 2 F m X p g I b q z n f a P S 2 v r G 5 l Z 5 u 7 K z u 7 d / U D 0 8 a p s 4 1 Z S 1 a C x i 3 Q 2 J Y Y I r 1 r L c C t Z N N C M y F K w T T u 5 y v / P E t O G x e r T T h A W S j B S P O C U 2 l / q S q 0 G 1 5 t W 9 O f A q 8 Q t S g w L N Q f W r P 4 x p K p m y V B B j e r 6 X 2 C A j 2 n I q 2 K z S T w 1 L C J 2 Q E e s 5 q o h k J s j m t 8 7 w m V O G O I q 1 K 2 X x X P 0 9 k R F p z F S G r l M S O z b L X i 7 + 5 / V S G 9 0 E G V d J a p m i i 0 V R K r C N c f 4 4 H n L N q B V T R w j V 3 N 2 K 6 Z h o Q q 2 L p + J C 8 J d f X i X t i 7 p / W b 9 6 u K w 1 b o s 4 y n A C p 3 A O P l x D A + 6 h C S 2 g M I Z n e I U 3 J N E L e k c f i 9 Y S K m a O 4 Q / Q 5 w 8 Y 2 Y 5 E &lt; / l a t e x i t &gt; max &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n P z I R J h E B j d U N u R Y w 3 9 W 7 Y B r 1 A E = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o g c P B S 8 e K 5 i 2 0 I a y 2 W 7 a p b u b s L s R S + h f 8 O J B E a / + I W / + G z d t D t r 6 Y O D x 3 g w z 8 8 K E M 2 1 c 9 9 s p r a 1 v b G 6 V t y s 7 u 3 v 7 B 9 X D o 7 a O U 0 W o T 2 I e q 2 6 I N e V M U t 8 w w 2 k 3 U R S L k N N O O L n N / c 4 j V Z r F 8 s F M E x o I P J I s Y g S b X O o L / D S o 1 t y 6 O w d a J V 5 B a l C g N a h + 9 Y c x S Q W V h n C s d c 9 z E x N k W B l G O J 1 V + q m m C S Y T P K I 9 S y U W V A f Z / N Y Z O r P K E E W x s i U N m q u / J z I s t J 6 K 0 H Y K b M Z 6 2 c v F / 7 x e a q L r I G M y S Q 2 V Z L E o S j k y M c o f R 0 O m K D F 8 a g k m i t l b E R l j h Y m x 8 V R s C N 7 y y 6 u k f V H 3 G v X L + 0 a t e V P E U Y Y T O I V z 8 O A K m n A H L f C B w B i e 4 R X e H O G 8 O O / O x 6 K 1 5 B Q z x / A H z u c P G 9 m O R g = = &lt; / l a t e x i t &gt; H &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 6 L S h G 6 A U w 4 N c G W / V D r X j c V l G t s = " &gt; A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 4 k o o V F w C Z l A u Y D k i P s b e a S N X t 7 x + 6 e E E J + g Y 2 F I r b + J D v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E i u D a u + + 3 k N j a 3 t n f y u 4 W 9 / Y P D o + L x S U v H q W L Y Z L G I V S e g G g W X 2 D T c C O w k C m k U C G w H 4 / u 5 3 3 5 C p X k s H 8 w k Q T + i Q 8 l D z q i x U q P W L 5 b c s r s A W S d e R k q Q o d 4 v f v U G M U s j l I Y J q n X X c x P j T 6 k y n A m c F X q p x o S y M R 1 i 1 1 J J I 9 T + d H H o j F x Y Z U D C W N m S h i z U 3 x N T G m k 9 i Q L b G V E z 0 q v e X P z P 6 6 Y m v P W n X C a p Q c m W i 8 J U E B O T + d d k w B U y I y a W U K a 4 v Z W w E V W U G Z t N w Y b g r b 6 8 T l p X Z a 9 S v m 5 U S t W 7 L I 4 8 n M E 5 X I I H N 1 C F G t S h C Q w Q n u E V 3 p x H 5 8 V 5 d z 6 W r T k n m z m F P 3 A + f w C e Q 4 z O &lt; / l a t e x i t &gt; (b) Right side view Visualization of notations in different object observation angles: (a) ? in Bird's Eye View, and (b) ? in right-side view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Depth prediction performance w.r.t. SILog (Scale invariant logarithmic error) and sqRel (Relative squared error) metrics on KITTI val set for all the car samples. Different depth ranges are considered in the performance evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Statistics on the KITTI train+val set for car samples, showing the number of samples (left) and cumulative proportions (right) w.r.t. different depths. Most samples are within 40m, while our method achieves significant depth improvements in this range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>11.84 10.06 24.47 17.17 15.40 w/ gt Dim 19.85 14.06 12.02 25.06 18.29 15.85 w/ gt Depth 79.82 70.91 62.41 88.60 82.66 75.41 Error analysis. Similar to the error analysis in</figDesc><table><row><cell>Method</cell><cell>3D Detection Easy Mod. Hard</cell><cell>BEV Easy Mod. Hard</cell></row><row><cell>Baseline</cell><cell>16.42</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>12.26 10.29 24.79 18.89 16.00 93.46 90.23 80.11</figDesc><table><row><cell>Method</cell><cell>Extra data</cell><cell cols="3">3D Detection Easy Mod. Hard</cell><cell cols="3">BEV Easy Mod. Hard</cell><cell cols="3">AOS Easy Mod. Hard</cell><cell>Runtime</cell></row><row><cell>MonoDLE[26]</cell><cell>-</cell><cell cols="10">17.23 -</cell></row><row><cell>GrooMeD-NMS[15]</cell><cell>-</cell><cell cols="2">18.10 12.32</cell><cell>9.65</cell><cell cols="6">26.19 18.27 14.05 90.05 79.93 63.43</cell><cell>-</cell></row><row><cell>DDMP-3D[42]</cell><cell>-</cell><cell cols="2">19.71 12.78</cell><cell>9.80</cell><cell cols="6">28.08 17.89 13.44 90.73 80.20 61.82</cell><cell>-</cell></row><row><cell>Decoupled-3D[5]</cell><cell>Yes</cell><cell>11.08</cell><cell>7.02</cell><cell>5.63</cell><cell cols="6">23.16 14.82 11.25 87.34 67.23 53.84</cell><cell>-</cell></row><row><cell>UR3D[38]</cell><cell>Yes</cell><cell>15.58</cell><cell>8.61</cell><cell>6.00</cell><cell>21.8</cell><cell>12.51</cell><cell>9.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>120ms</cell></row><row><cell>AM3D[25] PatchNet[24] DA-3Ddet[47]</cell><cell>Yes Yes Yes</cell><cell cols="6">16.50 10.74 15.68 11.12 10.17 22.97 16.86 14.97 9.52 25.03 17.32 14.91 16.80 11.50 8.9 ---</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>?400ms ?400ms -</cell></row><row><cell>D4LCN[10]</cell><cell>Yes</cell><cell cols="2">16.65 11.72</cell><cell>9.51</cell><cell cols="6">22.51 16.02 12.55 90.01 82.08 63.98</cell><cell>-</cell></row><row><cell>Kinematic3D[4] CaDDN[31]</cell><cell>Yes Yes</cell><cell cols="9">19.07 12.72 19.17 13.41 11.46 27.94 18.91 17.19 78.28 67.31 59.52 9.17 26.69 17.52 13.10 58.33 45.50 34.81</cell><cell>?120ms -</cell></row><row><cell>GS3D[18] MonoGRNet[30] MonoDIS[39]</cell><cell>No No No</cell><cell>4.47 9.61 10.37</cell><cell>2.90 5.74 7.94</cell><cell>2.47 4.25 6.40</cell><cell cols="3">8.41 18.19 11.17 6.08 17.23 13.19 11.12 4.94 8.73</cell><cell cols="4">85.79 75.63 61.85 ?2000ms ---?60ms ----</cell></row><row><cell>M3D-RPN[3]</cell><cell>No</cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell><cell cols="6">21.02 13.67 10.23 88.38 82.81 67.08</cell><cell>161ms</cell></row><row><cell>MonoPair[9]</cell><cell>No</cell><cell>13.04</cell><cell>9.99</cell><cell>8.65</cell><cell cols="6">19.28 14.83 12.89 91.65 86.11 76.45</cell><cell>57ms</cell></row><row><cell>RTM3D[20]</cell><cell>No</cell><cell cols="2">14.41 10.34</cell><cell>8.77</cell><cell cols="6">19.17 14.20 11.99 91.75 86.73 77.18</cell><cell>55ms</cell></row><row><cell>MoVi-3D[40]</cell><cell>No</cell><cell cols="2">15.19 10.90</cell><cell>9.26</cell><cell cols="3">22.76 17.03 14.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45ms</cell></row><row><cell>RAR-Net[22]</cell><cell>No</cell><cell cols="2">16.37 11.01</cell><cell>9.52</cell><cell cols="6">22.45 15.02 12.93 88.48 83.29 67.54</cell><cell>-</cell></row><row><cell>Our method</cell><cell>No</cell><cell cols="9">18.85 13.81 11.52 25.86 18.99 16.19 94.67 89.44 79.27</cell><cell>50ms</cell></row><row><cell>Improvement</cell><cell>-</cell><cell cols="9">+2.48 +2.80 +2.00 +3.10 +1.96 +1.34 +2.92 +2.71 +2.09</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>20.00 17.50 15.57 34.36 27.91 24.65 MonoGRNet [30] 11.90 7.56 5.76 19.72 12.81 10.15 47.59 32.28 25.50 52.13 35.99 28.72 12.30 10.42 24.12 18.17 15.76 55.38 42.39 37.99 61.06 47.63 41.92 Baseline 16.54 13.37 11.15 23.62 19.19 16.70 53.93 40.97 36.67 58.72 45.48 40.02 Our method 18.45 14.48 12.87 27.15 21.17 18.35 56.59 43.70 39.37 61.96 47.84 43.10</figDesc><table><row><cell>Method</cell><cell>3D Detection IoU?0.7 Easy Mod. Hard</cell><cell>BEV IoU?0.7 Easy Mod. Hard</cell><cell cols="3">3D Detection IoU?0.5 Easy Mod. Hard</cell><cell cols="3">BEV IoU?0.5 Easy Mod. Hard</cell></row><row><cell cols="3">CenterNet [49] 3.21 MonoDIS [39] 0.60 0.66 0.77 3.46 3.31 11.06 7.60 6.37 18.45 12.58 10.66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>M3D-RPN [3]</cell><cell>14.53 11.07 8.65</cell><cell cols="7">20.85 15.62 11.88 48.53 35.94 28.59 53.35 39.60 31.76</cell></row><row><cell>MoVi-3D [40]</cell><cell>14.28 11.13 9.68</cell><cell>22.36 17.87 15.73</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MonoPair [9]</cell><cell>16.28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>13.37 11.15 23.62 19.19 16.70 Enhanced baseline 16.54 13.37 11.15 23.62 19.19 16.70</figDesc><table><row><cell>description</cell><cell cols="2">3D Detection Easy Mod. Hard</cell><cell>BEV Easy Mod. Hard</cell></row><row><cell>Original baseline</cell><cell>12.78 9.83</cell><cell cols="2">8.27 18.32 14.18 12.11</cell></row><row><cell>+ Uncertainty</cell><cell cols="3">15.40 11.10 9.58 22.33 16.53 14.18</cell></row><row><cell>+ Center3d</cell><cell cols="3">16.22 12.88 10.94 22.61 17.89 16.17</cell></row><row><cell>+ Projected box</cell><cell>16.54</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>achieve impressive performance on the KITTI test set by introducing extra data or external networks. Although our method utilizes none of 18.45 14.48 12.87 27.15 21.17 18.35</figDesc><table><row><cell>Method</cell><cell>3D Detection Easy Mod. Hard</cell><cell>BEV Easy Mod. Hard</cell></row><row><cell>Baseline</cell><cell cols="2">16.54 13.37 11.15 23.62 19.19 16.70</cell></row><row><cell>+ 3D-CAT</cell><cell cols="2">15.87 11.80 10.33 21.85 16.90 14.51</cell></row><row><cell>+ Geo-SV1</cell><cell cols="2">17.25 13.38 11.29 24.33 18.57 16.06</cell></row><row><cell>+ Geo-SV2</cell><cell cols="2">17.10 13.22 11.13 25.02 18.62 16.48</cell></row><row><cell>Ours (full model)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>B. Additional Results and Analysis B.1. Additional Results for the Pedestrian/Cyclist Category Monocular 3D object detection results on the KITTI test set for the Pedestrian and Cyclist categories with the evaluation metric of AP40. The IoU threshold is set to 0.5. The bold black/blue color indicates the best/the second best performing method, respectively.</figDesc><table><row><cell>Cat.</cell><cell>Method</cell><cell>Easy</cell><cell cols="2">3D Detection/BEV Mod.</cell><cell>Hard</cell></row><row><cell></cell><cell>OFTNet [34]</cell><cell cols="2">0.63/1.28</cell><cell cols="2">0.36/0.81 0.35/0.51</cell></row><row><cell></cell><cell>SS3D [12]</cell><cell cols="2">2.31/2.48</cell><cell cols="2">1.78/2.09 1.48/1.61</cell></row><row><cell>Ped.</cell><cell>M3D-RPN [3] MoVi-3D [40]</cell><cell cols="2">4.92/5.65 8.99/10.08</cell><cell cols="2">3.48/4.05 2.94/3.29 5.44/6.29 4.57/5.37</cell></row><row><cell></cell><cell cols="5">MonoPair [9] 10.02/10.99 6.68/7.04 5.53/6.29</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">8.00/9.54</cell><cell cols="2">5.63/6.77 4.71/5.83</cell></row><row><cell></cell><cell>OFTNet [34]</cell><cell cols="2">0.14/0.36</cell><cell cols="2">0.06/0.16 0.07/0.15</cell></row><row><cell></cell><cell>SS3D [12]</cell><cell cols="2">2.80/3.45</cell><cell cols="2">1.45/1.89 1.35/1.44</cell></row><row><cell>Cyc.</cell><cell>M3D-RPN [3] MoVi-3D [40]</cell><cell cols="2">0.94/1.25 1.08/1.45</cell><cell cols="2">0.65/0.81 0.47/0.78 0.63/0.91 0.70/0.93</cell></row><row><cell></cell><cell>MonoPair [9]</cell><cell cols="2">3.79/4.76</cell><cell cols="2">2.12/2.87 1.83/2.42</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">4.73/5.93</cell><cell cols="2">2.93/3.87 2.58/3.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>92.72/92.57 84.92/84.38 70.35/69.82 M3D-RPN [3] 89.04/88.38 85.08/82.81 69.26/67.08 Ours 95.11/94.67 90.14/89.44 80.19/79.27 Ped. SS3D [12] 61.58/53.72 45.79/39.60 41.14/35.40 M3D-RPN [3] 56.64/44.33 41.46/31.88 37.31/28.55 Ours 58.49/52.87 44.63/39.76 40.41/35.83 /42.95 35.48/27.79 31.07/24.26 M3D-RPN [3] 61.54/48.11 41.54/31.09 35.23/26.10 Ours 65.42/55.58 50.48/42.05 42.48/35.48</figDesc><table><row><cell>Cat.</cell><cell>Method</cell><cell>Easy</cell><cell>2D Detection/AOS Mod.</cell><cell>Hard</cell></row><row><cell></cell><cell>SS3D [12]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SS3D [12]</cell><cell>52.97</cell><cell></cell><cell></cell></row><row><cell>Cyc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Depth values on the training set (in meter). We show the maximum (max) and minimum (min) depth values of the cars with the same height of 2D bounding boxes h and the same height of 3D bounding boxes, and the difference (diff.) between the maximum and minimum depth values.</figDesc><table><row><cell>h</cell><cell>depth</cell><cell cols="5">The height of 3D bounding boxes avg. 1.49m 1.50m 1.51m 1.52m</cell></row><row><cell></cell><cell cols="3">max 39.51 40.23</cell><cell>40.39</cell><cell>42.23</cell><cell>39.47</cell></row><row><cell>30</cell><cell>min</cell><cell cols="2">37.69 36.53</cell><cell>36.53</cell><cell>37.21</cell><cell>37.25</cell></row><row><cell></cell><cell>diff.</cell><cell>1.82</cell><cell>3.70</cell><cell>3.86</cell><cell>5.02</cell><cell>2.22</cell></row><row><cell></cell><cell cols="3">max 34.04 34.68</cell><cell>35.69</cell><cell>34.12</cell><cell>36.40</cell></row><row><cell>35</cell><cell>min</cell><cell cols="2">32.99 31.72</cell><cell>31.77</cell><cell>32.05</cell><cell>31.75</cell></row><row><cell></cell><cell>diff.</cell><cell>1.05</cell><cell>2.96</cell><cell>3.92</cell><cell>2.07</cell><cell>4.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Depth prediction results on the KITTI val set for all car samples. We show first the baseline and then ours (bold) for each row (i.e. each depth range). 'Num.' denotes the number of car samples on val set, which has in total 11,178 car samples.</figDesc><table><row><cell cols="6">Depth Range Num. SILog? absRel? sqRel? iRMSE?</cell></row><row><cell>0-10m</cell><cell>867</cell><cell>16.49 14.35</cell><cell>8.65 7.75</cell><cell>67.55 38.46</cell><cell>16.53 14.94</cell></row><row><cell>0-20m</cell><cell>4236</cell><cell>12.12 10.48</cell><cell>6.02 5.42</cell><cell>31.42 20.81</cell><cell>9.98 8.77</cell></row><row><cell>0-30m</cell><cell>7379</cell><cell>11.00 10.27</cell><cell>5.70 5.30</cell><cell>25.25 19.73</cell><cell>8.16 7.52</cell></row><row><cell>0-40m</cell><cell>9797</cell><cell>10.49 10.49</cell><cell>5.65 5.36</cell><cell>24.68 21.71</cell><cell>7.23 7.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>15.42 11.32 26.95 21.94 16.82 Ours 22.29 17.38 12.87 31.37 24.82 18.35</figDesc><table><row><cell>Description</cell><cell>15m</cell><cell>3D Detection 30m</cell><cell>all</cell><cell>15m</cell><cell>BEV 30m</cell><cell>all</cell></row><row><cell>Baselne</cell><cell cols="2">18.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>13.37 11.15 23.62 19.19 16.70 Pointwise MLP 17.09 13.12 11.05 23.79 18.20 16.26 Ours 18.79 14.53 12.77 26.48 20.75 18.04</figDesc><table><row><cell>Description</cell><cell>3D Detection Easy Mod. Hard</cell><cell>Easy</cell><cell>BEV Mod.</cell><cell>Hard</cell></row><row><cell>Baseline</cell><cell>16.54</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>Ablation study on KITTI val set for uncertainty and geometric modeling on the moderate setting of cars.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>learning with the image representations to boost the learning of the depth. Extensive experiments demonstrate the effectiveness of the proposed approach, and our results also achieve state-of-the-art performance with a large margin.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via geometric reasoning on keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Murashkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometry-aware learning of maps for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and heightguided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Geometry and uncertainty in deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Guy</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Groomed-nms: Grouped mathematically differentiable nms for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RTM3D: real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforced axial refinement network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving into localization errors for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Categorical depth distribution network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Categorical depth distribution network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised collaborative learning of keyframe detection and visual odometry towards monocular deep slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards generalization across depth for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depthconditioned dynamic message propagation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Moving slam: Fully unsupervised deep learning in non-rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Geometryaware video object detection for static cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via feature domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
