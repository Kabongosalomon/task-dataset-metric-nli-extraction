<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoseFix: Model-agnostic General Human Pose Refinement Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">ASRI Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
							<email>juyong.chang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of EI Kwangwoon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">ASRI Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoseFix: Model-agnostic General Human Pose Refinement Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person pose estimation from a 2D image is an essential technique for human behavior understanding. In this paper, we propose a human pose refinement network that estimates a refined pose from a tuple of an input image and input pose. The pose refinement was performed mainly through an end-to-end trainable multi-stage architecture in previous methods. However, they are highly dependent on pose estimation models and require careful model design. By contrast, we propose a model-agnostic pose refinement method. According to a recent study, state-of-the-art 2D human pose estimation methods have similar error distributions. We use this error statistics as prior information to generate synthetic poses and use the synthesized poses to train our model. In the testing stage, pose estimation results of any other methods can be input to the proposed method. Moreover, the proposed model does not require code or knowledge about other methods, which allows it to be easily used in the post-processing step. We show that the proposed approach achieves better performance than the conventional multi-stage refinement models and consistently improves the performance of various state-of-the-art pose estimation methods on the commonly used benchmark. The code is available in this https URL 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of human pose estimation is to localize semantic keypoints of a human body. It is an essential technique for human behavior understanding and human-computer interaction. Recently, many methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> utilize deep convolutional neural networks (CNNs) and achieved noticeable performance improvement. They are also updating performance limits in annual competitions for 2D human keypoint detection such as MS COCO keypoint detection challenge <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper, we propose a human pose refinement net- work that estimates a refined pose from a tuple of an input image and a pose. Conventionally, the pose refinement has been mainly performed by multi-stage architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. In other words, the initial pose and image features generated in the first stage go through subsequent stages, and each stage outputs a refined pose. These multistage architectures are usually trained in an end-to-end manner. However, the conventional multi-stage architecturebased refinement approach is highly dependent on the pose estimation model and requires careful design for successful refinement. By contrast, in this work, we propose a modelagnostic pose refinement method that does not depend on the pose estimation model. Recent research by Ronchi et al. <ref type="bibr" target="#b23">[24]</ref> gave us a clue on how to design a general model-agnostic pose refiner. They analyzed the results of the MS COCO 2016 keypoint detection challenge winners <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> by using new pose estimation evaluation metrics, i.e., keypoint similarity (KS) and object keypoint similarity (OKS). They taxonomized pose estimation errors into several types such as jitter, inversion, swap, and miss and described how frequently these errors occur and how much they can negatively affect performance. Although the winners <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> used very different approaches, their pose error distributions are very similar, which indicates that common issues exist for more accurate pose estimation.</p><p>Our basic idea is to use this error statistics as prior information to generate synthetic poses and use the synthesized poses to train the proposed pose refinement model (Pose-Fix). To train our model, we generate each type of the errors (i.e., jitter, inversion, swap, and miss) based on the pose error distributions from Ronchi et al. <ref type="bibr" target="#b23">[24]</ref>, and construct diverse and realistic poses. The generated input pose is fed to the PoseFix with the input image, and the PoseFix learns to refine the pose. We design our PoseFix as a single-stage architecture with a coarse-to-fine estimation pipeline. It takes the input pose in a coarse form and estimates the refined pose in a finer form. The coarse input pose enables the proposed model to focus not only on an exact location of the input pose but also around it, allowing our model to fix the error of the input pose. Furthermore, the finer form of the output pose enables the proposed model to localize the location of the pose more exactly compared to existing methods. After training, our PoseFix can be applied to and refine the pose estimation results of any single-or multi-person pose estimation method. <ref type="figure" target="#fig_0">Figure 1</ref> shows such a pose refinement pipeline of the proposed PoseFix.</p><p>Our contributions can be summarized as follows.</p><p>? We show that model-agnostic general pose refinement is possible. The PoseFix is trained independently of the pose estimation model. Instead, it is based on error statistics obtained through empirical analysis.</p><p>? Our PoseFix can take the pose estimation result of any pose detection method as the input. As the PoseFix does not require any code or knowledge about other methods, our model has very high flexibility and accessibility.</p><p>? We design the PoseFix as a coarse-to-fine estimation system. We empirically observed that this coarse-tofine pipeline is crucial for successful pose refinement.</p><p>? Our PoseFix achieves a better result than the conventional multi-stage architecture-based refinement methods. Also, the PoseFix consistently improves the performance of various state-of-the-art pose estimation methods on the commonly used benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Single-person pose estimation. Toshev et al. <ref type="bibr" target="#b27">[28]</ref> directly estimated the Cartesian coordinates of body joints by using a multi-stage deep network and achieved state-ofthe-art performance. Tompson et al. <ref type="bibr" target="#b26">[27]</ref> jointly trained a CNN and a graphical model. The CNN estimated 2D heatmaps for each joint, and they were used as the unary term for the graphical model. Liu et al. <ref type="bibr" target="#b28">[29]</ref> used multistage CNN which progressively enlarges receptive fields and refines the pose estimation result. Newell et al. <ref type="bibr" target="#b20">[21]</ref> proposed a stacked hourglass network which repeats downsampling and upsampling to exploit multi-scale information effectively. Carreria et al. <ref type="bibr" target="#b5">[6]</ref> proposed an iterative error feedback-based human pose estimation system. Chu et al. <ref type="bibr" target="#b7">[8]</ref> enhanced the stacked hourglass network <ref type="bibr" target="#b20">[21]</ref> by integrating it with a multi-context attention mechanism. Ke et al. <ref type="bibr" target="#b15">[16]</ref> proposed a multi-scale structure-aware network which achieved leading position in the publicly available human pose estimation benchmark <ref type="bibr" target="#b2">[3]</ref>.</p><p>Multi-person pose estimation. There are two main approaches in the multi-person pose estimation. The first one, top-down approach, relies on a human detector that predicts bounding boxes of humans. The detected human image is cropped and fed to the pose estimation network. The second one, bottom-up approach, localizes all human body keypoints in an input image and assembles them using proposed clustering algorithms in each work. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> are based on the top-down approach. He et al. <ref type="bibr" target="#b10">[11]</ref> proposed Mask R-CNN that can perform human detection and keypoint localization in a single model. Instead of cropping the detected humans in the input image, it crops human features from a feature map via the differentiable RoIAlign layer. Chen et al. <ref type="bibr" target="#b6">[7]</ref> proposed a cascaded pyramid network (CPN) which consists of two networks. The first one, GlobalNet, is based on deep backbone network and upsampling layers with skip connections. The second one, RefineNet, is built to refine the estimation results from the GlobalNet by focusing on hard keypoints. Xiao et al. <ref type="bibr" target="#b29">[30]</ref> used a simple pose estimation network that consists of a deep backbone network and several upsampling layers. Although it is based on a simple network architecture, it achieved state-of-the-art performance on the commonly used benchmark <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> are based on the bottom-up approach. DeepCut <ref type="bibr" target="#b22">[23]</ref> assigned the detected keypoints to each person in an image by formulating the assignment problem as an integer linear program. DeeperCut <ref type="bibr" target="#b22">[23]</ref> improves the DeepCut <ref type="bibr" target="#b22">[23]</ref> by introducing image-conditioned pair-wise terms. Cao et al. <ref type="bibr" target="#b4">[5]</ref> proposed part affinity fields (PAFs) that directly expose the association between human body keypoints. They assembled the localized keypoints of all persons in the input image by using the estimated PAFs. Newell et al. <ref type="bibr" target="#b19">[20]</ref> introduced a pixel-wise tag value to assign localized keypoints to a certain human. Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> proposed a pose residual network to assign detected keypoints to each person. Their model can jointly handle person detection, keypoint detection, and person segmentation. Human pose refinement. Many methods attempted to refine the estimated keypoint for more accurate performance. Newell et al. <ref type="bibr" target="#b20">[21]</ref>, Bulat and Tzimiropoulos <ref type="bibr" target="#b3">[4]</ref>, Liu et al. <ref type="bibr" target="#b28">[29]</ref>, and Chen et al. <ref type="bibr" target="#b6">[7]</ref> utilized an end-toend trainable multi-stage architecture-based network. Each stage tries to refine the pose estimation results of the previous stage via end-to-end learning. Carreria et al. <ref type="bibr" target="#b5">[6]</ref> iteratively estimated error feedback from a shared weight model. The output error feedback of the previous iteration is transformed into the input pose of the next iteration, which is repeated several times for progressive pose refinement. All of these methods combine pose estimation and refinement into a single model, and each refinement module is dependent on estimation. Therefore, the refinement modules have different structures, and they are not guaranteed to work successfully when they are combined with other estimation methods. On the other hand, our pose refinement method is independent of the estimation, and therefore the results can be consistently improved regardless of the prior pose estimation method.</p><p>Recently, Fieraru et al. <ref type="bibr" target="#b9">[10]</ref> proposed a post-processing network to refine the pose estimation results of other methods, which is conceptually similar to ours. They synthesized pose for training and employed simple network architecture that estimates refined heatmaps and offset vectors for each joint. While their method follows ad-hoc rules to generate input pose, our method is based on actual error statistics obtained through empirical analysis. Also, our network with coarse-to-fine structure achieves a much stronger refinement performance than their simpler one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the proposed model</head><p>The goal of the PoseFix is to refine the input 2D coordinates of the human body keypoints of all persons in an input image. To address this problem, our system is constructed based on the top-down pipeline which processes a tuple of a cropped human image and a given pose estimation result of that human instead of processing an entire image including multiple persons. In the training stage, the input pose is synthesized on the groundtruth pose realistically and diversely. In the testing stage, pose estimation results of any other methods can be the input pose to our system. The overall pipeline of the PoseFix is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Synthesizing poses for training</head><p>To train the PoseFix, we generate synthesized poses using the groundtruth poses. As the PoseFix should cover different pose estimation results from various methods in the testing stage, synthesized poses need to be diverse and realistic. To satisfy these properties, we generate synthesized poses randomly based on the error distributions of real poses as described in <ref type="bibr" target="#b23">[24]</ref>. The distributions include the frequency of each pose error (i.e., jitter, inversion, swap, and miss) according to the joint type, number of visible keypoints, and overlap in the input image. There may also be joints that do not have any error, which should be synthesized very close to the groundtruth to simulate correct esti- mations. Ronchi et al. <ref type="bibr" target="#b23">[24]</ref> called this status good. Considering most of the empirical distributions in <ref type="bibr" target="#b23">[24]</ref>, we compute the probability that each joint will have one of the pose errors or be in the good status.</p><p>The detailed error synthesis procedure on each groundtruth keypoint ? p j of joint j which belongs to a person p is described in below. For more clear description, we define j as a left/right inverted joint from the j, and p as a different person from the p in the input image. Also, d k j is defined as a L2 distance that makes KS with the groundtruth keypoint becomes k for joint j. Note that d k j depends on the type of joint j because the error distribution of each joint has different scale <ref type="bibr" target="#b23">[24]</ref>. For example, eyes require more precise localization than hips to obtain the same KS k. Good. Good status is defined as a very small displacement from the groundtruth keypoint. An offset vector whose angle and length are uniformly sampled from [0, 2?) and [0, d 0.85 j ), respectively, is added to the groundtruth ? p j . The synthesized keypoint position should be closer to the original groundtruth ? p j than ? p j , ? p j , and ? p j . Jitter. Jitter error is defined as a small displacement from the groundtruth keypoint. An offset vector whose angle and length are uniformly sampled from [0, 2?) and [d 0.85 j ,d 0.5 j ), respectively, is added to the groundtruth ? p j . Similar to the good status, the synthesized keypoint position should be closer to the original groundtruth ? p j than ? p j , ? p j , and ? p j . Inversion. Inversion error occurs when a pose estimation model is confused between semantically similar parts that belong to the same instance. We restrict the inversion error to the left/right body part confusion following <ref type="bibr" target="#b23">[24]</ref>. The jitter error is added to ? p j . The synthesized keypoint position should be closer to the ? p j than ? p j , ? p j , and ? p j . Swap. Swap error represents a confusion between the same or similar parts which belong to different persons. The jitter is added to ? p j or ? p j . The closest keypoint from the synthesized keypoint should be ? p j or ? p j , not any of ? p j and ? p j . Miss. Miss error represents a large displacement from the groundtruth keypoint position. An offset vector whose angle and length are uniformly sampled from [0, 2?) and </p><formula xml:id="formula_0">[d 0.5 j ,d 0.1 j ), respectively, is added to one of ? p j , ? p j , ? p j ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Architecture and learning of PoseFix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model design</head><p>We design the PoseFix to directly estimate a refined pose from a tuple of an input image and an input pose as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The input image and the input pose provide contextual and structured information to the PoseFix, respectively, and the PoseFix learns to use these information to fix pose errors in the input pose. Although some errors exist in the input pose, it still provides useful structured information because, as indicated by Ronchi et al. <ref type="bibr" target="#b23">[24]</ref>, most keypoints in the input pose are in good status or have jitter error which represent a small displacement from the groundtruth pose. This rough structured information acts like attention which tells the PoseFix where to focus on at the human body.</p><p>We observed that by learning to fix the pose errors in the input pose, the PoseFix learns where to focus on at the human body as in <ref type="figure" target="#fig_7">Figure 5</ref>. As it shows, although some errors exist in the input pose, the PoseFix initially focuses well on the reliable keypoint locations of the input pose. And then, it successfully localizes correct keypoints without being influenced by the errors of the input pose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Coarse-to-fine estimation</head><p>To make it more robust to errors, we design the proposed PoseFix to operate in a coarse-to-fine manner. We use the terms "coarse" and "fine" by the degree of uncertainty in representing the pose. For example, in representing the position of each joint constituting a pose, a Gaussian blob has a high uncertainty as much as the size of its standard deviation. On the other hand, a one-hot vector has relatively low uncertainty up to the size of a quantized grid. The coordinates of a keypoint has the least amount of uncertainty because it provides the exact information about the location itself. Therefore, in our work, the coarse-to-fine estimation implies that the coarse input pose (P = {P n } N n=1 ) represented by the set of Gaussian blobs is fed to the network, producing the finer pose in the form of the one-hot vector (H = {H n } N n=1 ), and then the finest pose in terms of the keypoint coordinates (C = {C n } N n=1 ) is generated as the final output as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. N denotes the number of keypoints. In this subsection, we describe this coarse-tofine estimation in more detail.</p><p>The input pose is constructed in a coarse form by a single-mode Gaussian heatmap representation as follows:</p><formula xml:id="formula_1">P n (i, j) = exp ? (i ? i n ) 2 + (j ? j n ) 2 2? 2 ,<label>(1)</label></formula><p>where P n and (i n ,j n ) are the input heatmap and 2D coordinates of nth keypoint, respectively, and ? is the standard deviation of the Gaussian peak. The generated input pose is concatenated with the input image and fed into the PoseFix.</p><p>This Gaussian heatmap representation is suitable for subsequent convolutional operations because it is pixel-wise aligned with the input image. Moreover, as the input pose can contain some errors, non-zero values around the center of the blob can be used to encourage the PoseFix to focus not only on the exact location of the input pose, but also around it. From the input Gaussian heatmap in a coarse form, the proposed network generates the heatmap H n and the keypoint coordinates C n for the nth keypoint, sequentially. To make H n a finer form, we supervise it using a one-hot vector. Then, soft-argmax operation <ref type="bibr" target="#b25">[26]</ref> is applied to H n to generate C n in a differentiable manner. Soft-argmax is defined as the element-wise product between the input heatmap and the meshgrid followed by the summation, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. More precisely, the 2D coordinates are calculated from H n as follows:</p><formula xml:id="formula_2">C n = ? ? w i=1 h j=1 iH n (i, j), w i=1 h j=1 jH n (i, j) ? ? T ,<label>(2)</label></formula><p>where w and h are the width and height of H n , respectively. Our network is trained by minimizing the cross-entropybased integral loss <ref type="bibr" target="#b25">[26]</ref>, which is defined as follows:</p><formula xml:id="formula_3">L = L H + L C ,<label>(3)</label></formula><p>where L is the cross-entropy-based integral loss, and two losses L H and L C are described below.</p><p>The L H is a cross-entropy loss which is calculated after applying the softmax function to the output heatmap along the spatial axis. The definition of the L H is as follows:</p><formula xml:id="formula_4">L H = ? 1 N N n=1 i,j H * n (i, j) log H n (i, j),<label>(4)</label></formula><p>where H * n and H n are the groundtruth and estimated heatmaps with softmax applied, respectively.</p><p>The groundtruth heatmap H * n is a one-hot vector if the groundtruth keypoint coordinates are integers. Otherwise, two grids for each x and y axis are selected by floor and ceil operations and are filled with probabilities by linear extrapolation. The L C is the sum of all L1 losses applied to the coordinates as follows:</p><formula xml:id="formula_5">L C = 1 N N n=1 C * n ? C n 1 ,<label>(5)</label></formula><p>where C * n is the groundtruth coordinates vector for nth keypoint. The L H forces the PoseFix to select a single grid point in the estimated heatmap, and the L C enables the PoseFix to localize keypoints more precisely because it is calculated in the continuous space which is free from quantization errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Network architecture</head><p>We used network architecture of Xiao et al. <ref type="bibr" target="#b29">[30]</ref> which consists of a deep backbone network (i.e., ResNet <ref type="bibr" target="#b12">[13]</ref>) and several upsampling layers. The final upsampling layer becomes heatmaps (H) after applying the softmax function. The soft-argmax operation extracts coordinates (C) from the heatmaps (H), and it becomes the final estimation of the PoseFix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementation details</head><p>Training. The proposed PoseFix is trained in an end-toend manner. The weights of the backbone part are initialized with the publicly released ResNet model pre-trained on the ImageNet dataset <ref type="bibr" target="#b24">[25]</ref>, and the weights of the remaining part are initialized from the zero-mean Gaussian distribution with ? = 0.01 and as in He et al. <ref type="bibr" target="#b11">[12]</ref>. The weights are updated by Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with a mini-batch size of 128. The initial learning rate is set to 5?10 ?4 and reduced by a factor of 10 at 90 and 120th epoch. We perform data augmentation including scaling (?30%), rotation (?40 ? ), and flip. To crop humans from an input image, groundtruth human bounding boxes are extended to a fixed aspect ratio (i.e., height:width = 4:3) and then cropped without distorting the aspect ratio. The cropped bounding box is resized to a fixed size, which becomes the input image. We train the PoseFix 140 epochs with four NVIDIA 1080 Ti GPUs, which took two days.</p><p>Testing. In the testing stage, the pose estimation result of other pose estimation methods becomes the input pose. To crop human bounding box from an image with multiple persons, we calculate bounding box coordinates from the keypoints coordinates of the input pose. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>, we used testing time flip augmentation.</p><p>Our model is implemented using TensorFlow <ref type="bibr" target="#b0">[1]</ref> deep learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Dataset and evaluation metric</head><p>The proposed PoseFix is trained and tested on the MS COCO <ref type="bibr" target="#b18">[19]</ref> 2017 keypoint detection dataset, which consists of training, validation, and test-dev sets. The training set includes 57K images and 150K person instances. The validation set and the test-dev sets include 5K and 20K images, respectively. The OKS-based AP metric is used to evaluate the accuracy of the keypoint localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Ablation study</head><p>To validate each component of the PoseFix, we tested the PoseFix on the validation set. The backbone of all the models are ResNet-50, and the size of the input image is  <ref type="table">Table 1</ref>: AP comparison between the conventional endto-end trainable multi-stage refinement model (E2E-refine) and the proposed model-agnostic refinement model (MArefine) on the validation set. The number in the parenthesis denotes the AP change from the input pose (i.e., CPN). set to 256?192. We used the CPN <ref type="bibr" target="#b6">[7]</ref> which is a state-ofthe-art human pose estimation method to generate the input poses.</p><p>Model-agnostic pose refinement. We compared the accuracy of the conventional end-to-end trainable multi-stage architecture-based pose refinement model (E2E-refine) and the proposed model-agnostic refinement model (MA-refine) in <ref type="table">Table 1</ref>. To train the E2E-refine, we added a refinement module which has the same network architecture as the PoseFix at the end part of the pre-trained CPN. And then, we fine-tuned it by additionally giving the cross-entropybased integral loss to the added module in an end-to-end manner. Both the input image and the output pose of the CPN are fed into the refinement module similarly to the PoseFix. We used a pre-trained CPN instead of training it from scratch because fine-tuning the pre-trained model yielded better performance.</p><p>As <ref type="table">Table 1</ref> shows, the MA-refine trained in a modelagnostic manner improves the accuracy greatly more than the conventional refinement model does. We believe that this is because the added refinement module can be easily overfitted to the output pose of the CPN when training the E2E-refine. In contrast, various input poses that are realisti-  cally synthesized in the training stage of the PoseFix lead to the effect of data augmentation, which makes the PoseFix more robust to unseen input poses in the testing stage. Instead of using the same network architecture of the PoseFix like in the E2E-refine, one can design their own refinement module. However, this approach requires careful network design because the amount of GPU memory available at one time is limited. By contrast, since the Pose-Fix is a decoupled model in both of the training and testing stages, it can serve as an add-on module, and thus provides more flexibility when building pose estimation models.</p><p>This analysis clearly demonstrates the benefits of using the model-agnostic pose refinement model compared with the conventional end-to-end trainable multi-stage architecture-based ones.</p><p>Coarse-to-fine estimation. To demonstrate the validity of the coarse-to-fine estimation, we compared the performance of fine-to-fine (i.e., F2F), coarse-to-fine (i.e., C2F, ours), and coarse-to-coarse (i.e., C2C) estimation pipelines in <ref type="figure" target="#fig_8">Figure 6</ref>. As described in Section 5.2, the Gaussian heatmap and one-hot vector are used as coarse and fine forms of the input pose, respectively. To estimate the refined pose in a coarse form, the model learns to estimate the Gaussian heatmap by minimizing mean square error following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. For the fine-form estimation, cross-entropybased integral loss is used as a loss function like ours. As <ref type="figure" target="#fig_8">Figure 6</ref> shows, the C2F (i.e., ours) exhibits a more accurate performance than F2F, which indicates that coarse input pose representation is more beneficial than fine input pose representation. Also, C2C fails to improve the input pose whereas F2F and C2F successfully refine the input pose. These results indicate that the fine-form estimation is crucial for a successful refinement.</p><p>To further analyze the benefit of the fine-form estimation, we additionally trained two models (C2F-L H and C2F-L C ). Instead of using both of the L C and L H like the C2F does, they are trained by minimizing only either L H or L C . The C2F-L H learns to estimate one-hot vector (H) by minimizing L H , and C2F-L C is supervised to estimate coordinate (C) by minimizing L C . Among C2C, C2F-L H , and C2F-L C , the target form of the C2C is the most coarse representation. On the other hand, that of C2F-L C is the finest representation as described in Section 5.2. <ref type="figure" target="#fig_8">Figure 6</ref> shows that C2C yields the worst performance while C2F-L C achieves the best among them. This finding shows that as the output representation of the PoseFix becomes a finer form, the performance improves. Thus, by integrating the two loss functions (i.e., L H and L C ) together, we can improve the performance much, as C2F shows.</p><p>This analysis clearly shows the benefit of the coarse-tofine estimation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Performance improvement of the state-of-theart methods by PoseFix</head><p>We report the performance improvement when the Pose-Fix is applied to the recent state-of-the-art human pose estimation methods. PAFs <ref type="bibr" target="#b4">[5]</ref>, AE <ref type="bibr" target="#b19">[20]</ref>, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>, CPN <ref type="bibr" target="#b6">[7]</ref>, and Simple <ref type="bibr" target="#b29">[30]</ref> are used to generate the input pose. To obtain the pose estimation results of the previous methods, we used their released codes and pre-trained models. We tested them by ourselves without ensembling and testing time augmentation. We also trained a pose estimation model (IntegralPose) with the same network architecture and loss function with the PoseFix to show that the PoseFix can improve a model trained from the same architecture. To analyze how the PoseFix changes the OKS and frequency of each error type, we tested the PoseFix on the validation set. We also report how much the PoseFix improves AP on the test-dev set. The ResNet-152 is used as  the backbone of the PoseFix, and the size of the input image is set to 384?288. OKS change. The graph in <ref type="figure" target="#fig_9">Figure 7</ref> shows the change of the OKS of the same instance when the PoseFix is applied to the baseline state-of-the-art methods.</p><p>Error frequency change. <ref type="figure" target="#fig_10">Figure 8</ref> shows how the frequency of each status or error type changes when the Pose-Fix is applied to the CPN.</p><p>AP improvement. <ref type="table" target="#tab_2">Table 2</ref> shows the improvements in AP when the PoseFix is applied to the recent state-of-theart human pose estimation methods. We also included the results of using different backbone networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref> for the Mask R-CNN, CPN, and Simple.</p><p>As <ref type="figure" target="#fig_9">Figures 7, 8</ref> and <ref type="table" target="#tab_2">Table 2</ref> show, the PoseFix consistently improves the performance of the state-of-the-art methods. The PoseFix corrects not only the small displacement error (i.e., jitter), but also the large displacement errors (i.e., inversion, miss, and swap) as in <ref type="figure" target="#fig_10">Figure 8</ref>. Taking into account the fact that the state-of-the-art methods used in the experiments vary in structure and learning strategies, we believe that our model has generalizability that can be applied to other pose estimation methods. It is also noticeable that the PoseFix does not require any code or knowledge of the pose estimation methods, which makes our model very easy and convenient to use in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We proposed a novel and powerful network, PoseFix, for human pose refinement. Unlike conventional end-toend multi-stage architecture models, the proposed PoseFix is a model-agnostic pose refinement network. To train the PoseFix, we generate the input pose by synthesizing pose errors according to empirical pose error distributions on the groundtruth pose. The PoseFix takes an input pose in a coarse form and estimates the refined pose in a finer form. Since PoseFix is model-agnostic, it does not require any code or knowledge about the target models. So, it can be used as a post-processing add-on module conveniently. We showed that the PoseFix achieves better performance than the conventional multi-stage architecture-based pose refinement module. Furthermore, the PoseFix consistently improves the accuracy of other methods on the commonly used pose estimation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Comparison with conventional end-to-end trainable multi-stage refinement</head><p>In <ref type="table">Table 1</ref> of the main manuscript, we compared the accuracy of the conventional end-to-end trainable multi-stage refinement model (E2E-refine) and the proposed modelagnostic refinement model (MA-refine). We tried to show the effectiveness of the proposed model-agnostic refinement model by making the number of parameters of the E2Erefine and MA-refine same.</p><p>However, as the conventional refinement requires careful model design, simply adding a refinement module which has the same network architecture with the PoseFix can result in sub-optimal performance. Therefore, we compare the accuracy of the refinement module of the state-of-the-art refinement-based method (i.e., CPN <ref type="bibr" target="#b6">[7]</ref>) and the PoseFix. The CPN consists of two parts. The first one, GlobalNet, is the baseline of the CPN. The second one, RefineNet, refines the pose estimation results of the GlobalNet. We use the GlobalNet as the pose estimation model and compare the accuracy improvement of the RefineNet and PoseFix. We trained and tested the CPN with GlobalNet only and both of the GlobalNet and RefineNet, using their released code. <ref type="table">Table 3</ref> shows our PoseFix improves AP more than stateof-the-art refinement module (i.e., RefineNet) by a large margin. This comparison demonstrates the benefit of the model-agnostic refinement over conventional end-to-end trainable multi-stage refinement more clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performance improvement of the state-ofthe-art methods by PoseFix</head><p>In <ref type="figure" target="#fig_10">Figure 8</ref> of the main manuscript, we showed how the frequency of each error type changes when the PoseFix is applied to the state-of-the-art method (i.e., CPN <ref type="bibr" target="#b6">[7]</ref>). We additionally show the changes of the AE <ref type="bibr" target="#b19">[20]</ref> and Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> in <ref type="figure" target="#fig_12">Figure 9</ref>  We demonstrate more generalizability by showing performance improvement on another 2D multi-person pose estimation dataset (i.e., PoseTrack 2018 <ref type="bibr" target="#b1">[2]</ref>). The Pose-Track 2018 dataset includes 66K frames, and they are split into training, validation and testing set. The state-of-theart human pose estimation method, Simple <ref type="bibr" target="#b29">[30]</ref>, is re-  <ref type="table">Table 3</ref>: AP comparison between state-of-the-art conventional end-to-end trainable multi-stage refinement model (RefineNet <ref type="bibr" target="#b6">[7]</ref>) and the proposed model-agnostic refinement model (PoseFix) on the MS COCO <ref type="bibr" target="#b18">[19]</ref> validation set. The number in the parenthesis denotes the AP change from the input pose (i.e., GlobalNet of the CPN <ref type="bibr" target="#b6">[7]</ref>).  implemented by ours 2 and its testing result is used as the input pose of the PoseFix. Both of the Simple <ref type="bibr" target="#b29">[30]</ref> and Pose-Fix is pre-trained on the COCO dataset and trained again on the PoseTrack 2018 training set without hyperparameter changes following <ref type="bibr" target="#b29">[30]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="table" target="#tab_6">Table 5</ref>   <ref type="table" target="#tab_5">Table 4</ref>: Comparison of APs with the state-of-the-art methods on the test-dev set. "*" means that the method involves extra data for training. "++" indicates results using ensemble. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Comparison with state-of-the-art methods</head><p>We compare the performance of the PoseFix with stateof-the-art methods, which include PAFs <ref type="bibr" target="#b4">[5]</ref>, G-RMI <ref type="bibr" target="#b21">[22]</ref>, AE <ref type="bibr" target="#b19">[20]</ref>, RMPE <ref type="bibr" target="#b8">[9]</ref>, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>, CFN <ref type="bibr" target="#b13">[14]</ref>, CPN <ref type="bibr" target="#b6">[7]</ref>, Integral <ref type="bibr" target="#b25">[26]</ref>, MultiPoseNet <ref type="bibr" target="#b17">[18]</ref>, and Simple <ref type="bibr" target="#b29">[30]</ref> on the MS COCO <ref type="bibr" target="#b18">[19]</ref> test-dev set. All the performance are from their papers. We used Simple <ref type="bibr" target="#b29">[30]</ref> as the input pose of the PoseFix. As they did not release the human detection model and result, we used our human detection model which achieves 57.2 AP for the human category on the testdev set. The Simple <ref type="bibr" target="#b29">[30]</ref> with our human detection model outputs slightly worse performance (73.3 AP) than the original Simple <ref type="bibr" target="#b29">[30]</ref> (73.7 AP).</p><p>As shown in   <ref type="table">Table 6</ref>: AP comparison between PoseRefiner <ref type="bibr" target="#b9">[10]</ref> and PoseFix on the PoseTrack 2018 validation set. The number in the parenthesis denotes the AP change from the input pose (i.e., Simple).</p><p>ing methods. It is noticeable that our method can achieve better performance when a new state-of-the-art method is proposed by using it as the input pose of our method. We also compare the performance of the PoseFix with PoseRefiner <ref type="bibr" target="#b9">[10]</ref> which has a similar approach to ours in <ref type="table">Table 6</ref>. <ref type="table">Table shows</ref> our PoseFix improve input pose significantly more than PoseRefiner <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Qualitative results</head><p>We show some qualitative results on the MS COCO <ref type="bibr" target="#b18">[19]</ref> test-dev set. <ref type="figure" target="#fig_0">Figure 12 and 13</ref> show the input images, input poses, and refined poses when the PoseFix is applied to Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_0">Figure 14</ref> shows final results when the PoseFix is applied to Simple <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input pose</head><p>Refined pose Image Input pose Refined pose  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>https://github.com/mks0601/PoseFix_RELEASE Testing pipeline of the PoseFix. It takes pose estimation results of any other method with an input image and outputs a refined pose. Note that the PoseFix does not require any code or knowledge about other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall pipeline of the PoseFix. In the training stage, the input pose is generated by synthesizing the pose errors based on the real pose error distributions on the groundtruth pose. In the testing stage, pose estimation results of any other methods become the input pose. The heatmaps are visualized by performing max pooling along the channel axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of synthesized pose errors for each type. The keypoint with pose error is highlighted by a yellow rectangle, and the groundtruth keypoints are drawn in a yellow circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 3 visualizes examples of synthesized pose errors of each type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the groundtruths and synthesized input poses. The synthesized poses are generated by adding errors to the groundtruth poses, which are used for training PoseFix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and ? p j . The synthesized keypoint position should be at least d 0.5 j away from all of ? p j , ? p j , ? p j , and ? p j . Some examples of synthesized input poses are shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of feature maps and final heatmaps of the PoseFix. The feature maps and heatmaps are reduced into one channel by max pooling along the channel axis for visualization. The order of the feature maps and heatmaps in the figure is the same with that of the feedforward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F2FFigure 6 :</head><label>6</label><figDesc>mAP comparison of various pipelines. The mAP is calculated on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>OKS change when the PoseFix is applied to stateof-the-art methods. The dotted line denotes identity function. OKS is calculated on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Frequency of each error type change when the PoseFix is applied to the CPN. The frequency is calculated on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>and 10, respectively. As the Figures show, our PoseFix improves the performance by fixing all types of pose errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Frequency of each error type change when the PoseFix is applied to the AE. The frequency is calculated on the MS COCO<ref type="bibr" target="#b18">[19]</ref> validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Frequency of each error type change when the PoseFix is applied to the Mask R-CNN. The frequency is calculated on the MS COCO [19] validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Frequency of each error type change when the PoseFix is applied to the Simple. The frequency is calculated on the PoseTrack 2018 [2] validation set. mance of the input pose. They show the proposed PoseFix can improve the input pose on variable datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Qualitative results of the PoseFix on the test-dev set. Qualitative results of the PoseFix on the test-dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Qualitative results of the PoseFix on the test-dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodsAP AP .50 AP .75 AP M AP L</figDesc><table><row><cell>E2E-refine</cell><cell>70.1 (+0.4)</cell><cell>87.3 (-1.0)</cell><cell>76.8 (-0.2)</cell><cell>66.8 (+0.6)</cell><cell>76.3 (+0.2)</cell></row><row><cell>MA-refine (Ours)</cell><cell>72.1 (+2.4)</cell><cell>88.5 (+0.2)</cell><cell>78.3 (+1.3)</cell><cell>68.6 (+2.4)</cell><cell>78.2 (+2.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodsAPAP .50 AP .75 AP M AP L AR AR .50 AR .75 AR M AR L</figDesc><table><row><cell>AE [20]</cell><cell>56.6</cell><cell>81.7</cell><cell>62.1</cell><cell>48.1</cell><cell>69.4</cell><cell>62.5</cell><cell>84.9</cell><cell>67.2</cell><cell>52.2</cell><cell>76.5</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>63.9</cell><cell>83.6</cell><cell>70.0</cell><cell>56.9</cell><cell>73.7</cell><cell>69.1</cell><cell>86.6</cell><cell>74.2</cell><cell>61.1</cell><cell>79.9</cell></row><row><cell>PAFs [5]</cell><cell>61.7</cell><cell>84.9</cell><cell>67.4</cell><cell>57.1</cell><cell>68.1</cell><cell>66.5</cell><cell>87.2</cell><cell>71.7</cell><cell>60.5</cell><cell>74.6</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>66.7</cell><cell>85.7</cell><cell>72.9</cell><cell>62.9</cell><cell>72.3</cell><cell>71.3</cell><cell>88.0</cell><cell>76.7</cell><cell>66.3</cell><cell>78.1</cell></row><row><cell>Mask R-CNN (ResNet-50) [11]</cell><cell>62.9</cell><cell>87.1</cell><cell>68.9</cell><cell>57.6</cell><cell>71.3</cell><cell>69.7</cell><cell>91.3</cell><cell>75.1</cell><cell>63.9</cell><cell>77.6</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>67.2</cell><cell>88.0</cell><cell>73.5</cell><cell>62.5</cell><cell>75.1</cell><cell>74.0</cell><cell>92.2</cell><cell>79.6</cell><cell>68.8</cell><cell>81.1</cell></row><row><cell>Mask R-CNN (ResNet-101)</cell><cell>63.4</cell><cell>87.5</cell><cell>69.4</cell><cell>57.8</cell><cell>72.0</cell><cell>70.2</cell><cell>91.8</cell><cell>75.6</cell><cell>64.3</cell><cell>78.2</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>67.5</cell><cell>88.4</cell><cell>73.8</cell><cell>62.6</cell><cell>75.5</cell><cell>74.3</cell><cell>92.6</cell><cell>79.9</cell><cell>69.1</cell><cell>81.4</cell></row><row><cell cols="2">Mask R-CNN (ResNeXt-101-64) 64.9</cell><cell>88.6</cell><cell>71.0</cell><cell>59.6</cell><cell>73.3</cell><cell>71.4</cell><cell>92.4</cell><cell>76.8</cell><cell>65.9</cell><cell>78.9</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>68.7</cell><cell>89.3</cell><cell>75.2</cell><cell>64.1</cell><cell>76.4</cell><cell>75.2</cell><cell>93.1</cell><cell>80.9</cell><cell>70.3</cell><cell>81.9</cell></row><row><cell cols="2">Mask R-CNN (ResNeXt-101-32) 64.9</cell><cell>88.4</cell><cell>70.9</cell><cell>59.5</cell><cell>73.2</cell><cell>71.3</cell><cell>92.2</cell><cell>76.7</cell><cell>65.8</cell><cell>78.9</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>68.5</cell><cell>88.9</cell><cell>75.0</cell><cell>64.0</cell><cell>76.2</cell><cell>75.0</cell><cell>92.9</cell><cell>80.7</cell><cell>70.1</cell><cell>81.8</cell></row><row><cell>IntegralPose</cell><cell>66.3</cell><cell>87.6</cell><cell>72.9</cell><cell>62.7</cell><cell>72.7</cell><cell>73.2</cell><cell>91.8</cell><cell>79.1</cell><cell>68.3</cell><cell>79.8</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>69.5</cell><cell>88.3</cell><cell>75.9</cell><cell>65.7</cell><cell>76.1</cell><cell>75.9</cell><cell>92.4</cell><cell>81.8</cell><cell>71.1</cell><cell>82.5</cell></row><row><cell>CPN (ResNet-50) [7]</cell><cell>68.6</cell><cell>89.6</cell><cell>76.7</cell><cell>65.3</cell><cell>74.6</cell><cell>75.6</cell><cell>93.7</cell><cell>82.6</cell><cell>70.8</cell><cell>82.0</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>71.8</cell><cell>89.8</cell><cell>78.9</cell><cell>68.3</cell><cell>78.1</cell><cell>78.2</cell><cell>93.9</cell><cell>84.3</cell><cell>73.5</cell><cell>84.6</cell></row><row><cell>CPN (ResNet-101)</cell><cell>69.6</cell><cell>89.9</cell><cell>77.6</cell><cell>66.3</cell><cell>75.6</cell><cell>76.6</cell><cell>93.9</cell><cell>83.5</cell><cell>72.0</cell><cell>82.9</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>72.6</cell><cell>90.2</cell><cell>79.7</cell><cell>69.0</cell><cell>78.9</cell><cell>78.9</cell><cell>94.1</cell><cell>85.0</cell><cell>74.2</cell><cell>85.1</cell></row><row><cell>Simple (ResNet-50) [30]</cell><cell>69.4</cell><cell>90.1</cell><cell>77.4</cell><cell>66.2</cell><cell>75.5</cell><cell>75.1</cell><cell>93.9</cell><cell>82.4</cell><cell>70.8</cell><cell>81.0</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>72.5</cell><cell>90.5</cell><cell>79.6</cell><cell>68.9</cell><cell>79.0</cell><cell>78.0</cell><cell>94.1</cell><cell>84.4</cell><cell>73.4</cell><cell>84.1</cell></row><row><cell>Simple (ResNet-101)</cell><cell>70.5</cell><cell>90.7</cell><cell>78.8</cell><cell>67.5</cell><cell>76.3</cell><cell>76.2</cell><cell>94.3</cell><cell>83.7</cell><cell>72.1</cell><cell>81.9</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>73.3</cell><cell>90.8</cell><cell>80.7</cell><cell>69.8</cell><cell>79.8</cell><cell>78.7</cell><cell>94.4</cell><cell>85.3</cell><cell>74.3</cell><cell>84.8</cell></row><row><cell>Simple (ResNet-152)</cell><cell>71.1</cell><cell>90.7</cell><cell>79.4</cell><cell>68.0</cell><cell>76.9</cell><cell>76.8</cell><cell>94.4</cell><cell>84.3</cell><cell>72.6</cell><cell>82.4</cell></row><row><cell>+ PoseFix (Ours)</cell><cell>73.6</cell><cell>90.8</cell><cell>81.0</cell><cell>70.3</cell><cell>79.8</cell><cell>79.0</cell><cell>94.4</cell><cell>85.7</cell><cell>74.8</cell><cell>84.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Improvement of APs when the PoseFix is applied to the state-of-the-art methods. The APs are calculated on the test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodsAP AP .50 AP .75 AP M AP L</figDesc><table><row><cell>RefineNet [7]</cell><cell>69.1 (+1.8)</cell><cell>87.9 (+0.4)</cell><cell>76.6 (+2.2)</cell><cell>65.7 (+1.6)</cell><cell>75.5 (+2.2)</cell></row><row><cell>PoseFix (Ours)</cell><cell>71.5 (+4.2)</cell><cell>88.0 (+0.5)</cell><cell>77.6 (+3.2)</cell><cell>68.0 (+3.9)</cell><cell>78.1 (+4.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>show performance improvement on the PoseTrack 2018 validation set. As they show, the PoseFix significantly improves the perfor-Methods AP AP .50 AP .75 AP M AP L AR AR .50 AR .75 AR M AR L</figDesc><table><row><cell>RMPE [9]</cell><cell>61.0</cell><cell>82.9</cell><cell>68.8</cell><cell>57.9</cell><cell>66.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAFs [5]</cell><cell>61.8</cell><cell>84.9</cell><cell>67.5</cell><cell>57.1</cell><cell>68.2</cell><cell>66.5</cell><cell>87.2</cell><cell>71.8</cell><cell>60.6</cell><cell>74.6</cell></row><row><cell cols="2">Mask R-CNN [11] 63.1</cell><cell>87.3</cell><cell>68.7</cell><cell>57.8</cell><cell>71.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AE [20]</cell><cell>65.5</cell><cell>86.8</cell><cell>72.3</cell><cell>60.6</cell><cell>72.6</cell><cell>70.2</cell><cell>89.5</cell><cell>76.0</cell><cell>64.6</cell><cell>78.1</cell></row><row><cell>Integral [26]</cell><cell>67.8</cell><cell>88.2</cell><cell>74.8</cell><cell>63.9</cell><cell>74.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>G-RMI [22]</cell><cell>64.9</cell><cell>85.5</cell><cell>71.3</cell><cell>62.3</cell><cell>70.0</cell><cell>69.7</cell><cell>88.7</cell><cell>75.5</cell><cell>64.4</cell><cell>77.1</cell></row><row><cell>G-RMI* [22]</cell><cell>68.5</cell><cell>87.1</cell><cell>75.5</cell><cell>65.8</cell><cell>73.3</cell><cell>73.3</cell><cell>90.1</cell><cell>79.5</cell><cell>68.1</cell><cell>80.4</cell></row><row><cell cols="2">MultiPoseNet [18] 69.6</cell><cell>86.3</cell><cell>76.6</cell><cell>65.0</cell><cell>76.3</cell><cell>73.5</cell><cell>88.1</cell><cell>79.5</cell><cell>68.6</cell><cell>80.3</cell></row><row><cell>CFN [14]</cell><cell>72.6</cell><cell>86.1</cell><cell>69.7</cell><cell>78.3</cell><cell>64.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPN [7]</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>78.5</cell><cell>95.1</cell><cell>85.3</cell><cell>74.2</cell><cell>84.3</cell></row><row><cell>CPN++ [7]</cell><cell>73.0</cell><cell>91.7</cell><cell>80.9</cell><cell>69.5</cell><cell>78.1</cell><cell>79.0</cell><cell>95.1</cell><cell>85.9</cell><cell>74.8</cell><cell>84.7</cell></row><row><cell>Simple [30]</cell><cell>73.7</cell><cell>91.9</cell><cell>81.1</cell><cell>70.3</cell><cell>80.0</cell><cell>79.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Simple [30]</cell><cell>73.3</cell><cell>91.2</cell><cell>80.9</cell><cell>69.8</cell><cell>79.7</cell><cell>78.7</cell><cell>94.8</cell><cell>85.4</cell><cell>74.2</cell><cell>84.8</cell></row><row><cell cols="2">+ PoseFix (Ours) 74.9</cell><cell>91.2</cell><cell>81.9</cell><cell>71.1</cell><cell>81.2</cell><cell>79.9</cell><cell>94.8</cell><cell>86.3</cell><cell>75.5</cell><cell>86.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>, our PoseFix outperforms all exist-Methods Head Shou Elb Wri Hip Knee Ankl Total Simple [30] 74.4 76.9 72.2 65.2 69.2 70.0 62.9 70.4</figDesc><table><row><cell>+ PoseFix (Ours)</cell><cell>79.0 81.6 76.4 69.7 75.2 74.3 67.0 75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Improvement of APs when the PoseFix is applied to the state-of-the-art method. The APs are calculated on the PoseTrack 2018 validation set.</figDesc><table><row><cell cols="9">Methods Head Shou Elb Wri Hip Knee Ankl Total</cell></row><row><cell>PoseRe-</cell><cell>74.0</cell><cell>76.8</cell><cell>72.2</cell><cell>65.4</cell><cell>70.5</cell><cell>69.7</cell><cell>63.7</cell><cell>70.6</cell></row><row><cell>finer [10]</cell><cell>(-0.4)</cell><cell>(-0.1)</cell><cell cols="3">(+0.0) (+0.2) (+1.3)</cell><cell>(-0.3)</cell><cell cols="2">(+0.8) (+0.2)</cell></row><row><cell>PoseFix</cell><cell>79.0</cell><cell>81.6</cell><cell>76.4</cell><cell>69.7</cell><cell>75.2</cell><cell>74.3</cell><cell>67.0</cell><cell>75.0</cell></row><row><cell>(Ours)</cell><cell cols="8">(+4.6) (+4.7) (+4.2) (+4.5) (+6.0) (+4.3) (+4.1) (+4.6)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/mks0601/TF-SimpleHumanPose</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "PoseFix: Model-agnostic General Human Pose</head><p>Refinement Network"</p><p>In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Integral human pose regression. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
