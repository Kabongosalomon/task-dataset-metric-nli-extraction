<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-08">8 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
							<email>hdai@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
							<email>yqsong@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
							<email>haixun.wang@instacart.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Instacart</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-08">8 Jun 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there is an effort to extend finegrained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained entity typing <ref type="bibr" target="#b14">(Ling and Weld, 2012)</ref> has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking <ref type="bibr" target="#b13">(Ling et al., 2015;</ref><ref type="bibr" target="#b18">Onoe and Durrett, 2020)</ref>, relation extraction <ref type="bibr" target="#b10">(Koch et al., 2014)</ref>, coreference resolution <ref type="bibr" target="#b18">(Onoe and Durrett, 2020)</ref>, etc. Recently, ultra-fine entity typing <ref type="bibr" target="#b1">(Choi et al., 2018)</ref> extends the effort to using a richer set of types <ref type="bibr">(e.g., person, actor, company, victim)</ref> to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and time-consuming for humans to annotate samples. As a result, most existing works use weak labels that are automatically generated <ref type="bibr" target="#b14">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b1">Choi et al., 2018;</ref><ref type="bibr" target="#b11">Lee et al., 2020)</ref>.</p><p>There are two main approaches to obtaining weakly labeled training examples. One approach is to find the Wikipedia pages that correspond to entity mentions, which can be done by using hyperlinks to Wikipedia or applying entity linking. Then the entity types can be obtained from knowledge bases. The other approach is to directly use the head words of nominal mentions as ultra-fine type labels. For example, if a nominal mention is "a famous actor," then the head word "actor" can be used as its type label.</p><p>Several problems exist when using these weak labels for the ultra-fine typing task. First, in the dataset created by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>, on average there are fewer than two labels (types) for each sample annotated through either entity linking or head word supervision. On the other hand, a human annotated sample has on average 5.4 labels. As a result, models trained from the automatically obtained labels have a low recall. Second, neither of the above approaches can create a large number of training samples for pronoun mentions. Third, it is difficult to obtain types that are highly dependent on the context. For example, in "I met the movie star Leonardo DiCaprio on the plane to L.A.," the type passenger is correct for "Leonardo DiCaprio." However, this type cannot be obtained by linking to knowledge bases.</p><p>In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns <ref type="bibr" target="#b6">(Hearst, 1992;</ref><ref type="bibr" target="#b22">Seitner et al., 2016)</ref> with a masked language model (MLM), such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, to generate weak labels for ultra-fine entity typing. Given a sentence that contains a mention, our approach Input Top Words for <ref type="bibr">[MASK]</ref> In late 2015, <ref type="bibr">[MASK]</ref> such as Leonardo DiCaprio starred in The Revenant. actors, stars, actor, directors, filmmakers At some clinics, they and some other <ref type="bibr">[MASK]</ref> are told the doctors don't know how to deal with AIDS, and to go someplace else. patients, people, doctors, kids, children Finkelstein says he expects the company to "benefit from some of the disruption faced by our competitors and any other <ref type="bibr">[MASK]</ref> ." company, business, companies, group, investors adds a short piece of text that contains a "[MASK]" token into it to construct an input to BERT. Then, the pretrained MLM will predict the hypernyms of the mention as the most probable words for " <ref type="bibr">[MASK]</ref>." These words can then be used as type labels. For example, consider the first example in <ref type="table" target="#tab_0">Table 1</ref>. The original sentence is "In late 2015, Leonardo DiCaprio starred in The Revenant." We construct an input for the BERT MLM by inserting "[MASK] such as" before the mention "Leonardo DiCaprio." With this input, the pretrained BERT MLM predicts "actors," "stars," "actor," "directors," and "filmmakers" as the five most probable words for " <ref type="bibr">[MASK]</ref>." Most of them are correct types for the mention after singularization. This approach can generate labels for different kinds of mentions, including named entity mentions, pronoun mentions, and nominal mentions. Another advantage is that it can produce labels that needs to be inferred from the context. This allows us to generate more context-dependent labels for each mention, such as passenger, patient, etc.</p><p>Then, we propose a method to select from the results obtained through different hypernym extraction patterns to improve the quality of the weak labels. We also use a weighted loss function to make better use of the generated labels for model training. Finally, we adopt a selftraining step to further improve the performance of the model. We evaluate our approach with the dataset created by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>, which to the best of our knowledge, is the only English ultra-fine entity typing dataset currently available. On this dataset, we achieve more than 4% absolute F1 improvement over the previously reported best result. Additionally, we also apply our approach to a traditional fine-grained entity typing dataset: Ontonotes <ref type="bibr" target="#b5">(Gillick et al., 2014)</ref>, where it also yields better performance than the state of the art.</p><p>Our contributions are summarized as follows.</p><p>? We propose a new way to generate weak labels for ultra-fine entity typing.</p><p>? We propose an approach to make use of the newly obtained weak labels to improve entity typing results.</p><p>? We conduct experiments on both an ultra-fine entity typing dataset and a traditional finegrained entity typing dataset to verify the effectiveness of our method.</p><p>Our code is available at https://github.com/HKUST-KnowComp/MLMET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The ultra-fine entity typing task proposed by <ref type="bibr" target="#b1">Choi et al. (2018)</ref> uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task <ref type="bibr" target="#b14">(Ling and Weld, 2012</ref>) that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by <ref type="bibr" target="#b17">(Onoe and Durrett, 2019)</ref> filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by <ref type="bibr" target="#b24">(Xiong et al., 2019)</ref> to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. <ref type="bibr" target="#b16">Onoe et al. (2021)</ref> use box embeddings to capture latent type hierarchies.</p><p>There is also some work on the applications of ultra-fine entity typing: <ref type="bibr" target="#b18">Onoe and Durrett (2020)</ref> apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation. The traditional fine-grained entity typing task <ref type="bibr" target="#b14">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b25">Yosef et al., 2012)</ref> is closely related to ultra-fine entity typing. Automatic annotation <ref type="bibr" target="#b14">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b5">Gillick et al., 2014;</ref> is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance.</p><p>For example, denoising the automatically generated labels <ref type="bibr" target="#b21">(Ren et al., 2016)</ref>, taking advantage of the entity type hierarchies or type inter-dependencies <ref type="bibr" target="#b15">Murty et al., 2018;</ref><ref type="bibr" target="#b12">Lin and Ji, 2019)</ref>, exploiting external resources such as the information of entities provided in knowledge bases <ref type="bibr" target="#b9">(Jin et al., 2019;</ref><ref type="bibr" target="#b2">Dai et al., 2019;</ref><ref type="bibr" target="#b23">Xin et al., 2018)</ref>, etc.</p><p>Our work is also related to recent studies <ref type="bibr" target="#b20">(Petroni et al., 2019;</ref><ref type="bibr" target="#b8">Jiang et al., 2020;</ref><ref type="bibr" target="#b26">Zhang et al., 2020)</ref> that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. <ref type="bibr" target="#b26">(Zhang et al., 2020)</ref> also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our methodology consists of two main steps. First, we obtain weak ultra-fine entity typing labels from a BERT masked language model. Second, we use the generated labels in model training to learn better ultra-fine entity typing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Labels from BERT MLM</head><p>Given a sentence and a mention of interest in the sentence, our goal is to derive the hypernym or the type of the mention using a BERT MLM. To do this, we insert into the sentence a few tokens to create an artificial Hearst pattern <ref type="bibr" target="#b6">(Hearst, 1992)</ref>. One of the inserted tokens is a special "[MASK]" token, which serves as the placeholder of the hypernym of the mention. As the BERT MLM predicts the "[MASK]" token, we derive the hypernyms of the mention.</p><p>Consider the first sentence in <ref type="table" target="#tab_0">Table 1</ref> as an example: "In late 2015, Leonardo DiCaprio starred  <ref type="table">Table 2</ref>: Hypernym extraction patterns. M denotes the hyponym; H denotes the hypernym. The F1 score is evaluated with the development set of the ultra-fine dataset <ref type="bibr" target="#b1">(Choi et al., 2018)</ref> for the labels generated with the corresponding pattern.</p><p>in The Revenant." To find the hypernym or the type of "Leonardo DiCaprio", we insert three tokens to create a "such as" pattern: "In late 2015,</p><p>[MASK] such as Leonardo DiCaprio starred in The Revenant." Applying the BERT MLM on the sentence, we derive hypernyms such as "actors," "stars," "directors," "filmmakers." <ref type="table" target="#tab_0">Table 1</ref> shows a few more examples.</p><p>We consider the 63 Hearst-like patterns <ref type="bibr" target="#b6">(Hearst, 1992)</ref> presented in <ref type="bibr" target="#b22">(Seitner et al., 2016</ref>) that express a hypernym-hypnonym relationship between two terms. <ref type="table">Table 2</ref> lists some of the patterns, wherein H and M denote a hypernym and a hyponym, respectively. For example, "M and some other H" can be used to match "Microsoft and some other companies."</p><p>The general procedure to use these patterns to create input samples for BERT MLM and obtain labels from its predictions is as follows. We first regard the mention as M . Then, we insert the rest of the pattern either before or after the mention, and we replace H with the special "[MASK]" token. After applying the BERT MLM on sentences with artificial Hearst patterns, we derive top k type labels from the prediction for " <ref type="bibr">[MASK]</ref>." To drive these k labels, we first sigularize the most probable words that are plural. Then, remove those that are not in the type vocabulary of the dataset. Finally, use the most probable k different words as k labels. For example, if we want to obtain 3 labels, and the most probable words are "people," "actors," "celebrities," "famous," "actor," etc. Then the 3 labels should be person, actor, celebrity. Because "actor" is the singluar form of "actors," and "famous" is not in the type vocabulary.</p><p>We show the performance of our method for obtaining 10 type labels for each mention with different patterns in <ref type="table">Table 2</ref>. A pre-trained BERT-Base-Cased MLM is used to obtain the results 1 .</p><p>For nominal mentions, directly applying the patterns that starts with "M " with the above procedure may sometimes be problematic. For example, consider the noun phrase "the factory in Thailand" as a mention. If we use the "M and some other H" pattern and insert "and other [MASK]" after the mention, the BERT MLM will predict the type country for Thailand instead of for the entire mention. To avoid such errors, while applying patterns that starts with "M " for nominal mentions, we regard the head word of the mention as M instead.</p><p>A more subtle and challenging problem is that the quality of the type labels derived from different patterns for different mentions can be very different. For example, for the mention "He" in sentence "He has won some very tough elections and he's governor of the largest state," the pattern "H such as M " leads to person, election, time, thing, leader as the top five types. But using the pattern "M and any other H," we get candidate, politician, man, person, governor. On the other hand, for mention "the Al Merreikh Stadium" in "It was only Chaouchi's third cap during that unforgettable night in the Al Merreikh Stadium," the results of using "H such as M " (the top five types are venue, place, facility, location, area) is better than using "M and any other H" (the top five types are venue, stadium, game, match, time).</p><p>To address the above problem, we do not use a same pattern for all the mentions. Instead, for each mention, we try to select the best pattern to apply from a list of patterns. This is achieved by using a baseline ultra-fine entity typing model, BERT-Ultra-Pre, which is trained beforehand without using labels generated with our BERT MLM based approach. Details of BERT-Ultra-Pre can be found in Section 5.2. Denote the pattern list as L. With each pattern in L, we can apply it on the given mention to derive a set of labels from the BERT MLM. Then, we find the set of labels that have the most overlap with the labels predicted by BERT-Ultra-Pre. Finally, the given mention is annotated with this set of labels.</p><p>It is not necessary to use all the patterns in <ref type="bibr" target="#b22">(Seitner et al., 2016)</ref>. To construct L, the list of patterns used for annotation, we perform the following procedure. <ref type="bibr">1</ref> We use the pretrained model provided in the Transformers library. We also tried using BERT-Large and RoBERTa models. However, they do not yield better performance.</p><p>Step 1: Initialize L to contain the best performing pattern (i.e., "M and any other H") only.</p><p>Step 2: From all the patterns not in L, find the one that may bring the greatest improvement in F1 score if it is added to L.</p><p>Step 3: Add the pattern found in Step 2 to the L if the improvement brought by it is larger than a threshold.</p><p>Step 4: Repeat steps 2-3 until no patterns can be added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Type Coverage</head><p>Since we only use one [MASK] token to generate labels, the model cannot produce multi-word types (e.g., football player) or single word types that are not present in the BERT MLM vocabulary. The BERT MLM vocabulary covers about 92% of the labels in the human annotated dataset constructed by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>. Type coverage is a known issue with weak supervision, and is tolerable if the generated labels can be used to achieve our final goal: improving the performance of the ultra-fine entity typing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>Our approach generates type labels for all three types of mentions: named entity mentions, pronoun mentions, and nominal mentions. For named entity mentions and nominal mentions, existing automatic annotation approaches can already provide some labels for them by using the entity types in knowledge bases or using the head words as types <ref type="bibr" target="#b14">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b1">Choi et al., 2018)</ref>. Thus, we combine these labels with the labels generated by us. For pronoun mentions, no other labels are used. Besides the automatically annotated samples, we can also use a small amount of human annotated samples provided by the dataset for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>Our ultra-fine entity typing model follows the BERT-based model in <ref type="bibr" target="#b17">(Onoe and Durrett, 2019)</ref>. Given a sentence that contains an entity mention, we form the sequence "[CLS] sentence [SEP] mention string [SEP]" as the input to BERT. Then, denoting the final hidden vector of the "[CLS]" token as u, we add a linear classification layer on top of u to model the probability of each type:</p><formula xml:id="formula_0">p = ?(W u),<label>(1)</label></formula><p>where ? is the sigmoid function, W is a trainable weight matrix. p ? R d , where d is the number of types used by the dataset. We assign a type t to the mention if p t , its corresponding element in p, is larger than 0.5. If no such types are found, we assign the one with the largest predicted probability to the mention.</p><p>To make use of the automatically labeled samples, some existing approaches mix them with high quality human annotated samples while training models <ref type="bibr" target="#b1">(Choi et al., 2018;</ref><ref type="bibr" target="#b17">Onoe and Durrett, 2019)</ref>. However, we find that better performance can be obtained by pretraining the model on automatically labeled samples, then fine-tuning it on human annotated samples.</p><p>Following <ref type="bibr" target="#b1">(Choi et al., 2018)</ref>, we partition the whole type vocabulary used by the dataset into three non-overlapping sets: general, fine, and ultrafine types, denoted with T g , T f and T u , respectively. Then, we use the following objective for training:</p><formula xml:id="formula_1">J (x) = L(x, T g )?(L, T g ) + L(x, T f )?(L, T f ) + L(x, T u )?(L, T u ),<label>(2)</label></formula><p>where x is a training sample; L denotes the set of type labels assigned to x through either human or automatic annotation. The function ?(L, T ) equals 1 when a type in L is in set T and 0 otherwise. This loss can avoid penalizing some false negative labels. Unlike existing studies, we define the function L differently for human annotated samples and automatically labeled samples. While pretraining with automatically labeled samples, the labels obtained through entity linking and head word supervision are usually of higher precision than those obtained through BERT MLM. Thus, we propose to assign different weights in the training objective to the labels generated with different methods:</p><formula xml:id="formula_2">L(x, T ) = ? t?T ?(t)[y t ? log(p t ) + (1 ? y t ) ? log(1 ? p t )],<label>(3)</label></formula><p>where y t equals to 1 if t is annotated as a type for x and 0 otherwise; p t is the probability of whether t should be assigned to x predicted by the model. The value of ?(t) indicates how confident we are about the label t for x. Specifically, it equals to a predefined constant value larger than 1 when t is a positive type for x obtained through entity linking or head word supervision, otherwise, it equals to 1.</p><p>While fine-tuning with human annotated samples, we directly use the binary cross entropy loss:</p><formula xml:id="formula_3">L(x, T ) = ? t?T [y t ?log(p t )+(1?y t )?log(1?p t )].</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Self-Training</head><p>Denote the ultra-fine entity typing model obtained after pretraining on the automatically labeled data as h, and the model obtained after fine-tuning h with human annotated data as m. A weakness of m is that at the fine-tuning stage, it is trained with only a small number of samples. Thus, we employ self-training to remedy this problem.</p><p>By using m as a teacher model, our self-training step fine-tunes the model h again with a mixture of the samples from the automatically labeled data and the human annotated data. This time, for the automatically annotated samples, we use pseudo labels generated based on the predictions of m instead of their original weak labels. The newly finetuned model should perform better than m, and is used for evaluation.</p><p>Denote the set of human annotated samples as H, the set of automatically labeled samples as A.</p><p>The training objective at this step is</p><formula xml:id="formula_4">J ST = 1 |H| x?H J (x) + ? 1 |A| x?A L ST (x),<label>(5)</label></formula><p>where ? is a hyperparameter that controls the strength of the supervision from the automatically labeled data. While computing loss for the samples in A, we only use the types that are very likely to be positive or negative. For a sample x, let p t be the probability of it belonging to type t predicted by the model m. We consider a type t very likely to be positive if p t is larger than a threshold P , or if t is a weak label of x and p t is larger than a smaller threshold P w . Denote the set of such types as? + (x). We consider a type t very likely to be negative if p t is smaller than 1 ? P . Denote the set of such types as? ? (x). Then we have:</p><formula xml:id="formula_5">L ST (x) = ? t?? + (x) log(p t ) ? t?? ? (x) log(1 ? p t ).<label>(6)</label></formula><p>Thus, we compute the binary cross entropy loss with only the types in? + (x) and? ? (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application to Traditional Fine-grained Entity Typing</head><p>Our approach to generating weak entity type labels with BERT MLM can also be applied to the traditional fine-grained entity typing task. Different from ultra-fine entity typing, traditional finegrained entity typing uses a manually designed entity type ontology to annotate mentions. The types in the ontology are organized in an hierarchical structure. For example, the ontology used by the Ontonotes dataset contains 89 types including /organization, /organization/company, /person, /person/politician, etc. On this dataset, our automatic annotation approach can mainly be helpful to generate better labels for nominal mentions. We still use the same method described in Section 3.1 to create input for BERT MLM based on the given mention. But with traditional finegrained entity typing, most mentions are assigned only one type path (e.g., a company mention will only be assigned labels {/organization, /organization/company}, which includes all the types along the path of /organization/company). Thus, while generating labels, we only use the most probable word predicted by the BERT MLM, which is mapped to the types used by the dataset if possible. For example, the word "company" and its plural form are both mapped to /organization/company. Such a mapping from free-form entity type words to the types used by the dataset can be created manually, which does not require much effort. We mainly construct the mapping with two ways: 1) Check each type used by the dataset, and think of a few words that should belong to it, if possible. For example, for the type /person/artist/author, corresponding words can be "author," "writer," etc. 2) Run the BERT MLM on a large number of inputs constructed with unannotated mentions, then try to map the words that are most frequently predicted as the most probable word to the entity type ontology.</p><p>Since only the most probable word predicted by the BERT MLM is used to produce labels, we also only use one hypernym relation pattern: "M and any other H."</p><p>For traditional fine-grained entity typing, we use our approach to generate labels for mentions that are not previously annotated with other auto-matic annotation approaches. While training, all the automatically labeled mentions are used together. The typing model is the same as the model described in 3.3. The binary cross entropy loss is directly employed as the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on our primary task: ultra-fine entity typing. In addition, we evaluate the performance of our approach when applied to traditional fine-grained entity typing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on Ultrafine</head><p>For ultra-fine entity typing, we use the dataset created by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>. It uses a type set that contains 10,331 types. These types are partitioned into three categories: 9 general types, 121 finegrained types, and 10,201 ultra-fine types. There are 5,994 human annotated samples. They are split into train/dev/test with ratio 1:1:1. It also provides 5.2M samples weakly labeled through entity linking and 20M samples weakly labeled through head word supervision.</p><p>We compare with the following approaches:</p><p>? UFET <ref type="bibr" target="#b1">(Choi et al., 2018)</ref>. This approach obtains the feature vector for classification by using a bi-LSTM, a character level CNN, and pretrained word embeddings.</p><p>? LabelGCN <ref type="bibr" target="#b24">(Xiong et al., 2019)</ref>. LabelGCN uses a graph propagation layer to capture label correlations.</p><p>? LDET <ref type="bibr" target="#b17">(Onoe and Durrett, 2019)</ref>. LDET learns a model that performs relabeling and sample filtering to the automatically labeled samples. Their typing model, which employs ELMo embeddings and a bi-LSTM, is train with the denoised labels.</p><p>? Box <ref type="bibr" target="#b16">(Onoe et al., 2021)</ref>. Box represents entity types with box embeddings to capture latent type hierarchies. Their model is BERTbased.</p><p>We use the BERT-Base-Cased version of BERT for both weak label generation and the typing model in Section 3.3. The hyperparameters are tuned through grid search using F1 on the dev set as criterion. The value of ?(t) in Equation <ref type="formula" target="#formula_2">(3)</ref> is set to 5.0 for positive types obtained through entity linking or head word supervision. ? in Equation (5) is set to 0.01. P and P w in Section 3.4 are  set to 0.9 and 0.7, respectively. Our approach to generate labels through BERT MLM is applied to each weak sample provided in the original dataset.</p><p>In addition, we also use our approach to annotate about 3.7M pronoun mentions, which are extracted through string matching from the English Gigaword corpus <ref type="bibr" target="#b19">(Parker et al., 2011)</ref>. We generate 10 types for each sample 2 . With the procedure described in Sectiton 3.1, three hypernym extraction patterns are used while generating labels with BERT MLM: "M and any other H," "H such as M ," "M and some other H." Specifically, adding "H such as M " and "M and some other H" improves the F1 score from 0.253 to 0.274, and from 0.274 to 0.279, respectively. Adding any more patterns cannot improve the F1 score for more than 0.007. Following existing work <ref type="bibr" target="#b16">(Onoe et al., 2021;</ref><ref type="bibr" target="#b17">Onoe and Durrett, 2019)</ref>, we evaluate the macroaveraged precision, recall, and F1 of different approaches on the manually annotated test set. The results are in <ref type="table" target="#tab_3">Table 3</ref>. Our approach achieves the best F1 score. It obtains more than 4% F1 score improvement over the existing best reported performance by Box in <ref type="bibr" target="#b16">(Onoe et al., 2021)</ref>. This demonstrates the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>For ablation study, we verify the effectiveness of the different techniques used in our full entity typing approach by evaluating the performance of the following variants: Ours (Single Pattern) only uses one pattern: M and any other H; Ours (Unweighted Loss) removes the ?(t) term in Equation (3); Ours (No Self-train) does not perform the self-training step. We also evaluate two baseline approaches: BERT-Ultra-Direct uses the same BERT based model described in Section 3.3, but <ref type="bibr">2</ref> The performance of the trained model is relatively insensitive with respect to the number of labels generated with MLM. The difference between the F1 scores of the models trained using 10 and 15 generated types is less than 0.005.  is trained with only the human annotated training samples; BERT-Ultra-Pre also uses the same BERT based model, but is first pretrained with the existing automatically generated training samples in the dataset provided by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>, then fine-tuned on the human annotated training data. First, the benefit of using the labels generated through BERT MLM can be verified by comparing Ours (No Self-train) and BERT-Ultra-Pre. Because the techniques employed in Ours <ref type="bibr">(No Selftrain)</ref>, including the use of multiple hypernym extraction patterns and the weighted loss, are both for better utilization of our automatic entity type label generation method.</p><p>The effectiveness of the use of multiple hypernym extraction patterns, the weighted loss, and the self-training step can be verified by comparing Ours with Ours (Single Pattern), Ours <ref type="bibr">(Unweighted Loss) and Ours (No Self-train)</ref>, respectively. Among them, self-training is most beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on Different Kinds of Mentions</head><p>It is also interesting to see how our approach performs on different kinds of mentions. <ref type="table" target="#tab_7">Table 5</ref> lists the performance of our full approach and two baseline systems on the three kinds of mentions in the dataset: named entity mention, pronoun mentions, and nominal mentions. Our approach performs much better than BERT-Ultra-Pre on all three kinds of mentions. The improvements in F1 on pronoun and nominal mentions are relatively more substantial.   bels, and the labels predicted by BERT-Ultra-Pre, BERT MLM, and our full approach. In the first example, the label prisoner is a type that depends on the context, and is usually not assigned to humans in knowledge bases. We think that since we can assign such labels to the training samples with our BERT MLM based approach, our model is better at predicting them than the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>The second and third examples demonstrate that our model may not only improve the recall by predicting more correct types, but also reduce incor-rect predictions that do not fit the mention or the context well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation on Ontonotes</head><p>The Ontonotes dataset uses an ontology that contains 89 types to label entity mentions. We use the version provided by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>. It includes 11,165 manually annotated mentions, which are split into a test set that contains 8,963 mentions, and a dev set that contain 2,202 mentions. It also provides about 3.4M automatically labeled mentions.</p><p>Since existing annotations for named entity mentions may be more accurate than the annotations obtained through our approach, we only apply our method to label nominal mentions. Applying the approach in Section 4, we create 1M new automatically labeled mentions with the head word supervision samples (such samples contain mostly nominal mentions) in the ultra-fine dataset. They are used together with the originally provided 3.4M mentions to train the typing model.</p><p>On this dataset, we compare with the following approaches: UFET <ref type="bibr" target="#b1">(Choi et al., 2018)</ref>, LDET <ref type="bibr" target="#b17">(Onoe and Durrett, 2019)</ref>, DSAM <ref type="bibr" target="#b7">(Hu et al., 2020)</ref>, LTRFET (Lin and Ji, 2019), BERT-Direct.</p><p>Where BERT-Direct uses the same BERT based model as our approach, but trains with only the weak samples provided in the dataset. LTRFET adopts a hybrid classification method to exploit type inter-dependency. DSAM is a diversified semantic attention model with both mention-level attention and context-level attention.</p><p>For our approach and BERT-Direct, we still use the pretrained BERT-Base-Cased model for initialization. Although a very large number of weakly labeled mentions are provided, not all of them are needed for training the models. In our experiments, for both our approach and BERT-Direct, the performance does not increase after training on about 0.3M mentions.</p><p>We report strict accuracy, macro-averaged F1,  <ref type="table">Table 7</ref>: Performance of different approaches on Ontonotes. We report strict accuracy, macro-averaged F1, and micro-averaged F1. and micro-averaged F1 <ref type="bibr" target="#b14">(Ling and Weld, 2012)</ref>. The results are in <ref type="table">Table 7</ref>. As we can see, our approach also achieves the best performance on this dataset. Comparing it with BERT-Direct demonstrates the benefit of the samples automatically labeled with BERT MLM. However, less improvement is achieved on OntoNotes than on the ultra-fine entity typing dataset. We think there are two main reasons. First, OntoNotes uses a much smaller entity type set (89 types) than the ultra-fine entity typing dataset (10,331 types). As a result, some finer grained types that can be produced by our approach become less beneficial. Second, generating type labels that are highly dependent on the context (e.g., types like criminal, speaker) is an advantage of our approach, and the ultra-fine entity typing dataset contains more such type labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a new approach to automatically generate ultra-fine entity typing labels. Given a sentence that contains a mention, we insert a hypernym extraction pattern with a "[MASK]" token in it, so that a pretrained BERT MLM may predict hypernyms of the mention for " <ref type="bibr">[MASK]</ref>." Multiple patterns are used to produce better labels for each mention. We also propose to use a weighted loss and perform a self-training step to learn better entity typing models. Experimental results show that our approach greatly outperforms state-of-the-art systems. Additionally, we also apply our approach to traditional finegrained entity typing, and verify its effectiveness with experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of constructed BERT MLM inputs for obtaining weak entity typing labels. Entity mentions are in bold and underlined. The texts highlighted with blue background are not in the original sentences. They are inserted to create inputs for BERT. The right column lists the five most probable words predicted by a pretrained BERT-Base-Cased MLM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Macro-averaged Precision, Recall, and F1 of different approaches on the test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Performance of different variants of our ap-</cell></row><row><cell>proach on the test set. BERT-Ultra-Direct and BERT-</cell></row><row><cell>Ultra-Pre are two baseline approaches that do not use</cell></row><row><cell>labels generated with our BERT MLM based method</cell></row><row><cell>in training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Pre 54.7 50.5 52.5 51.3 46.1 48.6 45.2 33.7 38.6 Ours 58.3 54.4 56.3 57.2 50.0 53.4 49.5 38.9 43.5</figDesc><table><row><cell>presents several ultra-fine entity typing</cell></row><row><cell>examples, along with the human annotated la-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance on named entity mentions, pronoun mentions, and nominal mentions, respectively.</figDesc><table><row><cell></cell><cell cols="4">Captured in 1795, he was con-</cell></row><row><cell></cell><cell cols="4">fined at Dunkirk, escaped, set</cell></row><row><cell></cell><cell cols="4">sail for India, was wrecked on the</cell></row><row><cell></cell><cell cols="4">French coast, and condemned to</cell></row><row><cell>Sentence</cell><cell cols="4">death by the decree of the French Directory.</cell></row><row><cell>Human</cell><cell cols="2">prisoner, person</cell><cell></cell></row><row><cell cols="5">BERT-Ultra-Pre person, soldier, man, criminal</cell></row><row><cell>BERT MLM</cell><cell cols="4">man, prisoner, person, soldier, offi-</cell></row><row><cell></cell><cell>cer</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="4">person, soldier, man, prisoner</cell></row><row><cell></cell><cell>Also</cell><cell>in</cell><cell>the</cell><cell>morning,</cell></row><row><cell></cell><cell cols="3">a roadside bomb</cell><cell>struck</cell><cell>a</cell></row><row><cell></cell><cell cols="4">police patrol on a main road in</cell></row><row><cell></cell><cell cols="4">Baghdad's northern neighbor-</cell></row><row><cell>Sentence</cell><cell cols="4">hood of Waziriya, damaging a police vehicle ...</cell></row><row><cell>Human</cell><cell cols="4">bomb, weapon, object, explosive</cell></row><row><cell cols="5">BERT-Ultra-Pre object, event, attack, bomb</cell></row><row><cell>BERT MLM</cell><cell cols="4">weapon, threat, evidence, device,</cell></row><row><cell></cell><cell>debris</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="3">object, weapon, bomb</cell></row><row><cell></cell><cell cols="4">In October 1917, Sutton was pro-</cell></row><row><cell></cell><cell cols="4">moted (temporarily) to the rank</cell></row><row><cell></cell><cell cols="4">of major and appointed Officer</cell></row><row><cell></cell><cell cols="4">Commanding No.7 Squadron, a</cell></row><row><cell>Sentence</cell><cell cols="4">position he held for the remained</cell></row><row><cell></cell><cell cols="2">of the War.</cell><cell></cell></row><row><cell>Human</cell><cell cols="4">soldier, officer, male, person</cell></row><row><cell cols="5">BERT-Ultra-Pre person, politician, male</cell></row><row><cell>BERT MLM</cell><cell cols="4">officer, pilot, man, unit, aircraft</cell></row><row><cell>Ours</cell><cell cols="4">person, soldier, male, officer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ultra-fine entity typing examples with the corresponding human annotated labels and predictions of three different systems. Entity mentions are in bold and underlined. For BERT MLM, we list the top five labels.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper was supported by the NSFC Grant (No. U20B2053) from China, the Early Career Scheme (ECS, No. 26206717), the General Research Fund (GRF, No. 16211520), and the Research Impact Fund (RIF, from the Research Grants Council (RGC) of Hong Kong, with special thanks to the WeChat-HKUST WHAT Lab on Artificial Intelligence Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving fine-grained entity typing with entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6211" to="6216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting semantic relations for fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Diversified semantic attention model for finegrained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing via hierarchical multi graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiansi</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4970" to="4979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Type-aware distantly supervised relation extraction with linked arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A chinese corpus for fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4451" to="4457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An attentive fine-grained entity typing model with latent type representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6198" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical losses and new resources for fine-grained entity typing and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modeling fine-grained entity types with box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00345</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2407" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable entity representations through large-scale typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="612" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large database of hypernymy relations extracted from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Seitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Meusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving neural fine-grained entity typing with knowledge attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imposing label-relational inductive bias for extremely fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="773" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyena: Hierarchical type classification for entity names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empower entity set expansion via language model probing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8151" to="8160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
