<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEMSEC: Graph Embedding with Self Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-02-12">12 Feb 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Davies</surname></persName>
							<email>ryan.davies@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GEMSEC: Graph Embedding with Self Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-02-12">12 Feb 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ?Computing methodologies ? Cluster analysis</term>
					<term>?Information systems ? Clustering</term>
					<term>KEYWORDS community detection, clustering, graph embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern graph embedding procedures can efficiently extract features of nodes from graphs with millions of nodes. e features are later used as inputs for downstream predictive tasks. In this paper we propose GEMSEC a graph embedding algorithm which learns a clustering of the nodes simultaneously with computing their features. e procedure places nodes in an abstract feature space where the vertex features minimize the negative log likelihood of preserving sampled vertex neighborhoods, while the nodes are clustered into a fixed number of groups in this space. GEMSEC is a general extension of earlier work in the domain as it is an augmentation of the core optimization problem of sequence based graph embedding procedures and is agnostic of the neighborhood sampling strategy. We show that GEMSEC extracts high quality clusters on real world social networks and is competitive with other community detection algorithms. We demonstrate that the clustering constraint has a positive effect on representation quality and also that our procedure learns to embed and cluster graphs jointly in a robust and scalable manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Finding communities in large networks has been studied extensively <ref type="bibr" target="#b7">[8]</ref>. Applications of community detection include the extraction of highly interconnected protein groups from PPI networks <ref type="bibr" target="#b30">[30]</ref>, identification of research collaborators from citation graphs <ref type="bibr" target="#b1">[2]</ref>, selection of people with shared interests from social networks <ref type="bibr" target="#b18">[19]</ref> or classification of products that are purchased together <ref type="bibr" target="#b25">[26]</ref>.</p><p>A community is a set of nodes in the graph which is highly interconnected compared to the graph as a whole. Community detection algorithms use various strategies to find groups of highly interconnected nodes. From a general point of view, community detection is analogous to clustering; one wants to find nodes (data points) which are similar to each other in terms of neighborhoods. Procedures such as the Louvain method greedily optimize a quantity called modularity representing the quality of extracted clusters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. Other algorithms start random processes and find communities based on the diffusion properties. Walktrap extracts closely knit neighborhoods with random walks <ref type="bibr" target="#b19">[20]</ref> while Label Propagation uses labels randomly propagated through the network to identify communities <ref type="bibr" target="#b9">[10]</ref>.</p><p>In a similar manner, sequence based graph embedding methods like DeepWalk and Node2Vec use random processes to generate representations of nodes in the graph <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Graph embedding procedures generally map nodes to a low dimensional abstract space where distances among nodes are approximately preserved. Besides distances, the neighborhood structure is also preserved to a certain extent. is means that nodes with overlapping neighborhoods are positioned close to each other in the low dimensional space. e learned representations are useful for machine learning tasks such as the labeling of the nodes, regression, edge prediction and graph visualization <ref type="bibr" target="#b8">[9]</ref>. Our contributions. We propose GEMSEC -a graph embedding procedure which learns an embedding of nodes and clusters the nodes at the same time. Sequence based graph embedding procedures create d dimensional feature representations of nodes in an abstract latent space that represent the likelihood of observing similar sampled neighborhoods of the nodes. is problem can be reformulated as minimizing the negative log likelihood of observed neighborhood samples in a probabilistic model on graphs. We extend this reformulated objective function by adding clustering cost to the negative log likelihood and solve the resulting optimization problem with a variant of mini-batch gradient descent <ref type="bibr" target="#b6">[7]</ref>. Simply, our approach creates a d dimensional representation for each node and C cluster centers in this d dimensional Euclidean space. Our model is agnostic of the way neighborhoods of vertices are being sampled. In our experiments we specifically se led on using first order truncated random walks. e fundamental idea in our work is that sequence based graph embedding procedures create similar representations for nodes which have similar sampled neighborhoods. Nodes with similar neighborhoods are expected to be in the same community. is means that clustering the representations can reveal the community structure. We directly exploit this feature by adding a cost element which enforces the created representation to be clustered. Looking at <ref type="figure" target="#fig_0">Figure 1</ref> the intuition behind our algorithm can be easily understood. By enforcing nodes to be clustered, GEMSEC reveals the natural community structure in the graph.</p><p>In addition we demonstrate that the objective function of sequence based graph embedding procedures can be extended with a smoothness regularization term. is procedure can additionally force nodes with high neighborhood overlaps to have similar representations. e experimental evaluation of GEMSEC primarily focuses on non-overlapping community detection. We demonstrate that GEM-SEC outperforms graph embedding procedures <ref type="bibr" target="#b20">[21]</ref> and community detection methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref> on a variety of real world social networks collected from Facebook when it comes to clustering. In terms of the clusterings' modularity, the performance advantage of our proposed method over the baseline community detection procedures varies between 0.24% and 10.74%. We also highlight with Deezer music streaming data that the clustering constraint has a positive effect on the representation quality of social network features for music genre recommendation. In terms of micro-averaged F1 score, the predictive performance increase compared to DeepWalk is between 3.74% and 8.79% in predicting genres liked by users using the embeddings. Our results support that the clustering performance of GEMSEC is robust to hyperparameter changes and also to perturbation. Finally, we show that the runtime complexity of our method is linear in the number of nodes.</p><p>Main contributions of our work are:</p><p>(1) We present GEMSEC, a sequence based graph embedding model which learns a graph embedding and a clustering of vertices in the embedding space jointly. (2) We introduce a procedure to train GEMSEC and demonstrate that its runtime scales linearly with the number of vertices. (3) We enhance GEMSEC with smoothness regularization in order to enforce nodes with high neighborhood overlap to have more similar representations. (4) We show strong clustering and embedding capabilities of GEMSEC on social networks extracted from Facebook and Deezer. e remainder of the paper is structured as follows. In Section 2 we overview the related literature. We discuss the graph clustering algorithm and its variants in Section 3. e evaluation of the introduced methods using real world social network data is carried out in Section 4. We look at the cluster quality obtained with our procedure and compare the clustering performance to other community detection methods. We show that the introduction of the clustering constraint increases representation quality on the task of multi-label node classification. We demonstrate the scalability of our algorithm, carry out a sensitivity analysis and highlight its robustness to perturbation. e paper concludes with Section 5 where we review our main findings and outline possible extensions of our work. e datasets that we collected and used in the experiments will be made available on h ps://snap.stanford.edu/. A high performance reference implementation of GEMSEC can be accessed on h ps://github.com/benedekrozemberczki.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recent advances in graph embedding procedures have made possible the effective and scalable unsupervised feature learning for large real world graphs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Features extracted with these sequence based graph embedding procedures can be used for predicting social network users' missing age <ref type="bibr" target="#b22">[23]</ref>, the category of scientific papers in citation networks <ref type="bibr" target="#b14">[15]</ref> and the function of proteins in protein-protein interaction networks <ref type="bibr" target="#b10">[11]</ref>. Besides supervised learning tasks on nodes the extracted features can be used for graph visualization <ref type="bibr" target="#b8">[9]</ref>, edge prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, community detection <ref type="bibr" target="#b8">[9]</ref> and structural role identification <ref type="bibr" target="#b26">[27]</ref>.</p><p>Sequence based graph embedding procedures were inspired by distributed word representations, specifically by the Skip-Gram model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. In this model representations of words are learned to minimize the negative log likelihood of observing words in the neighborhood (context) of a specific word. ese models use sentences in order to generate the contexts which one can think of as linear sequences of strings. e minimization problem itself is solved with stochastic gradient descent and made tractable by the use of negative sampling <ref type="bibr" target="#b11">[12]</ref>. In a similar manner, graph embedding models first sample sequences of vertices and extract noisy proximity statistics from these sequences within a fixed context. Co-occurrence statistics are encoded with a single hidden layer neural network and node specific weights in the input layer are used as vertex representations.</p><p>Research in sequence based graph embedding literature mainly considers the definition of the sampling strategy that is used to obtain vertex sequences. e most basic sampling strategy is the use of truncated random walks as used in Deepwalk <ref type="bibr" target="#b20">[21]</ref>. More involved sequence sampling methods include the use of secondorder random walks <ref type="bibr" target="#b10">[11]</ref>, the introduction of skips in random walks <ref type="bibr" target="#b21">[22]</ref> and branching processes <ref type="bibr" target="#b23">[24]</ref>. ese more sophisticated synthetic data generation models encode the structural role of nodes <ref type="bibr" target="#b26">[27]</ref>, to get a representation that is in line with the multi level structure of the graph and consequently to improve the predictive performance on downstream machine learning tasks.</p><p>ere is also a number of works focused on creating embeddings for specific types of graphs. Specifically, there is work looking at graphs with signed edges <ref type="bibr" target="#b36">[35]</ref> and rooted subgraphs <ref type="bibr" target="#b17">[18]</ref>. Another straightforward contribution to this area of research was the inclusion of generic vertex features in the creation of representations <ref type="bibr" target="#b35">[34]</ref>.</p><p>Besides sequence based graph embedding procedures other neural models were developed specifically to create representations of graphs for downstream machine learning tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">31]</ref>. Unlike sequence based embedding algorithms these models encode directly the adjacency matrix and other matrix representations of the graph. A major development in this direction was the introduction of scalable graph convolutional networks which besides the topology used generic features of vertices to create representations of nodes <ref type="bibr" target="#b14">[15]</ref>. Representations created with graph convolutional models have a shortcoming similar to the majority of sequence based graph embedding procedures. ey are not inductive, meaning that the learned representations cannot be transferred to other graphs. However, combined with branching process sampling, scalable inductive methods have been developed recently <ref type="bibr" target="#b13">[14]</ref>. e possibility of combining self clustering embeddings with these methods to create community detection procedures that use node features remains to be investigated.</p><p>ere has been previous work on using learned representations for community detection, but existing research is limited to clustering the nodes a er creating an embedding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">29]</ref>. Other methods aim to create representations and clusterings jointly that are consistent with precomputed clusterings <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">33]</ref>. Our approach on the other hand, takes clustering quality directly account while performing the embedding optimization, thus producing results faster and a more natural embedding with clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH EMBEDDING WITH SELF CLUSTERING</head><p>In this section we set up the optimization problem of jointly learning graph embedding and clustering. First, we focus on the optimization problem that allows for learning the embedding solely. Now let G = (V , E) be the graph of interest where V is the set of vertices and E is the set of edges. e graph embedding is a mapping f : V ? R d where d is the dimensionality of the embedding space. For each node ? V we create a d dimensional representation, essentially the embedding f is a |V | ?d real valued matrix. In the sequence based embedding, sequences of neighboring nodes are sampled from the graph. Within a sequence, a node occurs in the context of a window ? which we call neighbors of in the sequence. Given a strategy S to sample sequences, we refer to the collection of windows containing as N S ( ). Earlier works have proposed random walks, second order random walks or branching processes to obtain N S ( ). In our experiments presented later we will use unweighted first order random walks for sampling. Our goal is to minimize the negative log likelihood of observing these neighborhoods of source nodes conditional on the feature vectors that describe the position of nodes in the embedding space. e representation vector specific to node is f ( ). e optimization problem of interest is given by:</p><formula xml:id="formula_0">min f ?V ? log P(N S ( )| f ( )).<label>(1)</label></formula><p>is optimization is subject to two standard assumptions <ref type="bibr" target="#b10">[11]</ref>. First, we assume that P(N S ( )| f ( )) can be factorized in line with conditional independence with respect to f ( ). e consequence of this assumption is:</p><formula xml:id="formula_1">P(N S ( )| f ( )) = n i ?N S ( ) P(n i | f ( )).<label>(2)</label></formula><p>Second, we demand symmetry in the feature space, meaning that source and neighboring nodes have a symmetric effect on each other in the embedding space. We achieve this by using a so max function on the pairwise dot products of node representations with f ( ) to get P(n i | f ( )). is is expressed by</p><formula xml:id="formula_2">P(n i | f ( )) = exp(f (n i ) ? f ( )) u ?V exp(f (u) ? (f ( )) .<label>(3)</label></formula><p>Using Equations <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> we can reformulate the optimization problem as</p><formula xml:id="formula_3">min f ?V ? ? ? ? ? ? ln u ?V exp(f ( ) ? f (u)) ? n i ?N S ( ) f (n i ) ? f ( ) ? ? ? ? ? ? .<label>(4)</label></formula><p>e partition function in Equation (4) enforces nodes to be embedded in a low volume space around the origin, while the second term forces nodes with similar sampled neighborhoods to have similar representations, but farther out from the origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning to Cluster</head><p>We have formulated the sequence based graph embedding problem as a minimization task, and now we can extend it with a proper clustering cost and define the GEMSEC model. As we discussed in the introduction, a sequence based graph embedding procedure will place nodes with similar neighborhoods close to each other in the created latent space. Formally this means that</p><formula xml:id="formula_4">N S ( ) ? N S (u) implies that f ( ) ? f (u) conversely we can say that f ( ) ? f (u) implies that N S ( ) ? N S (u).</formula><p>e optimization problem described by Equation <ref type="formula" target="#formula_3">(4)</ref> can be augmented with a k-means like cost function which describes the clustering cost specifically in the embedding space.</p><p>is augmented optimization problem is described by</p><formula xml:id="formula_5">min f , ? ?V ? ? ? ? ? ? ln u ?V exp(f ( ) ? f (u)) ? n i ?N S ( ) f (n i ) ? f ( ) ? ? ? ? ? ? Embedding cost + ? ? ?V min c ?C f ( ) ? ? c 2 2 Clustering cost .<label>(5)</label></formula><p>In Equation <ref type="formula" target="#formula_5">(5)</ref> we have C cluster means -the c th cluster mean is denoted by ? c . Each of these cluster centers is a d-dimensional vector in the embedding space. For each node we calculate the distance from the cluster centers and we take the smallest of these distances. e weight coefficient of the clustering cost is given by the hyperparameter ? . Cluster means themselves are trainable parameters just as the embedding vectors are. Evaluating the partition function in the proposed objective function for all of the source nodes has a O(|V | 2 ) runtime complexity. Because of this, we have to approximate the partition function term with negative sampling which is a form of noise contrastive estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>e objective function has insightful gradients with respect to the node representation vectors and clusters centers. Assuming that all cluster centers are distinct, the minimum function in the clustering cost is differentiable. As a result we can obtain the gradients for node representations and cluster centers. Understanding these gradients is essential as we will use a first-order gradientbased optimizer to solve the problem proposed in <ref type="figure" target="#fig_9">Equation 5</ref>.</p><p>Specifically, the gradient of the objective function L with respect to the representation of node * ? V is described by Equation <ref type="formula">(6)</ref> if ? c is the closest cluster center to f ( * ).</p><formula xml:id="formula_6">? L ?f ( * ) = u ?V exp(f ( * ) ? f (u)) ? f (u) u ?V exp(f ( * ) ? f (u)) Partition function gradient ? n i ?N S ( * ) f (n i ) Neighbor direction + ? ? 2 ? (f ( * ) ? ? c )</formula><p>Closest cluster direction <ref type="bibr" target="#b5">(6)</ref> First, the gradient of the partition function pulls the representation of * towards the origin. e second term moves the representation of * closer to the representations of its neighbors in the embedding space while the third term moves the node closer to the closest cluster center. First, we have to note that if we set a high ? value the third term dominates the gradient. is will cause the node to gravitate towards the closest cluster center which might not contain the neighbors of * . is phenomenon can be understood by looking at Subfigure 2a. If node B is initially captured in the cluster on the le hand side it will never end up in a cluster with its neighbors on the right hand side. Second, only the position of the closest cluster center affects the representation of f ( * ).</p><p>If the set of nodes that belong to cluster center c is V c the gradient of the objective function with respect to ? c is described by</p><formula xml:id="formula_7">? L ?? c = ?? ? 2 ?Vc (f ( ) ? ? c ).<label>(7)</label></formula><p>First, looking at Equation 7 we see that the gradient moves the cluster center by the sum of coordinates of nodes in the embedding space that belong to cluster c. Second, if a cluster ends up empty it will not be updated as elements of the gradient would be zero. Because of this, the initialization of the cluster centers has to be chosen carefully. A wrong initialization just like the one with an empty cluster on Subfigure 2b can affect clustering performance considerably.  We want to create an embedding of nodes that preserves neighborhoods and at the same time we want the nodes to cluster in the embedding space around the cluster centers. In order to do so we have to set the cost coefficient of clustering such way that node representations are not captured by the closest cluster centers right a er initialization. e main difference between this procedure and simply clustering the nodes a er embedding them is the fact that the learned node representations can adapt to the clustering constraint.</p><p>We propose an efficient learning method to create GEMSEC embeddings which is described with pseudo-code by Algorithm 1. e main idea behind our procedure is the following. If initially the weight of clustering is too high, we will not be able to create meaningful representations and cluster centers at the same time. To resolve this, we anneal the weight coefficient of the clustering cost from a ? starting value to one.</p><p>is way the learned representations can adapt slowly to the cluster centers. </p><formula xml:id="formula_8">? -Learning rate. Result: f ( ), where ? V ? c , where c ? C 1 Model ? Initialize Model( |V |, d, C ) 2 for n in 1:N do 3 V ? Shuffle(V ) 4 for in V do 5 ? ? Update Cluster Cost Weight(? ) 6 ? ? Update Learning Rate(?) 7</formula><p>Sequence ? Sample Nodes(G, , l ) To train an embedding we do the following. Based on the number of vertices, embedding dimensions and clusters, we initialize the weights of our model. A er this we do N sampling repetitions in order to generate vertex sequences from every source node. Before starting a sampling epoch we shuffle the set of vertices. We iterate through the shuffled vertex set node by node. We increase the clustering cost coefficient ? (line 5) and decrease the learning rate ? (line 6). From the source node with our sampling strategy S we sample a vertex sequence with length l (line 7) and extract features using the context size ? (line 8). Using the extracted features, current learning rate and clustering cost coefficient we update the model weights using the gradients and our chosen optimizer (line 9). In our implementation we utilized a variant of stochastic gradient descent, specifically we used the Adam optimizer <ref type="bibr" target="#b6">[7]</ref>. As we said earlier to make the optimization problem tractable we approximate the first cost term with noise contrastive estimation, for each positive sample we draw k noise samples. e clustering cost coefficient is exponentially and the learning rate is linearly annealed. e clustering cost coefficient is being increased whereas the learning rate is being decreased. All in all, the runtime complexity of this procedure is O((? ? k + C) ? l ? d ? |V | ? N ) while DeepWalk with noise contrastive estimation has a O(? ? k ? l ? d ? |V | ? N ) runtime complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Smoothness Regularization</head><p>e graph embedding objective function can be augmented further by adding a smoothness regularization cost element which encourages learned representations to be similar if two sampled nodes share an edge <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">32]</ref>. is way we could increase the weight on having well represented strong first order relationships. We could add a smoothness regularization both to Equations (4) and <ref type="bibr" target="#b4">(5)</ref>. If ? is added to the objective function described by Equation <ref type="formula" target="#formula_3">(4)</ref> we get the smoothed graph embedding model. Later we reference this model as Smooth DeepWalk. We also add the regularization to the objective function of GEMSEC doing so we define Smooth GEMSEC.</p><formula xml:id="formula_9">? = ? ? ( ,u )?E S w ( ,u ) ? f ( ) ? f (u) 2<label>(8)</label></formula><p>In Equation <ref type="formula" target="#formula_9">(8)</ref> each node pair ( , u) has a regularization weight w <ref type="bibr">( ,u)</ref> . e weight can be arbitrarily chosen, later we discuss several different alternatives. e hyperparameter ? describes the regularization coefficient. If ? is high, distant representations of nodes with an edge between them are penalized heavily. e regularization term is being summed over the edges obtained with the sampling strategy.  e training procedure described by Algorithm 1 needs to be modified slightly to accommodate the introduction of the smoothness regularization term. First, we need to set a regularization coefficient. Using the sampled sequence of vertices we extract the edges on which we want to enforce the regularization term. For each sampled edge ( , u) we have to obtain the weight w ,u . e optimizer besides the regularization coefficient also needs this list of edges and weights for doing an update of model parameters.</p><p>In our work we consider different types of regularization weights that we use to obtain smooth representations. eir definitions are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Each of these regularization weights penalizes the difference of node representations with a different strategy.</p><p>? Unit: On each edge we have the same w ,u = 1 weight. ? Neighborhood overlap: If two nodes have a large overlapping neighborhood they should have similar representations and this weight increases linearly with the number of shared neighbors. is weighting strategy is biased towards pairs of nodes with high degree.</p><p>? Jaccards's Coefficient: e neighborhood overlap is normalized by the union of the neighborhood sets in order to down weight nodes with large neighborhood sets. Edges that are between nodes with no common neighbor have zero weight.</p><p>? Minimum normalized overlap: is weighting strategy gives high weight to an edge when one of the nodes has neighbors that are all neighbors of the other node.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>In this section we evaluate whether the cluster quality obtained by GEMSEC is competitive with other community detection procedures. We investigate the scalability and robustness of our method. We also explore how clustering affects the performance on downstream predictive tasks. Standard parameter settings. In order to help with reproducibility we have a standard parameter se ing which we use in our experiments to create embeddings. We emphasize when we deviate from these se ings. Unweighted truncated random walks with length 80 are used as a sampling strategy and we do 5 random walks per source node. From the random walks, features are extracted with a window size of 5. e embedding has 16 dimensions and it is assumed that we need to extract 20 cluster centers. We did a parameter sweep over a number of hyperparameters to obtain the highest average modularity on a given dataset. Initial learning rate values are chosen from 10 ?2 , 5 ? 10 ?3 , 10 ?3 , in a similar manner the final learning rate is chosen from 10 ?3 , 5 ? 10 ?4 , 10 ?4 . Noise contrastive estimation uses 10 negative examples per positive ones. e initial clustering cost coefficient is chosen from 10 ?1 , 10 ?2 , 10 ?3 and it is annealed to 1. Coefficient of the regularization term is 0.0625. Lastly, Jaccard's coefficient is used as a smoothness penalty weight as it consistently gives high quality results.</p><p>Datasets used. For the evaluation of GEMSEC real-world social network datasets are used that we collected from public API's specifically for this work. As you can see in <ref type="table" target="#tab_2">Table 2</ref> these social networks have a variety of size, density and level of clustering. We used graphs from two sources:</p><p>? Facebook page networks: ese graphs represent mutual like networks among verified Facebook pages -the types of sites included TV shows, politicians, athletes and artists among others.  ? Deezer user-user friendship networks: We collected friendship networks from the music streaming site Deezer and we included 3 European countries (Croatia, Hungary and Romania). For each user we curated the list of genres loved based on the songs liked by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cluster ality</head><p>Using the Facebook page networks we evaluate the clustering performance. We use the standard parameter se ings of our model to do clustering. We did a grid search over the standard learning rate and clustering cost coefficient values. Results obtained by k-means clustering DeepWalk embedding features and benchmarks using baseline community detection methods are also included. e cluster quality is evaluated by modularity -we assume that a node belongs to a single community. Our results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. Numbers report the mean modularity based on 10 randomly initialized experimental repetitions. Errors in parentheses correspond to two standard deviations. Baseline community detection methods used for comparison are:</p><p>(1) Overlap Factorization <ref type="bibr" target="#b0">[1]</ref>: We factorize the neighborhood overlap matrix to create continuous features. We create a 16 dimension factorization and cluster the factors with k-means clustering to create 20 communities. (2) Walktrap <ref type="bibr" target="#b19">[20]</ref>: Initiates random walks from every node and clusters vertices hierarchically based on the obtained neighborhoods. From each node we do truncated random walks with length 5. (3) Fast Greedy <ref type="bibr" target="#b5">[6]</ref>: Optimizes modularity greedily by clustering vertices with a hierarchical procedure. (4) Label Propagation <ref type="bibr" target="#b9">[10]</ref>: Creates unique labels for nodes that are propagated in the network using a random order majority rule based voting. We did label propagation that is randomly initialized and unweighted.</p><p>First, we see that Smooth GEMSEC consistently outperforms the embedding based methods and the benchmark community detection methods. Our procedure's performance advantage over the community detection benchmarks is the highest on the News Sites dataset the clustering's modularity is 10.73% higher and it is the lowest on the TV shows dataset with an advantage of 0.24%. Second, the basic GEMSEC method marginally outperforms DeepWalk on every dataset. ird, the use of smoothness regularization has sometimes non-significant, but definitely positive effect on the clustering performance of both Deepwalk and GEMSEC. Lastly, we assume that a careful tuning of hyperparameters could increase the clustering performance further. For example we fixed the cluster number at 20, but it is fairly plausible that other values would give considerably be er results on some of these graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Music Genre Recommendation</head><p>Graph embeddings are mainly used for extracting features of nodes for downstream predictive tasks. As we modified the graph embedding objective it is fair to assume that the performance on predictive problems might change due to the reformulation of the optimization problem itself. We might expect that accuracy increased or dropped by enforcing the clustering constraint. In order to investigate this we use social networks of Deezer users collected from European countries. We predict the genres of music liked by people using the embeddings. e number of distinct genres that users can like is 84 in each dataset. e exact experimental setup was as follows. Each graph was embedded with the standard parameter se ings that gave the highest modularity. DeepWalk embeddings used initial and final learning rate valuers such as 10 ?3 and 10 ?4 . For GEMSEC embeddings we modified the initial and final learning rates to be 10 ?3 and 5 ? 10 ?4 respectively and the initial cost coefficient of the clustering was 0.1. We used logistic regression with ? 2 regularization to predict each of the labels and 90% of the nodes were randomly selected for training. We evaluated the performance on the remaining users. e reported numbers in <ref type="table">Table 4</ref> are mean micro, macro and weighted average F1 scores calculated from 10 experimental repetitions.</p><p>When performance is evaluated by micro-averaged F1 score we see that Smooth GEMSEC significantly outperforms DeepWalk on all three datasets. Precisely this performance advantage varies between 3.73% and 8.79%. We also see that the basic GEMSEC is able to outperform DeepWalk on the Hungarian and Romanian Deezer datasets when performance is measured by micro averaged F1 score. Moreover, we see that based on this metric the performance of Smooth DeepWalk on this downstream task is as good as  <ref type="table">Table 4</ref>: Multi-label node classification performance of the embedding extracted features on the Deezer genre likes datasets. Performance is measured by average F1 score values. Models were trained on 90% of the data and evaluated on the remaining 10%. Errors in the parentheses correspond to two standard deviations. GEMSEC models consistently have good performance.</p><p>the performance of GEMSEC variants. e results obtained with micro and weighted average F1 score also underpin these findings.</p><p>ese results are quite interesting considering the fact that the number of naturally existing communities in these graphs might be quite different from the cluster number that we enforced in our experiment. In addition, we have evidence that introducing the clustering cost and smoothness regularization does not affect performance on the downstream predictive task adversely. On the contrary, it can increase the predictive accuracy significantly. <ref type="bibr" target="#b3">4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity Analysis</head><p>e formulation of GEMSEC involves the definition of a number of hyperparameters which affect the representation and henceforth the cluster quality. In the following experiments we test how the manipulation of hyperparameters affects the clustering performance of the introduced models. We embed the Politicians Facebook graph with the standard parameter se ings while the initial and final learning rates are set to be 10 ?2 and 5?10 ?3 respectively, the clustering cost coefficient is 0.1 and we perturb certain hyperparameters. In <ref type="figure" target="#fig_6">Figure 3</ref> each data point represents the mean modularity calculated from 10 experimental repetitions. We test the sensitivity of clustering performance to cluster center number, context size, dimension number, random walk length, clustering cost coefficient and initiated random walks per source node.</p><p>Based on the experimental results we can make two main observations. First, GEMSEC models give high quality clusterings for a wide range of parameter se ings. Second, introducing smoothness regularization makes GEMSEC more robust to hyperaparameter changes when performance is evaluated by cluster quality. Specifically, we observe that increasing the cluster number above 30 decreases slightly the average modularity. In case of the non smooth model the decrease in cluster quality is quite considerable.</p><p>We conclude that increasing the context size, the length of truncated random walks and the number of random walks per source node above a certain threshold has only marginal effect on the community detection performance. Interestingly, we also have empirical evidence for the node capture phenomenon -if the clustering cost coefficient is too high vanilla GEMSEC has a poor clustering performance. Finally, there is strong evidence that both GEMSEC and Smooth GEMSEC perform poorly when the number of dimensions used to create the embedding is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Smoothness Regularization Strategies</head><p>As we have seen the introduction of smoothness regularization can boost the clustering performance considerably. In Subsection 3.2 we defined different strategies for weighting edges in the smoothness regularization cost. It is presumed that these strategies work well with different values of the regularization coefficient. In this set of experiments we manipulate the regularization coefficient and look at the modularity under the earlier defined edge distance weighting strategies. We create embeddings of the Facebook Politcians graph with the parameter se ings used in Subsection 4.3 and we plo ed on <ref type="figure" target="#fig_8">Figure 4</ref> the mean modularity based on 10 experimental repetitions.</p><p>e main insights based on the experiments are summarized as follows. e clustering performance of DeepWalk is more sensitive to the regularization coefficient than that of GEMSEC. We observe that GEMSEC has high quality results with nearly every  possible weighting strategy and se ing of the regularization coefficient. Moreover, when the coefficient is too high the neighborhood overlap, minimum normalized overlap and unit weighting has considerable adverse effects on the clustering performance of Deep-Walk. Finally, the clustering performance under the standard setting of the regularization coefficient seems to be optimal for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Robustness to Perturbation</head><p>In a number of se ings the graph that we observe contains noisy edges or edges among vertices are not present. In a social network people who do not know each other might share an edge or conversely users who do know someone decide not to be linked on purpose. As such applications are highly relevant for real-world applications we will test the robustness of GEMSEC to these types of perturbations and compare it to the DeepWalk variants. Utilizing the Facebook Politicians graph we create synthetic graphs where a given fraction of nodes is randomly removed or added while the number of connected components is unchanged. Using these synthetic graphs we created a clustering of nodes and evaluated the modularity on the original graph. We used the hyperparameter se ings from Subsection 4.3 and each experiment was repeated 10 times. e plots on <ref type="figure" target="#fig_9">Figure 5</ref> report mean modularity and error bars correspond to two standard deviations. First, we can observe that all of the models are quite robust to edge removal -we see li le difference between the modularity values when we increased the fraction of removed edges from 10% to 20%. On the contrary the addition of edges that are non-existent in the original graph has a considerable adverse effect on clustering performance. Second, we see that the use of smoothness regularization with Jaccard's index weighting mitigates this effect. is is due to the fact that noisy edges between randomly selected nodes are expected to have a zero neighborhood overlap. Because of this, for noise edges we do not have a constraint on making representations to be similar. Finally, it should be considered that even when we add a considerable amount of edges randomly we are able to perform comparably to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Scalability</head><p>As scalability is an important aspect in real world applications we also investigated the scalability of our procedure. e quantity of interest was the time needed to do a single training epoch as the sampling procedure and the feature extraction is equivalent to that of DeepWalk (assuming that the regularization weights are precalculated). Our experiments were done on Erdos-Renyi graphs with a fixed average degree of 20. We created embeddings of this graph with both DeepWalk and GEMSEC variants and we used the baseline parameter se ings to create embeddings. We calculated mean optimization runtime in seconds per epoch based on ten epochs. e logarithms of these averages as a function of the log node number are plo ed on Figure 6. Most importantly, based on <ref type="figure" target="#fig_10">Figure 6</ref> we can conclude that doubling the size of the graph doubles the time needed for optimizing GEMSEC. e linear nature of the proposed algorithm is well demonstrated by the figure. We also observe that learning the clustering and embedding jointly increases optimization time, which was expected. Besides this, the presence of smoothness regularization also inflates the time needed for optimization, but each of the smooth algorithms scales linearly with the number of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper we proposed GEMSEC a novel algorithm that learns a graph embedding and a clustering of nodes jointly. e introduced model is a natural extension of earlier sequence based graph embedding procedures such as DeepWalk and Node2vec <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. We reformulated the objective function used for sequence based graph embedding by adding a constraint that enforces nodes to be clustered around a fixed number of cluster centers in the embedding space. As an additional extension we introduced smoothness regularization of embeddings which can enforce the representation of nodes with highly overlapping neighbourhoods to be similar.</p><p>Empirical evaluation of the proposed graph clustering methods on Facebook data shows that the quality of extracted communities is quite high. Our methods outperform a number of strong baselines and also they give be er results than simply clustering a DeepWalk embedding. We also demonstrate on social networks extracted from Deezer that GEMSEC with the right number of clusters is able to significantly improve the predictive accuracy on the downstream machine learning task of node labeling. Additional experiments establish that our proposed method is scalable, extracts high quality communities with a wide range of parameter se ings and the clustering performance is robust to perturbation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Zacahry's Karate club visualized using Deepwalk and GEMSEC embeddings. White nodes correspond to the instructor's community and blue ones to the president's community.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Node capture due to high cluster cost coefficient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Potential issues with cluster cost weighting and cluster initialization. Different background colors denote different sides of the clustering decision boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Data: G = (V , E) -Graph to be embedded. N -Number of sequence samples per node. l -Length of sequences. ? -Context size. d -Number of embedding dimensions. C -Number of clusters. k -Number of noise samples. ? -Initial clustering weight coefficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 Features? 9 Update 1 :</head><label>891</label><figDesc>Feature Extraction(Sequence, ?) Model Weights(Model, Features, ? , ?, k) GEMSEC training procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Minimum normalized overlap | B( ) ? B(u) | min( | B( ) | , | B(u) |)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Sensitivity of cluster quality to parameter changes measured by modularity. Smoothness regularization increases the robustness of the self-clustering embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Sensitivity of cluster quality to smoothness regularization coefficient on the Facebook politicians network. Smooth GEMSEC shows a stronger robustness to regularization coefficient changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Sensitivity of cluster quality to randomized edge removal and addition measured by modularity. Smoothness regularization increases robustness to edge addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Sensitivity of optimization runtime to graph size measured by seconds. Our proposed models' optimization has linear runtime complexity in the number of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Smoothness regularization weights for edge ( , u) with first order neighbor sets B( ) and B(u).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics of the datasets used in the paper for evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Facebook Verified Site Networks Politicians Companies Athletes News Sites Public Figures Artists Government TV Shows</figDesc><table><row><cell>Overlap Factorization</cell><cell>0.810 (?0.008)</cell><cell>0.553 (?0.010)</cell><cell>0.601 (?0.020)</cell><cell>0.471 (?0.016)</cell><cell>0.551 (?0.01)</cell><cell>0.474 (?0.018)</cell><cell>0.608 (?0.024)</cell><cell>0.786 (?0.008)</cell></row><row><cell>Walktrap</cell><cell>0.841 (?0.023)</cell><cell>0.639 (?0.016)</cell><cell>0.670 (?0.021)</cell><cell>0.514 (?0.023)</cell><cell>0.628 (?0.023)</cell><cell>0.554 (?0.026)</cell><cell>0.675 (?0.043)</cell><cell>0.790 (?0.036)</cell></row><row><cell>Fast greedy</cell><cell>0.819 (?0.008)</cell><cell>0.665 (?0.014)</cell><cell>0.605 (?0.026)</cell><cell>0.531 (?0.020)</cell><cell>0.630 (?0.011)</cell><cell>0.464 (?0.023)</cell><cell>0.615 (?0.046)</cell><cell>0.835 (?0.006)</cell></row><row><cell>Label Propagation</cell><cell>0.826 (?0.009)</cell><cell>0.647 (?0.075)</cell><cell>0.647 (?0.094)</cell><cell>0.243 (?0.159)</cell><cell>0.612 (?0.027)</cell><cell>0.393 (?0.018)</cell><cell>0.659 (?0.041)</cell><cell>0.839 (?0.004)</cell></row><row><cell>DeepWalk</cell><cell>0.840 (?0.017)</cell><cell>0.637 (?0.012)</cell><cell>0.649 (?0.012)</cell><cell>0.481 (?0.022)</cell><cell>0.631 (?0.011)</cell><cell>0.508 (?0.029)</cell><cell>0.703 (?0.010)</cell><cell>0.831 (?0.004)</cell></row><row><cell>Smooth DeepWalk</cell><cell>0.849 (?0.017)</cell><cell>0.667 (?0.007)</cell><cell>0.669 (?0.007)</cell><cell>0.541 (?0.006)</cell><cell>0.643 (?0.008)</cell><cell>0.523 (?0.020)</cell><cell>0.707 (?0.008)</cell><cell>0.835 (?0.008)</cell></row><row><cell>GEMSEC</cell><cell>0.851 (?0.009)</cell><cell>0.650 (?0.013)</cell><cell>0.674 (?0.009)</cell><cell>0.536 (?0.011)</cell><cell>0.636 (?0.014)</cell><cell>0.528 (?0.020)</cell><cell>0.705 (?0.020)</cell><cell>0.833 (?0.010)</cell></row><row><cell>Smooth GEMSEC</cell><cell>0.855 (?0.006)</cell><cell>0.683 (?0.009)</cell><cell>0.694 (?0.009)</cell><cell>0.588 (?0.009)</cell><cell>0.649 (?0.007)</cell><cell>0.559 (?0.011)</cell><cell>0.716 (?0.008)</cell><cell>0.841 (?0.004)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean modularity of clusterings on the Facebook datasets. Each embedding experiment was repeated ten times. Errors in the parentheses correspond to two standard deviations. In terms of modularity GEMSEC outperforms the baselines on most of the datasets. Smooth GEMSEC consistently does better than the baselines and DeepWalk.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Benedek Rozemberczki and Ryan Davies were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: membership, growth, and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hu Enlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Loup</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Lambio E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning community embedding with community detection and node embedding on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding Community Structure in Very Large Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR) (ICLR &apos;15)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR) (ICLR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02801</idno>
		<title level="m">Graph Embedding Techniques, Applications, and Performance: A Survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding Overlapping Communities in Networks by Label Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Gregory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">103018</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the irteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the irteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network lasso: Clustering and optimization in large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and eir Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santhoshkumar</forename><surname>Saminathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08928</idno>
		<title level="m">subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Community detection in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ploutarchos</forename><surname>Spyridonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="515" to="554" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pons</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Latapy</forename></persName>
		</author>
		<title level="m">Chapter Computing Communities in Large Networks Using Random Walks</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
	<note>International Symposium on Computer and Information Sciences</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepwalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Don&apos;T Walk, Skip!: Online Learning of Multi-scale Network Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="DOI">10.1145/3110025.3110086</idno>
		<ptr target="ps://doi.org/10.1145/3110025.3110086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (ASONAM &apos;17)</title>
		<meeting>the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (ASONAM &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exact Age Prediction in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742765</idno>
		<ptr target="ps://doi.org/10.1145/2740908.2742765" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web (WWW &apos;15 Companion</title>
		<meeting>the 24th International Conference on World Wide Web (WWW &apos;15 Companion<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised and Scalable Algorithm for Learning Node Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nivio</forename><surname>Ziviani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<ptr target="//jmlr.org/proceedings/papers/v70/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling a store&apos;s product space as a social network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Network Analysis and Mining, 2009. ASONAM&apos;09. International Conference on Advances in</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Struc2Vec: Learning Node Representations from Structural Identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098061</idno>
		<ptr target="ps://doi.org/10.1145/3097983.3098061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;17)</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</meeting>
		<imprint>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Deep Representations for Graph Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust community detection methods with resolution parameter for complex detection in protein protein interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Twan</forename><surname>Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Marchiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pa ern Recognition in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Embedding for Clustering Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Org</surname></persName>
		</author>
		<ptr target="//dl.acm.org/citation.cfm?id=3045390.3045442" />
		<imprint>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SNE: Signed Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
