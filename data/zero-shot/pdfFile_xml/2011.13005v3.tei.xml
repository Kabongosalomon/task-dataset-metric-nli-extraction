<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PREDATOR: Registration of 3D Point Clouds with Low Overlap</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Usvyatsov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">PREDATOR: Registration of 3D Point Clouds with Low Overlap</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce PREDATOR, a model for pairwise pointcloud registration with deep attention to the overlap region. Different from previous work, our model is specifically designed to handle (also) point-cloud pairs with low overlap. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds. The ability to focus on points that are relevant for matching greatly improves performance: PREDATOR raises the rate of successful registrations by more than 15 percent points in the low-overlap scenario, and also sets a new state of the art for the 3DMatch benchmark with 90.6% registration recall. [Code release]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent work has made substantial progress in fully automatic, 3D feature-based point cloud registration. At first glance, benchmarks like 3DMatch <ref type="bibr" target="#b55">[56]</ref> appear to be saturated, with multiple state-of-the-art (SoTA) methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3]</ref> reaching nearly 95% feature matching recall and successfully registering &gt;80% of all scan pairs. One may get the impression that the registration problem is solved-but this is actually not the case. We argue that the high success rates are a consequence of lenient evaluation protocols. We have been making our task too easy: existing literature and benchmarks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b22">23]</ref> consider only pairs of point clouds with ?30% overlap to measure performance. Yet, the lowoverlap regime is very relevant for practical applications. On the one hand, it may be difficult to ensure high overlap, for instance when moving along narrow corridors, or when closing loops in the presence of occlusions (densely builtup areas, forest, etc.). On the other hand, data acquisition is * First two authors contributed equally to this work.  <ref type="figure">Figure 1</ref>: PREDATOR is designed to focus attention on the overlap region, and to prefer salient points in that region, so as to enable robust registration in spite of low overlap.</p><p>often costly, so practitioners aim for a low number of scans with only the necessary overlap <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. Driven by the evaluation protocol, the high-overlap scenario became the focus of research, whereas the more challenging low-overlap examples were largely neglected (cf . <ref type="figure">Fig. 1</ref>). Consequently, the registration performance of even the best known methods deteriorates rapidly when the overlap between the two point clouds falls below 30%, see <ref type="figure">Fig. 2</ref>. Human operators, in contrast, can still register such low overlap point clouds without much effort.</p><p>This discrepancy is the starting point of the present work. To study its reasons, we have constructed a low-overlap dataset 3DLoMatch from scans of the popular 3DMatch benchmark, and have analysed the individual modules/steps of the registration pipeline <ref type="figure">(Fig. 2</ref>). It turns out that the effective receptive field of fully convolutional feature point descriptors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref> is local enough and the descriptors are hardly corrupted by non-overlapping parts of the scans. Rather than coming up with yet another way to learn better descriptors, the key to registering low overlap point clouds is learning where to sample feature points. A large performance boost can be achieved if the feature points are predominantly sampled from the overlapping portions of the scans <ref type="figure">(Fig. 2, right)</ref>.</p><p>We follow this path and introduce PREDATOR, a neural architecture for pairwise 3D point cloud registration that learns to detect the overlap region between two unregistered scans, and to focus on that region when sampling feature <ref type="figure">Figure 2</ref>: Registration with SoTA methods deteriorates rapidly for pairs with &lt;30% overlap (left). By increasing the fraction of points sampled in the overlap region, many failures can be avoided as shown here for FCGF <ref type="bibr" target="#b8">[9]</ref> (right).</p><p>points. The main contributions of our work are:</p><p>? an analysis why existing registration pipelines break down in the low-overlap regime ? a novel overlap attention block that allows for early information exchange between the two point clouds and focuses the subsequent steps on the overlap region ? a scheme to refine the feature point descriptors, by conditioning them also on the respective other point cloud ? a novel loss function to train matchability scores, which help to sample better and more repeatable interest points Moreover, we make available the 3DLoMatch dataset, containing the previously ignored scan pairs of 3DMatch that have low (10-30%) overlap. In our experiments, PREDATOR greatly outperforms existing methods in the low-overlap regime, increasing registration recall by &gt;15 percent points. It also sets a new state of the art on the 3DMatch benchmark, reaching a registration recall of &gt;90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We start this related-work section by reviewing the individual components of the traditional point cloud registration pipelines, before proceeding to newer, end-to-end pointcloud registration algorithms. Finally, we briefly cover recent advances in using contextual information to guide and robustify feature extraction and matching. Local 3D feature descriptors: Early local descriptors for point clouds <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref> aimed to characterise the local geometry by using hand-crafted features. While often lacking robustness against clutter and occlusions, they have long been a default choice for downstream tasks because they naturally generalise across datasets <ref type="bibr" target="#b19">[20]</ref>. In the last years, learned 3D feature descriptors have taken over and now routinely outperform their hand-crafted counterparts.</p><p>The pioneering 3DMatch method <ref type="bibr" target="#b55">[56]</ref> is based on a Siamese 3D CNN that extracts local feature descriptors from a signed distance function embedding. Others <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref> first extract hand-crafted features, then map them to a compact representation using multi-layer perceptrons. PPFNet <ref type="bibr" target="#b11">[12]</ref>, and its self-supervised version PPF-FoldNet <ref type="bibr" target="#b10">[11]</ref>, combine point pair features with a Point-Net <ref type="bibr" target="#b31">[32]</ref> architecture to extract descriptors that are aware of the global context. To alleviate artefacts caused by noise and voxelisation, <ref type="bibr" target="#b17">[18]</ref> proposed to use a smoothed density voxel grid as input to a 3D CNN. These early works achieved strong performance, but still operate on individual local patches, which greatly increases the computational cost and limits the receptive field to a predefined size.</p><p>Fully convolutional architectures <ref type="bibr" target="#b25">[26]</ref> that enable dense feature computation over the whole input in a single forward pass <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> have been adopted to design faster 3D feature descriptors. Building on sparse convolutions <ref type="bibr" target="#b7">[8]</ref>, FCGF <ref type="bibr" target="#b8">[9]</ref> achieves a performance similar to the best patchbased descriptors <ref type="bibr" target="#b17">[18]</ref>, while being orders of magnitude faster. D3Feat <ref type="bibr" target="#b2">[3]</ref> complements a fully convolutional feature descriptor with an salient point detector. Interest point sampling: The classic principle to sample salient rather than random points has also found its way into learned 2D <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> and 3D <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref> local feature extraction. All these methods implicitly assume that the saliency of a point fully determines its utility for downstream tasks. Here, we take a step back and argue that, while saliency is desirable for an interest point, it is not sufficient on its own. Indeed, in order to contribute to registration a point should not only be salient, but must also lie in the region where the two point clouds overlap-an essential property that, surprisingly, has largely been neglected thus far. Deep point-cloud registration: Instead of combining learned feature descriptors with some off-the-shelf robust optimization at inference time, a parallel stream of work aims to embed the differentiable pose estimation into the learning pipeline. PointNetLK <ref type="bibr" target="#b0">[1]</ref> combines a PointNetbased global feature descriptor <ref type="bibr" target="#b31">[32]</ref> with a Lucas/Kanadelike optimization algorithm <ref type="bibr" target="#b27">[28]</ref> and estimates the relative transformation in an iterative fashion. DCP <ref type="bibr" target="#b45">[46]</ref> use a DGCNN network <ref type="bibr" target="#b47">[48]</ref> to extract local features and computes soft correspondences before using the Kabsch algorithm to estimate the transformation parameters. To relax the need for strict one-to-one correspondence, DCP was later extended to PRNet <ref type="bibr" target="#b46">[47]</ref>, which includes a keypoint detection step and allows for partial correspondence. Instead of simply using soft correspondences, <ref type="bibr" target="#b54">[55]</ref> update the similarity matrix with a differentiable Sinkhorn layer <ref type="bibr" target="#b37">[38]</ref>. Similar to other methods, the weighted Kabsch algorithm <ref type="bibr" target="#b1">[2]</ref> is used to estimate the transformation parameters. Finally, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> complement a learned feature descriptor with an outlier filtering network, which infers the correspondence weights for later use in the weighted Kabsch algorithm. Contextual information: In the traditional pipeline, feature extraction is done independently per point cloud. Information is only communicated when computing pairwise similarities, although aggregating contextual information at an earlier stage could provide additional cues to robustify the descriptors and guide the matching step.</p><p>In 2D feature learning, D2D-Net <ref type="bibr" target="#b48">[49]</ref> use an attention mechanism in the bottleneck of an encoder-decoder scheme to aggregate the contextual information, which is later used to condition the output of the decoder on the second image. SuperGlue <ref type="bibr" target="#b35">[36]</ref> infuses the contextual information into the learned descriptors with a whole series of self-and crossattention layers, built upon the message-passing GNN <ref type="bibr" target="#b23">[24]</ref>. Early information mixing was previously also explored in the field of deep point cloud registration, where <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> use a transformer module to extract task-specific 3D features that are reinforced with contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>PREDATOR is a two-stream encoder-decoder network. Our default implementation uses residual blocks with KPConv-style point convolutions <ref type="bibr" target="#b39">[40]</ref>, but the architecture is agnostic w.r.t. the backbone and can also be implemented with other formulations of 3D convolutions, such as for instance sparse voxel convolutions <ref type="bibr" target="#b7">[8]</ref> (cf . Appendix). As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, the architecture of PREDATOR can be decomposed into three main modules: 1. encoding of the two point clouds into smaller sets of superpoints and associated latent feature encodings, with shared weights (Sec. 3.2); 2. the overlap attention module (in the bottleneck) that extracts co-contextual information between the feature encodings of the two point clouds, and assigns each superpoint two overlap scores that quantify how likely the superpoint itself and its soft-correspondence are located in the overlap between the two inputs (Sec. 3.3); 3. decoding of the mutually conditioned bottleneck repre-sentations to point-wise descriptors as well as refined per-point overlap and matchability scores (Sec. 3.4). Before diving into each component we lay out the basic problem setting and notation in Sec. 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem setting</head><p>Consider two point clouds P = {p i ? R 3 |i = 1..N }, and Q = {q i ? R 3 |i = 1..M }. Our goal is to recover a rigid transformation T Q P with parameters R ? SO(3) and t ? R 3 that aligns P to Q. By a slight abuse of notation we use the same symbols for sets of points and for their corresponding matrices P ? R N ?3 and Q ? R M ?3 .</p><p>Obviously T Q P can only ever be determined from the data if P and Q have sufficient overlap, meaning that after applying the ground truth transformation T Q P the overlap ratio</p><formula xml:id="formula_0">1 N (T Q P (p i ) ? NN(T Q P (p i ), Q) 2 ? v &gt; ? ,<label>(1)</label></formula><p>where NN denotes the nearest-neighbour operator w.r.t. its second argument, ? 2 is the Euclidean norm, |?| is the set cardinality, and v is a tolerance that depends on the point density. <ref type="bibr" target="#b1">2</ref> Contrary to previous work <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b22">23]</ref>, where the threshold to even attempt the alignment is typically ? &gt; 0.3, we are interested in low-overlap point clouds with ? &gt; 0.1. Fragments with different overlap ratios are shown in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder</head><p>We follow <ref type="bibr" target="#b39">[40]</ref> and first down-sample raw point clouds with a voxel-grid filter of size V , such that P and Q have reasonably uniform point density. In the shared encoder, a) overlap ratio = 0.1 b) overlap ratio = 0.3 c) overlap ratio = 0.5 <ref type="figure">Figure 4</ref>: Fragments with different overlap ratios. Overlap is computed relative to the source fragment (orange).</p><p>a series of ResNet-like blocks and strided convolutions aggregate the raw points into superpoints P ? R N ?3 and Q ? R M ?3 with associated features X P ? R N ?b and X Q ? R M ?b . Note that superpoints correspond to a fixed receptive field, so their number depends on the spatial extent of the input point cloud and may be different for the two inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overlap attention module</head><p>So far, the features X P , X Q in the bottleneck encode the geometry and context of the two point clouds. But X P has no knowledge of point cloud Q and vice versa. In order to reason about their respective overlap regions, some crosstalk is necessary. We argue that it makes sense to add that cross-talk at the level of superpoints in the bottleneck, just like a human operator will first get a rough overview of the overall shape to determine likely overlap regions, and only after that identifies precise feature points in those regions. Graph convolutional neural network: Before connecting the two feature encodings, we first further aggregate and strengthen their contextual relations individually with a graph neural network (GNN) <ref type="bibr" target="#b47">[48]</ref>. In the following, we describe the GNN for point cloud P . The GNN for Q is the same. First, the superpoints in P are linked into a graph in Euclidean space with the k-NN method. Let x i ? R b denote the feature encoding of superpoint p i , and (i, j) ? E the graph edge between superpoints p i and p j . The encoder features are then iteratively updated as</p><formula xml:id="formula_1">(k+1) x i = max (i,j)?E h ? cat[ (k) x i , (k) x j ? (k) x i ] ,<label>(2)</label></formula><p>where h ? (?) denotes a linear layer followed by instance normalization <ref type="bibr" target="#b42">[43]</ref> and a LeakyReLU activation <ref type="bibr" target="#b28">[29]</ref>, max(?) denotes element-/channel-wise max-pooling, and cat[?, ?] means concatenation. This update is performed twice with separate (not shared) parameters ?, and the final GNN fea-</p><formula xml:id="formula_2">tures x GNN i ? R d b are obtained as x GNN i = h ? (cat[ (0) x i , (1) x i , (2) x i ]) .<label>(3)</label></formula><p>Cross-attention block: Knowledge about potential overlap regions can only be gained by mixing information about both point clouds. To this end we adopt a cross-attention block <ref type="bibr" target="#b35">[36]</ref> based on the message passing formulation <ref type="bibr" target="#b15">[16]</ref>.</p><p>First, each superpoint in P is connected to all superpoints in Q to form a bipartite graph. Inspired by the Transformer architecture <ref type="bibr" target="#b44">[45]</ref>, vector-valued queries s i ? R b are used to retrieve the values v j ? R b of other superpoints based on their keys k j ? R b , where</p><formula xml:id="formula_3">k j = W k x GNN j v j = W v x GNN j s i = W s x GNN i<label>(4)</label></formula><p>and W k , W v , and W s are learnable weight matrices. The messages are computed as weighted averages of the values,</p><formula xml:id="formula_4">m i? = j:(i,j)?E a ij v j ,<label>(5)</label></formula><p>with attention weights <ref type="bibr" target="#b35">[36]</ref>. I.e., to update a superpoint p i one combines that point's query with the keys and values of all superpoints q j . In line with the literature, in practice we use a multi-attention layer with four parallel attention heads <ref type="bibr" target="#b44">[45]</ref>. The co-contextual features are computed as</p><formula xml:id="formula_5">a ij = softmax(s T i k j / ? b)</formula><formula xml:id="formula_6">x CA i = x GNN i + MLP(cat[s i , m i? ]) ,<label>(6)</label></formula><p>with MLP(?) denoting a three-layer fully connected network with instance normalization <ref type="bibr" target="#b42">[43]</ref> and ReLU <ref type="bibr" target="#b29">[30]</ref> activations after the first two layers. The same cross-attention block is also applied in reverse direction, so that information flows in both directions, P ? Q and Q ? P .</p><p>Overlap scores of the bottleneck points: The above update with co-contextual information is done for each superpoint in isolation, without considering the local context within each point cloud. We therefore, explicitly update the local context after the cross-attention block using another GNN that has the same architecture and underlying graph (within-point cloud links) as above, but separate parameters ?. This yields the final latent feature encodings F P ? R N ?b and F Q ? R M ?b , which are now conditioned on the features of the respective other point cloud. Those features are linearly projected to overlap scores o P ? R N and o Q ? R M , which can be interpreted as probabilities that a certain superpoint lies in the overlap region. Additionally, one can compute soft correspondences between superpoints and from the correspondence weights predict the cross-overlap score of a superpoint p i , i.e., the probability that its correspondence in Q lies in the overlap region:</p><formula xml:id="formula_7">o P i := w T i o Q , w ij := softmax 1 t f P i , f Q j ,<label>(7)</label></formula><p>where ?, ? is the inner product, and t is the temperature parameter that controls the soft assignment. In the limit t ? 0, Eq. (7) converges to hard nearest-neighbour assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder</head><p>Our decoder starts from conditioned features F P , concatenates them with the overlap scores o P ,? P , and outputs per-point feature descriptors F P ? R N ?32 and refined per-point overlap and matchability scores o P , m P ? R N . The matchability can be seen as a "conditional saliency" that quantifies how likely a point is to be matched correctly, given the points (resp. features) in the other point cloud Q.</p><p>The decoder architecture combines NN-upsampling with linear layers, and includes skip connections from the corresponding encoder layers. We deliberately keep the overlap score and the matchability separate to disentangle the reasons why a point is a good/bad candidate for matching: in principle a point can be unambiguously matchable but lie outside the overlap region, or it can lie in the overlap but have an ambiguous descriptor. Empirically, we find that the network learns to predict high matchability mostly for points in the overlap; probably reflecting the fact that the ground truth correspondences used for training, naturally, always lie in the overlap. For further details about the architecture, please refer to Appendix and the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss function and training</head><p>PREDATOR is trained end-to-end, using three losses w.r.t. ground truth correspondences as supervision.</p><p>Circle loss: To supervise the point-wise feature descriptors we follow 3 <ref type="bibr" target="#b2">[3]</ref> and use the circle loss <ref type="bibr" target="#b38">[39]</ref>, a variant of the more common triplet loss. Consider again a pair of overlapping point clouds P and Q, this time aligned with the ground truth transformation. We start by extracting the points p i ? P p ? P that have at least one (possibly multiple) correspondence in Q, where the set of correspondences E p (p i ) is defined as points in Q that lie within a radius r p around p i . Similarly, all points of Q outside a (larger) radius r s form the set of negatives E n (p i ). The circle loss is then computed from n p points sampled randomly from P p :</p><formula xml:id="formula_8">L P c = 1 n p np i=1 log 1 + j?Ep e ? j p (d j i ??p) ? k?En e ? k n (?n?d k i ) ,<label>(8)</label></formula><p>where d j i = ||f pi ? f qj || 2 denotes distance in feature space, and ? n , ? p are negative and positive margins, respectively. The weights ? j p = ?(d j i ?? p ) and ? k n = ?(? n ?d k i ) are determined individually for each positive and negative example, using the empirical margins ? p := 0.1 and ? n := 1.4 with hyper-parameter ?. The reverse loss L Q c is computed in the same way, for a total circle loss L c = 1 2 (L P c + L Q c ). Overlap loss: The estimation of the overlap probability is cast as binary classification and supervised using the over-</p><formula xml:id="formula_9">lap loss L o = 1 2 (L P o + L Q o ), where L P o = 1 |P| |P| i=1 o pi log(o pi ) + (1 ? o pi ) log(1 ? o pi ).<label>(9)</label></formula><p>3 Added to the repository after publication, not mentioned in the paper. </p><formula xml:id="formula_10">o pi = 1, ||T Q P (p i ) ? NN(T Q P (p i ), Q)|| 2 &lt; r o 0, otherwise ,<label>(10)</label></formula><p>with overlap threshold r o . The reverse loss L Q o is computed in the same way. The contributions from positive and negative examples are balanced with weights inversely proportional to their relative frequencies.</p><p>Matchability loss: Supervising the matchability scores is more difficult, as it is not clear in advance which are the right points to take into account during correspondence search. We follow a simple intuition: good keypoints are those that can be matched successfully at a given point during training, with the current feature descriptors. Hence, we cast the prediction as binary classification and generate the ground truth labels on the fly. Again, we sum the two symmetric losses,</p><formula xml:id="formula_11">L m = 1 2 (L P m + L Q m ), with L P m = 1 |P| |P| i=1 m pi log(m pi ) + (1 ? m pi ) log(1 ? m pi ),<label>(11)</label></formula><p>where ground truth labels m pi are computed on the fly via nearest neighbour search NN F (?, ?) in feature space:</p><formula xml:id="formula_12">m pi = 1, ||T Q P (p i )?NN F (p i , Q)|| 2 &lt; r m 0, otherwise.<label>(12)</label></formula><p>Implementation and training: PREDATOR is implemented in pytorch and can be trained on a single RTX 3090 GPU. At the start of the training we supervise PREDATOR only with the circle and overlap losses, the matchability loss is added only after few epochs, when the point-wise features are already meaningful (i.e., &gt;30% of interest points can be matched correctly). The three loss terms are weighted equally. For more details, please refer to Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate PREDATOR and justify our design choices on real point clouds, using 3DMatch <ref type="bibr" target="#b55">[56]</ref> and 3DLoMatch ( ? 4.1). Additionally, we compare PREDATOR to direct registration methods on the synthetic, object-centric Model-Net40 <ref type="bibr" target="#b49">[50]</ref> ( ? 4.2) and evaluate it on large outdoor scenes using odometryKITTI <ref type="bibr" target="#b14">[15]</ref>  <ref type="figure" target="#fig_1">( ? 4.3)</ref>. More details about the datasets and evaluation metrics are available in the Appndix. Qualitative results are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3DMatch</head><p>Dataset: <ref type="bibr" target="#b55">[56]</ref> is a collection of 62 scenes, from which we use 46 scenes for training, 8 scenes for validation and 8 for testing. Official 3DMatch dataset considers only scan pairs with &gt;30% overlap. Here, we add its counterpart in which we consider only scan pairs with overlaps between 10 and 30% and call this collection 3DLoMatch 4 . Metrics: Our main metric, corresponding to the actual aim of point cloud registration, is Registration Recall (RR), i.e., the fraction of scan pairs for which the correct transformation parameters are found with RANSAC. Following the literature <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>, we also report Feature Match Recall (FMR), defined as the fraction of pairs that have &gt;5% "inlier" matches with &lt;10 cm residual under the ground truth transformation (without checking if the transformation can be recovered from those matches), and Inlier Ratio (IR), the fraction of correct correspondences among the putative matches. Additionally, we use empirical cumulative distribution functions (ECDF) to evaluate the relative overlap ratio. At a specific overlap value, the (1 ? ECDF) curve shows the fraction of fragment pairs that have relative overlap greater or equal to that value. <ref type="bibr" target="#b3">4</ref> Due to a bug in the official implementation of the overlap computation for 3DMatch, a few (&lt;7%) scan pairs are included in both datasets.    <ref type="table">Table 1</ref>: Performance of PREDATOR with different interest point sampling strategies; om denotes the product of overlap score and matchability score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative overlap ratio:</head><p>We first evaluate if PREDATOR achieves its goal to focus on the overlap. We discard points with a predicted overlap score o i &lt; 0.5, compute the overlap ratio, and compare it to the one of the original scans. <ref type="figure" target="#fig_3">Fig. 6</ref> shows that more than half (71%) of the low-overlap pairs are pushed over the 30% threshold that prior works considered the lower limit for registration. On average, discarding points with low overlap scores almost doubles the overlap in 3DLoMatch (133% increase). Notably, it also increases the overlap in standard 3DMatch by, on average, &gt;50%. Interest point sampling: PREDATOR significantly increases the effective overlap, but does that improve registration performance? To test this we use the product of the overlap scores o and matchability scores m to bias interest point sampling. We compare two variants: top-k (om), where we pick the top-k points according to the multiplied scores; and prob. (om), where we instead sample points with probability proportional to the multiplied scores. For a more comprehensive assessment we follow <ref type="bibr" target="#b2">[3]</ref> and report performance with different numbers of sampled interest points. Tab. 1 shows that any of the informed sampling strategies greatly increases the inlier ratio, and as   a consequence also the registration recall. The gains are larger when fewer points are sampled. In the low-overlap regime the inlier ratios more than triple for up to 1000 points. We observe that, as expected, high inlier ratio does not necessarily imply high registration recall: our scores are apparently well calibrated, so that top-k (om) indeed finds most inliers, but these are often clustered and too close to each other to reliably estimate the transformation parameters ( <ref type="figure" target="#fig_4">Fig. 7)</ref>. We thus use the more robust prob. (om) sampling, which yields the best registration recall. It may be possible to achieve even higher registration recall by combining top-k (om) sampling with non-maxima suppression. We leave this for future work. Comparison to feature-based methods: We compare PREDATOR to recent feature-based registration methods: 3DSN <ref type="bibr" target="#b18">[19]</ref>, FCGF <ref type="bibr" target="#b8">[9]</ref> and D3Feat <ref type="bibr" target="#b2">[3]</ref>, see Tab. 2. Even though PREDATOR can not solve all the cases (cf . <ref type="figure" target="#fig_6">Fig. 8</ref>), it greatly outperforms existing methods on the low-overlap 3DLoMatch dataset, improving registration recall by 15.5-19.7 percent points (pp) over the closest competitorvariously FCGF or 3DFeat. Moreover, it also consistently reaches the highest registration recall on standard 3DMatch, showing that its attention to the overlap pays off even for scans with moderately large overlap. In line with our motivation, what matters is not so much the choice of descriptors, but finding interest points that lie in the overlap region -especially if that region is small. Comparison to direct registration methods: We also tried to compare PREDATOR to recent methods for direct registration of partial point clouds. Unfortunately, for both PRNet <ref type="bibr" target="#b46">[47]</ref> and RPM-Net <ref type="bibr" target="#b54">[55]</ref>, training on 3DMatch failed to converge to reasonable results, as already observed in <ref type="bibr" target="#b6">[7]</ref>. It appears that their feature extraction is specifically tuned to synthetic, object-centric point clouds. Thus, in a further  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ModelNet40</head><p>Dataset: <ref type="bibr" target="#b49">[50]</ref> contains 12,311 CAD models of man-made objects from 40 different categories. We follow <ref type="bibr" target="#b54">[55]</ref> to use 5,112 samples for training, 1,202 samples for validation, and 1,266 samples for testing. Partial scans are generated following <ref type="bibr" target="#b54">[55]</ref>. In addition to ModelNet which has 73.5% pairwise overlap on average, we generate ModelLoNet with lower (53.6%) average overlap. For more details see Appendix.</p><p>Metrics: We follow <ref type="bibr" target="#b54">[55]</ref> and measure the performance using the Relative Rotation Error (RRE) (geodesic distance between estimated and GT rotation matrices), the Relative Translation Error (RTE) (Euclidean distance between the  <ref type="table">Table 4</ref>: Evaluation results on ModelNet and ModelLoNet. 450 points are sampled for RANSAC with rand / prob.. estimated and GT translations), and the Chamfer distance (CD) between the two registered scans. Relative overlap ratio: We again evaluate if PREDATOR focuses on the overlap region. We extract 8,862 test pairs by varying the completeness of the input point clouds from 70 to 40%. <ref type="figure" target="#fig_7">Fig. 9</ref> shows that PREDATOR substantially increases the relative overlap and reduces the number of pairs with overlap &lt;70% by more than 40 pp. Comparison to direct registration methods: To be able to compare PREDATOR to RPM-Net <ref type="bibr" target="#b54">[55]</ref> and DCP <ref type="bibr" target="#b45">[46]</ref>, we resort to the synthetic, object-centric dataset they were designed for. We failed to train PRNet <ref type="bibr" target="#b46">[47]</ref> due to random crashes of the original code (also observed in <ref type="bibr" target="#b6">[7]</ref>).</p><p>Remarkably, PREDATOR can compete with methods specifically tuned for ModelNet, and in the low-overlap regime outperforms them in terms of RRE, see Tab. 4. Moreover, we observe a large boost by sampling points with overlap attention (prob. (om)) rather than randomly (rand). <ref type="figure" target="#fig_7">Fig. 9</ref> (right) further underlines the importance of sampling in the overlap: PREDATOR is a lot more robust in the low overlap regime (?8 ? lower RRE at completeness 0.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">odometryKITTI</head><p>Dataset: <ref type="bibr" target="#b14">[15]</ref> contains 11 sequences of LiDAR-scanned outdoor driving scenarios. We follow <ref type="bibr" target="#b8">[9]</ref> and use sequences 0-5 for training, 6-7 for validation, and 8-10 for testing. In line with <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref> we further refine the provided ground truth poses using ICP <ref type="bibr" target="#b4">[5]</ref> and only use point cloud pairs that are at most 10 m away from each other for evaluation.  <ref type="table">Table 5</ref>: Evaluation of PREDATOR on odometryKITTI, following the evaluation protocol employed by D3Feat <ref type="bibr" target="#b2">[3]</ref>.</p><p>Comparision to the SoTAs: We compare PREDATOR to 3DFeat-Net <ref type="bibr" target="#b53">[54]</ref>, FCGF <ref type="bibr" target="#b8">[9]</ref> and D3Feat* [3] 5 As shown in Tab. 5, PREDATOR performs on-par with the SoTA. The results also corroborate the impact of our overlap attention which again outperforms the random sampling baseline.</p><p>Computational complexity: With O(n 2 ) complexity the cross-attention module represents the memory bottleneck of PREDATOR. Furthermore, n cannot be selected freely but results from the interplay of (i) the resolution of the initial voxel grid, (ii) the network architecture (number of strided convolution layers), and (iii) the spatial extent of the scene. Nevertheless, by executing the cross-attention at the superpoint level, with greatly reduced n, we are able to apply PREDATOR to large outdoor scans like odometryKITTI on a single GPU. For even larger scenes, a simple engineering trick could be to split them into parts, as often done for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced PREDATOR, a deep model designed for pairwise registration of low-overlap point clouds. The core of the model is an overlap attention module that enables early information exchange between the point clouds' latent encodings, in order to infer which of their points are likely to lie in their overlap region.</p><p>There are a number of directions in which PREDATOR could be extended. At present it is tightly coupled to fully convolutional point cloud encoders, and relies on having a reasonable number of superpoints in the bottleneck. This could be a limitation in scenarios where the point density is very uneven. It would also be interesting to explore how our overlap-attention module can be integrated into direct point cloud registration methods and other neural architectures that have to handle two inputs with low overlap, e.g. in image matching <ref type="bibr" target="#b35">[36]</ref>. Finally, registration in the lowoverlap regime is challenging and PREDATOR cannot solve all the cases. A user study could provide a better understanding of how PREDATOR compares to human operators. Acknowledgements. This work was sponsored by the NVIDIA GPU grant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this supplementary material, we first provide rigorous definitions of evaluation metrics (Sec. A.1), then describe the data pre-processing step (Sec. A.2), network architectures (Sec. A.4) and training on individual datasets (Sec. A.3) in more detail. We further provide additional results (Sec. A.5), ablation studies (Sec. A.6) as well as a runtime analysis (Sec. A.7). Finally, we show more visualisations on 3DLoMatch and ModelLoNet benchmarks (Sec. A.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Evaluation metrics</head><p>The evaluation metrics, which we use to assess model performance in Sec. 4 of the main paper and Sec. A.5 of this supplementary material, are formally defined as follows:</p><p>Inlier ratio looks at the set of putative correspondences (p, q) ? K ij found by reciprocal matchingin feature space, and measures what fraction of them is "correct", in the sense that they lie within a threshold ? 1 = 10 cm after registering the two scans with the ground truth transformation T Q P :</p><formula xml:id="formula_13">IR = 1 |K ij | (p,q)?Kij ||T Q P (p) ? q|| 2 &lt; ? 1 ,<label>(13)</label></formula><p>with [?] the Iverson bracket. Feature Match recall (FMR) <ref type="bibr" target="#b11">[12]</ref> measures the fraction of point cloud pairs for which, based on the number of inlier correspondences, it is likely that accurate transformation parameters can be recovered with a robust estimator such as RANSAC. Note that FMR only checks whether the inlier ratio is above a threshold ? 2 = 0.05. It does not test if the transformation can actually be determined from those correspondences, which in practice is not always the case, since their geometric configuration may be (nearly) degenerate, e.g., they might lie very close together or along a straight edge. A single pair of point clouds counts as suitable for registration if IR &gt; ? 2 <ref type="bibr" target="#b13">(14)</ref> Registration recall <ref type="bibr" target="#b5">[6]</ref> is the most reliable metric, as it measures end-to-end performance on the actual task of point cloud registration. Specifically, it looks at the set of ground truth correspondences H * ij after applying the estimated transformation T Q P , computes their root mean square error,</p><formula xml:id="formula_14">RMSE = 1 H * ij (p,q)?H * ij ||T Q P (p) ? q|| 2 2 ,<label>(15)</label></formula><p>and checks for what fraction of all point pairs RMSE &lt; 0.2.</p><p>In keeping with the original evaluation script of 3DMatch, * First two authors contributed equally to this work.</p><p>immediately adjacent point clouds are excluded, since they have very high overlap by construction. Chamfer distance measures the quality of registration on synthetic data. We follow <ref type="bibr" target="#b54">[55]</ref> and use the modified Chamfer distance metric:</p><formula xml:id="formula_15">CD(P, Q) = 1 |P| p?P min q?Qraw T Q P (p) ? q 2 2 + 1 |Q| q?Q min p?Praw q ? T Q P (p) 2 2<label>(16)</label></formula><p>where P raw ? R 2048?3 and Q raw ? R 2048?3 are raw source and target point clouds, P ? R 717?3 and Q ? R 717?3 are input source and target point clouds.</p><p>Relative translation and rotation errors (RTE/RRE) measures the deviations from the ground truth pose as:</p><formula xml:id="formula_16">RTE = t ? t 2 RRE = arccos trace(R T R) ? 1 2<label>(17)</label></formula><p>where R and t denote the estimated rotation matrix and translation vector, respectively. Empirical Cumulative Distribution Function (ECDF) measures the distribution of a set of values:</p><formula xml:id="formula_17">ECDF(x) = {o i &lt; x} O<label>(18)</label></formula><p>where O is a set of values(ovelap ratios in our case) and</p><p>x ? [min{O}, max{O}].</p><p>A.2. Dataset preprocessing 3DMatch: <ref type="bibr" target="#b55">[56]</ref> is a collection of 62 scenes, combining earlier data from Analysis-by-Synthesis <ref type="bibr" target="#b43">[44]</ref>, 7Scenes <ref type="bibr" target="#b36">[37]</ref>, SUN3D <ref type="bibr" target="#b50">[51]</ref>, RGB-D Scenes v.2 <ref type="bibr" target="#b24">[25]</ref>, and Halber et al. <ref type="bibr" target="#b20">[21]</ref>. The official benchmark splits the data into 54 scenes for training and 8 for testing. Individual scenes are not only captured in different indoor spaces (e.g., bedrooms, offices, living rooms, restrooms) but also with different depth sensors (e.g., Microsoft Kinect, Structure Sensor, Asus Xtion Pro Live, and Intel RealSense). 3DMatch provides great diversity and allows our model to generalize across different indoor spaces. Individual scenes of 3DMatch are split into point cloud fragments, which are generated by fusing 50 consecutive depth frames using TSDF volumetric fusion <ref type="bibr" target="#b9">[10]</ref>. As a preprocessing step, we apply voxel-grid downsampling to all point clouds, and if multiple points fall into the same voxel, we randomly pick one.</p><p>ModelNet40: For each CAD model of ModelNet40, 2048 points are first generated by uniform sampling and scaled to fit into a unit sphere. Then we follow <ref type="bibr" target="#b54">[55]</ref> to produce partial scans: for source partial point cloud, we uniformly  sample a plane through the origin that splits the unit sphere into two half-spaces, shift that plane along its normal until 2048 ? p v points are on one side, and discard the points on the other side; the target point cloud is generated in the same manner; then the two resulting, partial point clouds are randomly rotated, translated and jittered with Gaussian noise. For the rotation, we sample a random axis and a random angle &lt;45 ? . The translation is sampled in the range [?0.5, 0.5]. Gaussian noise is applied per coordinate with ? = 0.05. Finally, 717 points are randomly sampled from the 2048 ? p v points. odometryKITTI: The dataset was captured using a Velodyne HDL-64 3D laser scanner by driving around the midsize city of Karlsruhe, in rural areas and on highways. The ground truth poses are provided by GPS/IMU system. We follow <ref type="bibr" target="#b2">[3]</ref> to use ICP to reduce the noise in the ground truth poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Implementation and training</head><p>For 3DMatch/Modelnet/KITTI, we train PREDATOR using Stochastic Gradient Descent for 30/ 200/ 150 epochs, with initial learning rate 0.005/ 0.01/ 0.05, momentum 0.98, and weight decay 10 ?6 . The learning rate is exponentially decayed by 0.05 after each epoch. Due to memory constraints we use batch size 1 in all experiments. The datasetdependent hyper-parameters which include number of negative pairs in circle loss n p , temperature factor ?, voxel size V , search radius for positive pair r p , safe radius r s , overlap and matchability radius r o and r m are given in Tab. 6. On odometryKITTI dataset, we take the curriculum learning <ref type="bibr" target="#b3">[4]</ref> strategy to gradually learn sharper local descriptors by adjusting n p . For more details please see our code. <ref type="bibr">3DSN</ref>     <ref type="table">Table 10</ref>: Ablation of the proposed overlap attention module with sparse convolution backbone. FCGF + OA denotes adding proposed overlap attention module to FCGF model. epochs, the results are shown in Tab. <ref type="bibr" target="#b9">10</ref>. It shows that FCGF can also greatly benefit from the overlap attention module. Registration recall almost doubles when sampling only 250 points on the challenging 3DLoMatch benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Timings</head><p>We compare the runtime of PREDATOR with FCGF 8 <ref type="bibr" target="#b8">[9]</ref> and D3Feat 9 <ref type="bibr" target="#b2">[3]</ref> on 3DMatch. For all three methods we set voxel size V = 2.5 cm and batch size 1. The test is run on a single GeForce GTX 1080 Ti with Intel(R) Core(TM) <ref type="bibr" target="#b7">8</ref> All experiments were done with MinkowskiEngine v0.4.2. <ref type="bibr" target="#b8">9</ref> We use its PyTorch implementation.  i7-7700K CPU @ 4.20GHz, 32GB RAM. The most timeconsuming step of our model, and also of D3Feat, is the data loader, as we have to pre-compute the neighborhood indices before the forward pass. With its smaller encoder and decoder, but the additional overlap attention module, PREDATOR is still marginally faster than D3Feat. FCGF has a more efficient data loader that relies on sparse convolution and queries neighbors during the forward pass. See Tab. 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Qualitative visualization</head><p>We show more qualitative results in <ref type="figure" target="#fig_8">Fig. 12</ref> and <ref type="figure" target="#fig_1">Fig. 13  for 3DLoMatch</ref> and ModelLoNet respectively. The input points clouds are rotated and translated here for better visualization of overlap and matchability scores. <ref type="figure">Figure 10</ref>: Feature matching recall in relation to inlier distance threshold ? 1 (left) and inlier ratio threshold ? 2 (right) <ref type="figure">Figure 11</ref>: Network architecture of PREDATOR for 3DMatch (middle) and ModelNet (bottom). In the cross attention module, for each (query s i ? R b?1 , key k i ? R b?1 , value v i ? R b?1 ), denotes first reshape them into shape (4, b 4 )(4 heads), then compute scores matrix S from s i and k i , finally get message update from v i and reshape back to (b, 1).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) Input point clouds b) Inferred overlap region c) Estimated registration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Network architecture of PREDATOR. Voxel-gridded point clouds P and Q are fed to the encoder, which extracts the superpoints P and Q and their latent features X P , X Q . The overlap-attention module updates the features with cocontextual information in a series of self-(GNN) and cross-attention (CA) blocks, and projects them to overlap o P , o Q and cross-overlap? P ,? Q scores. Finally, the decoder transforms the conditioned features and overlap scores to per-point feature descriptors F P , F Q , overlap scores o P , o Q , and matchability scores m P , m Q .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>odometryKITTIFigure 5 :</head><label>5</label><figDesc>a) Input point clouds b) Inferred overlap region c) Estimated registration 3DMatch ModelNet40Example results of PREDATOR that succeeds in attending to the overlap region to enable robust registration.The ground truth label o pi of point p i is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of the relative overlap ratio before and after filtering the points with the inferred overlap scores, 3DLoMatch (left) and 3DMatch (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Top-k (om) sampling yields clustered interest points, whereas the points obtained with prob. (om) sampling are more scattered and thus enable a more robust estimation of the transformation parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3DMatch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>An extreme case where the overlap is insufficient for registration even with the proposed attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Improved relative overlap ratio after filtering the points with the inferred overlap scores on 8862 ModelNet partial scans(left). Owing to the improved overlap ratio, PREDATOR is robust to the changes of partial value p v , while the performance of RPM-Net drops rapidly (right). rand and prob. denote the random and prob. (om) biased sampling of 450 interest points, respectively.ModelNet ModelLoNet Methods RRE RTE CD RRE RTE CD DCP-v2 [46] 11.975 0.171 0.0117 16.501 0.300 0.0268 RPM-Net [55] 1.712 0.018 0.00085 7.342 0.124 0.0050 PREDATOR (rand) 2.407 0.028 0.00120 10.985 0.175 0.0097 PREDATOR (prob. (om)) 1.739 0.019 0.00089 5.235 0.132 0.0083</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Example results on 3DLoMatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Example results on ModelLoNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the 3DMatch and 3DLoMatch datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation of the network architecture. ov. denotes upsampling the overlap scores; cond. denotes conditioning the bottleneck features on the respective other point cloud; ?ov. denotes upsampling the cross overlap scores. We ablate our overlap attention module in Tab. 3. We first compare PREDATOR with a baseline model, in which we completely remove the proposed overlap attention module. That baseline, combined with random sampling, achieves the 2 nd -highest FMR on both benchmarks, but only reaches 82.6%, respectively 38.9% RR. By adding the overlap scores, RR increases by 1.5, respectively 3.9 pp on 3DMatch and 3DLoMatch. Additionally upsampling conditioned feature scores or cross overlap scores further improves performance, especially on 3DLo-Match. All three parts combined lead to the best overall performance. For further ablation studies, see Appendix.</figDesc><table /><note>attempt we replaced the feature extractor of RPM-Net with FCGF. This brought the registration recall on 3DMatch to 54.9%, still far from the 85.1% that FCGF features achieve with RANSAC. We conclude that direct pairwise registra- tion is at this point only suitable for geometrically simple objects in controlled settings like ModelNet40. Ablations study:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters configurations for different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Detailed results on the 3DMatch and 3DLoMatch datasets. 51.6 86.0 74.9 20.4 43.3 96.1 54.0 89.2 75.5 21.9 52.2 96.2 56.7 89.1 78.3 26.1 57.4 96.7 58.0 89.0 78.6 26.7 59.8</figDesc><table><row><cell></cell><cell>3DMatch</cell><cell cols="2">3DLoMatch</cell><cell></cell></row><row><cell>matchability overlap FMR</cell><cell>IR</cell><cell>RR FMR</cell><cell>IR</cell><cell>RR</cell></row><row><cell>96.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Different combinations of scores used for probabilistic sampling. 84.7 83.3 81.6 71.4 40.1 41.7 38.2 35.4 26.8 FCGF+OA 89.1 88.9 88.7 87.5 85.4 57.8 58.3 59.8 58.7 55.9</figDesc><table><row><cell></cell><cell>3DMatch</cell><cell>3DLoMatch</cell></row><row><cell># Samples</cell><cell cols="2">5000 2500 1000 500 250 5000 2500 1000 500 250</cell></row><row><cell></cell><cell>Registration Recall (%)</cell></row><row><cell>FCGF [9]</cell><cell>85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Runtime per fragment pair in milli-seconds, averaged over 1623 test pairs of 3DMatch.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For efficiency, v is in practice determined after voxel-grid downsampling of the two point clouds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We find that the released D3Feat code fails to reproduce the results in the paper, possible due to hyper-parameter changes.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Network architecture</head><p>The detailed network architecture of PREDATOR is depicted in <ref type="figure">Fig. 11</ref>. Our model is built on the KPConv implementation from the D3Feat repository. <ref type="bibr" target="#b6">7</ref> We complement each KPConv layer with instance normalisation Leaky ReLU activations. The l-th strided convolution is applied to a point cloud dowsampled with voxel size 2 l ? V . Upsampling in the decoder is performed by querying the associated feature of the closest point from the previous layer.</p><p>With ?20k points after voxel-grid downsampling, the point clouds in 3DMatch are much denser than those of   <ref type="figure">Fig. 10</ref> shows that our descriptors are robust and perform well over a wide range of thresholds for the allowable inlier distance and the minimum inlier ratio. Notably, PREDATOR consistently outperforms D3Feat that uses a similar KPConv backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Additional ablation studies</head><p>Ablations of matchability score: We find that probabilistic sampling guided by the product of the overlap and matchability scores attains the highest RR. Here we further analyse the impact of each individual component. We first construct a baseline which applies random sampling (rand) over conditioned features, then we sample points with probability proportional to overlap scores (prob. (o)), to matchability scores (prob. (m)), and to the combination of the two scores (prob. (om)).  <ref type="table">449  106  159  182  78  26  234  45  160  128  524  283  222  210  138  42  237  70  191  154</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PointnetLK: Robust &amp; efficient point cloud registration using Pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Least-squares fitting of two 3-d point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="698" to="700" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D3feat: Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIG-GRAPH</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ppfnnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">D2-Net: A trainable CNN for joint detection and description of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multiview 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learned compact local feature descriptor for TLS-based geodetic monitoring of natural outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance evaluation of 3D local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Structured global registration of RGB-D scans in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08539</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rskdd-net: Random sample-based keypoint detector and descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongnan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3DRegNet: A deep neural network for 3d point registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Dias Pais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacinto</forename><forename type="middle">C</forename><surname>Venu Madhav Govindu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06195</idno>
		<title level="m">R2D2: Repeatable and reliable detector and descriptor</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoltan Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In CVPR, 2020. 3, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="876" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">KPconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PRNet: Self-supervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08480</idno>
		<title level="m">D2D: Learning to find good correspondences for image matching and manipulation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Extreme relative pose estimation for rgb-d scans via scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Jeffrey Z Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extreme relative pose network under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3dfeat-net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="630" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">RPM-Net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3DMatch: learning local geometric descriptors from RGB-D reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Relative Rotation Error ( ? ) ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
