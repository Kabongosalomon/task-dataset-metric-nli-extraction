<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lane Detection with Versatile AtrousFormer and Local Semantic Guidance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-08">8 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lane Detection with Versatile AtrousFormer and Local Semantic Guidance</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-08">8 Mar 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition March 9, 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lane Detection</term>
					<term>Local AtrousFormer</term>
					<term>Global AtrousFormer</term>
					<term>Local Semantic Guided Decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based tech-* Corresponding author * * ing point of each lane serves to guide the process. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>niques. A few have a try on incorporating the recent adorable, the seq2seq</head><p>Transformer <ref type="bibr" target="#b0">[1]</ref>. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately, in which the predicted Gaussian map of the start-  <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, in these methods, each lane is regarded as a semantic class. They really rely on a strong feature to infer the ideal shapes. Another group of them draw on the recent achievements in object detection. They <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> are developed towards two directions:</p><p>anchor-based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and anchor-free <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The anchor-based ones use beaming lines starting from either left, right, or bottom as anchor baseline, and then adjust offset to fit the real shape of lane line via a post-processing step. They heavily rely on pre-defined straight lines, thus struggling to handle more complex line shapes. As for those anchor-free, they equate lane detection to high-order polynomial regression, straightforward yet overly relying on certain parameters. The last main group <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, inspired by the fancy thoughts from human pose estimation, usually extract key points with lane semantic and then cluster them into different lane instances via complex post-processing methods. In general, methods other than sementic segmentation have difficulties in modeling more complex lane forms, like those described in BDD100K <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this paper, we walk further along the way of semantic segmentation based lane detection. Hereafter we get a closer look on its members. Early of them such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13]</ref> have a CNN-based encoder, a plain upsampling decoder, and a binary classifier. However, their quantitative and visualization results indicate that the segmented lanes are dragged down by low-quality feature map. The problem is generated due to their ignorance towards the spatial cues of lanes and incapability of conjecturing according to the environment information. To overcome, recent explorers such as SCNN <ref type="bibr" target="#b13">[14]</ref> and RESA <ref type="bibr" target="#b17">[18]</ref> propagate information between near or remote feature slices in four directions (upwards, downwards, rightwards, and leftwards). However, recurrent reinforcement manner does not treat features equally and therefore does not perform well (similar to LSTM). More advanced way still needs exploring. Another problem haunting the prediction of lane instances is their weak discrimination ability, that is to say, the classifiers feeding on global representation blunder in predicting the existence of lanes.</p><p>Instead of focusing on the temporally-recurrent CNN-based techniques, in this paper, we resort to designing Atrous Transformer (AtrousFormer). In the same spirit as ASPP <ref type="bibr" target="#b27">[28]</ref>, the AtrousFormer instills atrous experience to the transformer structure <ref type="bibr" target="#b0">[1]</ref> by following a two stage strategy. It collects information first along rows and then along columns. Compared to <ref type="bibr" target="#b0">[1]</ref>, our designing can save lots of computation cost (has reduced the number of key&amp;value entities) and improve performance, simultaneously. Note that AtrousFormer has two forms, namely global AtrousFormer and local Atrous-Former. The global one is installed on top of the feature extractor to enhance global representation. The local one is used to implement early extracting enhancement, in a local manner. We also propose Local Semantic Guided Decoder (LSGD), in which the Gaussian map of the starting point of each lane is employed as the attention map to guide classification process (the distance of each other in this position is long enough). Our contributions can be boiled down into the following points:</p><p>? We propose global AtrousFormer to collect information in an atrous yet global way, instilling the essence of the ASPP to the transformer structure and finally enhancing the ability to infer lane distribution.</p><p>? Global AtrousFormer is evolved into a more compact version, local AtrousFormer, by incorporating slice mechanism. Then it is early embedded into feature extractor like ResNet-18 to enhance feature extraction.</p><p>? We propose LSGD to obtain more representative feature vector, which then is used to better the lane existence prediction.</p><p>? Our network achieves favorable results against other state of the arts on the recent challenging datasets, achieving 78.08 F1 score on CULane <ref type="bibr" target="#b13">[14]</ref> and 96.71 Accuracy on TuSimple <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This section will discuss first the deep learning based lane detection methods. In general, the networks to conduct lane detection can be categorized into three classes, semantic segmentation based methods, detection based methods, and key point estimation based methods. Finally, the development of transformer in vision community is sketched.</p><p>Semantic Segmentation Based Methods.In this kind of doings, lane detection is modeled as a per-pixel prediction task with an additional classification branch <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. To improve performance, early they devise various enhancing mechanisms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref> and recently resort to feature aggregator modules <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. The <ref type="bibr" target="#b14">[15]</ref> uses the method of self-distillation to mine spatial properties of lanes. The <ref type="bibr" target="#b11">[12]</ref> designs a new powerful backbone for lane detection. The authors in <ref type="bibr" target="#b13">[14]</ref> use a in-layer, recurrent in four directions, and residual information passing mechanism to enhance the spatial feature representation, finally accommodating the thin and long lane shapes. Later, <ref type="bibr" target="#b17">[18]</ref> mitigates these problems in a sparse gleaning way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Based Methods. Motivated by the progresses in recent</head><p>object detection based methods, lane detection research in this direction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> can be classified as anchor-based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and anchor-free <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> use beaming lines from either the bottom, left, right to propose primary candidates, and then predicts offset maps to fit the real line shape. However, they are struggling to free themselves from the constraints of anchors. In <ref type="bibr" target="#b19">[20]</ref>, local cues are added to refine the problem. The works of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> abandon this kind of doing and directly assume that lanes in the scenes can be modeled as a polynomial equation estimation problem.</p><p>Although refreshing, they are infeasible to handle the complex scenes, due to overly relying on certain parameters.  26] usually determines the sparse or dense key points belonging to lanes first via a branch, and simultaneously in other branches predicts their identities to cluster them into different instances. In <ref type="bibr" target="#b23">[24]</ref>, the authors design a branched, multi-task network, including a binary segmentation branch and a identity embedding branch, to clustering key points into different lane instances in an end to end manner. However, in most cases it is suffered from occlusion problem. Therefore, <ref type="bibr" target="#b23">[24]</ref> stack several hourglass modules to enhance the inference ability of the network to alleviate the problem. Recently, <ref type="bibr" target="#b24">[25]</ref> proposes to use two affinity matrices to cluster the determined key points to further resolve the problem. The Vision Transformer <ref type="bibr" target="#b29">[30]</ref> is the primary one applying transformer structure on non-overlapping medium-size feature patches for image classification, to some degree achieving speed-accuracy balance. Later, one of its followups swin transformer <ref type="bibr" target="#b30">[31]</ref> uses window and shifting mechanisms at different scales, further alleviating the overdue computational problem and achieving good performance in different tasks. Others like <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> tinker it in various ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we show the overall architecture of our network (see <ref type="figure" target="#fig_2">Fig.2</ref>), the designed global AtrousFormer, local AtrousFormer enhanced extractor and Local Semantic Guided Decoder (LSGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>We first extract feature using local AtrousFormer enhanced extractor (enhanced ResNet-18, ResNet-34, etc). The max poolings in the third and fourth stages are replaced with dilations. After the raw image passes through the extractor, a feature with spatial size of 1/8 original image is generated. Its channel dimension followingly is compressed to C cmpr by one 1 ? 1 convolution, denoted as F of size H ? W ? C cmpr . We then send F to global AtrousFormer to further enhance the semantic representation, and the enhanced feature is finally fed to the decoder LSGD. The decoder consists of a segmentation branch and a classification branch. In the segmentation branch, the spatially enhanced feature is directly recovered to the original size by 8? bilinear upsampling. And in the sequel, the product is processed by a convolution manipulation to generate the final segmentation maps (the maps later are used to predict the distribution of lane instances). In the classification branch, we primarily get the Gaussian map of the starting point of each lane. Then the map is used to do spatial attention on the enhanced feature. Finally we obtain representative feature vector of each lane to clarify the existence of lanes. In the following context, the global AtrousFormer is first introduced for narrating consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global AtrousFormer</head><p>Let us at the beginning have a retrospect on the seq2seq Transformer. It fuses the thoughts of position-aware embedding, multi-head self-and crossattention mechanism, and residual connection, making the extracted features more expressive. However, in lane detection, lane itself is slender, and its spreading direction in either the real world or on the image plane follows certain geometric and human-setting rules. The ignorance of the seq2seq Transformer towards this observation not only causes computational overhead to surge, but also introduces more noises.</p><p>The global AtrousFormer imparts the prior knowledge to the seq2seq Transformer, decomposing the sampling process into a two-stage atrous form.</p><p>The process of the first stage is formulated in the next. The feature F extracted by the backbone is embedded into three tensors:</p><formula xml:id="formula_0">Q global = ConvQ 1?1 (F + P), E key = ConvK 1?1 (F + P), E val = ConvV 1?1 (F).</formula><p>(1) In <ref type="formula">(1)</ref>, ConvQ 1?1 refers to 1?1 convolution manipulation with stride and padding size equivalent to 1 and 0, respectively. Its output Q global is a tensor of size H ? W ? C cmpr and can be directly used as the query of the global AtrousFormer. The other two operations ConvK 1?1 and ConvV 1?1 denote 1 ? 1 convolutions with stride of 1 and padding size of H ? 0 (row and column wise paddings). Note that the padding serves only as placeholder that helps the generation of affinity matrices. Both E key and E val are of the same size 3H ? W ? C cmpr , serving to generate the final key K global and value V global .</p><p>The positional encoding tensor P of size H ? W ? C cmpr is generated using the same method in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Next we introduce the production of K global and V val . In global Atrous-Former, members of Q global in the ith row will query information from those that are not only in the ith row, but also those above and below the ith row in the distances of {d j } j?? (see <ref type="figure" target="#fig_4">Fig.3</ref>):</p><formula xml:id="formula_1">d j = H/2 (J?j ) , j = 0, 1, ..., J ? 1,<label>(2)</label></formula><p>where J is a hyper-parameter specifying the sampling density and d j ? 1.</p><p>The ? rounds down a float number to its closest integer. The K global therefore can be generated using the following pattern:</p><formula xml:id="formula_2">K global = Cat(E key [H : 2H, :, :], E key [H ? d 0 : 2H ? d 0 , :, :]</formula><p>, ...,</p><formula xml:id="formula_3">E key [H ? d J?1 : 2H ? d J?1 , :, :]).<label>(3)</label></formula><p>In <ref type="formula" target="#formula_3">(3)</ref>, we resort to python conservation word a : b to denote index range from a to b with the default interval of 1. The Cat concatenates the tensors along the second dimension. The output K global is of size H ? (2J + 1)W ? C cmpr . By replacing E key with E value in (3), we will get V global of the same size as K global . The heuristic experience is very effective considering the disconnected, thin, and forward-spreading distribution properties of lane instances. More members in the neighboring region are queried than those in the remote area.</p><p>Split Q global , K global , and V global into N heads heads along the channel dimension, each of which has C cmpr /N heads feature maps. As such, the affinity matrices can be computed as follows:</p><formula xml:id="formula_4">A global = Sof tmax(Q global @P erm(K global )) C cmpr /N heads ,<label>(4)</label></formula><p>in which P erm means to exchange the index order of the second and third dimension, Sof tmax represents the row-wise normalization, and the symbol @ denotes matrix multiplication. The produced affinity matrices A global is of size N heads ? H ? W ? (2J + 1)W. Similar to the seq2seq Transformer, we get the final output by using the following manipulations:</p><formula xml:id="formula_5">I global = N orm(F + Recover(A global @V global ), F global = N orm(I global + M LP (I global )).<label>(5)</label></formula><p>In <ref type="formula" target="#formula_5">(5)</ref>, N orm and M LP refer to layer normalization and multiple layer perceptrons, respectively. The Recover consisting of consecutive permute and reshape manipulations is used to recover the multi-head feature into the original size. As of now, how members in each row glean information by the proposed atrous way is introduced. The second stage is quite similar to the first stage, first exchanging the row and column indices of F global , then passing it through the equations from (1) to <ref type="bibr" target="#b4">(5)</ref>, and finally producing the global AtrousFormer enhanced feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local AtrousFormer for Early Enhancement</head><p>We further consider interleaving the global AtrousFormer into the early feature extraction stage to enhance the inference ability of our network.</p><p>However, directly inserting the global AtrousFormer into different stages of ResNet will be costly in terms of hardware and computation overhead.</p><p>Instead we develop local AtrousFormer by introducing rectangular box restriction to the global AtrousFormer, but in the meanwhile take the spatial distribution attributes of lanes into account.</p><p>We apply the local AtrousFormer to the second, third and fourth stages,  production to fuse to go to the next stage. This kind of multi-scale fusion objectively strengthens the local information communication, just like the window shifting mechanism in <ref type="bibr" target="#b30">[31]</ref>.</p><p>Next we give the details of local AtrousFormer in the ith stage (i ? {2, 3, 4}), which also has gleaning information process of two stages. During the first stage we split the primary feature into S col slices along column, and do the sparse attention only within the slice.  <ref type="formula" target="#formula_1">(2)</ref> is still adopted in the similar pattern. The key tensor can be generated (see <ref type="figure" target="#fig_6">Fig.4</ref>) as follows: , ...,</p><formula xml:id="formula_6">K local i = Cat(E key</formula><formula xml:id="formula_7">E key i [H ? d J?1 : 2H ? d J?1 , :, :, :]),<label>(6)</label></formula><p>where the operation of Cat will concatenate the tensors along the third dimension, and other manipulations are similar to those previously introduced.</p><formula xml:id="formula_8">The output K local i is of size H ? S col ? (2J + 1))W/S col ? C stage i . The value tensor V local i of size H ? S col ? (2J + 1))W/S col ? C stage i also can be produced by replacing E key i with E val i in (6).</formula><p>Split features into N heads i heads and the affinity matrices for the local AtrousFormer can be computed in the following way:</p><formula xml:id="formula_9">A local i = Sof tmax(Q local i @P erm(K local i )) C stage i /N heads i ,<label>(7)</label></formula><p>in which the P erm manipulation is used to exchange the third and fourth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Local Semantic Guided Decoder</head><p>The methods based on semantic segmentation tend to directly apply fully connected layers coupled with P ooling to clarify the existence of each lane in one branch, and Conv2D layers to segment the shapes in the other branch.</p><p>Both of them are on top of the semantically enhanced feature aggregator.</p><p>Although the designed feature aggregator is strong enough to handle the segmentation subtask, they ignore the negative influence on the current existence prediction from that of others. Differently from them, we adopt predicting the Gaussian map of the starting point of each lane to guide the classification process, which is named as Local Semantic Guided Decoder (LSGD).</p><p>Next we will introduce the details of LSGD. In lane detection, the number of lanes is a prior knowledge specified by the annotation rules, just like 4 in CULane and 6 in TuSimple. Therefore, LSGD is able to predict the Gaussian map of the starting point of each lane as follows: </p><formula xml:id="formula_10">G map = Sig((Conv(M axP ool 2d (F global )))),<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To verify the effectiveness of our proposed methods, we demonstrate their respective performance on three currently wide-used datasets, CULane <ref type="bibr" target="#b13">[14]</ref>, TuSimple <ref type="bibr" target="#b28">[29]</ref>, and BDD100 <ref type="bibr" target="#b26">[27]</ref>. They are challenging and encompass a variety of scenarios.</p><p>To be specific, CULane has 55 hours video clips, and consists of nine road conditions: normal, crowd, curve, dazzle night, night, no line, and arrow. The BDD100K originally designed for lane classification has binary annotation for lane semantics (provided by <ref type="bibr" target="#b14">[15]</ref>). The lanes are close to each other, and also embody in different forms, thus challenging to current algorithms. Different from CULane and TuSimple, the width of each lane in the training set is set to 8, while in the testing set it is set to 2. Following <ref type="bibr" target="#b14">[15]</ref>, we use the training set of 80000 frames of resolution 720 ? 1680 to train our model, and validation set of 10000 frames of the same resolution to test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We resize the original images to 288 ? 800 for CULane and 368 ? 640 for TuSimple, respectively. Furthermore, our training involves the following techniques of data augmentation: random scaling, cropping, random rotation, color jittering, and etc. The optimizer uses SGD with momentum 0.9 and weight decay 1e-4 to train our model. The learning rate is set to 2.5e-2 for CULane and 2.0e-2 for TuSimple, respectively. Warming-up strategy is used in the first 500 batches. Polynomial learning rate decay policy with power set to 0.9. The loss function is the summation of the focal loss multiplied by 0.2 and the one in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, which consists of segmentation BCE respectively. The F1-measure is adopted by CULane to evaluate the results, as follows:</p><formula xml:id="formula_11">F 1 = 2 ? Precision ? Recall Precision + Recall ,<label>(10)</label></formula><p>where Precision = TP/(TP + FP) and Recall = TP/(TP + FN).</p><p>The TuSimple evaluates the results by Accuracy, defined as follows:</p><formula xml:id="formula_12">Accuracy = clip C clip S clip ,<label>(11)</label></formula><p>in which C clip represents the number of correctly predicted lane points. To be specific, they are assigned the right identities. Their distances with the corresponding ground truth points are within certain range in a clip. The S clip denotes the number of ground truth points in a clip.</p><p>For BDD100K, following <ref type="bibr" target="#b14">[15]</ref>, we use per-pixel accuracy and mean IOU to evaluate the performance of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the SOTAs</head><p>In this section, we will demonstrate the performance of our network on CULane, TuSimple, and BDD100K, respectively. For CULane, the quantitative and visualization results under nine scenarios are showed in Tab.1 and SpinNet <ref type="bibr" target="#b35">[36]</ref>, UFAST <ref type="bibr" target="#b36">[37]</ref>, R18-E2E <ref type="bibr" target="#b37">[38]</ref>, R34-E2E <ref type="bibr" target="#b37">[38]</ref>, ERFNet-E2E <ref type="bibr" target="#b37">[38]</ref>, SCNN <ref type="bibr" target="#b13">[14]</ref>, ENet-SAD <ref type="bibr" target="#b14">[15]</ref>, ERFNet-IntRA-KD <ref type="bibr" target="#b39">[40]</ref>, PINet <ref type="bibr" target="#b23">[24]</ref>, Curvelanes-NAS <ref type="bibr" target="#b25">[26]</ref>, RESA <ref type="bibr" target="#b17">[18]</ref>, LaneATT <ref type="bibr" target="#b19">[20]</ref>, and SIM-CycleGan <ref type="bibr" target="#b12">[13]</ref>. All of the results are borrowed from their official publishings.      Selectrions of Atrous Experience and Slice Size. To avoid the laboriousness of super-parameter searching, we at first accept the in-layer fusion experience in <ref type="bibr" target="#b17">[18]</ref> to bootstrap, namely J = 4, tuning XR18-Ours in a greedy manner. Then we study the impact of slice size of local Atrous-Former on XR18-Ours and report the results in Tab.5. It turns out that when slice size is set to <ref type="bibr" target="#b17">(18,</ref><ref type="bibr" target="#b19">20)</ref>, the network gets the best performance. The results confirm that probing lane distribution needs big enough boxes. Then turning up or down J, we find that when J = 4, our network gets the best performance. Along the decreasing of sampling density, the performance of network deteriorates significantly. Related results is listed in Tab.6.</p><p>Compared to the Seq2Seq Transformer. In Tab.7, we provide comparison results of the global AtrousFormer and the seq2seq Transformer. All  are conducted on ResNet-18+LSGD by controlling the number of heads. It can be seen that compared to the seq2seq Transformer, in terms of the computation of affinity matrices and F1, the AtrousFormer is more efficient than the seq2seq Transformer given the sparsity of its queried positions. The number comparison is also listed (abbreviated as K&amp;V Pos. in the last column).</p><p>Effectiveness of the Two-stage Strategy. We validate the effectiveness of the two-stage strategy in the last two rows in Tab.7. The symbols of Formers, respectively. Individually using them will achieve 76.84 and 76.27</p><p>points of the F1 score, respectively. The fully global AtrousFormer surpasses the only row-wise one and only column-wise one by 0.35 and 0.98 point, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we design AtrousFormer to enhance network inference ability and mine the spatial distribution of lanes. The proposed AtrousFormer mainly has two representative forms, i.e., global AtrousFormer and local AtrousFormer, both of which glean information first by rows and then by columns according to the specified heuristic experience. In this paper, the former is installed on top of the extractor to enhance the global representation of the extracted feature. The latter one has a more compact form, and is inserted into the feature extractor to strengthen extraction. In addition, we design local semantic guided decoder to incorporate the local information into classification stage. Our network achieves impressive performance on CULane, TuSimple and BDD100K. It is able to handle complex situations like occullusion by other vehicles, bad weather conditions, abrasion, terrible illumination intensity, and compleax lane forms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of lanes in bad cases: Occlusion, Crowded, High Light, and Arrow Shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Key</head><label></label><figDesc>Point Estimation Based Methods. This approach[23, 24, 25,   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the architecture. It passes the raw image through local Atrous-Former Enhanced extractor, one 1 ? 1 Convolution, Global AtrousFormer, and Local Semantic Guided Decoder in sequence, finally generating segmentation maps and the corresponding classification scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Vision Transformers.</head><label></label><figDesc>The seq2seq Transformer<ref type="bibr" target="#b0">[1]</ref> is primarily proposed to resolve the problems long haunting around LSTM and various its posteriors, such as long-range information losing, weak semantic represen-tation, and sequential inflexibility. Recently, researchers begin introducing it into vision community to enhance the feature representation of backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The positions of query entities (in purple boxes) and key&amp;value entities (in both blue and purple boxes) in global AtrousFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>all of which generate representative features of size 1/8 original image (replacing the max poolings in the third and fourth stages with dilated convolution of atrous rates of 2 and 4, respectively). The unenhanced feature after each stage is sent to the local AtrousFormer, and then concatenated with the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The positions of query entities (in purple boxes) and key&amp;value entities (in both blue and purple boxes) in local AtrousFormer. The gleaning process happens within each slice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>d 0 : 2H ? d 0 , :, :, :]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>dimension of K local i .</head><label>i</label><figDesc>The output A local i is of size N heads i ? H ? S col ? W/S col ? (2J + 1)W/S col . Applying equation (5), we will get the locally row-wise enhanced feature. The second stage of the local AtrousFormer is first to transpose the row and column indices of input and the rest process is similar to what we have introduced in the end of the last subsection. Compared to attention mechanism in seq2seq Transformer, whose computation of self-attention involves (HW) 2 dot products, the computation in global AtrousFormer and local AtrousFormer have (HW) * (2J + 1)(H + W) and (HW) * (2J + 1)(H/S row + W/S col ), respectively. The S row denotes the slice number along row wise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>=</head><label></label><figDesc>in which Conv of consists of two consecutive 1 stride convolutions with kernels of size 3 ? 3 ? C cmpr ? C cmpr and 1 ? 1 ? C cmpr ? N lanes (number of lanes), respectively, M axP ool 2d represents 2? maxpooling, and Sig represents sigmoid manipulation. The output of size N lanes ? H/2 ? W/2 is supervised by the focal loss in<ref type="bibr" target="#b33">[34]</ref>. The ith Gaussian map G map[i,:,:] serves as the attention map to guide the classification process of the ith lane: U p 2 (G map [i,:,:] ) ? F global , where U p 2 is bilinear interpolation of 2? upsampling; M axP ool and AvgP ool refer to the max and average poolings, respectively; the M LP is three fully connected layers with hidden dimension as 64 and 16. The segmentation maps can be obtained by passing F global through a 8? bilinear interpolation and a convolution manipulation with stride 1 and kernel size 3 ? 3 ? C cmpr ? N lanes .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Most of the frames within are shot in the urban, and a few are in the rural or highway. In total, CULane has 133235 frames of size 590 ? 1640, of which 88880 are used for training, 9675 are used to validate, and the left are used to benchmark the performance of the model. The number of lanes in each frame is at most 4. The samples in TuSimple are shot under stable light situation by vehicles in highways. Compared to CULane, the dataset is relatively small. In total, it has 6408 frames. Of them, 3236 are used for training, 358 are used for validation, and 2782 are used to test. All are of size 720 ? 1280, and each frame contains at most 5 lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>loss and existence classification BCE loss. The batch size is set to 8 for CULane and 4 for TuSimple. The number of training epoch is set to 12 for CULane and 80 for TuSimple. All models are trained on 3 NVIDIA 2080Ti GPUs with Pytorch framework. For local AtrousFormer enhanced backbone, we set N heads in the second, third, and fourth stages to 2, 8, and 16, respectively, let J equal to 4, and set slice size to (18, 20) and (23, 20) on CULane and TuSimple, respectively. The feature extracted from the backbone are compressed to 128 using 1 ? 1 convolution. For global AtrousFormer, we set J to 4 and N heads to 16. The threshold values are set to 0.5 in both the classification branch and segmentation branch. The settings for BDD100K is slightly different from what we have adpted in CULane. Following [15], we resize the training image to 360?720 in training and use the unresized image to test. During the experiments on BDD100K, the branch of local semantic guidance is removed. For CULane, lanes are treated as a line of 30-pixel-width. The intersectionover-union (IOU) of the predictions and groundtruth lanes is used. In general, the predicted lanes that have a IOU with their corresponding targets larger or equivalent to the threshold value 0.5 are defined as the true positives (TP). The FP and FN are used to denote false positives and false negatives,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 ,</head><label>5</label><figDesc>respectively. The quantitative results for TuSimple and BDD100K are listed in Tab.2 and Tab.3. Algorithms to compare include FastDraw [35],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of our network versus other state of the arts under nine situations of CULane test set against the F1 metric. The Cross only reports FP. Note that red, blue, and green colors represents the highest, second, and third scores, respectively.The symbol of '-' represents the results are not reported officially.</figDesc><table><row><cell>Methods</cell><cell cols="11">Normal ? Crowded ? Dazzle ? Shadow? NoLine ? Arrow? Curve? Cross? Night? Total? Speed (ms)</cell></row><row><cell>Fast Draw [35]</cell><cell>85.90</cell><cell>63.60</cell><cell>57.00</cell><cell>59.90</cell><cell cols="5">40.60 79.40 65.20 7013 57.80</cell><cell>-</cell><cell>11</cell></row><row><cell>SpinNet [36]</cell><cell>90.50</cell><cell>71.70</cell><cell>62.00</cell><cell>72.90</cell><cell cols="3">43.20 85.00 50.70</cell><cell>-</cell><cell cols="2">68.10 74.20</cell><cell>-</cell></row><row><cell>R18-UFAST [37]</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell cols="6">40.20 81.00 57.90 1743 62.10 68.40</cell><cell>3</cell></row><row><cell>R34-UFAST [37]</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell cols="6">44.40 85.70 69.50 2037 66.70 72.30</cell><cell>6</cell></row><row><cell>R18-E2E [38]</cell><cell>90.00</cell><cell>69.70</cell><cell>60.20</cell><cell>62.50</cell><cell cols="6">43.20 83.20 70.30 2296 63.30 70.80</cell><cell>-</cell></row><row><cell>R34-E2E [38]</cell><cell>90.40</cell><cell>69.90</cell><cell>61.50</cell><cell>68.10</cell><cell cols="6">45.00 83.70 69.80 2077 63.20 71.50</cell><cell>-</cell></row><row><cell>ERFNet-E2E [38]</cell><cell>91.00</cell><cell>73.10</cell><cell>64.50</cell><cell>74.10</cell><cell cols="6">46.60 85.80 71.90 2022 67.90 74.00</cell><cell>-</cell></row><row><cell>R34-SCNN [14]</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell cols="6">43.40 84.10 64.40 1990 66.10 71.60</cell><cell>116</cell></row><row><cell>ERFNet-SAD [15]</cell><cell>90.10</cell><cell>68.80</cell><cell>60.20</cell><cell>65.90</cell><cell cols="6">41.60 84.00 65.70 1998 66.00 70.80</cell><cell>10</cell></row><row><cell>ERFNet-IntRA-KD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.40</cell><cell>-</cell></row><row><cell>PINet [24]</cell><cell>90.30</cell><cell>72.30</cell><cell>66.30</cell><cell>68.40</cell><cell>49.8</cell><cell cols="5">83.70 65.60 1427 67.70 74.40</cell><cell>40</cell></row><row><cell>SIM-CycleGAN[13]</cell><cell>91.80</cell><cell>71.80</cell><cell>66.40</cell><cell>76.20</cell><cell cols="6">46.10 87.80 67.10 2346 69.40 73.90</cell><cell>-</cell></row><row><cell>CurveLanes-NAS-S [26]</cell><cell>88.30</cell><cell>68.60</cell><cell>63.20</cell><cell>68.00</cell><cell cols="6">47.90 82.50 66.00 2817 66.20 71.40</cell><cell>-</cell></row><row><cell cols="2">CurveLanes-NAS-M [26] 90.20</cell><cell>70.50</cell><cell>65.90</cell><cell>69.30</cell><cell cols="6">48.80 85.70 67.50 2359 68.20 73.50</cell><cell>-</cell></row><row><cell>R34-RESA [18]</cell><cell>91.90</cell><cell>72.40</cell><cell>66.50</cell><cell>72.00</cell><cell cols="6">46.30 88.10 68.60 1896 69.80 74.50</cell><cell>22</cell></row><row><cell>R18-LaneATT [20]</cell><cell>91.11</cell><cell>72.96</cell><cell>65.72</cell><cell>70.91</cell><cell cols="6">48.35 85.49 63.37 1170 68.95 75.09</cell><cell>4</cell></row><row><cell>R34-LaneATT [20]</cell><cell>92.14</cell><cell>75.03</cell><cell cols="2">66.47 78.15</cell><cell cols="6">49.39 88.38 67.72 1330 70.72 76.68</cell><cell>6</cell></row><row><cell>R122-LaneATT [20]</cell><cell>91.74</cell><cell>76.16</cell><cell cols="8">69.47 76.31 50.46 86.29 68.40 1746 68.90 77.02</cell><cell>45</cell></row><row><cell>R18-Light</cell><cell>92.77</cell><cell>74.69</cell><cell>66.89</cell><cell>69.68</cell><cell cols="6">49.25 88.09 70.59 1096 72.85 77.03</cell><cell>20</cell></row><row><cell>XR18-Ours</cell><cell>92.72</cell><cell>75.56</cell><cell cols="2">68.16 73.67</cell><cell cols="6">50.07 88.82 70.32 1169 73.49 77.63</cell><cell>36</cell></row><row><cell>XR34-Ours</cell><cell>92.83</cell><cell>75.96</cell><cell cols="8">69.48 77.86 50.15 88.66 71.14 1054 73.74 78.08</cell><cell>44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison results with other state of the arts on the test set of TuSimple in terms of Accuracy, FP, and FN metrics.</figDesc><table><row><cell>Methods</cell><cell>Accuracy ?</cell><cell>FP?</cell><cell>FN?</cell></row><row><cell>SCNN [14]</cell><cell>96.53</cell><cell>6.17</cell><cell>1.80</cell></row><row><cell>EL-GAN [17]</cell><cell>94.90</cell><cell>4.12</cell><cell>3.36</cell></row><row><cell>PINet [24]</cell><cell>96.70</cell><cell>2.94</cell><cell>2.63</cell></row><row><cell>ENet-SAD [15]</cell><cell>96.64</cell><cell>6.02</cell><cell>2.05</cell></row><row><cell>ERF-E2E [38]</cell><cell>96.02</cell><cell>3.21</cell><cell>4.28</cell></row><row><cell>FastDraw [35]</cell><cell>95.20</cell><cell>7.60</cell><cell>4.50</cell></row><row><cell>R18-UFAST [37]</cell><cell>95.82</cell><cell cols="2">19.05 3.92</cell></row><row><cell>R34-UFAST [37]</cell><cell>95.86</cell><cell cols="2">18.91 3.75</cell></row><row><cell>PolyLaneNet [21]</cell><cell>93.36</cell><cell>9.42</cell><cell>9.33</cell></row><row><cell>LSTR [22]</cell><cell>96.18</cell><cell>2.91</cell><cell>3.38</cell></row><row><cell>R18-RESA [18]</cell><cell>96.82</cell><cell>3.95</cell><cell>2.83</cell></row><row><cell>R34-RESA [18]</cell><cell>96.70</cell><cell>3.96</cell><cell>2.48</cell></row><row><cell>R18-LaneATT [20]</cell><cell>95.57</cell><cell>3.56</cell><cell>3.01</cell></row><row><cell>R34-LaneATT [20]</cell><cell>95.63</cell><cell>3.53</cell><cell>2.92</cell></row><row><cell>XR18-Ours</cell><cell>96.59</cell><cell>2.83</cell><cell>3.26</cell></row><row><cell>XR34-Ours</cell><cell>96.71</cell><cell>2.82</cell><cell>3.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison results with other state of the arts on the test set of BDD100K in terms of average per-pexel accuracy and IOU of lane semantic of each frame. It is able to complete the lane instance according to the environment, even in the dark situation (see the ninth image). In terms of running time, R18-Light tops SCNN and RESA by 96 ms and 2 ms, respectively, while keeping satisfying performance. The XR34-Ours is approximately 3? faster than our predecessor SCNN on ResNet34. TuSimple Results Analysis. The quantitative results on TuSimple in Tab.2 to validate the performance of our network. It can be seen that our network obtains an accuracy of 96.59 and 96.71 on ResNet-18 and ResNet-34, respectively. The results are very approximate to the highest scores achieved by RESA. Under metric FP, our method gets the best performance of 2.82. BDD100K Results Analysis. On BDD100k, only some of the semantic segmentation methods can be reported due to the constraints of complex lane forms in Tab.<ref type="bibr" target="#b2">3</ref>. This is also one of the most important merits of our AtrousFormer, able to model more complex lanes. The results are showed in Tab3. It can be seen that XR18-Ours surpasses ERFNet-SAD by 22.53 points and 6.79 points in terms of accuracy and IOU metrics, respectively.4.4. Ablation StudiesEffectiveness of Main Contributions. To validate the effectiveness of the local AtrousFormer enhanced encoder, the global AtrousFormer, and LSGD, we provide detailed ablation studies on CULane in Tab.4. We stipulate ResNet-18+LSGD (without local attention guidance) as the baseline.The segmentation maps and their corresponding existence scores can be obtained after passing the parallel branches of 8? U pSampling&amp;Conv2D and 8? P ooling&amp;M LP . In Tab.4, the A.F. and XR are used to represent AtrousFormer and locally enhanced ResNet, respectively. We next give some analysis on the adopted techniques.It can be seen that AtrousFormer is very effective on improving the performance of lane detection. Compared to baseline, equiping global Atrous-Former results in 5.57 points of F1 gains on ResNet-18. The designed decoder LSGD coupled with global AtrousFormer surpasses the global Atrous-Former+LSGD without attention mechanism by 2.02 points in terms of F1 metric, which proves the effectiveness of the local attention guidance. Im-</figDesc><table><row><cell>Methods</cell><cell cols="2">Accuracy ? IOU?</cell></row><row><cell>ResNet18 [39]</cell><cell>30.66</cell><cell>11.07</cell></row><row><cell>ResNet34 [39]</cell><cell>30.92</cell><cell>12.24</cell></row><row><cell>SCNN [14]</cell><cell>35.79</cell><cell>15.04</cell></row><row><cell>R18-SAD [15]</cell><cell>31.10</cell><cell>13.29</cell></row><row><cell>R34-SAD [15]</cell><cell>32.68</cell><cell>14.56</cell></row><row><cell>ERFNet-SAD [15]</cell><cell>36.56</cell><cell>16.02</cell></row><row><cell>XR18-Ours</cell><cell>59.09</cell><cell>22.81</cell></row><row><cell>XR34-Ours</cell><cell>59.20</cell><cell>23.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the test set of CULane against F1, Precision, and Recall metrics.</figDesc><table><row><cell>Methods</cell><cell cols="3">F1? Precision? Recall?</cell></row><row><cell>Baseline</cell><cell>69.60</cell><cell>70.02</cell><cell>69.60</cell></row><row><cell>+Global A.F.</cell><cell>75.17</cell><cell>76.66</cell><cell>73.74</cell></row><row><cell>+Global A.F.+LSGD</cell><cell>77.19</cell><cell>82.56</cell><cell>72.49</cell></row><row><cell cols="2">XR18+global A.F.+LSGD 77.63</cell><cell>83.73</cell><cell>72.36</cell></row><row><cell cols="2">XR34+global A.F.+LSGD 78.08</cell><cell>84.36</cell><cell>72.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The impact of slice size on XR18-Ours.</figDesc><table><row><cell>Slice Size</cell><cell cols="2">(36, 100) (18, 20) (9, 10) (3, 5)</cell></row><row><cell>F1 (R18-Ours)</cell><cell>-</cell><cell>77.63 77.56 56.10</cell></row><row><cell cols="3">proving feature extractor from ResNet-18 to ResNet-34 will generate 0.48</cell></row><row><cell>point improvement.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The impact of experience J on XR18-Ours . Ours) 77.61 77.63 77.56 76.42 76.19 74.68</figDesc><table><row><cell>Experience J</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell>F1 (R18-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison results of the seq2seq Transformer and the global AtrousFormer along the number of heads on ResNet-18+LSGD (The symbol of '-' denotes running out of memory.).</figDesc><table><row><cell cols="5">Methods (Heads) F1? Precision? Recall? K&amp;V Pos. ?</cell></row><row><cell cols="3">Seq2seq Trm. (1) 76.19 82.19</cell><cell>71.01</cell><cell>3600</cell></row><row><cell>Global A.F. (1)</cell><cell cols="2">76.44 82.24</cell><cell>71.41</cell><cell>1224</cell></row><row><cell cols="3">Seq2seq Trm. (4) 76.37 82.12</cell><cell>71.38</cell><cell>14400</cell></row><row><cell>Global A.F. (4)</cell><cell cols="2">76.84 82.29</cell><cell>72.05</cell><cell>4896</cell></row><row><cell cols="3">Seq2seq Trm. (8) 76.42 81.89</cell><cell>71.65</cell><cell>28800</cell></row><row><cell>Global A.F. (8)</cell><cell cols="2">76.85 82.29</cell><cell>72.08</cell><cell>9792</cell></row><row><cell cols="2">Seq2seq Trm. (16) -</cell><cell>-</cell><cell>-</cell><cell>57600</cell></row><row><cell cols="3">Global A.F. (16) 77.19 82.56</cell><cell>72.49</cell><cell>19584</cell></row><row><cell>R.G. A.F. (16)</cell><cell cols="2">76.84 82.30</cell><cell>72.07</cell><cell>14400</cell></row><row><cell>C.G. A.F. (16)</cell><cell cols="2">76.27 82.97</cell><cell>70.57</cell><cell>5184</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recent progress in road and lane detection: a survey, Machine vision and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="727" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining statistical hough transform and particle filter for robust lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>W?rg?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Markeli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="993" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel lane detection based on geometrical model and gabor filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-lane detection in urban driving environments using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1297" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking in challenging scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New lane model and distance transform for lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1044" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking with ransac and kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3261" to="3264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer vision-based multiple-lane detection on straight road and in a curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Image Analysis and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<title level="m">17th international ieee conference on intelligent transportation systems (itsc)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
	<note>A novel curve lane detection based on improved river flow and ransa</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rbnet: A deep neural network for unified road and road boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lane detection in low-light conditions using an efficient data enhancement: Light conditions style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1394" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1947" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">El-gan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the european conference on computer vision (ECCV) Workshops</title>
		<meeting>the european conference on computer vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13719</idno>
		<title level="m">Resa: Recurrent feature-shift aggregator for lane detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attentionguided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6150" to="6156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bluvstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shlomo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05257</idno>
		<title level="m">Semi-local 3d lane detection and uncertainty estimation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12040</idno>
		<title level="m">Laneaf: Robust multi-lane detection with affinity fields</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tusimple lane detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<title level="m">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03889</idno>
		<title level="m">Conformer: Local features coupling global representations for visual recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
	<note>Cornernet: Detecting objects as paired keypoints</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11582" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spinnet: Spinning convolutional network for lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ultra fast structure-aware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="276" to="291" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV 16</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inter-region affinity distillation for road marking segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12486" to="12495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
