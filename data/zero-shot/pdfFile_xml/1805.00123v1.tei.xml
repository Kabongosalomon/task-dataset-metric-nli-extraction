<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrowdHuman: A Benchmark for Detecting Human in a Crowd</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
							<email>shaoshuai@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
							<email>zhaozijian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
							<email>liboxun@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><forename type="middle">Xiao</forename><surname>Gang</surname></persName>
							<email>yugang@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CrowdHuman: A Benchmark for Detecting Human in a Crowd</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still underrepresented in current human detection benchmarks. In this paper, we introduce a new dataset, called Crowd-Human 1 , to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of 470K human instances from the train and validation subsets, and 22.6 persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks. * Equal contribution.</p><p>1 Our CrowdHuman dataset can be downloaded from https:// sshao0516.github.io/CrowdHuman/ arXiv:1805.00123v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting people in images is among the most important components of computer vision and has attracted increasing attention in recent years <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>. A system that is able to detect human accurately plays an essential role in applications such as autonomous cars, smart surveillance, robotics, and advanced human machine interactions. Besides, it is a fundamental component for research topics like multiple-object tracking <ref type="bibr" target="#b12">[13]</ref>, human pose estimation <ref type="bibr" target="#b27">[28]</ref>, and person search <ref type="bibr" target="#b23">[24]</ref>. Coupled with the development and blooming of convolutional neural networks (CNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref>, modern human detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> have achieved remarkable performance on several major hu-man detection benchmarks.</p><p>However, as the algorithms improve, more challenging datasets are necessary to evaluate human detection systems in more complicated real world scenarios, where crowd scenes are relatively common. In crowd scenarios, different people occlude with each other with high overlaps and cause great difficulty of crowd occlusion. For example, when a target pedestrian T is largely overlapped with other pedestrians, the detector may fail to identify the boundaries of each person as they have similar appearances. Therefore, detector will treat the crowd as a whole, or shift the target bounding box of T to other pedestrians mistakenly. To make matters worse, even though the detectors are able to discriminate different pedestrians in the crowd, the highly overlapped bounding boxes will also be suppressed by the post process of non-maximum suppression (NMS). As a result, crowd occlusion makes the detector sensitive to the threshold of NMS. A lower threshold may lead to drastically drop on recall, while a higher threshold brings more false positives.</p><p>Current datasets and benchmarks for human detection, such as Caltech-USA <ref type="bibr" target="#b5">[6]</ref>, KITTI <ref type="bibr" target="#b24">[25]</ref>, CityPersons <ref type="bibr" target="#b30">[31]</ref>, and "person" subset of MSCOCO <ref type="bibr" target="#b16">[17]</ref>, have contributed to a rapid progress in the human detection. Nevertheless, crowd scenarios are still under-represented in these datasets. For example, the statistical number of persons per image is only 0.32 in Caltech-USA, 4.01 in COCOPersons, and 6.47 in CityPersons. And the average of pairwise overlap between two human instances (larger than 0.5 IoU) in these datasets is only 0.02, 0.02, and 0.32, respectively. Furthermore, the annotators for these datasets are more likely to annotate crowd human as a whole ignored region, which cannot be counted as valid samples in training and evaluation.</p><p>Our goal is to push the boundary of human detection by specifically targeting the challenging crowd scenarios. We collect and annotate a rich dataset, termed CrowdHuman, with considerable amount of crowded pedestrians. Crowd-Human contains 15, 000, 4, 370 and 5, 000 images for training, validation, and testing respectively. The dataset is exhaustively annotated and contains diverse scenes. There are <ref type="figure">Figure 1</ref>. Illustrative examples from different human dataset benchmarks. The images inside the green, yellow, blue boxes are from the COCO <ref type="bibr" target="#b16">[17]</ref>, Caltech <ref type="bibr" target="#b5">[6]</ref>, and CityPersons <ref type="bibr" target="#b30">[31]</ref> datasets, respectively. The images from the second row inside the red box are from our CrowdHuman benchmark with full body, visible body, and head bounding box annotations for each person.</p><p>totally 470k individual persons in the train and validation subsets, and the average number of pedestrians per image reaches 22.6. We also provide the visible region boundingbox annotation, and head region bounding-box annotation along with its full body annotation for each person. <ref type="figure">Fig. 1</ref> shows examples in our dataset compared with those in other human detection datasets.</p><p>To summarize, we propose a new dataset called Crowd-Human with the following three contributions:</p><p>? To the best of our knowledge, this is the first dataset which specifically targets to address the crowd issue in human detection task. More specifically, the average number of persons in an image is 22.6 and the average of pairwise overlap between two human instances (larger than 0.5 IoU) is 2.4, both of which are much larger than the existing benchmarks like CityPersons, KITTI and Caltech.</p><p>? The proposed CrowdHuman dataset provides annotations with three categories of bounding boxes: head bounding-box, human visible-region bounding-box, and human full-body bounding-box. Furthermore, these three categories of bounding-boxes are bound for each human instance.</p><p>? Experiments of cross-dataset generalization ability demonstrate our dataset can serve as a powerful pretraining dataset for many human detection tasks. A framework originally designed for general object detection without any specific modification provides state-of-the-art results on every previous benchmark including Caltech and CityPersons for pedestrian detection, COCOPerson for person detection, and Brainwash for head detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human detection datasets.</head><p>Pioneer works of pedestrian detection datasets involve INRIA <ref type="bibr" target="#b2">[3]</ref>, TudBrussels <ref type="bibr" target="#b26">[27]</ref>, and Daimler <ref type="bibr" target="#b6">[7]</ref>. These datasets have contributed to spurring interest and progress of human detection, However, as algorithm performance improves, these datasets are replaced by larger-scale datasets like Caltech-USA <ref type="bibr" target="#b5">[6]</ref> and KITTI <ref type="bibr" target="#b24">[25]</ref>. More recently, Zhang et al. build a rich and diverse pedestrian detection dataset CityPersons <ref type="bibr" target="#b30">[31]</ref> on top of CityScapes <ref type="bibr" target="#b1">[2]</ref> dataset. It is recorded by a car traversing various cities, contains dense pedestrians, and is annotated with high-quality bounding boxes.</p><p>Despite the prevalence of these datasets, they all suffer a problem of from low density. Statistically, the Caltech-USA and KITTI datasets have less than one person per image, while the CityPersons has ? 6 persons per image. In these datasets, the crowd scenes are significantly underrepresented. Even worse, protocols of these datasets allow annotators to ignore and discard the regions with a large number of persons as exhaustively annotating crowd regions is incredibly difficult and time consuming.</p><p>Human detection frameworks. Traditional human detectors, such as ACF <ref type="bibr" target="#b3">[4]</ref>, LDCF <ref type="bibr" target="#b18">[19]</ref>, and Checkerboard <ref type="bibr" target="#b31">[32]</ref>, exploit various filters based on Integral Channel Features (IDF) <ref type="bibr" target="#b4">[5]</ref> with sliding window strategy.</p><p>Recently, the CNN-based detectors have become a predominating trend in the field of pedestrian detection. In <ref type="bibr" target="#b28">[29]</ref>, self-learned features are extracted from deep neural networks and a boosted decision forest is used to detect pedestrians. Cai et al. <ref type="bibr" target="#b0">[1]</ref> propose an architecture which uses different levels of features to detect persons at various scales. Mao et al. <ref type="bibr" target="#b17">[18]</ref> propose a multi-task network to further improve detection performance. Hosang et al. <ref type="bibr" target="#b8">[9]</ref> propose a learning method to improve the robustness of NMS. Part-based models are utilized in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref> to alleviate occlusion problem. Repulsion loss is proposed to detect persons in crowd scenes <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CrowdHuman Dataset</head><p>In this section, we describe our CrowdHuman dataset including the collection process, annotation protocols, and informative statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>We would like our dataset to be diverse for real world scenarios. Thus, we crawl images from Google image search engine with ? 150 keywords for query. Exemplary keywords include "Pedestrians on the Fifth Avenue", "people crossing the roads", "students playing basketball" and "friends at a party". These keywords cover more than 40 different cities around the world, various activities (e.g., party, traveling, and sports), and numerous viewpoints (e.g., surveillance viewpoint and horizontal viewpoint). The number of images crawled from a keyword is limited to 500 to make the distribution of images balanced. We crawl ? 60, 000 candidate images in total. The images with only a small number of persons, or with small overlaps between persons, are filtered. Finally, ? 25, 000 images are collected in the CrowdHuman dataset. We randomly select 15, 000, 4, 370 and 5, 000 images for training, validation, and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Annotation</head><p>We annotate individual persons in the following steps.</p><p>? We annotate a full bounding box of each individual exhaustively. If the individual is partly occluded, the annotator is required to complete the invisible part and draw a full bounding box. Different from the existing datasets like CityPersons, where the bounding boxes annotated are generated via drawing a line from top of the head and the middle of feet with a fixed aspect ratio (0.41), our annotation protocol is more flexible in real world scenarios which have various human poses. We also provide bounding boxes for human-like objects, e.g., statue, with a specific label. Following the metrics of <ref type="bibr" target="#b5">[6]</ref>, these bounding-boxes will be ignored during evaluation.</p><p>? We crop each annotated instance from the images, and send these cropped regions for annotators to draw a visible bounding box.</p><p>? We further send the cropped regions to annotate a head bounding box. All the annotations are double-checked by at least one different annotator to ensure the annotation quality. We compare our CrowdHuman dataset with previous datasets in terms of annotation types in <ref type="table" target="#tab_1">Table 1</ref>. Besides from the popular pedestrian detection datasets, we also include the COCO <ref type="bibr" target="#b16">[17]</ref> dataset with only a "person" class. Compared with CrowdHuman, which provides various types of annotations, Caltech and CityPersons have only normalized full bounding boxes and visible boxes, KITTI has only full bounding boxes, and COCOPersons has only visible bounding boxes. More importantly, none of them has head bounding boxes associated with each individual person, which may serve as a possible means to address the crowd occlusion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Statistics</head><p>Dataset Size. The volume of the CrowdHuman training subset is illustrated in the first three lines of <ref type="table">Table 2</ref>. In a total of 15, 000 images, there are ? 340k person and ? 99k ignore region annotations in the CrowdHuman training subset. The number is more than 10x boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others.</p><p>Density. In terms of density, on average there are ? 22.6 persons per image in CrowdHuman dataset, as shown in the fourth line of <ref type="table">Table 2</ref>. We also report the density from the existing datasets in <ref type="table" target="#tab_2">Table 3</ref>. Obviously, CrowdHuman dataset is of much higher crowdness compared with all previous datasets. Caltech and KITTI suffer from extremely low-density, for that on average there is only ? 1 person per   image. The number in CityPersons reaches ? 7, a significant boost while still not dense enough. As for COCOPersons, although its volume is relatively large, it is insufficient to serve as a ideal benchmark for the challenging crowd scenes. Thanks to the pre-filtering and annotation protocol of our dataset, CrowdHuman can reach a much better density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech KITTI CityPersons COCOPersons CrowdHuman Full BBox</head><formula xml:id="formula_0">? ? Visible BBox ? Head BBox ? ? ? ?</formula><p>Diversity. Diversity is an important factor of a dataset. COCOPersons and CrowdHuman contain people in unlimited poses in a wide range of domains, while Caltech, KITTI and CityPersons are all recorded by a car traversing on streets. The number of identical persons is also critical. As reported in the fifth line in <ref type="table">Table 2</ref>, this number amounts to ?33k in CrowdHuman while images in Caltech and KITTI are not sparsely sampled, resulting in less amount of identical persons.</p><p>Occlusion. To better analyze the distribution of occlusion levels, we divide the dataset into the "bare" sub-set (occlusion ? 30%), the "partial" subset (30% &lt; occlusion ? 70%), and the "heavy" subset (occlusion &gt; 70%). In <ref type="figure" target="#fig_2">Fig. 3</ref>, we compare the distribution of persons at different occlusion levels for CityPersons 2 . The bare subset and partial subset in CityPersons constitute 46.79% and 24.19% of entire dataset respectively, while the ratios for CrowdHuman are 29.89% and 32.13%. The occlusion levels are more balanced in CrowdHuman, in contrary to those in CityPersons, which have more persons with low occlusion.</p><p>We also provide statistics on pair-wise occlusion. For each image, We count the number of person pairs with different intersection over union (IoU) threshold. The results are shown in <ref type="table">Table 4</ref>. In average, few person pairs with an IoU threshold of 0.3 are included in Caltech, KITTI or COCOPersons. For CityPersons dataset, the number is less than one pair per image. However, the number is 9 for CrowdHuman. Moreover, There are averagely 2.4 pairs whose IoU is greater than 0.5 in the CrowdHuman dataset. <ref type="bibr" target="#b1">2</ref> The statistics is computed without group people We further count the occlusion levels for triples of persons. As shown in <ref type="table">Table 5</ref>, such cases can be hardly found in previous datasets, while they are well-represented in Crowd-Human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will first discuss the experiments on our CrowdHuman dataset, including full body detection, visible body detection and head detection. Meanwhile, the generalization ability of our CrowdHuman dataset will be evaluated on standard pedestrian benchmarks like Caltech and CityPersons, person detection benchmark on CO-COPersons, and head detection benchmark on Brainwash dataset. We use FPN <ref type="bibr" target="#b14">[15]</ref> and RetinaNet <ref type="bibr" target="#b15">[16]</ref> as two baseline detectors to represent the two-stage algorithms and onestage algorithms, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline Detectors</head><p>Our baseline detectors are Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> and Reti-naNet <ref type="bibr" target="#b15">[16]</ref>, both based on the Feature Pyramid Network (FPN) <ref type="bibr" target="#b14">[15]</ref> with a ResNet-50 <ref type="bibr" target="#b7">[8]</ref> back-bone network. Faster R-CNN and RetinaNet are both proposed for general object detection, and they have dominated the field of object detection in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>The training and validation subsets of CrowdHuman can be downloaded from our website. In the following experiments, our algorithms are trained based on CrowdHuman train subset and the results are evaluated in the validation subset. An online evaluation server will help to evaluate the performance of the testing subset and a leaderboard will be maintained. The annotations of testing subset will not be made publicly available.</p><p>We follow the evaluation metric used for Caltech <ref type="bibr" target="#b5">[6]</ref>, denoted as mMR, which is the average log miss rate over false positives per-image ranging in 10 ?2 , 10 0 . mMR is a good indicator for the algorithms applied in the real world applications. Results on ignored regions will not considered in the evaluation. Besides, Average Precision (AP) and recall of the algorithms are included for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We use the same setting of anchor scales as <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>. For all the experiments related to full body detection, we modify the height v.s. width ratios of anchors as {1 : 1, 1.5 : 1, 2 : 1, 2.5 : 1, 3 : 1} in consideration of the human body shape. While for visible body detection and human head detection, the ratios are set to {1 : 2, 1 : 1, 2 : 1}, in comparison with the original papers. The input image sizes of Caltech and CityPersons are set to 2? and 1? of the original images according to <ref type="bibr" target="#b30">[31]</ref>. As the images of CrowdHuman and MSCOCO are both collected from the Internet with various sizes, we resize the input so that their short edge is at 800 pixels while the long edge should be no more than 1400 pixels at the same time. The input sizes of Brainwash is set as 640 ? 480.</p><p>We train all datasets with 600k and 750k iterations for FPN and RetinaNet, respectively. The base learning rate is set to 0.02 and decreased by a factor of 10 after 150k and 450k for FPN, and 180k and 560k for RetinaNet. The Stochastic Gradient Descent (SGD) solver is adopted to optimize the networks on 8 GPUs. A mini-batch involves 2 images per GPU, except for CityPersons where a mini-batch involves only 1 image due to the physical limitation of GPU memory. Weight decay and momentum are set to 0.0001 and 0.9. We do not finetune the batch normalization <ref type="bibr" target="#b10">[11]</ref> layers. Multi-scale training/testing are not applied to ensure fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Detection results on CrowdHuman</head><p>Visible Body Detection As the human have different poses and occlusion conditions, the visible regions may be quite different for each individual person, which brings many difficulties to human detection. <ref type="table" target="#tab_4">Table 6</ref> illustrates the results for the visible part detection based on FPN and RetinaNet. FPN outperforms RetinaNet in this case. According to Table 6, the proposed CrowdHuman dataset is a challenging benchmark, especially for the state-of-the-art human detection algorithms. The illustrative examples of visible body detection based on FPN are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Body Detection</head><p>Detecting full body regions is more difficult than detecting the visible part as the detectors should predict the occluded boundaries of the full body. To make matters worse, the ground-truth annotation might be suffered from high variance caused by different decisionmakings by different annotators.</p><p>Different from the visible part detection, the aspect ratios of the anchors for the full body detection are set as [1.0, 1.5, 2.0, 2.5, 3.0] to make the detector tend to predict the slim and tall bounding boxes. Another important thing is that the RoIs are not clipped into the limitation of the image boundaries, as there are many full body bounding boxes extended out of images. The results are shown in <ref type="table" target="#tab_5">Table 7</ref> and the illustrative examples of FPN are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Similar to the Visible body detection, FPN has a significant gain over RetinaNet.</p><p>In <ref type="table" target="#tab_5">Table 7</ref>, we also report the FPN pedestrian detection results 3 on Caltech, i.e., 10.08 mMR, and CityPersons, i.e., 14.81 mMR. It shows that our CrowdHuman dataset is much challenging than the standard pedestrian detection benchmarks based on the detection performance.  0.01 1.01 iou&gt;0.7 0.00 0.08 0.00 0.33 iou&gt;0.8 0.00 0.02 0.00 0.07 iou&gt;0.9 0.00 0.00 0.00 0.01 <ref type="table">Table 4</ref>. Comparison of pair-wise overlap between two human instances.</p><p>Head Detection Head is one of the most obvious parts of a whole body. Head detection is widely used in the practical applications such as people number counting, face detection and tracking. We compare the results of FPN and RetinaNet as shown in 0.00 0.12 iou&gt;0.5 0.00 0.00 0.00 0.03 <ref type="table">Table 5</ref>. Comparison of high-order overlaps among three human instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cross-dataset Evaluation</head><p>As shown in Section 3, the size of CrowdHuman dataset is obviously larger than the existing benchmarks, like Caltech and CityPersons. In this section, we evaluate that the   Recall AP mMR FPN <ref type="bibr" target="#b14">[15]</ref> 81.10 77.95 52.06 RetinaNet <ref type="bibr" target="#b15">[16]</ref> 78.43 71.36 60.64 generalization ability of our CrowdHuman dataset. More specifically, we first train the model on our CrowdHuman dataset and then finetune it on the visible body detection benchmarks like COCOPersons <ref type="bibr" target="#b16">[17]</ref>, full body detection benchmarks like Caltech <ref type="bibr" target="#b5">[6]</ref> and CityPersons <ref type="bibr" target="#b30">[31]</ref>, and head detection benchmarks like Brainwash <ref type="bibr" target="#b22">[23]</ref>. As reported in Section 4.4, FPN is superior to RetinaNet in all three cases. Therefore, in the following experiments, we adopt FPN as our baseline detector.</p><p>COCOPersons COCOPersons is a subset of MSCOCO from the images with groundtruth bounding box of "person". The other 79 classes are ignored in our evaluation. After the filtering process, there are 64115 images from the trainval minus minival for training, and the other 2639 images from minival for validation. All the persons in CO-COPersons are annotated as the visible body with different type of human poses. The results are illustrated in <ref type="table" target="#tab_6">Table 9</ref>. Based on the pretraining of our CrowdHuman dataset, our algorithm has superior performance on the COCOPersons Caltech and CityPersons Caltech and CityPersons are widely used benchmarks for pedestrian detection, both of them are usually adopted to evaluate full body detection algorithms. We use the reasonable set for Caltech dataset where the object size is larger than 50 pixels. <ref type="table" target="#tab_1">Table 10</ref> and <ref type="table" target="#tab_1">Table 11</ref> show the results on Caltech and CityPersons, respectively. We compare the algorithms in the first part of the tables with:</p><p>? FPN trained on the Caltech</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? FPN trained on CityPersons</head><p>? FPN trained on CrowdHuman</p><p>? FPN model pretrained on CrowdHuman and then finetuned on the corresponding target training set Also, state-of-art algorithms on Caltech and CityPersons are reported in the second part of tables as well. To summarize, the results illustrated in <ref type="table" target="#tab_1">Table 10</ref> and <ref type="table" target="#tab_1">Table 11</ref> demonstrate that our CrowdHuman dataset can serve as an effective pretraining dataset for pedestrian detection task on Caltech and CityPersons 4 for full body detection.</p><p>Brainwash Brainwash <ref type="bibr" target="#b22">[23]</ref> is a head detection dataset whose images are extracted from the video footage at every 100 seconds. Following the step of <ref type="bibr" target="#b22">[23]</ref>, the training  [23] -78.0 set has 10,917 images with 82,906 instances and the validation set has 500 images with 3318 instances. Similar to visible body detection and full body detection, Brainwash dataset is evaluated to validate the generalization ability of our CrowdHuman dataset for head detection. <ref type="table" target="#tab_1">Table 12</ref> shows the results of head detection task on Brainwash dataset. By using the FPN as the head detector, the performance is already much better than the state-ofart in <ref type="bibr" target="#b22">[23]</ref>. On top of that, pretraining on the CrowdHuman dataset further boost the result by 2.5% of mMR, which validates the generalization ability of our CrowdHuman dataset for head detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new human detection benchmark designed to address the crowd problem. There are three contributions of our proposed CrowdHuman dataset. Firstly, compared with the existing human detection benchmark, the proposed dataset is larger-scale with much higher crowdness. Secondly, the full body bounding box, the visible bounding box, and the head bounding box are annotated for each human instance. The rich annotations enables a lot of potential visual algorithms and applications. Last but not least, our CrowdHuman dataset can serve as a powerful pretraining dataset. State-of-the-art results have been reported on benchmarks of pedestrian detection benchmarks like Caltech and CityPersons, and Head detection benchmark like Brainwash. The dataset as well as the code and models discussed in the paper will be released 5 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>shows the three kinds of bounding boxes associated with an individual person as well as an example of annotated image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) provides an illustrative example of our three kinds of annotations: Head Bounding-Box, Visible Bounding-Box, and Full Bounding-Box. (b) is an example image with our human annotations where magenta mask illustrates the ignored region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the visible ratio between our CrowdHuman and CityPersons dataset. Visible Ratio is defined as the ratio of visible bounding box to the full bounding box. pair/img Cal City COCO CrowdHuman iou&gt;0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results for the full body detection of FPN based on CrowdHuman dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for the visible body detection of FPN based on CrowdHuman dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results for the head detection of FPN based on CrowdHuman dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Caltech</cell><cell>KITTI</cell><cell cols="3">CityPersons COCOPersons CrowdHuman</cell></row><row><cell># images</cell><cell>42, 782</cell><cell>3, 712</cell><cell>2, 975</cell><cell>64, 115</cell><cell>15, 000</cell></row><row><cell># persons</cell><cell>13, 674</cell><cell>2, 322</cell><cell>19, 238</cell><cell>257, 252</cell><cell>339, 565</cell></row><row><cell># ignore regions</cell><cell>50, 363</cell><cell>45</cell><cell>6, 768</cell><cell>5, 206</cell><cell>99, 227</cell></row><row><cell># person/image</cell><cell>0.32</cell><cell>0.63</cell><cell>6.47</cell><cell>4.01</cell><cell>22.64</cell></row><row><cell># unique persons</cell><cell>1, 273</cell><cell>&lt; 2, 322</cell><cell>19, 238</cell><cell>257, 252</cell><cell>339, 565</cell></row></table><note>Comparison of different annotation types for the popular human detection benchmarks.? : Aligned to a certain ratio.Table 2. Volume, density and diversity of different human detection datasets. For fair comparison, we only show the statistics of training subset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the human density against the widely used human detection dataset. The first column refers to the number of human instances in the image.</figDesc><table><row><cell>person/img ?</cell><cell cols="2">Caltech</cell><cell></cell><cell>KITTI</cell><cell cols="2">CityPersons</cell><cell cols="2">COCOPersons</cell><cell cols="2">CrowdHuman</cell></row><row><cell>1</cell><cell cols="10">7839 18.3% 969 26.1% 2482 83.4% 64115 100.0% 15000 100.0%</cell></row><row><cell>2</cell><cell cols="10">3257 7.6% 370 10.0% 2082 70.0% 39283 61.3% 15000 100.0%</cell></row><row><cell>3</cell><cell cols="10">1265 3.0% 273 7.4% 1741 58.5% 28553 44.5% 14996 100.0%</cell></row><row><cell>5</cell><cell>282</cell><cell cols="9">0.7% 164 4.4% 1225 41.2% 18775 29.3% 14220 94.8%</cell></row><row><cell>10</cell><cell>36</cell><cell>0.1%</cell><cell>19</cell><cell>0.5%</cell><cell cols="3">610 20.5% 9604</cell><cell cols="3">15.0% 10844 72.3%</cell></row><row><cell>20</cell><cell>0</cell><cell>0.0%</cell><cell>0</cell><cell>0.0%</cell><cell>227</cell><cell>7.6%</cell><cell>0</cell><cell>0.0%</cell><cell>5907</cell><cell>39.4%</cell></row><row><cell>30</cell><cell>0</cell><cell>0.0%</cell><cell>0</cell><cell>0.0%</cell><cell>94</cell><cell>3.2%</cell><cell>0</cell><cell>0.0%</cell><cell>3294</cell><cell>21.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="3">pair/img Cal City COCO CrowdHuman</cell></row><row><cell>iou&gt;0.1 0.02 0.30</cell><cell>0.02</cell><cell>8.70</cell></row><row><cell>iou&gt;0.2 0.00 0.11</cell><cell>0.00</cell><cell>2.09</cell></row><row><cell>iou&gt;0.3 0.00 0.04</cell><cell>0.00</cell><cell>0.51</cell></row><row><cell>iou&gt;0.4 0.00 0.01</cell><cell></cell><cell></cell></row><row><cell>. The illustrative examples of</cell><cell></cell><cell></cell></row><row><cell>head detection on CrowdHuman by FPN detector are shown</cell><cell></cell><cell></cell></row><row><cell>in Fig. 6.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Evaluation of visible body detection on CrowdHuman benchmark.</figDesc><table><row><cell></cell><cell>Recall AP mMR</cell></row><row><cell>FPN [15]</cell><cell>91.51 85.60 55.94</cell></row><row><cell cols="2">RetinaNet [16] 90.96 77.19 65.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Evaluation of full body detection on CrowdHuman benchmark.</figDesc><table><row><cell></cell><cell>Recall AP mMR</cell></row><row><cell>FPN [15]</cell><cell>90.24 84.95 50.42</cell></row><row><cell>RetinaNet [16]</cell><cell>93.80 80.83 63.33</cell></row><row><cell>FPN on Caltech</cell><cell>99.76 89.95 10.08</cell></row><row><cell cols="2">FPN on CityPersons 97.97 94.35 14.81</cell></row><row><cell cols="2">Table 8. Evaluation of Head detection on CrowdHuman bench-</cell></row><row><cell>mark.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Experimental results on COCOPersons.</figDesc><table><row><cell>Train-set</cell><cell>Recall AP mMR</cell></row><row><cell cols="2">COCOPersons 95.57 83.83 41.89</cell></row><row><cell cols="2">Crowd?COCO 95.87 85.02 39.79</cell></row><row><cell cols="2">benchmark against the one without CrowdHuman pretrain-</cell></row><row><cell>ing.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .Table 11 .Table 12 .</head><label>101112</label><figDesc>Experimental results on Caltech dataset. Experimental reslts on CityPersons. Experimental results on Brainwash.</figDesc><table><row><cell>Train-set Caltech</cell><cell cols="3">Recall AP mMR 99.76 89.95 10.08</cell><cell>Train-set</cell><cell cols="3">Recall AP mMR</cell></row><row><cell>CityPersons</cell><cell cols="3">99.05 85.81 14.69</cell><cell>Caltech</cell><cell cols="3">87.21 65.87 45.52</cell></row><row><cell>CrowdHuman</cell><cell cols="3">99.88 90.58 8.81</cell><cell>CityPersons</cell><cell cols="3">97.97 94.35 14.81</cell></row><row><cell>Crowd?Calt</cell><cell cols="3">99.88 95.69 3.46</cell><cell>CrowdHuman</cell><cell cols="3">98.73 98.10 21.18</cell></row><row><cell cols="2">CityPersons?Calt [31] -</cell><cell>-</cell><cell>5.1</cell><cell>Crowd?City</cell><cell cols="3">97.78 95.58 10.67</cell></row><row><cell>Repulsion [26]</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell cols="2">CityPersons [31] -</cell><cell>-</cell><cell>14.8</cell></row><row><cell>[18]</cell><cell>-</cell><cell>-</cell><cell>5.5</cell><cell>Repulsion [26]</cell><cell>-</cell><cell>-</cell><cell>13.2</cell></row><row><cell>Train-set</cell><cell cols="3">Recall AP mMR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Brainwash</cell><cell cols="3">98.52 95.74 19.77</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Crowd?Brain 98.66 96.15 17.24</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The results are evaluated on the standard reasonable set</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The evaluation is based on 1? scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://sshao0516.github.io/CrowdHuman/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07155</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4073" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<editor>Ian Reid Stefan Roth Konrad Schindler Laura Leal-Taix, Anton Milan</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08160</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1134</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3258" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao Wang Liang Lin Xiaogang Wang Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas Geigerand Philip Lenzand Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen Chunhua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="794" to="801" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng Zhiqiang Zhang Gang Yu Jian Sun Yilun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07032</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Citypersons: A diverse dataset for pedestrian detection. 1, 2, 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-label learning of part detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3486" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
