<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiSubs: A Large-scale Multimodal and Multilingual Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lucia Specia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Pranava Madhyastha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lucia Specia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiel</forename><surname>Figueiredo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lucia Specia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Chiraag</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lucia Specia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lucia Specia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MultiSubs: A Large-scale Multimodal and Multilingual Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.5281/zenodo.5034604</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodality ? Visual grounding ? Multilinguality ? Multimodal Dataset</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. We show the utility of the dataset on two automatic tasks: (i) fill-in-the blank; (ii) lexical translation. Results of the human evaluation and automatic models demonstrate that images can be a useful complement to the textual context. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences, and can be obtained from https://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>around us, we need to be able to interpret such multimodal signals together. Learning and understanding languages is not an exception: humans make use of multiple modalities when doing so. In particular, words are generally learned with visual (among others) input as additional modality. Research on computational models of language grounding using visual information has led to many interesting applications, such as Image Captioning <ref type="bibr" target="#b37">[35]</ref>, Visual Question Answering <ref type="bibr" target="#b1">[2]</ref> and Visual Dialog <ref type="bibr" target="#b7">[8]</ref>.</p><p>Various multimodal datasets comprising images and text have been constructed for different applications. Many of these are made up of images annotated with text labels, and thus do not provide a context in which to apply the text and/or images. More recent datasets for image captioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">36]</ref> go beyond textual labels and annotate images with sentence-level text. While these sentences provide a stronger context for the image, they suffer from one primary shortcoming: Each sentence 'explains' an image given as a whole, while most often focusing on only some of the elements depicted in the image. This makes it difficult to learn correspondences between elements in the text and their visual representation. Indeed, the connection between images and text is multifaceted, i.e. the former is not strictly a textual representation of the latter, thus making it hard to describe a whole image in a single sentence or to illustrate a whole sentence with a single image. A tighter, local correspondence between images and text segments is therefore needed in order to learn better groundings between words and images. Additionally, the texts are limited to very specific domains (image descriptions), while the images are also constrained to very few and very specific object categories or human activities; this makes it very hard to generalise to the diversity of possible real-world scenarios.  <ref type="figure">Fig. 1</ref> An example instance from our proposed large-scale multimodal and multilingual dataset. MultiSubs comprises predominantly conversational or narrative texts from movie subtitles, with text fragments illustrated with images and aligned across languages.</p><p>In this paper we propose MultiSubs, a new largescale multimodal and multilingual dataset that facilitates research on grounding words to images in the context of their corresponding sentences ( <ref type="figure">Figure 1)</ref>. In contrast to previous datasets, ours ground words not only to images but also to their contextual usage in language, potentially giving rise to deeper insights into real-world human language learning. More specifically, (i) text fragments and images in MultiSubs have a tighter local correspondence, facilitating the learning of associations between text fragments and their corresponding visual representations; (ii) the images are more general and diverse in scope and not constrained to particular domains, in contrast to image captioning datasets; (iii) multiple images are possible for each given text fragment and sentence; (iv) the text comprises a grammar or syntax similar to free-form, realworld text; and (v) the texts are multilingual and not just monolingual or bilingual. Starting from a parellel corpus of movie subtitles ( ?3), we propose a crosslingual multimodal disambiguation method to illustrate text fragments by exploiting the parallel multilingual texts to disambiguate the meanings of words in the text <ref type="figure">(Figure 2</ref>) ( ?4). To the best of our knowledge, this has not been previously explored in the context of text illustration. We also evaluate the quality of the dataset and illustrated text fragments via human judgment by casting it as a game ( ?6).</p><p>We propose and demonstrate two different multimodal applications using MultiSubs:</p><p>1. A fill-in-the-blank task to guess a missing word from a sentence, with or without image(s) of the word as clues ( ?7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Lexical translation, where we translate a source word in the context of a sentence to a target word in a foreign language, given the source sentence and zero or more images associated with the source word ( ?7.2).</p><p>The dataset can be obtained from https://doi. org/10.5281/zenodo.5034604 under a Creative Commons licence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most existing multimodal grounding datasets consist of images/videos annotated with noun labels 1 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">23]</ref>. The main applications of these datasets include multimedia annotation/indexing/retrieval <ref type="bibr" target="#b36">[34]</ref> and object recognition/detection <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b34">32]</ref>. They also enable research on grounded semantic representation or concept learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Besides nouns, other work and datasets focus on labelling and recognising actions <ref type="bibr" target="#b15">[14]</ref> and verbs <ref type="bibr" target="#b16">[15]</ref>. These works, however, are limited to single word labels independent of a contextual usage.</p><p>Recently multimodal grounding work has been moving beyond textual labels to include free-form sentences or paragraphs. Various datasets were constructed for these tasks, including image and video descriptions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1]</ref>, news articles <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b33">31]</ref>, cooking recipes <ref type="bibr" target="#b27">[25]</ref>, among others. These datasets, however, ground whole images to the whole text, and making it difficult to identify correspondences between text fragments and elements in the image. In addition, the text also does not explain all elements in the images.</p><p>Apart from monolingual text, there has also been work on multimodal grounding on multilingual text. One primary application of such work is in bilingual lexicon induction using visual data <ref type="bibr" target="#b21">[20]</ref>, where the task is to find words in different languages sharing the same meaning. Hewitt et al . <ref type="bibr" target="#b18">[17]</ref> has recently developed a large-scale dataset to investigate bilingual lexicon learning for 100 languages. However, this dataset is limited to single word tokens; no textual context is provided with the words. Beyond word tokens, there are also multilingual datasets that are provided at sentence level, primarily extended from existing image description/captioning datasets <ref type="bibr" target="#b12">[12,</ref><ref type="bibr">27]</ref>. Schamoni et al . <ref type="bibr" target="#b35">[33]</ref> also introduce a dataset with images from Wikipedia and their captions in multiple languages; however, the captions are not parallel across languages. These datasets are either very small or use machine translation to generate texts in a different language. More important, they are literal descriptions of images gathered for a specific set of object categories or activities and written by users in a constrained setting (A woman is standing beside a bicycle with a dog). Like monolingual image  <ref type="figure">Fig. 2</ref> Overview of the MultiSubs construction process. Starting from parallel corpora, we selected 'visually salient' English words (weapon and trunk in this example). We automatically align the words across languages (e.g. trunk to cajuela, coffre etc.), and queried BabelNet with the words to obtain a list of synsets. In this example, trunk in English is ambiguous, but cajuela in Spanish is not. We thus disambiguated the sense of trunk by finding the intersection of synsets across languages (bn:00007381n), and illustrate trunk with images associated with the intersecting synset, as provided by BabelNet.</p><p>descriptions, whole sentences are associated with whole images. This makes it hard to ground image elements to text fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus and text fragment selection</head><p>MultiSubs is based on the OpenSubtitles 2016 (OPUS) corpus <ref type="bibr" target="#b26">[24]</ref>, which is a large-scale dataset of movie subtitles in 65 languages obtained from OpenSubtitles <ref type="bibr" target="#b31">[29]</ref>. We use a subset by restricting the movies 2 to five categories that we believe are potentially more 'visual': adventure, animation, comedy, documentaries, and family. The mapping of IMDb identifiers (used in OPUS) to their corresponding categories are obtained from IMDb's official list <ref type="bibr" target="#b20">[19]</ref>. Most of the subtitles are conversational (dialogues) or narrative (story narration or documentaries). The subtitles are further filtered to only a subset of English subtitles that has been aligned in OPUS to subtitles from at least one of the top 30 non-English languages in the corpus. This resulted in 45,482 movie instances overall with ?38M English sentences. The number of movies ranges from 2,354 to 31,168 for the top 30 languages.</p><p>We aim to select text fragments that are potentially 'visually depictable', and which can therefore be illustrated with images. We start by chunking the English subtitles 3 to extract nouns, verbs, compound nouns, and simple adjectivial noun phrases. The fragments are ranked by imageability scores obtained via bootstrapping from the MRC Psycholinguistic database <ref type="bibr" target="#b32">[30]</ref>; for multi-word phrases we average the imageability score of each individual word, assigning a zero score to each unseen word. We retain text fragments with an imageability score of at least 500, which is determined by manual inspection of a subset of words. After removing fragments occurring only once, the output is a set of 144,168 unique candidate fragments (more than 16M instances) across ?11M sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Illustration of text fragments</head><p>Our approach for illustrating MultiSubs obtains images for a subset of text fragments: single word nouns. Such nouns occur substantially more often in the corpus and are thus more suitable for learning algorithms. Additionally, single nouns (dog) make it more feasible to obtain good representative images than longer phrases (a medium-sized white and brown dog). This filtering step results in 4,099 unique English nouns occurring in ?10.2M English sentences.</p><p>We aim to obtain images that illustrate the correct sense of these nouns in the context of the sentence. For that, we propose a novel approach that exploits the aligned multilingual subtitle corpus for sense disambiguation using BabelNet <ref type="bibr" target="#b30">[28]</ref> ( ?4.1), a multilingual sense dictionary. <ref type="figure">Figure 2</ref> illustrates the process.</p><p>MultiSubs is designed as a subtitle corpus illustrated with general images. Taking images from the video from where the subtitle comes is not possible since we do not have access to the copyrighted materials. In addition, there are no guarantees that the concepts mentioned in the text would be depicted in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-lingual sense disambiguation</head><p>The key intuition to our proposed text illustration approach is that an ambiguous English word may be unambiguous in the parallel sentence in the target language. For example, the correct word sense of drill in an English sentence can be inferred from a parallel Portuguese sentence based on the occurrence of the word broca (the machine) or treino (training exercise).</p><p>Cross-lingual word alignment. We experiment with up to four target languages in selecting the correct images to illustrate our candidate text fragments (nouns): Spanish (ES) and Brazilian Portuguese (PT), which are the two most frequent languages in OPUS; and French (FR) and German (DE), both commonly used in existing Machine Translation (MT) and Multimodal Machine Translation (MMT) research <ref type="bibr" target="#b11">[11]</ref>. For each language, subtitles are selected such that (i) each is aligned with a subtitle in English; (ii) each contains at least one noun of interest.</p><p>For English and each target language, we trained fast_align [10] on the full set of parallel sentences (regardless of whether the sentence contains a candidate fragment) to obtain alignments between words in both languages (symmetrised by the intersection of alignments in both directions). This generates a dictionary which maps English nouns to words in the target language. We filter this dictionary to remove pairs with infrequent target phrases (under 1% of the corpus). We also group words in the target language that share the same lemma 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sense disambiguation.</head><p>A noun being translated to different words in the target language does not necessarily mean it is ambiguous. The target phrases may simply be synonyms referring to the same concept. Thus, we further attempt to group synonyms on the target side, while also determining the correct word sense by looking at the aligned phrases across multilingual corpora.</p><p>For word senses, we use BabelNet <ref type="bibr" target="#b30">[28]</ref>, which is a large semantic network and multilingual encyclopaedic dictionary covering many languages and unifies other semantic networks. We query BabelNet with the English noun and its possible translations in each target language from our automatically aligned dictionary. The output (queried separately per language) is a list of BabelNet synset IDs matching the query.</p><p>To help us identify the correct sense of an English noun for a given context, we use the aligned word in the parallel sentence in the target language for disambiguation. We compute the intersection between the Babel-Net synset IDs returned from both queries. For example, the English query bank could contain the synsets financial-bank and river-bank, and the Spanish query for the corresponding translation banco only returns the synset financial-bank. In this case, the intersection of both synset sets allows us to decide that bank, when translated to banco, refers to its financial-bank sense. Therefore, we can annotate the respective parallel sentence in the corpus with the correct sense. Where multiple synset IDs intersect, we take the union of all intersecting synsets as possible senses for the particular alignment. This potentially means that (i) the term is ambiguous and the ambiguity is carried over to the target language; or (ii) the distinct BabelNet synsets actually refer to the same or similar sense, as BabelNet unifies word senses from multiple sources automatically. We name this dataset intersect 1 .</p><p>If the above is only performed for one language pair, this single target language may not be sufficient to disambiguate the sense of the English term, as the term might be ambiguous in both languages (e.g. coffre is also ambiguous in <ref type="figure">Figure 2</ref>). This is particularly true for closely related languages such as Portuguese and Spanish. Thus, we propose exploiting multiple target languages to further increase our confidence in disambiguating the sense of the English word. Our assumption is that more languages will eventually allow the correct context of the word to be identified.</p><p>More specifically, we examine subtitles containing parallel sentences for up to four target languages. For each English phrase, we retain instances with at least one intersection between the synset IDs across all N languages, and discard if there is no intersection. We name these datasets intersect N , which comprise sentences that have valid synset alignments to at least N languages. Note that intersect N +1 ? intersect N . <ref type="table">Table 1</ref> shows the final dataset sizes of intersect N . Our experiments in ?7 will focus on comparing models trained on different intersect N datasets, and test them on intersect 4 .</p><p>Image selection. The final step to constructing MultiSubs is to assign at least one image to each disambiguated English term, and by design the term in the aligned target language(s). As BabelNet generally provides multiple images for a given synset ID, we illustrate the term with all Creative Commons images associated with the synset. <ref type="table">Table 1</ref> Number of sentences for the intersect N subset of MultiSubs, where N is the minimum number of target languages used for disambiguation. The slight variation in the final column is due to differences in how the aligned sentences are combined or split in OPUS across languages.  5 MultiSubs statistics and analysis <ref type="table">Table 1</ref> shows the number of sentences in MultiSubs, according to their degree of intersection. On average, there are 1.10 illustrated words per sentence in MultiSubs, where about 90-93% sentences contain one illustrated word per sentence (depending on the target language). The number of images for each BabelNet synset ranges from 1 to 259, with an average of 15.5 images (excluding those with no images). <ref type="table" target="#tab_2">Table 2</ref> shows some statistics of the sentences in MultiSubs. MultiSubs is substantially larger and less repetitive than Multi30k <ref type="bibr" target="#b12">[12]</ref> (?300k tokens, ?11-19k types, and only ?5-11k singletons), even though the sentence length remains similar. <ref type="figure" target="#fig_1">Figure 3</ref> shows an example of how multilingual corpora is beneficial for disambiguating the correct sense of a word and subsequently illustrating it with an image. The top example shows an instance from intersect 1 , where the English sentence is aligned to only one target language (French). In this case, the word sceau is ambiguous in BabelNet, covering different but mostly related senses, and in some cases is noisy (terms are obtained by automatic translation). The bottom example shows an example where the English sentence is aligned to four target languages, which came to a consensus on a single BabelNet synset (and illustrated with the correct image). A manual inspection of a randomly selected subset of the data to assess our automated dis-bn:00070012n (seal wax), bn:00070013n (stamp), bn:00070014n (sealskin), ... EN : stamp my heart with a seal of love ! FR: frapper mon coeur d' un sceau d' amour ! bn:00021163n (animal) EN : even the seal 's got the badge . ES : que hasta la foca tiene placa . PT : at? a foca tem um distintivo . FR: m?me l' otarie a un badge . DE : sogar die robbe hat das abzeichen .  ambiguation procedure showed that intersect 4 is of high quality. We found many interesting cases of ambiguities, some of which are shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><formula xml:id="formula_0">N = 1 N = 2 N = 3 N = 4 ES 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human evaluation</head><p>To quantitatively assess our automated cross-lingual sense disambiguation cleaning procedure, we collect human annotations to determine whether images in MultiSubs are indeed useful for predicting a missing word in a fill-in-the-blank task. The annotation also serves as a human upperbound to the task (detailed in ?7.1), measuring whether images are useful for helping humans guess the missing word.</p><p>We set up the annotation task as The Gap Filling Game ( <ref type="figure">Figure 5</ref>). In this game, users are given three attempts at guessing the exact word removed from a sentence from MultiSubs. In the first attempt, the game shows only the sentence (along with a blank space for the missing word). In the second attempt, the game additionally provides one image for the missing word as a clue. In the third and final attempt, the system shows all images associated with the missing word. At each attempt, users are awarded a score of 1.0 if the word they entered is an exact match to the original word, or otherwise a partial score (between 0.0 and 1.0) computed as the cosine similarity between pre-trained CBOW word2vec <ref type="bibr" target="#b28">[26]</ref> embeddings of the predicted and <ref type="figure">Fig. 5</ref> A screenshot of The Gap Filling Game, used to evaluate our automated cleaning procedure, as an upperbound to how well humans can perform the task without images, and to evaluate whether images are actually useful for the task. the original word. Each 'turn' (one sentence) ends when the user enters an exact match or after he or she has exhausted all three attempts, whichever occurs first. The score at the second and third attempts are multiplied by a penalty factor (0.90 and 0.80 respectively) to encourage users to guess the word correctly as early as possible. A user's score for a single turn is the maximum over all three attempts, and the final cumulative score per user is the sum of the score across all annotated sentences. This final score determines the winner and runner-up at the end of the game (after a pre-defined cut-off date), both of whom are awarded an Amazon voucher each. Users are not given an exact 'current top score' table during the game, but are instead provided the percentage of all users who has a lower score than the user's current score.</p><p>For the human annotations, we also introduce the intersect 0 dataset where the words are not disambiguated, i.e. images from all matching BabelNet synsets are used. This is to evaluate the quality of our automated filtering process. Annotators are allocated 100 sentences per batch, and are able to request for more batches once they complete their allocated batch. Sentences are selected at random. To select one image for the second attempt, we select the image most similar to the majority of other images of the synset, by computing the cosine distance of each image's ResNet152 pool5 fea-ture <ref type="bibr" target="#b17">[16]</ref> against all remaining images in the synset, and averaged the distance across these images.</p><p>Users are allowed to complete as many sentences as they like. The annotations were collected over 24 days in December 2018, and participants are primarily staff and student volunteers from the University of Sheffield, UK. 238 users participated in our annotation, resulting in 11,127 annotated instances (after filtering out invalid annotations). <ref type="table" target="#tab_3">Table 3</ref> shows the results of human annotation, comparing the proportion of instances correctly predicted by annotators at different attempts: (1) no image; (2) one image; (3) many images; and also those that fail to be correctly predicted after three attempts. We consider a prediction correct if the predicted word is an exact match to the original word. Overall, out of 11, 127 instances, 21.89% of instances were predicted correctly with only the sentence as context, 20.49% with one image, and 15.21% with many images. The annotators failed to guess the remaining 42.41% of instances. Thus, we can estimate a human upper bound of 57.59% for correctly predicting missing words in the dataset, regardless of the cue provided. Across different intersect N splits, there is an improvement in the proportion of correct predictions as N increases, from 54.55% for intersect 0 to 60.83% for intersect 4 . We have also tested sampling each split to have an equal number of instances <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">598)</ref> to ensure that the proportion is not an indirect effect of imbalance in the number of instances; we found the proportions to be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of human evaluation</head><p>A user might fail to predict the exact word, but the word might be semantically similar to the correct word (e.g. a synonym). Thus, we also evaluate the annotations with the cosine similarity between word2vec embeddings of the predicted and correct word. <ref type="table" target="#tab_4">Table 4</ref> shows the average word similarity scores at different attempts across intersect N splits. Across attempts, the average similarity score is lowest for attempt 1 (textonly, 0.36), compared to attempts 2 (one image) and 3 (many images) -0.48 and 0.49 respectively. Again, we verified that the scores are not affected by the imbalanced number of instances, by sampling equal number of instances across splits and attempts. We also observe a generally higher average score as we increase N , albeit marginal. <ref type="figure">Figure 6</ref> shows a few example human annotations, with varying degrees of success. In some cases, textual context alone is sufficient for predicting the correct word. In other cases, like in the second example, it is difficult to guess the missing word purely from textual context. In this case, images are useful.   <ref type="figure">6</ref> Example annotations from our human experiment, with the masked word boldfaced. Users' guesses are italicised, with the word similarity score in brackets. The first example was guessed correctly without any images. The second was guessed correctly after one image was shown. The third was only guessed correctly after all images were shown. The final example was not guessed correctly after all three attempts.</p><p>We conclude that the task of filling in the blanks in MultiSubs is quite challenging even for humans, where only 57.59% instances were correctly guessed. This inspired us to introduce fill-in-the-blank as a task to eval-uate how well automatic models can perform the same task, with or without images as cues ( ?7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental evaluation</head><p>We demonstrate how MultiSubs can be used to train models to learn multimodal text grounding with images on two different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Fill-in-the-blank task</head><p>The first task we present for MultiSubs is a fill-inthe-blank task. The objective of the task is to predict a word that has been removed from a sentence in MultiSubs, given the masked sentence as textual context and optionally one or more images depicting the missing word as visual context. Our hypothesis is that images in MultiSubs can provide additional contextual information complementary to the masked sentence, and that models that utilize both textual and visual contextual cues will be able to recover the missing word better than models that use either alone. Formally, given a sequence S={w 1 , . . . , w t?1 , w t , w t+1 , . . . , ...w T } of length T , where w t is unobserved while the others are observed, the task is to predict w t given S and optionally one or more images {I 1 , I 2 , . . . , I K }.</p><p>This task is similar to the human annotation ( ?6). Thus, we use the statistics from human evaluation as an estimated human upperbound for the task. We observe that this task is challenging even for humans who successfully predicted the missing word for only 57.59% of instances, regardless of whether they use images as contextual cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Models</head><p>We train three computational models for this task: (i) a text-only model, where we follow Lala et al . <ref type="bibr" target="#b23">[21]</ref> and use a bidirectional recurrent neural network (BRNN) that takes in the sentence with blanks and predicts at each time step either null or the word corresponding to the blank; (ii) an image-only baseline, where we treat the task as image labelling and build a two layer feedforward neural network, with ResNet152 pool5 layer <ref type="bibr" target="#b17">[16]</ref> as image features; and (iii) a multimodal model, where we follow Lala et al . <ref type="bibr" target="#b23">[21]</ref> and use simple multimodal fusion to initialize the BRNN model with the image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Dataset and settings</head><p>We blank out each illustrated word of a sentence as a fill-in-the-blank instance. If a sentence contains multiple illustrated nouns, we replicate the sentence and generate a blank per sentence for each noun, treating each as a separate instance.</p><p>The number of validation and test instances is fixed at 5, 000 each. These comprise sentences from intersect 4 , which we consider to be the cleanest subset of MultiSubs. The validation and test sets are made more challenging by (i) uniformly sampling nouns from intersect 4 to increase their diversity; (ii) sampling an instance for each possible BabelNet sense of a sampled noun; this increases the semantic (and visual) variety for each word (e.g. sampling both the financial institution sense and the river sense of the noun 'bank'). The training set comprises all remaining instances.</p><p>We sample one image at random from the corresponding synset to illustrate each sense-disambiguated noun. Our preliminary analysis showed that, in most cases, an image tends to correspond to only a single word label. This makes it less challenging for an image classifier which simply performs an exact matching of a test image to a training image, as the same image is repeated frequently across instances of the same noun. To circumvent this problem, we ensured that the images in the validation and test sets are both disjoint from the images in the training set. This is done by reserving 10% of all unique images for each synset in the validation and test sets respectively, and removing all these images from the training set. Our final training set consists of 4, 277, 772 instances with 2, 797 unique masked words. The number of unique words in the validation and test set is 496 and 493 respectively, signifying their diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Evaluation metrics</head><p>The models are evaluated using two metrics: (i) accuracy; (ii) average word similarity. The accuracy measures the proportion of correctly predicted words (exact token match) across test instances. The word similarity score measures the average semantic similarity across test instances between the predicted word and the correct word. For this paper, the cosine similarity between word2vec embeddings is used. Our evaluation script can be found on https://github.com/ josiahwang/multisubs-eval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Results</head><p>We trained different models on four disjoint subsets of the training samples. These are selected such that each corresponds to words whose sense have been disambiguated using exactly N languages, i.e. intersect =N ? intersect N ( ?4.1). This allows us to investigate whether our automated cross-lingual disambiguation process helps improve the quality of the images used to unambiguously illustrate the words. During development, our models encountered issues with predicting words that are unseen in the training set, resulting in lower than expected accuracies. This is especially true when training with the intersect =N subsplits. Thus, we report results on a subset of the full test set that only contains output words that have been seen across all training subsplits intersect =N . This test subset contains 3, 262 instances with 169 unique words, and is used across all training subsets.</p><p>Baseline models. Our baseline models are (i) a random baseline that predicts a random target word from the full training set; (ii) a random-multinomial baseline that randomly samples a target word based on its frequency distribution in the full training set; (iii) a classic n-gram model with back-off. The n-gram model learns the most frequent target word from the full training set given the previous n ? 1 context words; the context window is iteratively reduced if the context is not found. In the case of n = 1, the model always predicts the most frequent blanked-out word (man for our dataset). We report results for n ? 9 (the predictions do not change after n = 9). <ref type="table" target="#tab_5">Table 5</ref> presents the baseline results on the test subset. As expected, randomly guessing the blank word does not get the system far. It is useful to note that the word similarity score has a lower-bound of 0.10. Always guessing man (1-gram) is slightly better than randomly guessing, although the accuracy is still low at 1%. Surprisingly, the simple n-gram models with backoff actually perform well, with an accuracy of 23.67% for 4-grams and up to 30.35% for 9-grams. The word similarity scores show a similar trend, with a maximum score of 0.44 with 9-grams.</p><p>Neural-based models. <ref type="table" target="#tab_6">Table 6</ref> shows the accuracies of our automatic models on the fill-in-the-blank task, compared to the estimated human upperbound of 57.59%.</p><p>Overall, text-only models perform better than their imageonly counterparts. Multimodal models that combine both text and image signals perform better in some cases (intersect =3 and intersect =4 ), especially with the cleaner splits where the images have been filtered with more languages. This can be observed when both the performance of image-only models and multimodal models improve as the number of languages used to filter the images increases. This suggests that our automated cross-lingual disambiguation process is beneficial. The text-only models appear to give lower accuracy as the number of languages increases; this is naturally expected as the size of the training set is smaller for larger number of intersecting languages. However, the opposite is true for our multimodal model -we observe substantial improvements instead as the number of languages increases (and thus fewer training examples). This demonstrates that the additional image modality actually helped improve the accuracy, even with a smaller training set. <ref type="table">Table 7</ref> shows the average word similarity scores of the models on the task, to account for predictions that are semantically correct despite not being an exact match. Again, a similar trend is observed: images become more useful as the image filtering process becomes more robust. Thus, we conclude that, given cleaner versions of the dataset, images may prove to be a useful, complementary signal for the fill-in-the-blank task.</p><p>It is interesting that the complex neural-based models (even the multimodal ones) did not perform better than our simpler n-gram based model. This may <ref type="table">Table 7</ref> Word similarity scores for the fill-in-the-blank task on the test subset, comparing text-only, image-only, and multimodal models trained on different subsets of the data. be because the n-gram models are trained on the full dataset rather than the subsplits, although we did not observe any better accuracy when training our BRNN text model on the full training set (17.6% accuracy).</p><p>To further investigate this, we compute the statistics for the n-gram models on the different intersection =N subsplits. The 9-gram models still achieved accuracies comparable to the BRNN text model, between 16.1% to 16.6% for intersection =1 , intersection =2 , and intersection =3 . The accuracies are already close to this level even for 5-grams. The 9-gram model for intersection =4 actually achieved a higher accuracy of 22.75% despite being the smaller subsplit, suggesting that perhaps some of the instances here are useful for predicting the test set; indeed, the test set was taken from this subsplit. There is also a chance that the neural-based models might just need more tweaking to perform better than the n-gram models; we leave this as future work to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Lexical translation</head><p>As MultiSubs is a multimodal and multilingual dataset, we explore a second application for MultiSubs, that is lexical translation (LT). The objective of the LT task is to translate a given word w s in a source language s to a word w f in a specified target language f . The translation is performed in the context of the original sentence in the source language, and optionally with one or more images corresponding to the word w s . The prime motivation for this task is to investigate challenges in multimodal machine translation (MMT) at a lexical level, i.e. for dealing with words that are ambiguous in the target language, or for tackling outof-vocabulary words. For example, the word hat can be translated to German as hut (stereotypical hat with a brim all around the bottom), kappe (a cap), or m?tze (winter hat). Textual context alone may not be sufficient to inform translation in such cases, and the hypothesis that images can be used to complement the sentences in helping translate a source word to its correct sense in the target language.</p><p>For this paper, we fix English as the source language, and explore translating words to the four tar-get languages in MultiSubs: Spanish (ES), Portuguese (PT), French (FR), and German (DE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Models</head><p>We follow the models as described in ?7.1.1. The text based models are based on a similar BRNN model but in this case, the input contains the entire English source sentence with a marked word (marked with an underscore). The marked word in this case is the word to be translated. The BRNN model is trained with the objective of predicting the translation for the marked word and null for all words that are not marked. Our hypothesis is that in the current setting the model is able to maximally exploit context for the translation of the source word.</p><p>For the image-only model, we use a two-layered neural network where the input is the source word and the image feature is used to condition the hidden layer. The multimodal model is a variant of the text-only BRNN, but initialized with the image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Dataset and settings</head><p>We use the same procedure as ?7.1.2 to prepare the dataset. Instead of being masked, the English words now act as the word to be translated. The corresponding word in the target language is obtained from our automatic alignment procedure ( ?4.1). For each target language, we use a subset of MultiSubs where the sentences are aligned to the target language. We also remove unambiguous instances where a word can only be translated to the same word in the target language.</p><p>Like ?7.1.2, the number of validation and test instances is fixed at 5, 000 each. Again, to tackle issues with unseen labels, we use a version of the test set which contains only a subset where the output target words are seen during training.</p><p>The number of training instances per language are: 2, 356, 787 (ES), 1, 950, 455 (PT), 1, 143, 608 (FR), and 405, 759 (DE). The number of unique source and target words are between 131 ? 153 and 173 ? 223 respectively for the test set.</p><p>Like ?7.1.2, we train models across different intersect =N subsplits. We also subsample each split to be of equal sizes to keep the number of training examples consistent across the subsplits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Evaluation metric</head><p>For the LT task, we propose a metric that rewards correctly translated ambiguous words and penalises words translated to the wrong sense. We name this metric Ambiguous Lexical Index (ALI). An ALI score is awarded per English word to measure how well the word has been correctly translated, and the score is averaged across all words. More specifically, for each English word, a score of +1 is awarded if the correct translation of the word is found in the output translation, a score of ?1 is assigned if a known incorrect translation (from our dictionary) is found, and 0 if none of the candidate words are found in the translation. The ALI scores for each English word is obtained by averaging the scores of individual instances, and an overall score is obtained by averaging across all English words. Thus, a per-word ALI score of 1 indicates that the word is always translated correctly, ?1 indicates that the word is always translated to the wrong sense, and 0 means the word is never translated to any of the potential target words in our dictionary for the word.</p><p>Being a per-word metric, another advantage of ALI is that you can choose only a subset of words to evaluate. For example, you may evaluate on only a subset of words that are highly ambiguous and more difficult. ALI will provide a measure of how difficult it is for a system to correctly translate this set of words to its correct sense in the target language on average, without overly biasing the evaluation towards words that appear more frequently in the test dataset. We will not test this in this paper, but will leave this as potential future work.</p><p>This metric is similar to the accuracy metric proposed by Lala &amp; Specia <ref type="bibr" target="#b24">[22]</ref>, where they evaluate the capabilities of MT systems at disambiguating words. However, they penalize equally translations with an incorrect sense (-1 in ALI) and translations that rephrase the text and do not contain any of the possible senses (0 in ALI). In addition, they only consider matches at token level, and do not consider matches at the lemma level. Finally, their metric is computed per instance, while ALI is computed per English word.</p><p>An implementation of the ALI metric can be found in our evaluation script at https://github.com/josiahwang/multisubs-eval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Results</head><p>Baseline models. Like the fill-in-the-blank task, <ref type="table" target="#tab_8">Table 8</ref> reports the results of a simple n-gram with backoff baseline, trained on the full training set. The model will predict the translation given the target word and the n ? 1 words before the word in the source language, and will back-off as in the fill-in-the-blank model. The 1-gram model is equivalent to a Most Frequent Translation (MFT) baseline trained on the full dataset. We report the results for up to n = 5, as the ALI scores are approximately the same beyond that. The ALI scores are generally quite high, and just by using one or two more context words in the source language (2-gram and 3-gram), we can already achieve reasonably high ALI scores.</p><p>Neural-based models. <ref type="table" target="#tab_9">Table 9</ref> shows the ALI scores for our models, evaluated on the test set. We compare the model scores to a most frequent translation (MFT) baseline obtained from the respective intersect =N subsplits of the training data. Interestingly, the MFT baselines actually performed better than the BRNN textonly model in general. The exception is for the intersect =4 split, where the opposite is observed: the ALI scores for MFT drastically drop while the scores for the textonly model drastically improve. Further investigation is needed to ascertain the reason for the drastic change in scores. Our suspicion is that there are many English words in the test set with only a few test instances; ALI is computed per-word thus weights equally English words that occur frequently and those that occur once or twice in the test set. A variation in the predictions for these infrequent words, coupled with the small number of source words in the test set in general, might swing the scores drastically. For example, 31% of the 143 source words in the Spanish test set have only 1-3 instances.</p><p>For our models, we observe a general trend where as we go from intersect =1 to intersect =4 the ALI score over all models tend to consistently increase. The textonly models performed better than image-only models. Thus, as a lexical translation task, images do not seem to be as useful as text for translation. Indeed, like observations in multimodal machine translation research, textual cues play a stronger role in translation, as the space of possible lexical translations is already narrowed down by knowing the source word. There does not appear to be any significant improvement when adding image signals to text models. It still remains to be seen whether this is due to images not being useful or that the multimodal model is not effectively using images during translation. It is also worth noting that our n-gram with backoff baselines trained on the full training set <ref type="table" target="#tab_8">(Table 8)</ref> achieved higher ALI scores than all our neural-based models (with the exception of the irregular score for the text and multimodal models trained on intersect =4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We introduced MultiSubs, a large-scale multimodal and multilingual dataset aimed at facilitating research on grounding words to images in the context of their corresponding sentences. The dataset consists of a parallel corpus of subtitles in English and four other languages, and selected words are illustrated with one or more images in the context of the sentence. This provides a tighter local correspondence between text and images, allowing the learning of associations between text fragments and their corresponding images. The structure of the text is also less constrained than existing multilingual and multimodal datasets, making it more representative of multimodal grounding in real-world scenarios.</p><p>Human evaluation in the form of a fill-in-the-blank game showed that the task is quite challenging, where humans failed to guess a missing word 42.41% of the time, and could correctly guess only 21.89% of instances without any images. We applied MultiSubs on two tasks: fill-in-the-blank and lexical translation, and compared automatic models that use and do not use images as contextual cues for both tasks. We plan to further develop MultiSubs to annotate more phrases with images, and to improve the quality and quantity of images associated with the text fragments. MultiSubs will benefit research on visual grounding of words especially in the context of free-form sentences, and is made publicly available under a Creative Commons licence on https://doi.org/10.5281/zenodo.5034604.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>any chance you're ever gonna get that kid out of the trunk or what ? une chance que vous sortiez ce gosse un jour de la malle ou pas ? alguma chance de tirar aquela crian?a do ba? ou o qu? ? EN FR PT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Example of using multilingual corpora to disambiguate and illustrate a phrase. they knew the gods put dewdrops on plants in the night. sabiam que os deues punham orvalho nas plantas ? noite today we are announcing the closing of 11 of our older plants. hoje anunciamos o encerramento de 11 das f?bricas mais antigas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Example disambiguation in the EN-PT portion of MultiSubs. In both cases, plants were correctly disambiguated using 4 languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Token/type statistics on the sentences of intersect 1 MultiSubs.</figDesc><table><row><cell></cell><cell>tokens</cell><cell>types</cell><cell cols="2">avg length singletons</cell></row><row><cell cols="3">EN 27,423,227 152,520</cell><cell>12.70</cell><cell>2,005,874</cell></row><row><cell>ES</cell><cell cols="2">25,616,482 245,686</cell><cell>11.86</cell><cell>2,012,476</cell></row><row><cell cols="3">EN 23,110,285 138,487</cell><cell>12.87</cell><cell>1,685,102</cell></row><row><cell cols="3">PT 20,538,013 205,410</cell><cell>11.43</cell><cell>1,687,903</cell></row><row><cell cols="3">EN 13,523,651 104,851</cell><cell>12.72</cell><cell>1,012,136</cell></row><row><cell cols="3">FR 12,956,305 149,372</cell><cell>12.19</cell><cell>1,004,304</cell></row><row><cell>EN</cell><cell>4,670,577</cell><cell>62,138</cell><cell>12.15</cell><cell>364,656</cell></row><row><cell>DE</cell><cell>4,311,350</cell><cell>123,087</cell><cell>11.21</cell><cell>364,613</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Distribution across different attempts by humans in the fill-in-the-blank task.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Correct at attempt</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>Failed</cell><cell></cell></row><row><cell>intersect 0</cell><cell>611 (18.75%)</cell><cell>660 (20.26%)</cell><cell>503 (15.44%)</cell><cell>1484 (45.55%)</cell><cell>3258</cell></row><row><cell>intersect 1</cell><cell>534 (21.86%)</cell><cell>481 (19.69%)</cell><cell>378 (15.47%)</cell><cell>1050 (42.98%)</cell><cell>2443</cell></row><row><cell>intersect 2</cell><cell>462 (22.35%)</cell><cell>408 (19.74%)</cell><cell>303 (14.66%)</cell><cell>894 (43.25%)</cell><cell>2067</cell></row><row><cell>intersect 3</cell><cell>432 (24.53%)</cell><cell>388 (22.03%)</cell><cell>260 (14.76%)</cell><cell>681 (38.67%)</cell><cell>1761</cell></row><row><cell>intersect 4</cell><cell>397 (24.84%)</cell><cell>343 (21.46%)</cell><cell>248 (15.52%)</cell><cell>610 (38.17%)</cell><cell>1598</cell></row><row><cell>all</cell><cell cols="5">2436 (21.89%) 2280 (20.49%) 1692 (15.21%) 4719 (42.41%) 11127</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Average word similarity scores of human evaluation of the fill-in-the-blank task.</figDesc><table><row><cell></cell><cell cols="3">Average scores for attempt</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>intersect 0</cell><cell>0.33 (3258)</cell><cell cols="2">0.47 (2647) 0.47 (1987)</cell></row><row><cell>intersect 1</cell><cell>0.36 (2443)</cell><cell cols="2">0.47 (1909) 0.49 (1428)</cell></row><row><cell>intersect 2</cell><cell>0.37 (2067)</cell><cell cols="2">0.48 (1605) 0.48 (1197)</cell></row><row><cell>intersect 3</cell><cell>0.38 (1761)</cell><cell>0.51 (1329)</cell><cell>0.50 (941)</cell></row><row><cell>intersect 4</cell><cell>0.39 (1598)</cell><cell>0.50 (1201)</cell><cell>0.52 (858)</cell></row><row><cell>all</cell><cell cols="3">0.36 (11127) 0.48 (8691) 0.49 (6411)</cell></row></table><note>he was one of the best pitchers in baseball . baseball (1.00) uh , you know , i got to fix the sink , catch the game . car (0.06), sink (1.00) i saw it at the supermarket and i thought that maybe you would have liked it . market (0.18) shop (0.50), supermarket (1.00) It's mac , the night watchman . before (0.07), police (0.31), guard (0.26) Fig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Accuracy and word similarity scores for our baseline (text-only) models on the fill-in-the-blank task, evaluated on the test subset and trained on the full training set.</figDesc><table><row><cell></cell><cell cols="2">Accuracy (%) Word similarity</cell></row><row><cell>random</cell><cell>0.00</cell><cell>0.10</cell></row><row><cell>random-multinomial</cell><cell>0.03</cell><cell>0.12</cell></row><row><cell>1-gram</cell><cell>1.07</cell><cell>0.17</cell></row><row><cell>2-gram</cell><cell>8.74</cell><cell>0.22</cell></row><row><cell>3-gram</cell><cell>16.03</cell><cell>0.31</cell></row><row><cell>4-gram</cell><cell>23.67</cell><cell>0.38</cell></row><row><cell>5-gram</cell><cell>27.35</cell><cell>0.41</cell></row><row><cell>6-gram</cell><cell>29.28</cell><cell>0.43</cell></row><row><cell>7-gram</cell><cell>30.07</cell><cell>0.43</cell></row><row><cell>8-gram</cell><cell>30.32</cell><cell>0.44</cell></row><row><cell>9-gram</cell><cell>30.35</cell><cell>0.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Accuracy scores (%) for the fill-in-the-blank task on the test subset, comparing text-only, image-only, and multimodal model trained on different subsets of the data.</figDesc><table><row><cell>text</cell><cell cols="2">image multimodal</cell></row><row><cell>intersect =1 16.49</cell><cell>9.84</cell><cell>14.53</cell></row><row><cell>intersect =2 18.82</cell><cell>11.62</cell><cell>16.00</cell></row><row><cell>intersect =3 17.72</cell><cell>12.91</cell><cell>19.19</cell></row><row><cell>intersect =4 15.57</cell><cell>13.70</cell><cell>30.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>ALI scores on the lexical translation task for the ngram with back-off baseline models trained on the full training set. The results are reported on the test subset. Note that 1-gram is equivalent to a Most Frequent Translation (MFT) baseline trained on the full dataset.</figDesc><table><row><cell></cell><cell cols="5">1-gram 2-gram 3-gram 4-gram 5-gram</cell></row><row><cell>ES</cell><cell>0.58</cell><cell>0.64</cell><cell>0.65</cell><cell>0.64</cell><cell>0.64</cell></row><row><cell>PT</cell><cell>0.54</cell><cell>0.64</cell><cell>0.69</cell><cell>0.69</cell><cell>0.69</cell></row><row><cell>FR</cell><cell>0.68</cell><cell>0.73</cell><cell>0.73</cell><cell>0.70</cell><cell>0.70</cell></row><row><cell>DE</cell><cell>0.50</cell><cell>0.56</cell><cell>0.59</cell><cell>0.59</cell><cell>0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>ALI scores for the lexical translation task on the test set, comparing an MFT baseline, text-only, image-only, and multimodal models trained on different subsets of the data.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">MFT text image multimodal</cell></row><row><cell></cell><cell>intersect=1</cell><cell>0.53</cell><cell>0.42</cell><cell>0.19</cell><cell>0.43</cell></row><row><cell></cell><cell>intersect=2</cell><cell>0.54</cell><cell>0.48</cell><cell>0.30</cell><cell>0.54</cell></row><row><cell>ES</cell><cell>intersect=3</cell><cell>0.64</cell><cell>0.55</cell><cell>0.33</cell><cell>0.51</cell></row><row><cell></cell><cell>intersect=4</cell><cell>0.40</cell><cell>0.81</cell><cell>0.34</cell><cell>0.81</cell></row><row><cell></cell><cell>intersect=1</cell><cell>0.52</cell><cell>0.40</cell><cell>0.21</cell><cell>0.41</cell></row><row><cell></cell><cell>intersect=2</cell><cell>0.55</cell><cell>0.47</cell><cell>0.30</cell><cell>0.45</cell></row><row><cell>PT</cell><cell>intersect=3</cell><cell>0.55</cell><cell>0.47</cell><cell>0.32</cell><cell>0.48</cell></row><row><cell></cell><cell>intersect=4</cell><cell>0.36</cell><cell>0.79</cell><cell>0.37</cell><cell>0.80</cell></row><row><cell></cell><cell>intersect=1</cell><cell>0.59</cell><cell>0.44</cell><cell>0.22</cell><cell>0.46</cell></row><row><cell></cell><cell>intersect=2</cell><cell>0.66</cell><cell>0.50</cell><cell>0.30</cell><cell>0.54</cell></row><row><cell>FR</cell><cell>intersect=3</cell><cell>0.75</cell><cell>0.59</cell><cell>0.31</cell><cell>0.55</cell></row><row><cell></cell><cell>intersect=4</cell><cell>0.31</cell><cell>0.81</cell><cell>0.33</cell><cell>0.81</cell></row><row><cell></cell><cell>intersect=1</cell><cell>0.35</cell><cell>0.41</cell><cell>0.27</cell><cell>0.43</cell></row><row><cell></cell><cell>intersect=2</cell><cell>0.45</cell><cell>0.52</cell><cell>0.27</cell><cell>0.50</cell></row><row><cell>DE</cell><cell>intersect=3</cell><cell>0.58</cell><cell>0.54</cell><cell>0.34</cell><cell>0.53</cell></row><row><cell></cell><cell>intersect=4</cell><cell>0.27</cell><cell>0.92</cell><cell>0.37</cell><cell>0.94</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the term 'movie' to cover all types of shows such as movies, TV series, and mini series.<ref type="bibr" target="#b2">3</ref> PoS tagger from spaCy v2: en_core_web_md from https: //spacy.io/models/en.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We used the lemmas provided by spaCy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video description: A survey of methods, datasets and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aafaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno>abs/1806.00186</idno>
		<ptr target="http://arxiv.org/abs/1806.00186" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279.URLhttp:</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2798607</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grounding distributional semantics in the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.1111/lnc3.12170</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal grounding for language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beinborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1197" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, NM, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2325" to="2339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="409" to="442" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1504.00325v2</idno>
		<ptr target="http://arxiv.org/abs/1504.00325v2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.121</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2017/html/Das_Visual_Dialog_CVPR_2017_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miami</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-4718" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-3210</idno>
		<ptr target="http://www.aclweb.org/anthology/W16-3210" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
		<meeting>the 5th Workshop on Vision and Language<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
				<ptr target="https://www.aclweb.org/anthology/N10-1011" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of action recognition datasets for language and vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
	<note>Canada</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual sense disambiguation for verbs using multimodal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning translations via images with a massively multilingual image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-1239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A corpus of images and text in online news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bedjeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">; K</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<editor>J. Odijk, S. Piperidis</editor>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accessed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imdb</surname></persName>
		</author>
		<ptr target="https://www.imdb.com/interfaces/" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2018" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual bilingual lexicon induction with transferred ConvNet features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<idno type="DOI">DOI10.18653/v1/D15-1015</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grounded word sense translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Shortcomings in Vision and Language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal lexical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">; K</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2018/summaries/629.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<editor>J. Odijk, S. Piperidis, T. Tokunaga</editor>
		<meeting>the Language Resources and Evaluation Conference<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3810" to="3817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">doi.org/10.1007/978-3-319-10602-1_48</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars</editor>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">; K</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<editor>J. Odijk, S. Piperidis</editor>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Recipe1m: A dataset for learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mar?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1810.06553</idno>
		<ptr target="http://arxiv.org/abs/1810.06553" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shimizu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K.Q. Weinberger</editor>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">DOI10.18653/v1/P16-1168</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BabelNet: The automatic construction, evaluation and application of a widecoverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accessed</title>
		<ptr target="http://www.opensubtitles.org/" />
	</analytic>
	<monogr>
		<title level="m">OpenSubtitles: Subtitles -download movie and TV Series subtitles</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2018" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inferring psycholinguistic properties of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1050</idno>
		<ptr target="http://www.aclweb.org/anthology/N16-1050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BreakingNews: Article annotation by image and text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno>10. 1109/TPAMI.2017.2721945</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1072" to="1085" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>10.1007/ s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dataset and reranking method for multimodal MT of user-generated image captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W18-1814" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 13th Conference of the Association for Machine Translation in the Americas<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Research Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal video indexing: A review of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:MTAP.0000046380.27575.a5</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="35" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298935</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/229" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
