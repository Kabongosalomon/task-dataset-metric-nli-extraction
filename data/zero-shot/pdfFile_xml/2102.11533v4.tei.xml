<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACCURATE LEARNING OF GRAPH REPRESENTATIONS WITH GRAPH MULTISET POOLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
							<email>jinheon.baek@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minki</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aitrics</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">ACCURATE LEARNING OF GRAPH REPRESENTATIONS WITH GRAPH MULTISET POOLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b53">(Zhou et al., 2018;</ref><ref type="bibr" target="#b44">Wu et al., 2019)</ref>, which work with graph structured data, have recently attracted considerable attention, as they can learn expressive representations for various graph-related tasks such as node classification, link prediction, and graph classification. While the majority of the existing works on GNNs focus on the message passing strategies for neighborhood aggregation <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017)</ref>, which aims to encode the nodes in a graph accurately, graph pooling <ref type="bibr" target="#b48">Ying et al., 2018)</ref> that maps the set of nodes into a compact representation is crucial in capturing a meaningful structure of an entire graph.</p><p>As a simplest approach for graph pooling, we can average or sum all node features in the given graph <ref type="bibr" target="#b1">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref>  <ref type="figure" target="#fig_0">(Figure 1 (B)</ref>). However, since such simple aggregation schemes treat all nodes equally without considering their relative importance on the given tasks, they can not generate a meaningful graph representation in a task-specific manner. Their flat architecture designs also restrict their capability toward the hierarchical pooling or graph compression into few nodes. To tackle these limitations, several differentiable pooling operations have been proposed to condense the given graph. There are two dominant approaches to pooling a graph. Node drop methods <ref type="bibr" target="#b23">Lee et al., 2019b)</ref>  <ref type="figure" target="#fig_0">(Figure 1 (C)</ref>) obtain a score of each node using information from graph convolutional layers, and then drop unnecessary nodes with lower scores at each pooling step. Node clustering methods <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019)</ref>  <ref type="figure" target="#fig_0">(Figure 1 (D)</ref>), on the other hand, cluster similar nodes into a single node by exploiting their hierarchical structure. Both graph pooling approaches have obvious drawbacks. First, node drop methods unnecessarily drop some nodes at every pooling step, leading to information loss on those discarded nodes. On the other hand, node clustering methods compute the dense cluster assignment matrix with an adjacency matrix. This prevents them from exploiting sparsity in the graph topology, leading to excessively high computational complexity <ref type="bibr" target="#b23">(Lee et al., 2019b)</ref>. Furthermore, to accurately represent the graph, the GNNs should obtain a representation that is as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b43">(Weisfeiler &amp; Leman, 1968)</ref>, such that it can map two different graphs onto two distinct embeddings. While recent message-passing operations satisfy this constraint <ref type="bibr" target="#b27">(Morris et al., 2019;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref>, most deep graph pooling works <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b23">Lee et al., 2019b;</ref><ref type="bibr" target="#b15">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019)</ref> overlook graph isomorphism except for a few .</p><p>To obtain accurate representations of graphs, we need a graph pooling function that is as powerful as the WL test in distinguishing two different graphs. To this end, we first focus on that the graph representation learning can be regarded as a multiset encoding problem, which allows for possibly repeating elements, since a graph may have redundant node representations (See <ref type="figure" target="#fig_0">Figure 1, right)</ref>. However, since a graph is more than a multiset due to its structural constraint, we further define the problem as a graph multiset encoding, whose goal is to encode two different graphs, given as multisets of node features with auxiliary structural dependencies among them (See <ref type="figure" target="#fig_0">Figure 1, right)</ref>, into two unique embeddings. We tackle this problem by utilizing a graph-structured attention unit. By leveraging this unit as a fundamental building block, we propose the Graph Multiset Transformer (GMT), a pooling mechanism that condenses the given graph into the set of representative nodes, and then further encodes relationships between them to enhance the representation power of a graph. We theoretically analyze the connection between our pooling operations and WL test, and further show that our graph multiset pooling function can be easily extended to node clustering methods.</p><p>We then experimentally validate the graph classification performance of GMT on 10 benchmark datasets from biochemical and social domains, on which it significantly outperforms existing methods on most of them. However, since graph classification tasks only require discriminative information, to better quantify the amount of information about the graph in condensed nodes after pooling, we further validate it on graph reconstruction of synthetic and molecule graphs, and also on two graph generation tasks, namely molecule generation and retrosynthesis. Notably, GMT outperforms baselines with even larger performance gap on graph reconstruction, which demonstrates that it learns meaningful information without forgetting original graph structure. Finally, it improves the graph generation performance on two tasks, which shows that GMT can be well coupled with other GNNs for graph representation learning. In sum, our main contributions are summarized as follows:</p><p>? We treat a graph pooling problem as a multiset encoding problem, under which we consider relationships among nodes in a set with several attention units, to make a compact representation of an entire graph only with one global function, without additional message-passing operations.</p><p>? We show that existing GNN with our parametric pooling operation can be as powerful as the WL test, and also be easily extended to the node clustering approaches with learnable clusters.</p><p>? We extensively validate GMT for graph classification, reconstruction, and generation tasks on synthetic and real-world graphs, on which it largely outperforms most graph pooling baselines.</p><p>Graph Neural Network Existing graph neural network (GNN) models generally encode the nodes by aggregating the features from the neighbors <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017;</ref><ref type="bibr" target="#b49">You et al., 2019)</ref>, and have achieved a large success on node classification and link prediction tasks. Recently, there also exist transformer-based GNNs <ref type="bibr" target="#b29">(Nguyen et al., 2019;</ref><ref type="bibr" target="#b35">Rong et al., 2020)</ref> that further consider the relatedness between nodes in learning the node embeddings. However, accurately representing the given graph as a whole remains challenging. While using mean or max over the node embeddings allow to represent the entire graph for graph classification <ref type="bibr" target="#b11">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b8">Dai et al., 2016)</ref>, they are mostly suboptimal, and may output the same representation for two different graphs. To resolve this problem, recent GNN models <ref type="bibr" target="#b46">(Xu et al., 2019;</ref><ref type="bibr" target="#b27">Morris et al., 2019)</ref> aim to make the GNNs to be as powerful as the Weisfeiler-Lehman test <ref type="bibr" target="#b43">(Weisfeiler &amp; Leman, 1968)</ref> in distinguishing graph structures. Yet, they also rely on simple operations, and we need a more sophisticated method to represent the entire graph.</p><p>Graph Pooling Graph pooling methods play an essential role of representing the entire graph. While averaging all node features is directly used as simplest pooling methods <ref type="bibr" target="#b1">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b36">Simonovsky &amp; Komodakis, 2017)</ref>, they result in a loss of information since they consider all node information equally without considering key features for graphs. To overcome this limitation, there have been recent studies on graph pooling to compress the given graph in a task specific manner. Node drop methods use learnable scoring functions to drop nodes with lower scores <ref type="bibr" target="#b15">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b23">Lee et al., 2019b)</ref>. Moreover, node clustering methods cast the graph pooling problem into the node clustering problem to map the nodes into a set of clusters <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019;</ref><ref type="bibr" target="#b50">Yuan &amp; Ji, 2020)</ref>. Some methods combine these two approaches by first locally clustering the neighboring nodes, and then dropping unimportant clusters <ref type="bibr" target="#b34">(Ranjan et al., 2020)</ref>. Meanwhile, edge clustering gradually merges nodes by contracting high-scoring edges between them <ref type="bibr" target="#b10">(Diehl, 2019)</ref>. In addition, Ahmadi et al. (2020) model the memory layer to aggregate nodes without utilizing message-passing after pooling. Finally, there exists a semi-supervised pooling method  that scores nodes with an attention scheme <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref>, to weight more on the important nodes on pooling.</p><p>(Multi-)Set Representation Learning Note that a set of nodes in a graph forms a multiset <ref type="bibr" target="#b46">(Xu et al., 2019)</ref>; a set that allows possibly repeating elements. Therefore, contrary to the previous setencoding methods, which mainly consider non-graph problems <ref type="bibr" target="#b31">(Qi et al., 2017a;</ref><ref type="bibr" target="#b47">Yi et al., 2019;</ref><ref type="bibr" target="#b37">Snell et al., 2017)</ref>, we regard the graph representation learning as a multi-set encoding problem. Mathematically, <ref type="bibr" target="#b51">Zaheer et al. (2017);</ref><ref type="bibr" target="#b32">Qi et al. (2017b)</ref> provide the theoretical grounds on permutation invariant functions for the set encoding. Further, <ref type="bibr" target="#b22">Lee et al. (2019a)</ref> propose Set Transformer, which uses attention mechanism on the set encoding. Building on top of these theoretical grounds on set, we propose the multiset encoding function that explicitly considers the graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH MULTISET POOLING</head><p>We posit the graph representation learning problem as a multiset encoding problem, and then utilize the graph-structured attention to consider the global graph structure when encoding the given graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PRELIMINARIES</head><p>We begin with the general descriptions of graph neural network, and graph pooling.</p><p>Graph Neural Network A graph G can be represented by its adjacency matrix A ? {0, 1} n?n and the node set V with |V| = n nodes, along with the c dimensional node features X ? R n?c . Graph Neural Networks (GNNs) learn feature representation for different nodes using neighborhood aggregation schemes, which are formalized as the following Message-Passing function:</p><formula xml:id="formula_0">H (l+1) u = UPDATE (l) H (l) u , AGGREGATE (l) H (l) v , ?v ? N (u) ,<label>(1)</label></formula><p>where H (l+1) ? R n?d is the node features computed after l-steps of the GNN simplified as follows:</p><formula xml:id="formula_1">H (l+1) = GNN (l) (H (l) , A (l)</formula><p>), UPDATE and AGGREGATE are arbitrary differentiable functions, N (u) denotes a set of neighboring nodes of u, and H</p><p>u is initialized as the input node features X u .</p><p>Graph Pooling While message-passing functions can produce a set of node representations, we need an additional READOUT function to obtain an entire graph representation h G ? R d as follows:</p><formula xml:id="formula_3">h G = READOUT ({H v | v ? V}) .<label>(2)</label></formula><p>As a READOUT function, we can simply use the average or sum over all node features H v , ?v ? V from the given graph <ref type="bibr" target="#b1">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref>. However, since such aggregation schemes take all node information equally without considering the graph structures, they lose structural information that is necessary for accurately representing a graph. To tackle this limitation, Node Drop methods <ref type="bibr" target="#b15">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b23">Lee et al., 2019b)</ref> select the high scored nodes i (l+1) ? R n l+1 with learnable score function s at layer l, to drop the unnecessary nodes, denoted as follows:</p><formula xml:id="formula_4">y (l) = s(H (l) , A (l) ); i (l+1) = top k (y (l) ),<label>(3)</label></formula><p>where function s depends on specific implementations, and top k function samples the top k nodes by dropping nodes with low scores y (l) ? R n l . Whereas Node Clustering methods <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019</ref>) learn a cluster assignment matrix C (l) ? R n l ?n l+1 with node features H (l) ? R n l ?d , to coarsen the nodes and the adjacency matrix A (l) ? R n l ?n l at layer l as follows:</p><formula xml:id="formula_5">H (l+1) = C (l) T H (l) ; A (l+1) = C (l) T A (l) C (l) ,<label>(4)</label></formula><p>where generating an assignment matrix C (l) depends on specific implementations. While these two approaches obtain decent performances on graph classification tasks, they are suboptimal since node drop methods unnecessarily drop arbitrary nodes, and node clustering methods have limited scalability to large graphs <ref type="bibr" target="#b5">(Cangea et al., 2018;</ref><ref type="bibr" target="#b23">Lee et al., 2019b)</ref>. Therefore, we need a sophisticated graph pooling layer that coarsens the graph with sparse implementation without discarding nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GRAPH MULTISET TRANSFORMER</head><p>We now describe the Graph Multiset Transformer (GMT) architecture, which can accurately represent the entire graph, given a multiset of node features. We first introduce a multiset encoding scheme that allows to embed two different graphs into distinct embeddings, and then describe the graph multi-head attention that reflects the graph topology in the attention-based multiset encoding.</p><p>Multiset Encoding The input of the graph pooling function READOUT consists of nodes in a graph, and they form a multiset (i.e. a set that allows for repeating elements) since different nodes can have identical feature vectors. To design a graph pooling function that is as powerful as the WL test, it needs to satisfy the permutation invariance and injectiveness over the multiset, since two non-isomorphic graphs should be embedded differently through the injective function. While the simple sum pooling satisfies the injectiveness over a multiset <ref type="bibr" target="#b46">(Xu et al., 2019)</ref>, it may treat all node embeddings equally without consideration of their relevance to the task. To resolve this issue, we consider attention mechanism on the multiset pooling function to capture structural dependencies among nodes within a graph, in which we can provably enjoy the expressive power of the WL test.</p><p>Graph Multi-head Attention To overcome the inability of simple pooling methods (e.g. sum) on distinguishing important nodes, we use the attention mechanism as the main component in our pooling scheme. Assume that we have n node vectors, and the input of the attention function (Att) consists of query Q ? R nq?d k , key K ? R n?d k and value V ? R n?dv , where n q is the number of query vectors, n is the number of input nodes, d k is the dimensionlity of the key vector, and d v is the dimensionality of the value vector. Then we compute the dot product of the query with all keys, to put more weights on the relevant values, namely nodes, as follows:</p><formula xml:id="formula_6">Att(Q, K, V ) = w(QK T )V ,</formula><p>where w is an activation function. Instead of computing a single attention, we can further use a multi-head attention <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, by linearly projecting the query Q, key K, and value V h times respectively to yield h different representation subspaces. The output of the multi-head attention function (MH) then can be denoted as follows:</p><formula xml:id="formula_7">MH(Q, K, V ) = [O 1 , ..., O h ] W O ; O i = Att(QW Q i , KW K i , V W V i ),<label>(5)</label></formula><p>where the operations for h parallel projections are parameter matrices</p><formula xml:id="formula_8">W Q i ? R d k ?d k , W K i ? R d k ?d k , and W V i ? R dv?dv . Also, the output projection matrix is W O ? R hdv?d model , where d<label>model</label></formula><p>is the output dimensionality for the multi-head attention (MH) function.</p><p>While multi-head attention is superior to trivial pooling methods such as sum or mean as it considers global dependencies among nodes, the MH function suboptimally generates the key K and value  <ref type="figure">Figure 2</ref>: Graph Multiset Transformer. Given a graph passed through several message passing layers, we use an attention-based pooling block (GMPool) and a self-attention block (SelfAtt) to compress the nodes into few important nodes and consider the interaction among them respectively, within a multiset framework.</p><p>V for Att, since it linearly projects the obtained node embeddings H from equation 1 to further obtain the key and value pairs. To tackle this limitation, we newly define a novel graph multi-head attention block (GMH). Formally, given node features H ? R n?d with their adjacency information A, we construct the key and value using GNNs, to explicitly leverage the graph structure as follows:</p><formula xml:id="formula_9">GMH(Q, H, A) = [O 1 , ..., O h ] W O ; O i = Att(QW Q i , GNN K i (H, A), GNN V i (H, A)</formula><p>), (6) where the output of GNN i contains neighboring information of the graph, compared to the linearly projected node embeddings KW K i and V W V i in equation 5, for key and value matrices in Att. Graph Multiset Pooling with Graph Multi-head Attention Using the ingredients above, we now propose a graph pooling function that satisfies the injectiveness and permutation invariance, such that the overall architecture can be at most as powerful as the WL test, while taking the graph structure into account. Given node features H ? R n?d from GNNs, we define a Graph Multiset Pooling (GMPool), which is inspired by the Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b22">Lee et al., 2019a)</ref>, to compress the n nodes into the k typical nodes, with a parameterized seed matrix S ? R k?d for the pooling operation that is directly optimized in an end-to-end fashion, as follows ( <ref type="figure">Figure 2</ref></p><formula xml:id="formula_10">-GMPool): GMPool k (H, A) = LN(Z + rFF(Z)); Z = LN(S + GMH(S, H, A)),<label>(7)</label></formula><p>where rFF is any row-wise feedforward layer that processes each individual row independently and identically, and LN is a layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>. Note that the GMH function in equation 7 considers interactions between k seed vectors (queries) in S and n nodes (keys) in H, to compress n nodes into k clusters with their attention similarities between queries and keys. Also, to extend the pooling scheme from set to multiset, we simply consider redundant node representations.</p><p>Self-Attention for Inter-node Relationship While previously described GMPool condenses entire nodes into k representative nodes, a major drawback of this scheme is that it does not consider relationships between nodes. To tackle this limitation, one should further consider the interactions among n or condensed k different nodes. To this end, we propose a Self-Attention function (SelfAtt), inspired by the Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b22">Lee et al., 2019a)</ref>, as follows <ref type="figure">(</ref> </p><p>where, compared to GMH in equation 7 that considers interactions between k vectors and n nodes, SelfAtt captures inter-relationships among n nodes by putting node embeddings H on both query and key locations in MH of equation 8. To satisfy the injectiveness property of SelfAtt, it might not consider interactions among n nodes, which we discuss in Proposition 3 of Appendix A.1.</p><p>Overall Architecture We now describe the full structure of Graph Multiset Transformer (GMT) consisting of GNN and pooling layers using ingredients above (See <ref type="figure">Figure 2)</ref>. For a graph G with node features X and an adjacency matrix A, the Encoder : G ? H ? R n?d is denoted as follows:</p><formula xml:id="formula_12">Encoder(X, A) = GNN 2 (GNN 1 (X, A), A),<label>(9)</label></formula><p>where we can stack several GNNs to construct the deep structures. After obtaining a set of node features H from an encoder, the pooling layer aggregates the features into a single vector form;</p><formula xml:id="formula_13">Pooling : H, A ? h G ? R d .</formula><p>To deal with a large number of nodes, we first condense the entire graph into k representative nodes with Graph Multiset Pooling (GMPool), which is also adaptable to the varying size of nodes, and then utilize the interaction among them with Self-Attention Block (SelfAtt). Finally, we get the entire graph representation by using GMPool with k = 1 as follows:</p><formula xml:id="formula_14">Pooling(H, A) = GMPool 1 (SelfAtt(GMPool k (H, A)), A ),<label>(10)</label></formula><p>where A ? R k?k is the identity or coarsened adjacency matrix since adjacency information should be adjusted after compressing the nodes from n to k with GMPool k (See Appendix B for detail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CONNECTION WITH WEISFEILER-LEHMAN GRAPH ISOMORPHISM TEST</head><p>Weisfeiler-Lehman (WL) test <ref type="bibr" target="#b43">(Weisfeiler &amp; Leman, 1968</ref>) is known for its ability to efficiently distinguish two different graphs. Recent studies <ref type="bibr" target="#b27">(Morris et al., 2019;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref> show that GNNs can be made to be as powerful as the WL test, by using an injective function over a multiset to map two different graphs into distinct spaces. Building on previous powerful GNNs, if our graph pooling function is injective, then our overall architecture can be at most as powerful as the WL test. To do so, we first recount the theorem from <ref type="bibr" target="#b46">Xu et al. (2019)</ref>, as formalized in Theorem 1.</p><p>Theorem 1 (Non-isomorphic Graphs to Different Embeddings). Let A : G ? R d be a GNN, and Weisfeiler-Lehman test decides two graphs G 1 ? G and G 2 ? G as non-isomorphic. Then, A maps two different graphs G 1 and G 2 to distinct vectors if node aggregation and update functions are injective, and graph-level readout, which operates on a multiset of node features {H i }, is injective.</p><p>Since we focus on the representation of graphs through pooling, we deal with the injectiveness of the READOUT function. Our next Lemma 2 states that GMPool can represent the injective function.</p><p>Lemma 2 (Injectiveness on Graph Multiset Pooling). Assume the input feature space H is a countable set. Then the output of GMPool i k (H, A) with GMH(S i , H, A) for a seed vector S i can be unique for each multiset H ? H of bounded size. Further, the output of full GMPool k (H, A) constructs a multiset with k elements, which are also unique on the input multiset H.</p><p>All proofs for the WL test are provided in Appendix A.1. Based upon the injectiveness of GM-Pool, we show injectiveness of SelfAtt, to make an overall architecture (sequence of GMPool and SelfAtt with GNNs) as powerful as the WL test, formalized in Proposition 3. To satisfy the injectiveness of SelfAtt, we might not care about interactions among multiset elements (See Appendix A.1).</p><p>Proposition 3 (Injectiveness on Pooling Function). The overall Graph Multiset Transformer with multiple GMPool and SelfAtt can map two different graphs G 1 and G 2 to distinct embedding spaces, such that the resulting GNN with proposed pooling functions can be as powerful as the WL test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CONNECTION WITH NODE CLUSTERING APPROACHES</head><p>Node clustering is widely used for coarsening a graph in a hierarchical manner, as described in the equation 4. However, since they require to store and even multiply the adjacency matrix A with the soft assignment matrix C: A (l+1) = C (l) T A (l) C (l) , they need a quadratic space O(n 2 ) for n nodes, which is problematic for large graphs. Meanwhile, our GMPool does not compute a coarsened adjacency matrix A (l+1) , such that graph pooling is possible only with a sparse implementation, as formalized in Theorem 4. All proofs regarding node clustering are provided in Appendix A.2.</p><p>Theorem 4 (Space Complexity of Graph Multiset Pooling). Graph Multiset Pooling condsense a graph with n nodes to k nodes in O(nk) space complexity, which can be further optimized to O(n).</p><p>In spite of this huge strength on space complexity, our GMPool can be further approximated to the node clustering methods by manipulating an adjacency matrix, as formalized in Proposition 5.</p><p>Proposition 5 (Approximation to Node Clustering). Graph Multiset Pooling GMPool k can perform hierarchical node clustering with learnable k cluster centroids by Seed Vector S in equation 7.</p><p>Note that, contrary to previous node clusterings <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019)</ref>, GMPool learns data dependent k cluster centroids that might be more meaningful to capture graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>To validate the proposed Graph Multiset Transformer (GMT) for graph representation learning, we evaluate it on classification, reconstruction and generation tasks of synthetic and real-world graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH CLASSIFICATION</head><p>Objective The goal of graph classification is to predict a label y i ? Y of a given graph G i ? G, with a mapping function f : G ? Y. To this end, we use a set of node representations {H v | v ? V} to obtain an entire graph representation h G that is used to classify a label f (G) =?. We then learn f with a cross-entropy loss, to minimize the negative log likelihood as follows: min i=1 ?y i log? i . <ref type="table">Table 1</ref>: Graph classification results on test sets. The reported results are mean and standard deviation over 10 different runs. Best performance and its comparable results (p &gt; 0.05) from the t-test are marked in bold. Hyphen (-) denotes out-of-resources that take more than 10 days (See <ref type="figure">Figure 4</ref> for the time efficiency analysis).   Datasets Among TU datasets , we select 6 datasets including 3 datasets (D&amp;D, PROTEINS, and MUTAG) on Biochemical domain, and 3 datasets (IMDB-B, IMDB-M, and COL-LAB) on Social domain with accuracy for evaluation metric. Also, we use 4 molecule datasets (HIV, Tox21, ToxCast, BBBP) from the OGB datasets <ref type="bibr" target="#b18">(Hu et al., 2020)</ref> with ROC-AUC for evaluation metric. Statistics are reported in the <ref type="table">Table 1</ref>, and more details are described in the Appendix C.2.</p><p>Models 1) GCN. 2) GIN. GNNs with mean or sum pooling <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref>. 3) Set2Set. Set pooling baseline <ref type="bibr" target="#b40">(Vinyals et al., 2016)</ref>. 4) SortPool. 5) SAGPool. 6) TopKPool. 7) ASAP. The methods <ref type="bibr" target="#b23">Lee et al., 2019b;</ref><ref type="bibr" target="#b15">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b34">Ranjan et al., 2020</ref>) that use the node drop, by dropping nodes (or clusters) with lower scores using scoring functions. 8) DiffPool. 9) MinCutPool. 10) HaarPool. 11) StructPool. The methods <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019;</ref><ref type="bibr" target="#b50">Yuan &amp; Ji, 2020</ref>) that use the node clustering, by grouping a set of nodes into a set of clusters using a cluster assignment matrix. 12) EdgePool. The method <ref type="bibr" target="#b10">(Diehl, 2019</ref>) that gradually merges two adjacent nodes that have a high score edge. 13) GMT. The proposed Graph Multiset Transformer (See Appendix C.1 for detailed descriptions).</p><p>Implementation Details For a fair comparison of pooling baselines <ref type="bibr" target="#b23">(Lee et al., 2019b)</ref>, we fix the GCN (Kipf &amp; Welling, 2017) as a message passing layer. We evaluate the model performance on TU datasets for 10-fold cross validation <ref type="bibr" target="#b46">Xu et al., 2019)</ref> with LIBSVM <ref type="bibr" target="#b7">(Chang &amp; Lin, 2011)</ref>. Also, we use the initial node features following the fair comparison setup <ref type="bibr" target="#b14">(Errica et al., 2020)</ref>. We evaluate the performance on OGB datasets with their original feature extraction and data split settings <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. Experimental details are described in the Appendix C.2.</p><p>Classification Results <ref type="table">Table 1</ref> shows that our GMT outperforms most baselines, or achieves comparable performance to the best baseline results. These results demonstrate that our method is simple yet powerful as it only performs a single global operation at the final layer, unlike several baselines that use multiple pooling with a sequence of message passing (See <ref type="figure" target="#fig_9">Figure 9</ref> for the detailed model architectures). Note that, since graph classification tasks mostly require the discriminative information to predict the labels of a graph, GNN baselines without parametric pooling, such as GCN and GIN, sometimes outperform pooling baselines on some datasets. In addition, recent work <ref type="bibr" target="#b26">(Mesquita et al., 2020)</ref>, which reveals that message-passing layers are dominant in the graph classification, supports this phenomenon. Therefore, we conduct experiments on graph reconstruction to directly quantify the amount of retained information after pooling, which we describe in the next subsection. Ablation Study To see where the performance improvement comes from, we conduct an ablation study on GMT by removing graph attention, self attention, and message-passing operations. <ref type="table" target="#tab_1">Table 2</ref> shows that using graph attention with self-attention helps significantly improve the performances from the mean pooling. Further, performances of the GMT without message-passing layers indicate that our pooling layer well captures the graph multiset structure only with pooling without GNNs.</p><p>Efficiency While node clustering methods achieve decent performances in <ref type="table">Table 1</ref>, they are known to suffer from large memory usage since they cannot work with sparse graph implementations. To compare the GPU Memory Efficiency of GMT with baseline models, we test it on the Erdos-Renyi graphs <ref type="bibr" target="#b13">(Erd?s &amp; R?nyi, 1960)</ref> (See Appendix C.2 for detail setup). <ref type="figure">Figure 3</ref> shows that our GMT is highly efficient in terms of memory thanks to its compatibility with sparse graphs, making it more practical over memory-heavy pooling baselines. In addition to this, we measure the Time Efficiency to further validate the practicality of GMT in terms of time complexity. We validate it with the same Erdos-Renyi graphs (See Appendix C.2 for detail setup). <ref type="figure">Figure 4</ref> shows that GMT takes less than (or nearly about) a second even for large graphs, compared to the slowly working models such as HaarPool and EdgePool. This result further confirms that our GMT is practically efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GRAPH RECONSTRUCTION</head><p>Graph classification does not directly measure the expressiveness of GNNs since identifying discriminative features may be more important than accurately representing graphs. Meanwhile, graph reconstruction directly quantifies the graph information retained by condensed nodes after pooling.</p><p>Objective For graph reconstruction, we train an autoencoder to reconstruct the input node features X ? R n?c from their pooled representations X pool ? R k?c . The learning objective to minimize the discrepancy between the original graph X and the reconstructed graph X rec with a cluster assignment matrix C ? R n?k is denoted as follows: min X ? X rec , where X rec = CX pool .</p><p>Experimental Setup We first experiment with Synthetic Graph, such as ring and grid <ref type="bibr" target="#b4">(Bianchi et al., 2019)</ref>, that can be represented in a 2-D Euclidean space, where the goal is to restore the location of each node from pooled features, with an adjacency matrix. We further experiment with real-world Molecule Graph, namely ZINC datasets <ref type="bibr" target="#b19">(Irwin et al., 2012)</ref>, which consists of 12K molecular graphs. See Appendix C.3 for the experimental details including model descriptions.  Reconstruction Results <ref type="figure">Figure 5</ref> shows the original and the reconstructed graphs for Synthetic Graph of ring and grid structures. The noisy results of baselines indicate that the condensed node features do not fully capture the original graph structure. Whereas our GMPool yields almost perfect reconstruction, which demonstrates that our pooling operation learns meaningful representation without discarding the original graph information. We further validate the reconstruction performance of the proposed GMPool on the real-world Molecule Graph, namely ZINC, by varying the compression ratio. <ref type="figure">Figure 6</ref> shows reconstruction results on the molecule graph, on which GMPool largely outperforms all compared baselines in terms of validity, exact match, and accuracy (High score indicates the better, and see Appendix C.3 for the detailed description of evaluation metrics). With given results, we demonstrate that our GMPool can be easily extended to the node clustering schemes, while it is powerful enough to encode meaningful information to reconstruct the graph.   <ref type="table">Table 3</ref>: Top-k accuracy for Retrosynthesis experiment on USPTO-50k data, for cases where the reaction class is given as prior information (Bottom) and not given (Top).</p><p>Qualitative Analysis We visualize the reconstruction examples from ZINC in <ref type="figure" target="#fig_6">Figure 7</ref>, where colors in the left figure indicate the assigned clusters on each atoms, and red dashed circles indicate the incorrectly predicted atoms on the reconstructed molecule. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, GMPool yields more calibrated clustering than MinCutPool, capturing the detailed substructures, which results in the successful reconstruction (See <ref type="figure" target="#fig_0">Figure 11</ref> in Appendix D for more reconstruction examples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GRAPH GENERATION</head><p>Objective Graph generation is used to generate a valid graph that satisfies the desired properties, in which graph encoding is used to improve the generation performances. Formally, given a graph G with graph encoding function f , the goal here is to generate a valid graph? ? G of desired property y with graph decoding function g as follows: min d(y,?), where? = g(f (G)). d is a distance metric between the generated graph and desired properties, to guarantee that the graph has them.</p><p>Experimental Setup To evaluate the applicability of our model, we experiment on Molecule Generation to stably generate the valid molecules with <ref type="bibr">MolGAN (Cao &amp; Kipf, 2018)</ref>, and Retrosynthesis to empower the synthesis performances with Graph Logic Network (GLN) , by replacing their graph embedding function f (G) to ours. In both experiments, we replace the average pooling to either the MinCutPool or GMT. See Appendix C.4 for more experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation Results</head><p>The power of a discriminator distinguishing whether a molecule is real or fake is highly important to create a valid molecule in MolGAN. <ref type="figure" target="#fig_7">Figure 8</ref> shows the validity curve on the early stage of MolGAN training for Molecule Generation, and the representation power of GMT significantly leads to the stabilized generation of valid molecules than baselines. Further, <ref type="table">Table 3</ref> shows Retrosynthesis results, where we use the GLN as a backbone architecture. Similar to the molecule generation, retrosynthesis with GMT further improves the performances, which suggests that GMT can replace existing pooling methods for improved performances on diverse graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we pointed out that existing graph pooling approaches either do not consider the task relevance of each node (sum or mean) or may not satisfy the injectiveness (node drop and clustering methods). To overcome such limitations, we proposed a novel graph pooling method, Graph Multiset Transformer (GMT), which not only encodes the given set of node embeddings as a multiset to uniquely embed two different graphs into two distinct embeddings, but also considers both the global structure of the graph and their task relevance in compressing the node features. We theoretically justified that the proposed pooling function is as powerful as the WL test, and can be extended to the node clustering schemes. We validated the proposed GMT on 10 graph classification datasets, and our method outperformed state-of-the-art graph pooling models on most of them. We further showed that our method is superior to the existing graph pooling approaches on graph reconstruction and generation tasks, which require more accurate representations of the graph than classification tasks. We strongly believe that the proposed pooling method will bring substantial practical impact, as it is generally applicable to many graph-learning tasks that are becoming increasingly important. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS A.1 PROOFS REGARDING WEISFEILER-LEHMAN TEST</head><p>We first recount the theorem of <ref type="bibr" target="#b46">Xu et al. (2019)</ref> to define a GNN that is injective over a multiset (Theorem 1). We then prove that the proposed Graph Multiset Pooling (GMPool) can map two different graphs to distinct spaces (Lemma 2). Finally, we show that our overall architecture Graph Multiset Transformer (GMT) with a sequence of proposed Graph Multiset Pooling (GMPool) and Self-Attention (SelfAtt) can represent the injective function over the input multiset (Proposition 3).</p><p>Theorem 1 (Non-isomorphic Graphs to Different Embeddings). Let A : G ? R d be a GNN, and Weisfeiler-Lehman test decides two graphs G 1 ? G and G 2 ? G as non-isomorphic. Then, A maps two different graphs G 1 and G 2 to distinct vectors if node aggregation and update functions are injective, and graph-level readout, which operates on a multiset of node features {H i }, is injective.</p><p>Proof. To map two non-isomorphic graphs to distinct embedding spaces with GNNs, we recount the theorem on Graph Isomorphism Network. See Appendix B of <ref type="bibr" target="#b46">Xu et al. (2019)</ref> for details.</p><p>Lemma 2 (Uniqueness on Graph Multiset Pooling). Assume the input feature space H is a countable set. Then the output of the GMPool i k (H, A) with GMH(S i , H, A) for a seed vector S i can be unique for each multiset H ? H of bounded size. Further, the output of the full GMPool k (H, A) constructs a multiset with k elements, which are also unique on the input multiset H.</p><p>Proof. We first state that the GNNs of the Graph Multi-head Attention (GMH) in a GMPool can represent the injective function over the multiset H with an adjacency information A, by selecting proper message-passing functions that satisfy the WL test <ref type="bibr" target="#b46">(Xu et al., 2019;</ref><ref type="bibr" target="#b27">Morris et al., 2019)</ref>, denoted as follows: H <ref type="figure">= GNN(H, A)</ref>, where H ? H. Then, given enough elements, a GMPool i k (H, A) can express the sum pooling over the multiset H defined as follows: ?( h?H f (h)), where f and ? are mapping functions (see the proof of PMA in <ref type="bibr" target="#b22">Lee et al. (2019a)</ref>).</p><p>Since H is a countable set, there is a mapping from the elements to prime numbers denoted by p(h) : H ? P. If we let f (h) = ? log p(h), then h?H f (h) = log h?H 1 p(h) which constitutes an unique mapping for every multiset H ? H (see <ref type="bibr" target="#b41">Wagstaff et al. (2019)</ref>). In other words, h?H f (h) is injective. Also, we can easily construct a function ?, such that GMPool i k (H, A) = ?( h?H f (h)) = ?(log h?H 1 p(h) ) is the injective function for every multiset H ? H, where H is derived from the GNN component in the GMPool; H = GNN(H, A).</p><p>Furthermore, since a GMPool considers multiset elements without any order, it satisfies the permutation invariance condition for the multiset function.</p><p>Finally, each GMPool block has k components such that the output of it consists of k elements as follows:</p><formula xml:id="formula_15">GMPool = GMPool i k (H, A) k i=1</formula><p>, which allows multiple instances for its elements. Then, since each GMPool i k <ref type="figure">(H, A)</ref> is unique on the input multiset H, the output of the GMPool that consists of k outputs is also unique on the input multiset H.</p><p>Thanks to the universal approximation theorem <ref type="bibr" target="#b17">(Hornik et al., 1989)</ref>, we can construct such functions p and ? using multi-layer perceptrons (MLPs).</p><p>Proposition 3 (Injectiveness on Pooling Function). The overall Graph Multiset Transformer with multiple GMPool and SelfAtt can map two different graphs G 1 and G 2 to distinct embedding spaces, such that the resulting GNN with proposed pooling functions can be as powerful as the WL test.</p><p>Proof. By Lemma 2, we know that a Graph Multiset Pooling (GMPool) can represent the injective function over the input multiset H ? H. If we can also show that a Self-Attention (SelfAtt) can represent the injective function over the multiset, then the sequence of the GMPool and SelfAtt blocks can satisfy the injectiveness.</p><p>Let W O be a zero matrix in SelfAtt function. SelfAtt(H) then can be approximated to the any instance-wise feed-forward network denoted as follows: SelfAtt(H) = rFF(H). Therefore, this rFF is a suitable transformation ? : R d ? R d that can be easily constructed over the multiset elements h ? H, to satisfy the injectiveness.</p><p>To maximize the discriminative power of the Graph Multiset Transformer (GMT) by satisfying the WL test, we assume that SelfAtt does not consider the interactions among multiset elements, namely nodes. While proper GNNs with the proposed pooling function can be at most as powerful as the WL test with this assumption, our experimental results with the ablation study show that the interaction among nodes is significantly important to distinguish the broad classes of graphs (See <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOFS REGRADING NODE CLUSTERING</head><p>We first prove that the space complexity of the Graph Multiset Pooling (GMPool) without GNNs can be approximated to the O(n) with n nodes (Theorem 4). After that, we show that the GMPool can be extended to the node clustering approaches with learnable cluster centroids (Proposition 5).</p><p>Theorem 4 (Space Complexity of Graph Multiset Pooling). Graph Multiset Pooling condsense a graph with n nodes to k nodes in O(nk) space complexity, which can be further optimized to O(n).</p><p>Proof. Assume that we have key K ? R n?d k and value V ? R n?dv matrices in the Att function of Graph Multi-head Attention (GMH) for the simplicity, which is described in the equation 6. Also, Q is defined as a seed vector S ? R k?d in the GMPool function of the equation 7. To obtain the weights on the values V , we multiply the query Q with key K: QK T . This matrix multiplication then maps a set of n nodes into a set of k nodes, such that it requires O(nk) space complexity. Also, we can further drop the constant term k: O(n), by properly setting the small k values; k n.</p><p>The multiplication of the attention weights QK T with value V also takes the same complexity, such that the overall space complexity of GMPool is O(nk), which can be further optimized to O(n).</p><p>The space complexity of GNNs with sparse implementation requires O(n + m) space complexity, where n is the number of nodes, and m is the number of edges in a graph. Therefore, multiple GNNs followed by our GMPool require the total space complexity of O(n + m) due to the space complexity of the GNN operations. However, GNNs with our GMPool are more efficient than node clustering methods, since node clustering approaches need O(n 2 ) space complexity.</p><p>Proposition 5 (Approximation to Node Clustering). Graph Multiset Pooling GMPool k can perform hierarchical node clustering with learnable k cluster centroids by Seed Vector S in equation 7.</p><p>Proof. Node clustering approaches are widely used to coarsen a given large graph in a hierarchical manner with several message-passing functions. The core part of the node clustering schemes is to generate a cluster assignment matrix C, to coarsen nodes and adjacency matrix as in an equation 4. Therefore, if our Graph Multiset Pooling (GMPool) can generate a cluster assignment matrix C, then the proposed GMPool can be directly approximated to the node clustering approaches.</p><p>In the proposed GMPool, query Q is generated from a learnable set of k seed vectors S, and key K and value V are generated from node features H with GNNs in the Graph Multi-head Attention (GMH) block, as in an equation 6. In this function, if we decompose the attention function Att(Q, K, V ) = w(QK T )V into the dot products of the query with all keys, and the corresponding weighted sum of values, then the first dot product term inherently generates a soft assignment matrix as follows: C = w(QK T ). Therefore, the proposed GMPool can be easily extended to the node clustering schemes, with the inherently generated cluster assignment matrix; C = w(QK T ), where one of the proper choices for the activation function w is the softmax function as follows:</p><formula xml:id="formula_16">w(QK T ) i,j = exp(Q i K T j ) k n=1 exp(Q n K T j ) .<label>(11)</label></formula><p>Furthermore, through the learnable seed vectors S for the query Q, we can learn data dependent k different cluster centroids in an end-to-end fashion.</p><p>Note that, as shown in the section 4.2 of the main paper, the proposed GMPool significantly outperforms the previous node clustering approaches <ref type="bibr" target="#b48">(Ying et al., 2018;</ref><ref type="bibr" target="#b4">Bianchi et al., 2019)</ref>. This is because, contrary to them, the proposed GMPool can explicitly learn data dependent k cluster centroids by learnable seed vectors S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS FOR GRAPH MULTISET TRANSFORMER COMPONENTS</head><p>In this section, we describe the Graph Multiset Pooling (GMPool) and Self-Attention (SelfAtt), which are the components of the proposed Graph Multiset Transformer, in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Multiset Pooling</head><p>The core components of the Graph Multiset Pooling (GMPool) is the Graph Multi-head Attention (GMH) that considers the graph structure into account, by constructing the key K and value V using GNNs, as described in the equation 6. As shown in the <ref type="table" target="#tab_1">Table 2</ref> of the main paper, this graph multi-head attention significantly outperforms the naive multi-head attention (MH in equation 5). However, after compressing the n nodes into the k nodes with GMPool k , we can not directly perform further GNNs since the original adjacency information is useless after pooling. To tackle this limitation, we can generate the new adjacency matrix A for the compressed nodes, by performing node clustering as described in Proposition 5 of the main paper as follows:</p><formula xml:id="formula_17">GMPool 1 (GMPool k (H, A), A ); A = C T AC,<label>(12)</label></formula><p>where C is the generated cluster assignment matrix, and A is the coarsened adjacency matrix as described in the equation 4. However, this approach is well known for their scalability issues <ref type="bibr" target="#b23">(Lee et al., 2019b;</ref><ref type="bibr" target="#b5">Cangea et al., 2018)</ref>, since they require quadratic space O(n 2 ) to store and even multiply the adjacency matrix A with the soft assignment matrix C. Therefore, we leave doing this as a future work, and use the following trick. By replacing the adjacency matrix A with the identity matrix I in the GMPool except for the first block, we can easily perform multiple GMPools without any constraints, which is approximated to the GMPool with MH in the equation 5, rather than GMH in the equation 6, as follows:</p><formula xml:id="formula_18">GMPool 1 (GMPool k (H, A), A ); A = I.<label>(13)</label></formula><p>Self-Attention The Self-Attention (SelfAtt) function can consider the inter-relationships between nodes in a set, which helps the network to take the global graph structure into account. Because of this advantage, the self-attention function significantly improves the proposed model performance on the graph classification tasks, as shown in the <ref type="table" target="#tab_1">Table 2</ref> of the main paper. From a different perspective, we can regard the Self-Attention function as a graph neural network (GNN) with a complete graph. Specifically, given k nodes from the previous layer, the Multi-head Attention (MH) of the Self-Attention function first constructs the adjacency matrix among all nodes with their similarities, through the matrix multiplication of the query with key: QK T , and then computes the outputs with the sum of the obtained weights on the value. In other words, the self-attention function can be considered as one message passing function with a soft adjacency matrix, which might be further connected to the Graph Attention Network .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL SETUP</head><p>In this section, we first introduce the baselines and our model, and then describe the experimental details about graph classification, reconstruction, and generation tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 BASELINES AND OUR MODEL</head><p>1) GCN. This method <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> is the mean pooling baseline with Graph Convolutional Network (GCN) as a message passing layer.</p><p>2) GIN. This method <ref type="bibr" target="#b46">(Xu et al., 2019)</ref> is the sum pooling baseline with Graph Isomorphism Network (GIN) as a message passing layer.</p><p>3) Set2Set. This method <ref type="bibr" target="#b40">(Vinyals et al., 2016)</ref> is the set pooling baseline that uses a recurrent neural network to encode a set of all nodes, with content-based attention over them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>SortPool. This method  is the node drop baseline that drops unimportant nodes by sorting their representations, which are directly generated from the previous GNN layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>SAGPool. This method <ref type="bibr" target="#b23">(Lee et al., 2019b)</ref> is the node drop baseline that selects the important nodes, by dropping unimportant nodes with lower scores that are generated by the another graph convolutional layer, instead of using scores from the previously passed layers. Particularly, this method has two variants. 6.1) SAGPool(G) is the global node drop method that drops unimportant nodes one time at the end of their architecture. 6.2) SAGPool(H) is the hierarchical node drop method that drops unimportant nodes sequentially with multiple graph convolutional layers.</p><p>6) TopkPool. This method <ref type="bibr" target="#b15">(Gao &amp; Ji, 2019)</ref> is the node drop baseline that selects the top-ranked nodes using a learnable scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7)</head><p>ASAP. This method <ref type="bibr" target="#b34">(Ranjan et al., 2020)</ref> is the node drop baseline that first locally generates the clusters with neighboring nodes, and then drops the lower score clusters using a scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8)</head><p>DiffPool. This method <ref type="bibr" target="#b48">(Ying et al., 2018)</ref> is the node clustering baseline that produces the hierarchical representation of the graphs in an end-to-end fashion, by clustering similar nodes into the few nodes through graph convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9)</head><p>MinCutPool. This method <ref type="bibr" target="#b4">(Bianchi et al., 2019)</ref> is the node clustering baseline that applies the spectral clustering with GNNs, to coarsen the nodes and the adjacency matrix of a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10)</head><p>HaarPool. This method  is the spectral-based pooling baseline that compresses the node features with a nonlinear transformation in a Haar wavelet domain. Since it directly uses the spectral clustering to generate a coarsened matrix, the time complexity cost is relatively higher than other pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11)</head><p>StructPool. This method <ref type="bibr" target="#b50">(Yuan &amp; Ji, 2020)</ref> is the node clustering baseline that integrates the concept of the conditional random field into the graph pooling. While this method can be used with a hierarchical scheme, we use it with a global scheme following their original implementation, which is similar to the SortPool .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12)</head><p>EdgePool. This method <ref type="bibr" target="#b10">(Diehl, 2019)</ref> is the edge clustering baseline that gradually merges the nodes, by contracting the high score edge between two adjacent nodes.</p><p>13) GMT. Our Graph Multiset Transformer that first condenses all nodes into the important nodes by GMPool, and then considers interactions between nodes in a set. Since it operates on the global READOUT layer, it can be coupled with hierarchical pooling methods by replacing their last layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 GRAPH CLASSIFICATION</head><p>Dataset Among TU datasets , we select the 6 datasets including 3 datasets (D&amp;D, PROTEINS, and MUTAG) on Biochemical domain, and 3 datasets (IMDB-B, IMDB-M, and COLLAB) on Social domain. We use the classification accuracy as an evaluation metric. As suggested by <ref type="bibr" target="#b14">Errica et al. (2020)</ref> for a fair comparison, we use the one-hot encoding of their atom types as initial node features in the bio-chemical datasets, and the one-hot encoding of node degrees as initial node features in the social datasets. Moreover, we use the recently suggested 4 molecule graphs (HIV, Tox21, ToxCast, BBBP) from the OGB datasets <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. We use the ROC-AUC for an evaluation metric, and use the additional atom and bond features, as suggested by <ref type="bibr" target="#b18">Hu et al. (2020)</ref>. Dataset statistics are reported in the <ref type="table">Table 1</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details on Classification Experiments</head><p>For all experiments on TU datasets, we evaluate the model performance with a 10-fold cross validation setting, where the dataset split is based on the conventionally used training/test splits <ref type="bibr" target="#b30">(Niepert et al., 2016;</ref><ref type="bibr" target="#b46">Xu et al., 2019)</ref>, with LIBSVM <ref type="bibr" target="#b7">(Chang &amp; Lin, 2011)</ref>. In addition, we use the 10 percent of the training data as a validation data following the fair comparison setup <ref type="bibr" target="#b14">(Errica et al., 2020)</ref>. For all experiments on OGB datasets, we evaluate the model performance following the original training/validation/test dataset splits <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. We use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs, for the TU datasets. Further, the maximum number of epochs is set to 500. We then report the average performances on the validation and test sets, by performing overall experiments 10 times with different seeds.</p><p>For all experiments on TU datasets except the D&amp;D, the learning rate is set to 5 ? 10 ?4 , hidden size is set to 128, batch size is set to 128, weight decay is set to 1 ? 10 ?4 , and dropout rate is set to 0.5.</p><p>Since the D&amp;D dataset has a large number of nodes (See <ref type="table">Table 1</ref> in the main paper), node clustering methods can not perform clustering operations on large graphs with large batch sizes, such that the hidden size is set to 32, and batch size is set to 10 on the D&amp;D dataset. For all experiments on OGB datasets except the HIV, the learning rate is set to 1 ? 10 ?3 , hidden size is set to 128, batch size is set to 128, weight decay is set to 1 ? 10 ?4 , and dropout rate is set to 0.5. Since the HIV dataset contains a large number of graphs compared to others (See <ref type="table">Table 1</ref> in the main paper), the batch size is set to 512 for fast training. Then we optimize the network with Adam optimizer <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2014)</ref>. For a fair comparison of baselines <ref type="bibr" target="#b23">(Lee et al., 2019b)</ref>, we use the three GCN layers <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> as a message passing function for all models with jumping knowledge strategies <ref type="bibr" target="#b45">(Xu et al., 2018)</ref>, and only change the pooling architecture throughout all models, as illustrated in <ref type="figure" target="#fig_9">Figure 9</ref>. Also, we set the pooling ratio as 25% in each pooling layer for both baselines and our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details on Efficiency Experiments</head><p>To compare the GPU memory efficiency of GMT against baseline models including node drop and node clustering methods, we first generate the Erdos-Renyi graphs <ref type="bibr" target="#b13">(Erd?s &amp; R?nyi, 1960)</ref> by varying the number of nodes n, where the edge size m is twice the number of nodes: m = 2n. For all models, we compress the given n nodes into the k = 4 nodes at the first pooling function.</p><p>To compare the time efficiency of GMT against baseline models, we first generate the Erdos-Renyi graphs <ref type="bibr" target="#b13">(Erd?s &amp; R?nyi, 1960)</ref> by varying the number of nodes n with m = n 2 /10 edges, following the setting of HaarPool . For all models, we set the pooling ratio as 25% except for HaarPool, since it compresses the nodes according to the coarse-grained chain of a graph. We measure the forward time, including CPU and GPU, for all models with 50 graphs over one batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 GRAPH RECONSTRUCTION</head><p>Dataset We first experiment with synthetic graphs represented in a 2-D Euclidean space, such as ring and grid structures. The node features of a graph consist of their location in a 2-D coordinate space, and the adjacency matrix indicates the connectivity pattern of nodes. The goal here is to restore all node locations from compressed features after pooling, with the intact adjacency matrix.</p><p>While synthetic graphs are appropriate choices for the qualitative analysis, we further do the quantitative evaluation of models with real-world molecular graphs. Specifically, we use the subset <ref type="bibr" target="#b12">(Dwivedi et al., 2020)</ref> of the ZINC dataset <ref type="bibr" target="#b19">(Irwin et al., 2012)</ref>, which consists of 12K real-world molecular graphs, to further conduct a graph reconstruction on the large number of various graphs. The goal of the molecule reconstruction task is to restore the exact atom types of all nodes in the given graph, from the compressed representations after pooling.</p><p>Common Implementation Details Following Bianchi et al. <ref type="formula" target="#formula_0">(2019)</ref>, we use the two message passing layers both right before the pooling operation and right after the unpooling operation. Also, both pooling and unpooling operations are performed once and sequentially connected, as illustrated in  the <ref type="figure" target="#fig_9">Figure 9</ref>. We compare our methods against both the node drop (TopKPool <ref type="bibr" target="#b15">(Gao &amp; Ji, 2019)</ref>) and node clustering (DiffPool <ref type="bibr" target="#b48">(Ying et al., 2018) and</ref><ref type="bibr">MinCutPool (Bianchi et al., 2019)</ref>) methods. For the node drop method, we use the unpooling operation proposed in the graph U-net <ref type="bibr" target="#b15">(Gao &amp; Ji, 2019)</ref>. For the node clustering methods, we use the graph coarsening schemes described in the equation 4, with their specific implementations on generating an assignment matrix. For our proposed method, we only use the one Graph Multiset Pooling (GMPool) without SelfAtt, where we follow the node clustering approaches as described in the subsection 3.4 by generating a single soft assignment matrix with one head h = 1 in the multi-head attention function. For experiments of both synthetic and molecule reconstructions, the learning rate is set to 5 ? 10 ?3 , and hidden size is set to 32. We then optimize the network with Adam optimizer <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details on Synthetic Graph</head><p>We set the pooling ratio of all models as 25%. For the loss function, we use the Mean Squared Error (MSE) to train models. We use the early stopping criterion, where we stop the training if there is no further improvement on the training loss during 1,000 epochs. Further, the maximum number of epochs is set to 10,000. Note that, there is no other available graphs for validation of the synthetic graph, such that we train and test the models only with the given graph in the <ref type="figure" target="#fig_0">Figure 10</ref>. The baseline results are adopted from <ref type="bibr" target="#b4">Bianchi et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details on Molecule Graph</head><p>We set the pooling ratio of all models as 5%, 10%, 15%, and 25%, and plot all results in the <ref type="figure">Figure 6</ref> of the main paper. Note that, in the case of molecule graph reconstruction, a softmax layer is appended at the last layer of the model architecture to classify the original atom types of all nodes. For the loss function, we use the cross entropy loss to train models. We use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs. Further, the maximum number of epochs is set to 500, and batch size is set to 128. Note that, in the case of molecule graph reconstruction on the ZINC dataset, we strictly separate the training, validation and test sets, as suggested by <ref type="bibr" target="#b12">Dwivedi et al. (2020)</ref>. We perform all experiments 5 times with 5 different random seeds, and then report the averaged result with the standard deviation. Note that, in addition to baselines mentioned in the common implementation details paragraph, we compare two more baselines: GCN with a random assignment matrix for pooling, which is adopted from <ref type="bibr" target="#b26">Mesquita et al. (2020)</ref>, and StructPool <ref type="bibr" target="#b50">(Yuan &amp; Ji, 2020)</ref>, for the real-world molecule graph reconstruction.</p><p>Evaluation Metrics for Molecule Reconstruction For quantitative evaluations, we use the three metrics as follows: 1) validity indicates the number of reconstructed molecules that are chemically valid, 2) exact match indicates the number of reconstructed molecules that are exactly same as the original molecules, and 3) accuracy indicates the classification accuracy of atom types of all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 GRAPH GENERATION</head><p>Common Implementation Details In the graph generation experiments, we replace the graph embedding function f (G) from existing graph generation models to the proposed Graph Multiset Transformer (GMT), to evaluate the applicability of our model on generation tasks, as described <ref type="table">Table 4</ref>: Graph classification results on validation sets with standard deviations. All results are averaged over 10 different runs. Best performance and its comparable results (p &gt; 0.05) from the t-test are marked in blod. Hyphen (-) denotes out-of-resources that take more than 10 days (See <ref type="figure">Figure 4</ref> for the time efficiency analysis). For MinCutPool, since it cannot directly compress the all n nodes into the 1 cluster to represent the entire graph, we use the following trick to replace the simple pooling operation (e.g. sum or mean) with it. We first condense the graph into the k clusters (k = 4) using one MinCutPool layer, and then average the condensed nodes to get a single representation of the given graph. However, our proposed Graph Multiset Transformer (GMT) can directly compress the all n nodes into the 1 node with one learnable seed vector, by using the single GMPool 1 block. In other words, we use the one GMPool 1 to represent the entire graph by replacing their simple pooling (e.g. sum or mean), in which we use the following softmax activation function for computing attention weights:</p><formula xml:id="formula_19">w(QK T ) i,j = exp(Q i K T j ) n k=1 exp(Q i K T k ) .<label>(14)</label></formula><p>Implementation Details on Molecule Generation For the molecule generation experiment with the MolGAN, we replace the average pooling in the discriminator with GMPool 1 . We use the QM9 dataset <ref type="bibr" target="#b33">(Ramakrishnan et al., 2014)</ref> following the original MolGAN paper <ref type="bibr" target="#b6">(Cao &amp; Kipf, 2018)</ref>. To evaluate the models, we report the validity of 13,319 generated molecules at the early stage of the MolGAN training, over 4 different runs. As depicted in <ref type="figure" target="#fig_7">Figure 8</ref> of the main paper, each solid curve indicates the average validity of each model with 4 different runs, and the shaded area indicates the half of the standard deviation for 4 different runs.</p><p>Implementation Details on Retrosynthesis For the retrosynthesis experiment with the Graph Logic Network (GLN), we replace the average pooling in the template and subgraph encoding functions with GMPool 1 . We use the USPTO-50k dataset following the original paper .</p><p>For an evaluation metric, we use the Top-k accuracy for both reaction class is not given and given cases, following the original paper . We reproduce all results in <ref type="table">Table 3</ref> with published codes from the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL EXPERIMENTAL RESULTS</head><p>Validation Results on Graph Classification We additionally provide the graph classification results on validation sets. As shown in <ref type="table">Table 4</ref>, the proposed GMT outperforms most baselines, or <ref type="table">Table 6</ref>: Quantitative results of the graph reconstruction task on reconstructing the node features and the adjacency matrix for synthetic graphs, with two different minimization objectives and error calculation metrics: X ? X rec and A ? A rec . * indicates the model without using adjacency normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data: Grid Graph Ring Graph</head><p>Objective: min X ? X rec min A ? A rec min X ? X rec min A ? A rec Error Calculation: achieves comparable performances to the best baseline results even in the validation sets. While validation results can not directly measure the generalization performance of the model for unseen data, these results further confirm that our method is powerful enough, compared to baselines. Regarding the results of test sets on the graph classification task, please see <ref type="table">Table 1</ref> in the main paper. Leaderboard Results on Graph Classification For a fair comparison, we experiment with all baselines and our models in the same setting, as described in the implementation details of Appendix C.2. Specifically, we average the results over 10 different runs with the same hidden dimension (128, while leaderboard uses 300), and the same number of message-passing layers (3, while leaderboard uses 5) with 10 different seeds for all models. Therefore, the reproduced results can be slightly different from the leaderboard results, as shown in <ref type="table" target="#tab_5">Table 5</ref>, since the leaderboard uses different hyper-parameters with different random seeds (See Hu et al.</p><formula xml:id="formula_20">X ? X rec A ? A rec X ? X rec A ? A rec X ? X rec A ? A rec X ? X rec A ? A</formula><p>(2020) for more details). However, our reproduction results are almost the same as the leaderboard results, and sometimes outperform the leaderboard results (See the GCN results for the HIV dataset in <ref type="table" target="#tab_5">Table 5</ref>). Therefore, while we conduct all experiments under the same setting for a fair comparison, where specific hyperparameter choices are slightly different from the leaderboard setting, these results indicate that there is no significant difference between reproduced and leaderboard results.</p><p>Quantitative Results on Graph Reconstruction for Synthetic Graphs While we conduct experiments on reconstructing node features on the given graph, to quantify the retained information on the condensed nodes after pooling (See Section 4.2 for experiments on the graph reconstruction task), we further reconstruct the adjacency matrix to see if the pooling layer can also condense the adjacency structure without loss of information. The learning objective to minimize the discrepancy between the original adjacency matrix A and the reconstructed adjacency matrix A rec with a cluster assignment matrix C ? R n?k is defined as follows: min A ? A rec , where A rec = CA pool C T .</p><p>Then we design the following two experiments. First, pooling layers are trained to minimize the objective in Section 4.2: min X ? X rec . After that, we measure the discrepancy between the original and the reconstructed node features: X ? X rec , and also measure the discrepancy between the original and the reconstructed adjacency matrix: A ? A rec . Second, pooling layers are trained to minimize the objective described in the previous paragraph: min A ? A rec , and then we measure the aforementioned two discrepancies in the same way.</p><p>We experiment with synthetic grid and ring graphs, illustrated in <ref type="figure" target="#fig_0">Figure 10</ref>. <ref type="table">Table 6</ref> shows that the error is large when the objective and the error metric are different, which indicates that there is a high discrepancy between the required information for condensing node and the required information for condensing adjacency matrix. In other words, the compression for node and the compression for adjacency matrix might be differently performed to reconstruct the whole graph information.</p><p>Also, <ref type="table">Table 6</ref> shows that there are some cases where there is no significant difference in the calculated adjacency error ( A ? A rec ), when minimizing nodes discrepancies and minimizing adjacency discrepancies (See 0.0331 and 0.0324 for the proposed GMT on the Ring Graph). Furthermore, calculated errors for the adjacency matrix when minimizing adjacency discrepancies are generally larger than the calculated errors for node features when minimizing nodes discrepancies.</p><p>These results indicate that the adjacency matrix is difficult to reconstruct after pooling. This might be because the reconstructed adjacency matrix should be further transformed from continuous values to discrete values (0 or 1 for the undirected simple graph), while the reconstructed node features can be directly represented as continuous values. We leave further reconstructing adjacency matrices and visualizing them as a future work.</p><p>Additional Examples for Molecule Reconstruction We visualize the additional examples for molecule reconstruction on the ZINC dataset in <ref type="figure" target="#fig_0">Figure 11</ref>. Molecules on the left side indicate the original molecule, where the transparent color denotes the assigned cluster for each node, which is obtained by the cluster assignment matrix C with node (atom) representations in a graph (molecule) (See Proposition 5 for more detail on generating the cluster assignment matrix). Also, molecules on the right side indicate the reconstructed molecules with failure cases denoted as a red dotted circle.</p><p>As visualized in <ref type="figure" target="#fig_0">Figure 11</ref>, we can see that the same atom or the similarly connected atoms obtain the same cluster (color). For example, the atom type O mostly obtains the yellow cluster, and the atom type F obtains the green cluster. Furthermore, ring-shaped substructures that do not contain O or N mostly receive the blue cluster, whereas ring-shaped substructures that contain O and N receive the green and yellow clusters respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Concepts (Left): Conceptual comparison of graph pooling methods. Grey box indicates the readout layer, which is compatible with our method. Also, green check icon indicates the model that can be as powerful as the WL test. (Right): An illustration of set, multiset, and graph multiset encoding for graph representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2-SelfAtt): SelfAtt(H) = LN(Z + rFF(Z)); Z = LN(H + MH(H, H, H)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Memory efficiency of GMT compared with baselines. X indicates out-of-memory error. Time efficiency of GMT compared with baselines. X indicates out-of-memory error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Reconstruction results of ring and grid synthetic graphs, compared to node drop and clustering methods. SeeFigure 10for high resolution. Reconstruction results on the ZINC molecule dataset by varying the compression ratio. Solid lines denote the mean, and shaded areas denote the variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Reconstruction example with assigned clusters as colors on left and reconstructed molecules on right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Validity curve for molecule generation on QM9 dataset from MolGAN. Solid lines denote the mean and shaded areas denote the variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2018R1A5A1059921).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Illustration of High-level Model Architectures. (Top): Global Graph Classification; GCN, GIN, Set2Set, SortPool, SAGPool(G), StructPool, GMT. (Middle:) Hierarchical Graph Classification; DiffPool, SAGPool(H), TopKPool, MinCutPool, ASAP, EdgePool, HaarPool. (Bottom:) Graph Reconstruction; Diff-Pool, TopKPool, MinCutPool, GMT. MP denotes the message passing layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>High resolution images for synthetic graph reconstruction results inFigure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Molecule Reconstruction Examples (Left): Original molecules with the assigned cluster on each node represented as color, where cluster is generated from Graph Multiset Pooling (GM-Pool). (Right): Reconstructed molecules. Red dotted circle indicates the incorrect atom prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.55 73.24 ? 0.73 69.50 ? 1.78 76.81 ? 1.01 75.04 ? 0.80 60.63 ? 0.51 65.47 ? 1.73 73.26 ? 0.46 50.39 ? 0.41 80.59 ? 0.27 3 / 10 GIN 70.79 ? 1.17 71.46 ? 1.66 81.39 ? 1.53 75.95 ? 1.35 73.27 ? 0.84 60.83 ? 0.46 67.65 ? 3.00 72.78 ? 0.86 48.13 ? 1.36 78.19 ? 0.63 2 / 10 Set2Set 71.94 ? 0.56 73.27 ? 0.85 69.89 ? 1.94 74.70 ? 1.65 74.10 ? 1.13 59.70 ? 1.04 66.79 ? 1.05 72.90 ? 0.75 50.19 ? 0.39 79.55 ? 0.39 1 / 10 SortPool 75.58 ? 0.72 73.17 ? 0.88 71.94 ? 3.55 71.82 ? 1.63 69.54 ? 0.75 58.69 ? 1.71 65.98 ? 1.70 72.12 ? 1.12 48.18 ? 0.83 77.87 ? 0.47 0 / 10 DiffPool 77.56 ? 0.41 73.03 ? 1.00 79.22 ? 1.02 75.64 ? 1.86 74.88 ? 0.81 62.28 ? 0.56 68.25 ? 0.96 73.14 ? 0.70 51.31 ? 0.72 78.68 ? 0.43 3 / 10 SAGPool(G) 71.54 ? 0.91 72.02 ? 1.08 76.78 ? 2.12 74.56 ? 1.69 71.10 ? 1.06 59.88 ? 0.79 65.16 ? 1.93 72.16 ? 0.88 49.47 ? 0.56 78.85 ? 0.56 0 / 10 SAGPool(H) 74.72 ? 0.82 71.56 ? 1.49 73.67 ? 4.28 71.44 ? 1.67 69.81 ? 1.75 58.91 ? 0.80 63.94 ? 2.59 72.55 ? 1.28 50.23 ? 0.44 78.03 ? 0.31 1 / 10 TopKPool 73.63 ? 0.55 70.48 ? 1.01 67.61 ? 3.36 72.27 ? 0.91 69.39 ? 2.02 58.42 ? 0.91 65.19 ? 2.30 71.58 ? 0.95 48.59 ? 0.72 77.58 ? 0.85 0 / 10 MinCutPool 78.22 ? 0.54 74.72 ? 0.48 79.17 ? 1.64 75.37 ? 2.05 75.11 ? 0.69 62.48 ? 1.33 65.97 ? 1.13 72.65 ? 0.75 51.04 ? 0.70 80.87 ? 0.34 4 / 10 StructPool 78.45 ? 0.40 75.16 ? 0.86 79.50 ? 1.75 75.85 ? 1.81 75.43 ? 0.79 62.17 ? 1.61 67.01 ? 2.65 72.06 ? 0.64 50.23 ? 0.53 77.27 ? 0.51 3 / 10 ASAP 76.58 ? 1.04 73.92 ? 0.63 77.83? 1.49 72.86 ? 1.40 72.24 ? 1.66 58.09 ? 1.62 63.50 ? 2.47 72.81 ? 0.50 50.78 ? 0.75 78.64 ? 0.50 1 / 10 EdgePool 75.85 ? 0.58 75.12 ? 0.76 74.17? 1.82 72.66 ? 1.70 73.77 ? 0.68 60.70 ? 0.92 67.18 ? 1.97 72.46 ? 0.74 50.79 ? Ours) 78.72 ? 0.59 75.09 ? 0.59 83.44 ? 1.33 77.56 ? 1.25 77.30 ? 0.59 65.44 ? 0.58 68.31 ? 1.62 73.48 ? 0.76 50.66 ? 0.82 80.74 ? 0.54</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Biochemical Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Social Domain</cell><cell></cell><cell>Significance</cell></row><row><cell></cell><cell>D&amp;D</cell><cell cols="3">PROTEINS MUTAG</cell><cell>HIV</cell><cell>Tox21</cell><cell>ToxCast</cell><cell>BBBP</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>COLLAB</cell></row><row><cell># graphs</cell><cell>1,178</cell><cell cols="2">1,113</cell><cell>188</cell><cell>41,127</cell><cell>7,831</cell><cell>8,576</cell><cell>2,039</cell><cell>1,000</cell><cell>1,500</cell><cell>5,000</cell><cell>-</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell></cell><cell>2</cell><cell>2</cell><cell>12</cell><cell>617</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>-</cell></row><row><cell>Avg # nodes</cell><cell>284.32</cell><cell cols="2">39.06</cell><cell>17.93</cell><cell>25.51</cell><cell>18.57</cell><cell>18.78</cell><cell>24.06</cell><cell>19.77</cell><cell>13.00</cell><cell>74.49</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="10">72.05 0.59</cell><cell>-</cell><cell>3 / 9</cell></row><row><cell>HaarPool</cell><cell>-</cell><cell>-</cell><cell cols="2">66.11? 1.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">66.11 ? 0.82 73.29 ? 0.34 49.98 ? 0.57</cell><cell>-</cell><cell>1 / 5</cell></row><row><cell cols="13">GMT (10 / 10</cell></row><row><cell>Model</cell><cell></cell><cell cols="3">D&amp;D PROTEINS BBBP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GMT</cell><cell></cell><cell>78.72</cell><cell>75.09</cell><cell>68.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">w/o message passing 78.06</cell><cell>75.07</cell><cell>65.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">w/o graph attention 78.08</cell><cell>74.50</cell><cell>66.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o self-attention</cell><cell>75.13</cell><cell>74.22</cell><cell>64.53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">mean pooling</cell><cell>72.05</cell><cell>73.24</cell><cell>65.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation Study of GMT on the D&amp;D, PROTEINS, and BBBP datasets for graph classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>67.55 74.92 83.48 88.64 92.37 Class MinCutPool 51.17 67.47 75.59 83.68 89.31 92.31 Unknown GMT (Ours) 51.83 68.20 75.17 83.20 89.33 92.47 Reaction GLN 63.53 78.27 84.32 89.51 92.17 93.17 Class MinCutPool 63.91 79.19 84.76 89.69 92.13 93.23 as Prior GMT (Ours) 64.17 79.61 85.32 89.97 92.31 93.25</figDesc><table><row><cell>Top-k accuracy:</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell>Reaction GLN</cell><cell>51.41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 0.65 77.13 ? 0.44 76.56 ? 1.75 81.27 ? 0.92 78.80 ? 0.40 65.66 ? 0.40 93.35 ? 1.08 77.93 ? 0.28 54.29 ? 0.23 83.08 ? 0.13 GIN 76.85 ? 0.61 78.43 ? 0.45 94.44 ? 0.52 82.10 ? 1.01 78.20 ? 0.45 66.29 ? 0.42 94.64 ? 0.36 78.38 ? 0.26 54.04 ? 0.29 82.19 ? 0.25 Set2Set 76.32 ? 0.40 77.64 ? 0.41 79.72 ? 2.40 80.07 ? 0.93 79.13 ? 0.75 66.39 ? 0.49 91.89 ? 1.48 78.13 ? 0.30 54.39 ? 0.19 82.34 ? 0.23 SortPool 80.68 ? 0.59 77.92 ? 0.42 81.33 ? 3.00 81.17 ? 2.30 75.97 ? 0.76 64.26 ? 1.17 94.21 ? 1.04 77.46 ? 0.60 52.95 ? 0.62 80.58 ? 0.25 DiffPool 81.33 ? 0.33 79.09 ? 0.36 87.94 ? 1.93 83.16 ? 0.44 80.02 ? 0.38 69.73 ? 0.79 96.32 ? 0.36 77.86 ? 0.39 54.77 ? 0.19 81.69 ? 0.31 SAGPool(G) 76.73 ? 0.80 77.01 ? 0.58 88.11 ? 1.21 80.55 ? 1.89 77.03 ? 0.76 65.51 ? 0.91 95.59 ? 1.22 78.09 ? 0.58 53.73 ? 0.42 81.91 ? 0.45 SAGPool(H) 79.56 ? 0.67 77.24 ? 0.56 86.06 ? 2.07 79.21 ? 1.50 75.36 ? 2.63 64.05 ? 0.83 93.05 ? 3.00 77.11 ? 0.46 53.49 ? 0.65 80.55 ? 0.56 TopKPool 78.54 ? 0.73 75.47 ? 0.90 75.06 ? 2.12 79.24 ? 1.84 75.06 ? 2.30 64.56 ? 0.56 93.31 ? 2.32 76.12 ? 0.79 52.75 ? 0.58 79.94 ? 0.86 MinCutPool 81.96 ? 0.39 79.23 ? 0.66 87.22 ? 1.72 83.12 ? 1.27 81.10 ? 0.42 69.09 ? 1.12 95.99 ? 0.47 77.76 ? 0.36 54.94 ? 0.19 83.37 ? 0.18 StructPool 82.56 ? 0.37 80.00 ? 0.27 91.5 ? 0.95 81.09 ? 1.26 79.61 ? 0.70 66.49 ? 1.59 95.18 ? 0.59 77.14 ? 0.31 54.13 ? 0.39 79.90 ? 0.18 ASAP 81.58 ? 0.38 78.71 ? 0.45 91.33? 0.65 79.80 ? 1.88 77.33 ? 1.34 63.82 ? 0.75 92.96 ? 1.09 77.89 ? 0.51 55.17 ? 0.33 82.11 ? 0.33 EdgePool 80.32 ? 0.44 79.61 ? 0.25 87.28? 1.18 81.84 ? 1.32 78.92 ? 0.29 66.21 ? 0.64 94.98 ? 0.62 77.50 ? 0.25 54.69 ? Ours) 82.19 ? 0.40 80.01 ? 0.21 91.00 ? 0.82 83.54 ? 0.78 80.91 ? 0.41 69.77 ? 0.67 95.14 ? 0.48 78.43 ? 0.22 55.14 ? 0.25 83.37 ? 0.11 in the subsection 4.3 of the main paper. As baselines, we first use the original models with their implementations. Specifically, we use the MolGAN 2 (Cao &amp; Kipf, 2018) for molecule generation, and Graph Logic Network (GLN) 3 (Dai et al., 2019) for retrosynthesis. For both experiments, we directly follow the experimental details of original papers (Cao &amp; Kipf, 2018; Dai et al., 2019) for a fair comparison. Furthermore, to compare our models with another strong pooling method, we use the MinCutPool (Bianchi et al., 2019)  as an additional baseline for generation tasks, since it shows the best performance among baselines in the previous two classification and reconstruction tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Biochemical Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Social Domain</cell><cell></cell></row><row><cell></cell><cell>D&amp;D</cell><cell cols="2">PROTEINS MUTAG</cell><cell>HIV</cell><cell>Tox21</cell><cell>ToxCast</cell><cell>BBBP</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>COLLAB</cell></row><row><cell># graphs</cell><cell>1,178</cell><cell>1,113</cell><cell>188</cell><cell>41,127</cell><cell>7,831</cell><cell>8,576</cell><cell>2,039</cell><cell>1,000</cell><cell>1,500</cell><cell>5,000</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>12</cell><cell>617</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell>Avg # nodes</cell><cell>284.32</cell><cell>39.06</cell><cell>17.93</cell><cell>25.51</cell><cell>18.57</cell><cell>18.78</cell><cell>24.06</cell><cell>19.77</cell><cell>13.00</cell><cell>74.49</cell></row><row><cell>GCN</cell><cell cols="9">76.17 0.40</cell><cell>-</cell></row><row><cell>HaarPool</cell><cell>-</cell><cell>-</cell><cell>68.22? 0.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">89.98 ? 0.58 76.72 ? 0.60 53.03 ? 0.14</cell><cell>-</cell></row><row><cell>GMT (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Graph classification results for OGB test datasets with standard deviations. Leaderboard GCN 76.06 ? 0.97 75.29 ? 0.69 GIN 75.58 ? 1.40 74.91 ? 0.51 Reproduced GCN 76.81 ? 1.01 75.04 ? 0.80 GIN 75.95 ? 1.35 73.27 ? 0.84 Ours GMT 77.56 ? 1.25 77.30 ? 0.59</figDesc><table><row><cell>Model</cell><cell>HIV</cell><cell>Tox21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/yongqyu/MolGAN-pytorch 3 https://github.com/Hanjun-Dai/GLN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memorybased graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khas</forename><surname>Amir Hosein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quaid</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00481</idno>
		<title level="m">Spectral clustering with graph neural networks for graph pooling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrosynthesis prediction with conditional graph logic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="8870" to="8880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Edge contraction pooling for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10990</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erd?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PUBLICATION OF THE MATHE-MATICAL INSTITUTE OF THE HUNGARIAN ACADEMY OF SCIENCES</title>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page" from="17" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ZINC: A free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised graph classification: A hierarchical graph perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13" />
			<biblScope unit="page" from="972" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11418</idno>
		<title level="m">Rethinking pooling in graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised universal self-attention network for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ASAP: adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GROVER: self-supervised message passing transformer on large-scale molecular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02835</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the limitations of representing functions on sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6487" to="6494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11580</idno>
		<title level="m">Xiaosheng Zhuang, and Yanan Fan. Haarpooling: Graph pooling with compressive haar basis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GSPN: generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structpool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
