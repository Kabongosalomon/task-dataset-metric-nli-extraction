<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-SUPERVISED TRANSFORMERS FOR UNSUPERVISED OBJECT DISCOVERY USING NORMALIZED CUT AUTHOR VERSION OF CVPR22 PAPER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Lab 3 LIGM (</orgName>
								<orgName type="laboratory" key="lab2">UMR 8049</orgName>
								<orgName type="institution" key="instit1">AI</orgName>
								<orgName type="institution" key="instit2">Ecole des Ponts</orgName>
								<address>
									<settlement>UPE</settlement>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Vaufreydaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-SUPERVISED TRANSFORMERS FOR UNSUPERVISED OBJECT DISCOVERY USING NORMALIZED CUT AUTHOR VERSION OF CVPR22 PAPER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers trained with self-supervision using selfdistillation loss (DINO) have been shown to produce attention maps that highlight salient foreground objects. In this paper, we show a graph-based method that uses the self-supervised transformer features to discover an object from an image. Visual tokens are viewed as nodes in a weighted graph with edges representing a connectivity score based on the similarity of tokens. Foreground objects can then be segmented using a normalized graphcut to group self-similar regions. We solve the graph-cut problem using spectral clustering with generalized eigendecomposition and show that the second smallest eigenvector provides a cutting solution since its absolute value indicates the likelihood that a token belongs to a foreground object.</p><p>Despite its simplicity, this approach significantly boosts the performance of unsupervised object discovery: we improve over the recent state-of-the-art LOST by a margin of 6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The performance can be further improved by adding a second stage class-agnostic detector (CAD). Our proposed method can be easily extended to unsupervised saliency detection and weakly supervised object detection. For unsupervised saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS, DUT-OMRON respectively compared to state-of-the-art. For weakly supervised object detection, we achieve competitive performance on CUB and ImageNet. Our code is available at: https://www.m-psi.fr/Papers/TokenCut2022/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is a key enabling technology for realworld vision systems for tasks such as robotics, autonomous driving, traffic monitoring, manufacturing, and embodied artificial intelligence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. However, * Corresponding Author (a) DINO <ref type="bibr" target="#b5">[6]</ref> (b) LOST <ref type="bibr" target="#b44">[45]</ref>.</p><p>(c) TokenCut (ours) (d) Attention maps associated to different patches <ref type="figure">Figure 1</ref>: The attention map of the class token used in DINO <ref type="bibr" target="#b5">[6]</ref> (a) and the map of the inverse degrees used in LOST <ref type="bibr" target="#b44">[45]</ref> (b) are noisy for foreground / background separation. Our proposed method provides a clean attention map that can be used to segment salient objects (c). Considering the attention map associated to different patches highlight different regions of the object (d), it is reasonable to build a graph computing attention maps from multiple patches.</p><p>the performance of current state-of-the-art object detectors is limited by the high cost of annotating sufficient training data <ref type="bibr" target="#b32">[33]</ref> for supervised learning. This limitation becomes even more apparent when using transfer learning to adapt a pre-trained object detector to a new application domain. Approaches such as active learning <ref type="bibr" target="#b0">[1]</ref>, semisupervised learning <ref type="bibr" target="#b33">[34]</ref>, and weakly-supervised learning <ref type="bibr" target="#b38">[39]</ref> have attempted to overcome this barrier by providing more efficient learning, but with only limited success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2202.11539v2 [cs.CV] 24 Mar 2022</head><p>In this work, we focus on object discovery in natural images with no human annotations. This is an important problem and a critical step for many downstream applications <ref type="bibr" target="#b55">[56]</ref>. Poor object discovery can lead to poor overall system performance. Current approaches for this problem adopt some forms of bounding box proposal mechanism <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61]</ref> and formulate object discovery as an optimization problem. However, this approach can be computationally expensive <ref type="bibr" target="#b54">[55]</ref> as each pair of bounding box proposals across different images needs to be compared, and the optimization may fail to scale to larger datasets due to the quadratic computation overhead <ref type="bibr" target="#b56">[57]</ref>.</p><p>Transformers have recently been shown to outperform convolutional neural networks for visual recognition. Vision Transformers, such as ViT <ref type="bibr" target="#b17">[18]</ref> accept image patches as input tokens and use stacked layers of encoders with self-attention to map tokens to image-level class labels.</p><p>Recent results with DINO <ref type="bibr" target="#b5">[6]</ref> have shown that when trained with self-distillation loss <ref type="bibr" target="#b24">[25]</ref>, the attention maps associated to the class token from the last layer indicate salient foreground regions. However, as illustrated in <ref type="figure">Fig. 1a</ref>, such attention maps are noisy and it is not clear that they can be used for unsupervised object discovery.</p><p>With LOST <ref type="bibr" target="#b44">[45]</ref>, the authors propose construction of a graph and use the inverse degrees of nodes to segment objects. A heuristic seed expansion strategy is used to overcome noise ( <ref type="figure">Fig. 1b)</ref> and detect a single bounding box for a foreground object. The attention maps associated with different nodes often contain meaningful information, as illustrated in <ref type="figure">Fig. 1d</ref>. We have investigated whether it is possible to use the information in the entire graph by projecting the graph into a low dimensional subspace using eigendecomposition. We have discovered that such a projection can be used with Normalized Cut <ref type="bibr" target="#b42">[43]</ref> (Ncut) to significantly improve foreground / background segmentation ( <ref type="figure">Fig. 1c</ref>).</p><p>In this paper, we propose TokenCut, a simple but effective graph-based approach for unsupervised object discovery. We build on the self-supervised vision transformer trained with DINO <ref type="bibr" target="#b5">[6]</ref> as our backbone feature encoder and locate objects with the resulting features. Instead of using only the class token, we use all token features. We construct an undirected graph based on the token features in the last self-attention layer, where the visual tokens are viewed as graph nodes with edges representing a connectivity score based on similarity of the features. We then use a normalized graph-cut to group self-similar regions and delimit the foreground objects. We solve the graph-cut problem using spectral clustering with generalized eigendecomposition and show that the second smallest eigenvector provides a cutting solution indicating the likelihood that a token belongs to a foreground object. Our approach can be considered as a run-time adaptation, which means that the model is able to adapt to each specific test image despite the shared training model.</p><p>Despite its simplicity, our approach significantly improves unsupervised object discovery. The method achieves 68.8%, 72.1% and 58.8% on VOC07 <ref type="bibr" target="#b18">[19]</ref>, VOC12 <ref type="bibr" target="#b19">[20]</ref>, COCO20K <ref type="bibr" target="#b32">[33]</ref> respectively, thus outperforming LOST <ref type="bibr" target="#b44">[45]</ref> by a margin of 6.9%, 8.1% and 8.1% respectively. TokenCut with second stage CAD further improves the performance to 71.4%, 75.3% and 62.6% on VOC07, VOC12, COCO20k respectively, which outperforms LOST + CAD by 5.7%, 4.9% and 5.1% respectively.</p><p>In addition, we show that TokenCut can be easily extended to weakly supervised object detection and unsupervised saliency detection. For weakly supervised object detection, the goal is to detect objects using only image-level annotations. We freeze the encoder and fine-tune a linear classifier with weakly-supervised image labels. We then apply TokenCut on the features extracted from the finetuned encoder. Our approach produces clearly improved results on the CUB dataset <ref type="bibr" target="#b58">[59]</ref> and competitive performance on ImageNet-1K <ref type="bibr" target="#b13">[14]</ref>. For unsupervised saliency detection, we use the foreground region discovered by the proposed approach and apply Bilateral Solver <ref type="bibr" target="#b4">[5]</ref> as a post-processing step to refine edges of the foreground region. In terms of results, our approach significantly improves previous state-of-the-art methods on ECSSD <ref type="bibr" target="#b43">[44]</ref>, DUTS <ref type="bibr" target="#b59">[60]</ref> and DUT-OMRON <ref type="bibr" target="#b66">[67]</ref>.</p><p>In summary, our main contributions are as follows:</p><p>? We propose a simple and effective method to discover objects in images without supervision based on the selfsupervised vision transformers. This method significantly outperforms previous state-of-the-art methods for unsupervised object discovery when tested on multiple datasets; ? We extend the proposed method to weakly-supervised object detection and show that the simple approach can achieve competitive performance; ? We also show that this method can be used for unsupervised saliency detection. The results demonstrate that TokenCut significantly improves the previous state-ofthe-art performance on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-supervised vision transformers. ViT <ref type="bibr" target="#b17">[18]</ref> has shown that a transformer architecture <ref type="bibr" target="#b52">[53]</ref> can be used as an effective encoder for images and provide useful features for supervised vision tasks. MoCo-v3 <ref type="bibr" target="#b7">[8]</ref> demonstrated that ViT can provide self-supervised representation learning and achieve strong results using contrastive learning. Recently, DINO <ref type="bibr" target="#b5">[6]</ref> proposed to train transformers with self-distillation loss <ref type="bibr" target="#b24">[25]</ref>, showing that ViT contains explicit information that can be used for semantic segmentation of an image. Inspired by BERT <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref> proposed MST, which dynamically masks some tokens and learns to recover missing tokens using a global image decoder. Also motivated by BERT <ref type="bibr" target="#b15">[16]</ref>, BEIT <ref type="bibr" target="#b3">[4]</ref> first tokenizes the original image into visual tokens then randomly mask some tokens and learn to recover them using a transformer. Recently, MAE <ref type="bibr" target="#b22">[23]</ref> masks a high propor-2 <ref type="figure">Figure 2</ref>: An overview of the TokenCut approach. We construct a graph where the nodes are tokens and the edges are similarities between the tokens using transformer features. The foreground and background segmentation can be solved by Ncut <ref type="bibr" target="#b42">[43]</ref>. Performing bi-partition on the second smallest eigenvector allows to detect foreground object. tion of images and reconstructs the missing pixels with an asymmetric encoder-decoder.</p><p>Unsupervised object discovery. Given a group of images, unsupervised object discovery seeks to discover and delimit similar objects that appear in multiple images. Some methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref> are designed to segment common repeated objects in an image collection, but rely on strong assumptions about the frequency of appearance of an object. Other approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> use bounding box proposals and formulate the object discovery as an optimization problem. <ref type="bibr" target="#b56">[57]</ref> proposed a novel formulation of unsupervised object discovery as a ranking problem and showed that discovery could be scaled to datasets with more than 10K images. Recently, LOST <ref type="bibr" target="#b44">[45]</ref> significantly improved over state-of-the-art for unsupervised object discovery. LOST extracts features using a selfsupervised transformer based on DINO <ref type="bibr" target="#b5">[6]</ref> and designs a heuristic seed expansion strategy to obtain a single object region. Our work is closely related to LOST <ref type="bibr" target="#b44">[45]</ref>, as we also use self-supervised transformer features. However, rather than relying on the attention map of some specific nodes, we propose a graph-based method that employs the attention scores of all the nodes and can be used with Ncut <ref type="bibr" target="#b42">[43]</ref> to obtain a more precise segmentation of the image object.</p><p>Weakly supervised object detection. Weakly supervised object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b71">72]</ref> can be used to locate image objects using only image-level annotation. Early approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr">77]</ref> mainly relied on a Class Activation Map (CAM) which is introduced in [77] to generate class-specific localization maps and find discriminant regions. Several methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b74">75]</ref> have been proposed to improve CAM by erasing the discriminant regions and forcing the networks to capture additional object regions. Data augmentation techniques such as Cutout <ref type="bibr" target="#b16">[17]</ref> and CutMix <ref type="bibr" target="#b69">[70]</ref> have been shown to provide improvement for both classification and localization performance. Some methods achieve both classification and localization using two separate networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b70">71]</ref>. <ref type="bibr" target="#b70">[71]</ref> trained the localization network using pseudo bounding boxes generated by <ref type="bibr" target="#b60">[61]</ref>. <ref type="bibr" target="#b70">[71]</ref> first learns a classifier, then freeze its weights and train another detector. <ref type="bibr" target="#b21">[22]</ref> learns a regressor and a classifier using the consistency of CAM between two transformations. Unlike these approaches, which are specifically designed for weakly supervised object detection, we propose an unified solution to both unsupervised object discovery and weakly supervised object detection based on transformer.</p><p>Unsupervised saliency detection. Unsupervised saliency detection seeks to segment a salient objects within an image. Earlier work on this problem <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">78]</ref> used classical techniques such as color contrast <ref type="bibr" target="#b9">[10]</ref>, certain background priors <ref type="bibr" target="#b61">[62]</ref>, or super-pixels <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b66">67]</ref>. More recently, unsupervised deep models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b72">73]</ref> have incorporated heuristic saliency methods as pseudo ground truth to train deep CNN models. However, these methods rely on a CNN model pretrained with supervised training. <ref type="bibr" target="#b57">[58]</ref> has proposed an unsupervised Large-Scale GAN that does not make use of labels during training. In the following, we show that incorporating a simple post-processing step into our unsupervised object discovery can provide a strong baseline method for unsupervised saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach: TokenCut</head><p>The TokenCut algorithm can be used to predict bounding boxes that locate a salient object in an image. Our approach, illustrated in <ref type="figure">Fig. 2</ref>, is based on a graph where the nodes are tokens and the edges are similarities between the tokens using features based on the latent variables of the transformer. In the following, we first briefly present vision transformers in Section 3.1.1 and Normalized Cut in Section 3.1.2. We then introduce our solution and the implementation details in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Vision Transformers</head><p>Given an image of size H ? W , vision transformers (ViT) <ref type="bibr" target="#b17">[18]</ref> take non-overlapping 2D image patches of resolutions K ? K as inputs, with the number of patches N = HW/K 2 . Each patch is represented as a token, described by a vector of numerical features, referred to as an embedding. An extra learnable token, denoted as a class token CLS, is used to represent the aggregated information of the entire set of patches. This CLS token and the 3 set of patch tokens are fed to a standard transformer network with a "pre-norm" layer normalization <ref type="bibr" target="#b1">[2]</ref>.</p><p>The vision transformer is composed of a multiple layers of encoders, each with feed-forward networks and multiple attention heads for self-attention, paralleled with skip connections. For the unsupervised object discovery task, we use a vision transformer trained with self-supervised learning using DINO <ref type="bibr" target="#b5">[6]</ref> and extract latent variables from the final layer as the input features for our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Normalized Cut (Ncut)</head><p>Graph partitioning. Ncut <ref type="bibr" target="#b42">[43]</ref> can be used to partition a graph into two disjoint sets A and B. The method partitions the graph so as to minimizing the Ncut energy <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_0">N cut(A, B) = C(A, B) C(A, V) + C(A, B) C(B, V)<label>(1)</label></formula><p>where C measures the degree of similarity between two sets. C(A, B) = vi?A,vj ?B E i,j and C(A, V) is the total connection from nodes in A to all nodes in the graph.</p><p>As shown by Shi and Malik <ref type="bibr" target="#b42">[43]</ref>, the optimization problem in Eqn 1 is equivalent to:</p><formula xml:id="formula_1">min x N cut(x) = min y y T (D ? E)y y T Dy .<label>(2)</label></formula><p>With the condition y ? {1, ?b} N , b satisfies y T D1 = 0. D is a diagonal matrix with d i = j E i,j on its diagonal.</p><p>Ncut solution with the relaxed constraint. Taking z = D 1 2 y. Eqn 2 can be rewrite as:</p><formula xml:id="formula_2">min z z T D ? 1 2 (D ? E)D ? 1 2 z z T z .<label>(3)</label></formula><p>Indicating in <ref type="bibr" target="#b42">[43]</ref>, the formulation in Eqn 3 is equivalent to the Rayleigh quotient <ref type="bibr" target="#b51">[52]</ref>, which is equivalent to solve</p><formula xml:id="formula_3">D ? 1 2 (D ? E)D ? 1 2 z = ?z,</formula><p>where D ? E is the Laplacian matrix and known to be positive semidefinite <ref type="bibr" target="#b37">[38]</ref>. Therefore z 0 = D 1 2 1 is an eigenvector associated to the smallest eigenvalue ? = 0. According to Rayleigh quotient <ref type="bibr" target="#b51">[52]</ref>, the second smallest eigenvector z 1 is perpendicular to the smallest one (z 0 ) and can be used to minimize the energy in Eqn 3,</p><formula xml:id="formula_4">z 1 = argmin z T z0 z T D ? 1 2 (D ? E)D ? 1 2 z z T z .</formula><p>Taking z = D 1 2 y,</p><formula xml:id="formula_5">y 1 = argmin y T D1=0 y T (D ? E)y y T Dy .</formula><p>Thus, the second smallest eigenvector of the generalized eigensystem (D ? E)y = ?Dy is the real valued solution to the Ncut <ref type="bibr" target="#b42">[43]</ref> problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TokenCut Algorithm</head><p>Graph construction. Our method uses the vision transformers as described in Section 3.1.1 to produce a vector of features for each K?K image patch. A fully connected undirected graph G = (V, E) of patches is constructed where each V represents a patch with a feature vector {v i } N i=1 , and each patch is linked to adjacent patches by labeled edges, E. Edge labels represent a similarity score S based on the cosine similarity of the feature vectors of the two patches.</p><formula xml:id="formula_6">E i,j = 1, if S(v i , v j ) ? ? , else ,<label>(4)</label></formula><p>where ? is a hyper-parameter and equals a small value 1e ? 5 to assure that the graph is fully connected and S is the cosine similarity between features. Note that the spatial information has been implicitly included in the features via positional encoding in transformer.</p><formula xml:id="formula_7">S(v i , v j ) = v i v j v i 2 v j 2 .<label>(5)</label></formula><p>We apply Ncut algorithm, as described in Section 3.1.2, on constructed graph G and obtain the second smallest eigenvector of the generalized eigensystem, which can be seen as an attention map of the potential objects. We provide visualization of this attention map in Section 4.</p><p>Discovering Salient Object with TokenCut. We assume that there is at least one object in the image and the object occupies the foreground region. To successfully segment the foreground objects from the image, we must solve three problems: i) We must determine a means to partition the graph into two subgraphs and ii) given a bipartition of the graph, we must determine which partition represents the foreground. iii) In case of detecting multiple connected components in the foreground, we must identify the most salient object.</p><p>For the first problem, in our initial experiments we have used a simple average value of the projection onto the second smallest eigenvector to determine the similarity value for cutting the graph</p><formula xml:id="formula_8">y 1 = 1 N i y i 1 . Formally, A = {v i |y i 1 ? y 1 } and B = {v i |y i 1 &gt; y 1 }.</formula><p>We have compared this to using the classical clustering algorithms of K-means and EM to cluster the second smallest eigen vector into 2 partitions. The comparison is available in the supplementary material <ref type="table" target="#tab_7">Table 7</ref>, indicating that the mean generally provides better results.</p><p>For the second problem, the foreground contains the salient object and is assumed to be less connected to the 4 (a) DINO CLS Token Attention</p><formula xml:id="formula_9">(b) DINO Detection (c) LOST Inverse Degree Attention (d) LOST Detection (e) Our</formula><p>Eigen Attention (f) Our Detection <ref type="figure">Figure 3</ref>: Visual results of unsupervised single object discovery on VOC12. In (a), we show the attention of the CLS token in DINO <ref type="bibr" target="#b5">[6]</ref> which is used for detection (b). LOST <ref type="bibr" target="#b44">[45]</ref> is mainly relied on the map of inverse degrees (c) to perform detection (d). For our approach, we illustrate the eigenvector in (e) and our detection in (f). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively. entire graph. Intuitively, d i &lt; d j if v i belongs to the foreground while v j is the background token. Therefore, the eigenvector of the foreground object should have a larger absolute value than the one of the background. We use the maximum absolute value v max to select the foreground partition and the most salient object. The partition that contains v max is taken as foreground. As our graph has no explicit spatial constraint, the foreground might contain more than one connected regions. We simply select the largest connected component existing in the foreground containing the maximum absolute value v max as our final object region.</p><p>In summary, the TokenCut algorithm consists of the following steps:</p><p>1. Given an image, build a graph G = (V, E) according to Equation 4 and 5.</p><p>2. Solve the generalized eigensystem (D ? E)y = ?Dy for the eigenvector associated to the second smallest eigenvalue y 1 .</p><p>3. Compute bi-partition using the average over y 1 :</p><formula xml:id="formula_10">y 1 = i y i 1 N . A = {v i |y i 1 ? y 1 } and B = {v i |y i 1 &gt; y 1 } 4.</formula><p>Find the largest connected component associated to the maximum absolute value of y 1 .</p><p>Implementation details. For our experiments, we use the ViT-S/16 model <ref type="bibr" target="#b17">[18]</ref> trained with self-distillation loss (DINO) <ref type="bibr" target="#b5">[6]</ref> to extract features of patches. We employ the keys features of the last layer as the input features v. Ablations on different features as well as transformers trained with self-supervised learning are provided in the supplementary material <ref type="table" target="#tab_5">Table 5</ref>. We set ? = 0.2 for all datasets, the dependency on ? is provided in Section 4.4. In terms of running time, our un-optimised implementation takes approximately 0.32 seconds to detect a bounding box of a single image with resolution 480 ? 480 on a single GPU QUADRO RTX 8000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our approach on three tasks: unsupervised single object discovery, weakly supervised object detection and unsupervised saliency detection. We present results of unsupervised single object discovery in Section 4.1. The results of weakly supervised object detection are in Section 4.2. The results of unsupervised saliency detection in Section 4.3. We provide analysis of ? in Sec-5 <ref type="table">Table 1</ref>: Comparisons for unsupervised single object discovery. We compare TokenCut to state-of-the-art object discovery methods on VOC07 <ref type="bibr" target="#b18">[19]</ref>, VOC12 <ref type="bibr" target="#b19">[20]</ref> and COCO20K <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref> datasets. Model performances are evaluated with CorLoc metric. "Inter-image Simi." means the model leverages information from the entire dataset and explores inter-image similarities to localize objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inter-image Simi. DINO <ref type="bibr" target="#b5">[6]</ref> Feat. VOC07 <ref type="bibr" target="#b18">[19]</ref> VOC12 <ref type="bibr" target="#b19">[20]</ref> COCO20K <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref> Selective Search <ref type="bibr">[</ref> ViT-S/16 <ref type="bibr" target="#b17">[18]</ref> 71.4 (? 5.7) 75.3 (? 4.9) 62.6 (? 5.1) +CAD indicates to train a second stage class-agnostic detector with "pseudo-boxes" labels. tion 4.4, other ablation studies will be presented in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Single Object Discovery</head><p>Evaluation metric. We report performance using the CorLoc metric for precise localisation, as used by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b60">61]</ref>. CorLoc counts a predicted bounding box as correct if the intersection over union (IoU) score between the predicted bounding box and one of the ground truth bounding boxes is superior to 0.5.</p><p>Quantitative Results. We evaluate our approach on three commonly used benchmarks for unsupervised single object discovery: VOC07 <ref type="bibr" target="#b18">[19]</ref> , VOC12 <ref type="bibr" target="#b19">[20]</ref> and COCO20K <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref>. The qualitative results are provided in Tab. 1. We evaluate the CorLoc scores in comparison with previous state-of-the-art single object discovery methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b73">74</ref>, 79] on VOC07, VOC12, and COCO20K datasets. These methods can be roughly divided into two groups based on whether the model leverages information from the entire dataset and explores inter-image similarities or not. Because of quadratic complexity of region comparison among images, models with inter-image similarities are generally difficult to scale to larger datasets. The selective search <ref type="bibr" target="#b50">[51]</ref>, edge boxes [79], LOST <ref type="bibr" target="#b44">[45]</ref> and TokenCut do not require inter-image similarities and are thus much more efficient. As shown in the table, TokenCut consistently outperforms all previous methods on all datasets by a large margin. Specifically, TokenCut improves the stateof-the-art by 6.9%, 8.1% and 8.1% in VOC07, VOC12 and COCO20K respectively using the same ViT-S/16 features.</p><p>We also list a set of results that including a second stage unsupervised training strategy to boost the performance, This is referred to as class-agnostic detection (CAD). A CAD is trained by assigning the same "foreground" category to all the boxes produced by the first stage single object discovery model. As shown in Tab. 1, TokenCut + CAD outperforms the state-of-the-art by 5.7%, 4.9% and 5.1% on VOC07, VOC12 and COCO20k respectively.</p><p>Qualitative Results. In <ref type="figure">Fig. 3</ref>, we provide visualization for DINO-seg <ref type="bibr" target="#b5">[6]</ref>, LOST <ref type="bibr" target="#b44">[45]</ref> and Tokencut. For each method, we visualize the heatmap that is used to perform object detection. For DINO-seg, the heatmap is the attention map associated to the CLS token. For LOST, the detection is mainly based on the map of inverse degree ( 1 di ). For TokenCut, we display the second smallest eigenvector. The visual result demonstrates that Tokencut can extract a high quality segmentation of the salient object. Comparing with DINO-seg and LOST, TokenCut is able to extract a more complete segmentation as can be seen in the first and the third samples in <ref type="figure">Fig. 3</ref>. In some other cases, when all the methods have a high quality map, To-kenCut has the strongest intensity on the object, this phenomenon can be viewed in the last sample in <ref type="figure">Fig. 3</ref>. More visual results can be found in the supplementary material <ref type="figure" target="#fig_4">Fig. 7</ref> and <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weakly Supervised Object Localization</head><p>Evaluation metrics. We report three standard metrics: Top-1 Cls, GT Loc and Top-1 Loc. Top-1 Cls represents the top-1 accuracy of image classification. GT Loc is similar to CorLoc in which a predicted box is counted as correct if the IoU score is superior to 0.5 between the predicted bounding box and one of the ground-truth bounding boxes. Top-1 Loc is the most important metric as it 6 uses ten crop augmentations to get final classification results. and ? learn a classifer and a detector separately. considers measuring both the classification and the detection: a predicted bounding box is counted as a true positive if the class of the image is correctly predicted and the IoU is superior to 0.5 between the predicted bounding box and the ground-truth bounding box.</p><p>Results. We use two datasets to evaluate model performances on weakly supervised object localization: CUB-200-2011 <ref type="bibr" target="#b58">[59]</ref> (CUB) and ImangeNet-1k <ref type="bibr" target="#b39">[40]</ref>. The finetuning details can be found in supplementary material. In Tab. 2, we compare TokenCut to the state-of-theart weakly-supervised object localization approaches on CUB and ImageNet-1K datasets. The methods can be divided to two groups: models initialized with ImageNet-1K supervised pre-training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr">76</ref>, 77] and models inilialized with ImageNet-1K self-supervised pretraining <ref type="bibr" target="#b44">[45]</ref>.</p><p>On the CUB dataset, TokenCut achieves the best performance over all methods, and outperforms the stateof-the-art LOST method by 2.1% and 1.6% on GT Loc and Top-1 Loc. Interestingly, all the ImageNet-1K selfsupervised pretraining models are better than the supervised pretrained models. We believe that this is because supervised pretraining learns a more discriminative representation of the pretrained dataset than self-supervised pretraining, leading to a reduction in transferability to downstream datasets such as CUB. In comparison, selfsupervised pretraining can learn a more general represenatation and thus provides better transferbility.</p><p>On the ImageNet-1K dataset, TokenCut outperforms LOST by 5.4% and 4.4% on GT Loc and Top-1 Loc, and achieves a comparable performance with the ImageNet-1K supervised pretrain model. If the downstream task is ImageNet-1K, then the supervised pretraining with ImageNet-1K can provide discriminative features that improve the localization task as they are tuned to the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised Saliency detection</head><p>Evaluation Metrics We report three standard metrics: F-measure, IoU and Accuracy. F-measure is a standard measure in saliency detection. It is computed as F ? =  lowing previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58]</ref>, we set ? = 0.3 for consistency. IoU(Intersection over Union) score is computed based on the binary predicted mask and the ground-truth, the threshold is set to 0.5. Accuracy measures the proportion of pixels that have been correctly assigned to the object/background. The binarization threshold is set to 0.5 for masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We further evaluate TokenCut on three commonly datasets for unsupervised saliency detection: EC-SSD <ref type="bibr" target="#b43">[44]</ref>, DUTS <ref type="bibr" target="#b59">[60]</ref> and DUT-OMRON <ref type="bibr" target="#b66">[67]</ref>. The qualitative results are in Tab. 3. TokenCut significantly outperforms previous state-of-the-art. Adding Bilateral Solver <ref type="bibr" target="#b4">[5]</ref> refines the boundary of an object and further boosts the performance over TokenCut, which can also be seen from the visual results presented in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis and Discussion</head><p>Analysis of ? . In Tab. 4, we provide an analysis on ? defined in Equation <ref type="bibr" target="#b3">4</ref>. The results indicate that the effects of variations in ? value are not significant and that a suitable threshold is ? = 0.2.</p><p>Internet Images We further test TokenCut on Internet images. The results are in <ref type="figure" target="#fig_1">Figure 5</ref>. We can see even  <ref type="figure">Figure 6</ref>: Failure cases on VOC12 (1st and 2nd row) and COCO (3rd row). LOST <ref type="bibr" target="#b44">[45]</ref> mainly relies on the map of inverse degrees (a) to perform detection (b). For our approach, we illustrate the eigenvector in (c) and our detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.</p><p>the inputs images are with noisy background, our algorithm can still provide precise attention map cover the object leading to accurate bounding box prediction, which demonstrates again the robustness of our approach.</p><p>Limitations Despite the good performance of Token-Cut, it has several limitations. Examples of failure cases are shown in <ref type="figure">Fig. 6</ref>: i) TokenCut focuses on the largest salient part in the image, which may not be the desired object <ref type="figure">(Fig. 6, 1st row)</ref>. ii) Similar to LOST <ref type="bibr" target="#b44">[45]</ref>, To-kenCut assumes that a single salient object occupies the foreground. If multiple overlapping objects are present in an image, both LOST and our approach will fail to detect one of the object <ref type="figure">(Fig. 6, 2nd row)</ref>. iii) Neither LOST nor our approach can handle occlusion ( <ref type="figure">Fig. 6, 3rd</ref> row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced TokenCut, a simple but effective approach for unsupervised object discovery. TokenCut uses self-supervised learning with transformers to constructs a graph where nodes are patches and edges represent similarities between patches. We showed that salient objects can be directly detected and delimited using Ncut. We evaluated this approach on unsupervised single object discovery, weakly supervised object detection and unsupervised saliency detection, and showed that it provides a significant improvement over previous approaches. Our results indicate that self-supervised transformers can provide a rich and general set of features that may likely be used for a variety of computer vision problems. 8 A Analysis of backbones.</p><p>In Tab. 5, we provide an ablation study on different transformer backbones. The "-S" and "-B" are ViT small <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> and ViT base <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> architecture respectively. The "-16" and "-8" represents patch sizes 16 and 8 respectively. The "MocoV3" is another pre-trained self-supervised transformer model <ref type="bibr" target="#b7">[8]</ref>. The ? value is set to 0.3 for MoCov3, while for Dino the best tau value is 0.2. We observe that although the result of MoCov3 is slightly worse than the results of TokenCut with Dino, MoCov3 still outperforms previous state-of-the-art, indicating that TokenCut can provide similar results when used with other self-supervised Transformer architectures. Besides, the results demonstrates that a patch size of 16 provides better results than a patch size of 8. Several insights can be found: i) TokenCut outperforms LOST for different backbones. ii) As LOST relies on a heuristic seeds expansion strategy, the performance varies significantly using different backbones. While our approach is more robust. We provide another an ablation study on different backbones for weakly supervised object localization. Results are shown in Tab. 6.</p><p>The "-S" and "-B" designate ViT small <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> and ViT base <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> architecture respectively. The "-16" and "-8" indicate patch sizes 16 and 8 respectively. For our approach, we report results with ? = 0.2, which is the same on all the datasets. Note that LOST with ViT-S/8 achieves much worse results, because the seed expansion strategy in LOST relies on the top-100 patches which are with lowest degrees. When the total number of patches is large, the proposed seed expansion strategy is not able to cover entire objects. While our approach provides more robust performance on different datasets across different backbones. + B Analysis of bi-partition strategies.</p><p>In Tab. 7, we study different strategies to separate the nodes in our graph into two groups using the second smallest eigenvector. We consider three natural methods: mean value (Mean), Expectation-Maximisation (EM), K-means clustering (K-means). We use python sklearn library for EM and K-means algorithm implementation. For EM algorithm, we set number of iteration to 300 and each component has its own general covariance matrix. The convergence threshold is set to 1e-3. For K-means algorithm, we use "k-means++" for initialization. The maximum number of iterations is set to 300. The convergence threshold is set to 1e-4. The result suggests that the simple mean value as the splitting point performs well for most cases. We have also tried to search for the splitting point based on the best Ncut(A,B) value. Due to the quadratic complexity, this approach requires substantially more computations. Thus, we finally obsolete it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of Graph edge weight</head><p>In this section, we provide an ablation study on graph edge weight defininig on equation 4. We have tested to directly use the similarity score as edge weights (i.e., Eij = S(xi, xj))). However, it is not possible because there may exist negative edge values, which violates the Normalized Cut algorithm assumption. Thus, we also tried thresholding the similarity score (i.e., Eij = S(xi, xj) if S(xi, xj) &gt; ? , else ). We obtain 68.9% on VOC07 dataset and 72% on VOC12 dataset, which is similar to our reported results.</p><p>D Visual results for unsupervised single object discovery on VOC07 and COCO12</p><p>We show visual results for unsupervised single object discovery on VOC07 <ref type="bibr" target="#b18">[19]</ref> and COCO12 <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref>, which are illustrated in <ref type="figure" target="#fig_4">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> respectively.</p><p>For each dataset, we compare both attention maps and bounding box predictions among DINO <ref type="bibr" target="#b5">[6]</ref>, LOST <ref type="bibr" target="#b44">[45]</ref> and TokenCut. The attention map for DINO is extracted from the CLS token attention map of the last layer of key features. The attention map for LOST is the inverse degree map used in LOST for detection. The TokenCut attention map is the second smallest eigenvector of Equation 2. These results show that TokenCut provides clearly better segmentation of the object.   <ref type="bibr" target="#b18">[19]</ref> In (a), we show the attention of the CLS token in DINO <ref type="bibr" target="#b5">[6]</ref> which is used for detection (b). LOST <ref type="bibr" target="#b44">[45]</ref> is mainly relied on the map of inverse degrees (c) to perform detection (d). For our approach, we illustrate the eigenvector in (e) and our detection in (f). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.  <ref type="figure">Figure 8</ref>: Visual results of unsupervised single object discovery on COCO20K <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref>. In (a), we show the attention of the CLS token in DINO <ref type="bibr" target="#b5">[6]</ref> used for detection (b). LOST <ref type="bibr" target="#b44">[45]</ref> mainly relies on the map of inverse degrees (c) to perform detection (d). For TokenCut, we illustrate the eigenvector in (e) and the detection in (f). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Fine-tuning self-supervised transformers</head><p>For weakly supervised object localization, we use a pre-trained DINO model as our backbone and learn a linear classifier on the training set where we only have access to the class labels. We freeze the backbone weights and fine-tune a linear classifier, as shown in Tab. 2. For CUB, We train with a SGD optimizer for 1000 epochs and set the batch size to 256 per GPU, distributed over 4 GPUs. The learning rate is linearly warmed during the first 50 epochs, then follows a cosine learning rate scheduler. We decay the learning rate from batch size 256 ?5e-4 to 1e-6. The weight decay is set to 0.005. For ImageNet-1K, we use the models released by DINO. Other training setups and details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visual results for weakly supervised object localizatio on CUB and Imagenet-1k</head><p>We present visual results for weakly supervised object localization on CUB <ref type="bibr" target="#b58">[59]</ref> and Imagenet-1k <ref type="bibr" target="#b13">[14]</ref> in <ref type="figure">Fig. 9 and Fig. 10</ref> respectively.</p><p>For each dataset, we compare the attention map and bounding box prediction with LOST <ref type="bibr" target="#b44">[45]</ref> and our approach. The eigenvector of TokenCut provides better segmentation on objects and leads to better detection results.  <ref type="figure">Figure 9</ref>: Visual results for weakly supervised object localization on CUB <ref type="bibr" target="#b58">[59]</ref>. In (a), we show the map of inverse degrees used to perform detection with LOST (b) <ref type="bibr" target="#b44">[45]</ref>. For TokenCut, we illustrate the eigenvector in (c) used for detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.  <ref type="figure">Figure 10</ref>: Visual results of unsupervised single object discovery on Imagenet-1k <ref type="bibr" target="#b13">[14]</ref>. In (a), we show LOST <ref type="bibr" target="#b44">[45]</ref> the map of inverse degrees, which is used to perform detection (b). For TokenCut, we illustrate the eigenvector in (c) and the detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively. 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Failure cases on CUB and Imagenet-1k</head><p>We illustrate additional failure cases in <ref type="figure">Fig. 11</ref>. Those failure cases can be organised into three categories: 1) Where TokenCut focus on the largest salient object, whereas the annotation is highlights a different object, shown in the first and the second column in <ref type="figure">Fig. 11</ref>. 2) Similar to LOST, Tokencut is not able to differentiate the connected objects, such as the third and the fourth column in <ref type="figure">Fig. 11</ref>. 3) In case of occlusion, neither LOST nor our approach can't detect the entire object, such as the last two columns in <ref type="figure">Fig. 11</ref>.  <ref type="figure">Figure 11</ref>: Failure cases on Imagenet-1k <ref type="bibr" target="#b13">[14]</ref> and CUB <ref type="bibr" target="#b58">[59]</ref>. In (a), we show LOST <ref type="bibr" target="#b44">[45]</ref> the map of inverse degrees, which is used to perform detection (b). For TokenCut, we illustrate the eigenvector in (c) and the detection in (d). Blue and Red bounding boxes indicate the ground-truth and the predicted bounding boxes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Datasets</head><p>We present in this section the details of the datasets used in our experiments:</p><p>? VOC07 and VOC12 correspond to the training and validation set of PASCAL-VOC07 and PASCAL-VOC12. VOC07 and VOC12 contain 5 011 and 11 540 images respectively which belong to 20 categories. They are commonly evaluated for unsupervised object discovery <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b60">61]</ref>. ? COCO20K consists of 19 817 randomly chosen images from the COCO2014 dataset <ref type="bibr" target="#b32">[33]</ref>. It is used as a benchmark in <ref type="bibr" target="#b55">[56]</ref> for a large scale evaluation. ? CUB consists of 200 bird species, including 6 033 and 5 755 images in training and test sets respectively, which is commonly used to evaluate weakly supervised object localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr">77</ref>]. ? ImageNet <ref type="bibr" target="#b13">[14]</ref> is a widely used benchmark for image classification and object detection, which consists of 1 000 different categories. The number of images in training and validation sets are 1.3 million and 50,000 respectively. Each image contains a single object supposed to be detected. During the training, only class labels are available. ? ECSSD contains 1 000 real-world images of complex scenes for testing. ? DUTS contains 10 553 train and 5 019 test images. The training set is collected from the ImageNet detection train/val set. The test set is collected from ImageNet test, and the SUN dataset <ref type="bibr" target="#b64">[65]</ref>. Following the previous works <ref type="bibr" target="#b41">[42]</ref>, we report the performance on the DUTS-test subset. ? DUT-OMRON [67] contains 5 168 images of high quality natural images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Visual results for unsupervised saliency detecion on ECSSD, DUTS and DUT-OMRON</head><p>We present visual results for unsupervised saliency detecion on ECSSD <ref type="bibr" target="#b43">[44]</ref>, DUTS <ref type="bibr" target="#b59">[60]</ref> and DUT-OMRON <ref type="bibr" target="#b66">[67]</ref> in <ref type="figure" target="#fig_9">Fig. 12, 13</ref> and 14 respectively.</p><p>For each dataset, we compare LOST segmentation, LOST + Bilateral Solver and our approch. The TokenCut provides better segmentation on objects. The performance is further improved with Bilateral Solver.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(1+? 2 Figure 4 :</head><label>24</label><figDesc>)P recision?Recall ? 2 P recision+Recall , where the Precision and Recall are defined based on the binarized predicted mask and ground truth mask. The maxF ? is the maximum value of 255 uniformly distributed binarization thresholds. Fol-7 (a) Input (b) Ours (c) Ours + BS (d) GT Visual results of unsupervised segments on ECSSD<ref type="bibr" target="#b43">[44]</ref>. In (a), we show the input image. TokenCut detection result is presented in (b). TokenCut + Bilateral Solver results is shown in (c). (d) is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Visual results of images taken from the Internet. We show the input images, our eigen attention and final detection in (a) (b) (c) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visual results of unsupervised single object discovery on VOC07</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Visual results of unsupervised segments on ECSSD<ref type="bibr" target="#b43">[44]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Visual results of unsupervised segments on DUTS [60](a) Input (b) LOST (c) LOST + BS (d) Ours (e) Ours + BS (f) GT Visual results of unsupervised segments on DUT-OMRON [67]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons for weakly supervised object localization. We report Top-1 Cls, GT Loc and Top-1 Loc on CUB<ref type="bibr" target="#b58">[59]</ref> and ImageNet-1K<ref type="bibr" target="#b13">[14]</ref> datasets. Compared state-of-the-art methods are divided into two groups: with ImageNet-1K supervised pretraining and with ImageNet-1K self-supervised pretraining.</figDesc><table><row><cell>Pretrained Dataset</cell><cell>Method</cell><cell>Backbone</cell><cell cols="3">CUB [59], Acc. (%) Top-1 Cls GT Loc Top-1 Loc</cell><cell cols="3">ImageNet-1K [14], Acc. (%) Top-1 Cls GT Loc Top-1 Loc</cell></row><row><cell></cell><cell>CAM [77]</cell><cell>GoogLeNet [48]</cell><cell>73.8</cell><cell>-</cell><cell>41.1</cell><cell>65</cell><cell>-</cell><cell>43.6</cell></row><row><cell></cell><cell cols="2">HaS-32 [46] + [3] GoogLeNet [48]</cell><cell>75.4</cell><cell>61.1</cell><cell>47.4</cell><cell>68.9</cell><cell>60.6</cell><cell>44.6</cell></row><row><cell>ImageNet-1K [14] supervised pretrain</cell><cell>ADL [13] + [3] ADL [13] I2C [76]</cell><cell cols="2">ResNet50 [24] InceptionV3 [49] 74.6 75.0 InceptionV3 [49] -</cell><cell>77.6 -72.6</cell><cell>59.5 53.0 56</cell><cell>75.8 72.8 73.3</cell><cell>62.2 -68.5</cell><cell>49.4 48.7 53.1</cell></row><row><cell></cell><cell>PSOL [71]  ?</cell><cell cols="2">InceptionV3 [49] -</cell><cell>-</cell><cell>65.5</cell><cell>-</cell><cell>65.2</cell><cell>54.8</cell></row><row><cell></cell><cell>SLT-Net [22]</cell><cell cols="2">InceptionV3 [49] 76.4</cell><cell>86.5</cell><cell>66.1</cell><cell>78.1</cell><cell>67.6</cell><cell>55.7</cell></row><row><cell>ImageNet-1K [14]</cell><cell>LOST [45]</cell><cell>ViT-S/16 [18]</cell><cell>79.5</cell><cell>89.7</cell><cell>71.3</cell><cell>77.0</cell><cell>60.0</cell><cell>49</cell></row><row><cell>self-supervised pretrain</cell><cell>TokenCut</cell><cell>ViT-S/16 [18]</cell><cell>79.5</cell><cell cols="3">91.8 (? 2.1) 72.9 (? 1.6) 77.0</cell><cell cols="2">65.4 (? 5.4) 52.3 (? 3.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons for unsupervised saliency detection We compare TokenCut to state-of-the-art unsupervised saliency detection methods on ECSSD<ref type="bibr" target="#b43">[44]</ref>, DUTS<ref type="bibr" target="#b59">[60]</ref> and DUT-OMRON<ref type="bibr" target="#b66">[67]</ref>. TokenCut achieves better results comparing with other competitive approaches.</figDesc><table><row><cell>Method</cell><cell>maxF ? (%)</cell><cell>ECSSD [44] IoU(%)</cell><cell>Acc.(%)</cell><cell>maxF ? (%)</cell><cell>DUTS [60] IoU(%)</cell><cell>Acc.(%)</cell><cell cols="2">DUT-OMRON [67] maxF ? (%) IoU(%)</cell><cell>Acc.(%)</cell></row><row><cell>HS [66]</cell><cell>67.3</cell><cell>50.8</cell><cell>84.7</cell><cell>50.4</cell><cell>36.9</cell><cell>82.6</cell><cell>56.1</cell><cell>43.3</cell><cell>84.3</cell></row><row><cell>wCtr [78]</cell><cell>68.4</cell><cell>51.7</cell><cell>86.2</cell><cell>52.2</cell><cell>39.2</cell><cell>83.5</cell><cell>54.1</cell><cell>41.6</cell><cell>83.8</cell></row><row><cell>WSC [31]</cell><cell>68.3</cell><cell>49.8</cell><cell>85.2</cell><cell>52.8</cell><cell>38.4</cell><cell>86.2</cell><cell>52.3</cell><cell>38.7</cell><cell>86.5</cell></row><row><cell>DeepUSPS [37]</cell><cell>58.4</cell><cell>44.0</cell><cell>79.5</cell><cell>42.5</cell><cell>30.5</cell><cell>77.3</cell><cell>41.4</cell><cell>30.5</cell><cell>77.9</cell></row><row><cell>BigBiGAN [58]</cell><cell>78.2</cell><cell>67.2</cell><cell>89.9</cell><cell>60.8</cell><cell>49.8</cell><cell>87.8</cell><cell>54.9</cell><cell>45.3</cell><cell>85.6</cell></row><row><cell>E-BigBiGAN [58]</cell><cell>79.7</cell><cell>68.4</cell><cell>90.6</cell><cell>62.4</cell><cell>51.1</cell><cell>88.2</cell><cell>56.3</cell><cell>46.4</cell><cell>86.0</cell></row><row><cell>LOST [42, 45]</cell><cell>75.8</cell><cell>65.4</cell><cell>89.5</cell><cell>61.1</cell><cell>51.8</cell><cell>87.1</cell><cell>47.3</cell><cell>41.0</cell><cell>79.7</cell></row><row><cell>LOST [42, 45]+Bilateral Solver [5]</cell><cell>83.7</cell><cell>72.3</cell><cell>91.6</cell><cell>69.7</cell><cell>57.2</cell><cell>88.7</cell><cell>57.8</cell><cell>48.9</cell><cell>81.8</cell></row><row><cell>TokenCut</cell><cell>80.3</cell><cell>71.2</cell><cell>91.8</cell><cell>67.2</cell><cell>57.6</cell><cell>90.3</cell><cell>60.0</cell><cell>53.3</cell><cell>88.0</cell></row><row><cell>TokenCut + Bilateral Solver [5]</cell><cell>87.4 (? 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>7) 77.2(? 4.9) 93.4 (? 1.8) 75.5(? 5.8) 62.4 (? 5.2) 91.4 (? 2.7) 69.7 (? 11.9) 61.8 (? 12.9) 89.7 (? 7.9)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Analysis of ? . We report CorLoc for unsupervised single object discovery on VOC07, VOC12, COCO20K, and Top-1 Loc for weakly supervised object detection on CUB and ImageNet-1K.</figDesc><table><row><cell>?</cell><cell cols="5">CorLoc VOC07 VOC12 COCO20K CUB ImageNet-1K Top-1 Loc</cell></row><row><cell>0</cell><cell>67.4</cell><cell>71.3</cell><cell>56.1</cell><cell>73.0</cell><cell>53.8</cell></row><row><cell>0.1</cell><cell>68.6</cell><cell>72.1</cell><cell>58.2</cell><cell>73.2</cell><cell>53.4</cell></row><row><cell>0.2</cell><cell>68.8</cell><cell>72.1</cell><cell>58.8</cell><cell>72.9</cell><cell>52.3</cell></row><row><cell>0.3</cell><cell>67.7</cell><cell>72.1</cell><cell>58.2</cell><cell>70.8</cell><cell>50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>[ 76 ]</head><label>76</label><figDesc>Xiaolin Zhang, Yunchao Wei, and Yi Yang. Inter-image communication for weakly supervised localization. In ECCV, 2020. 7 [77] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016. 3, 7, 16 [78] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun. Saliency optimization from robust background detection.</figDesc><table><row><cell>In CVPR, 2014. 3, 7</cell></row><row><cell>[79] C Lawrence Zitnick and Piotr Doll?r. Edge boxes: Locat-</cell></row><row><cell>ing object proposals from edges. In ECCV, 2014. 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Analysis of different backbones.</figDesc><table><row><cell></cell><cell cols="4">We report CorLoc for unsupervised single object discovery on VOC07,</cell></row><row><cell>VOC12, COCO20K.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>VOC07</cell><cell>VOC12</cell><cell>COCO20K</cell></row><row><cell cols="2">LOST [45] ViT-S/16 [6, 18]</cell><cell>61.9</cell><cell>64.0</cell><cell>50.7</cell></row><row><cell cols="3">TokenCut MoCoV3-ViT-S/16 [8, 18] 66.2</cell><cell>66.9</cell><cell>54.5</cell></row><row><cell cols="2">TokenCut ViT-S/16 [6, 18]</cell><cell>68.8 (? 6.9)</cell><cell>72.1 (? 8.1)</cell><cell>58.8 (? 8.1)</cell></row><row><cell cols="2">LOST [45] ViT-S/8 [6, 18]</cell><cell>55.5</cell><cell>57.0</cell><cell>49.5</cell></row><row><cell cols="2">TokenCut ViT-S/8 [6, 18]</cell><cell cols="3">67.3 (? 11.8) 71.6 (? 14.6) 60.7 (? 11.2)</cell></row><row><cell cols="2">LOST [45] ViT-B/16 [6, 18]</cell><cell>60.1</cell><cell>63.3</cell><cell>50.0</cell></row><row><cell cols="2">TokenCut ViT-B/16 [6, 18]</cell><cell>68.8 (? 8.7)</cell><cell>72.4 (? 9.1)</cell><cell>59.0 (? 9.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Analysis of backbones for weakly supervised object localization. We report Top-1 Cls, GT Loc and Top-1 Loc on CUB<ref type="bibr" target="#b58">[59]</ref> and Imagenet-1k<ref type="bibr" target="#b13">[14]</ref> datasets.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>?</cell><cell cols="3">CUB [50], Acc. (%) Top-1 Cls GT Loc Top-1 Loc</cell><cell cols="3">ImageNet-1K [11], Acc. (%) Top-1 Cls GT Loc Top-1 Loc</cell></row><row><cell cols="3">LOST [45] ViT-S/16 [6, 18] -</cell><cell>79.5</cell><cell>89.7</cell><cell>71.3</cell><cell>77.0</cell><cell>60.0</cell><cell>49.0</cell></row><row><cell cols="4">TokenCut ViT-S/16 [6, 18] 0.2 79.5</cell><cell>91.8 (? 2.1)</cell><cell cols="2">72.9 (? 1.6) 77.0</cell><cell>65.4 (? 5.4)</cell><cell>53.4 (? 4.4)</cell></row><row><cell cols="2">LOST [45] ViT-S/8 [6, 18]</cell><cell>-</cell><cell>82.3</cell><cell>78.0</cell><cell>64.4</cell><cell>79.4</cell><cell>45.8</cell><cell>38.1</cell></row><row><cell cols="2">TokenCut ViT-S/8 [6, 18]</cell><cell cols="2">0.2 82.3</cell><cell cols="3">89.9 (? 11.9) 74.2 (? 9.8) 79.4</cell><cell cols="2">66.0 (? 20.2) 55.0 (? 16.9)</cell></row><row><cell cols="3">LOST [45] ViT-B/16 [6, 18] -</cell><cell>80.3</cell><cell>90.7</cell><cell>72.8</cell><cell>78.3</cell><cell>58.6</cell><cell>48.3</cell></row><row><cell cols="4">TokenCut ViT-B/16 [6, 18] 0.2 80.3</cell><cell>90.0 (? 0.7)</cell><cell cols="2">72.5 (? 0.3) 78.3</cell><cell>63.2 (? 4.8)</cell><cell>52.3 (? 4.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Analysis of different bi-partition methods. We report CorLoc for unsupervised single object discovery.</figDesc><table><row><cell cols="4">Bi-partition VOC07 VOC12 COCO20K</cell></row><row><cell>Mean</cell><cell>68.8</cell><cell>72.1</cell><cell>58.8</cell></row><row><cell>EM</cell><cell>63.0</cell><cell>65.7</cell><cell>59.3</cell></row><row><cell>K-means</cell><cell>67.5</cell><cell>69.2</cell><cell>61.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work has been partially supported by the MIAI Multidisciplinary AI Institute at the Univ.</p><p>Grenoble Alpes (MIAI@Grenoble Alpes -ANR-19-P3IA-0003), and by the EU H2020 ICT48 project Humane AI Net under contract EU #952026.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning for deep detection neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking class activation mapping for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyug</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beit: Bert pretraining of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Show, match and segment: Joint weakly supervised learning of semantic matching and object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.2" />
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.2" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fang Wan, and Dingwen Zhang. Strengthen learning tolerance for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Masked autoencoders are scalable vision learners. arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coattention cnns for unsupervised object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised detection of regions of interest using iterative link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A weighted sparse coding framework for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Masked self-supervised transformer for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometry constrained weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Erasing integrated learning: A simple yet effective approach for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepusps: Deep robust unsupervised saliency prediction via self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaithanya</forename><surname>Kumar Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi Hoai Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partitioning sparse matrices with eigenvectors of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pothen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang-Pu</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on matrix analysis and applications</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instance-aware, context-focused, and memoryefficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning co-segmentation by segment swapping for retrieval and discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15904</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Localizing objects with selfsupervised transformers and no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colocalization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles F Van Loan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The Johns Hopkins University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised image matching and object discovery as optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Toward unsupervised, multi-object discovery in large-scale image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Large-scale unsupervised object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sizikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Object segmentation without labels with large-scale generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unsupervised object discovery and co-localization by deep descriptor transformation. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal dynamic graph lstm for action-driven video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rethinking the route towards weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization and detection: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Object discovery from a single unlabeled image by mining frequent itemsets with multi-scale features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingji</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
