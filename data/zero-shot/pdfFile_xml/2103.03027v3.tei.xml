<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Multi-Label Action Dependencies for Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Tirupattur</surname></persName>
							<email>praveentirupattur@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
							<email>kevinduarte@knights.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Rawat</surname></persName>
							<email>yogesh@crcv.ucf.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Multi-Label Action Dependencies for Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world videos contain many complex actions with inherent relationships between action classes. In this work, we propose an attention-based architecture that models these action relationships for the task of temporal action localization in untrimmed videos. As opposed to previous works that leverage video-level co-occurrence of actions, we distinguish the relationships between actions that occur at the same time-step and actions that occur at different time-steps (i.e. those which precede or follow each other). We define these distinct relationships as action dependencies. We propose to improve action localization performance by modeling these action dependencies in a novel attention-based Multi-Label Action Dependency (MLAD) layer. The MLAD layer consists of two branches: a Cooccurrence Dependency Branch and a Temporal Dependency Branch to model co-occurrence action dependencies and temporal action dependencies, respectively. We observe that existing metrics used for multi-label classification do not explicitly measure how well action dependencies are modeled, therefore, we propose novel metrics that consider both co-occurrence and temporal dependencies between action classes. Through empirical evaluation and extensive analysis, we show improved performance over state-of-theart methods on multi-label action localization benchmarks (MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding and localizing actions in complex video sequences is a heavily researched problem in computer vision. The task of action localization in the untrimmed video involves predicting the action, or actions, present at each time-step of the video sequence. Several works propose top-down methods, that propose temporal regions of a video <ref type="bibr">Figure 1</ref>. Two action sequences from the MultiTHUMOS dataset. The first sequence (top) shows action dependencies within a given time-step: "Basketball Dribble" and "Run" co-occur. The bottom sequence shows action dependencies across time-steps: "Fall" follows "Jump". The table above each frame shows the comparison of probability scores predicted by our model with the I3D baseline for each action class present at that time-step. Modeling both types of dependencies is beneficial for correctly detecting actions.</p><p>which are then classified and refined <ref type="bibr">[5,</ref><ref type="bibr" target="#b4">12,</ref><ref type="bibr" target="#b33">38,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b6">14,</ref><ref type="bibr" target="#b48">53]</ref>. Other approaches produce bottom-up predictions for each time-step directly from the frame-level or clip-level features <ref type="bibr" target="#b19">[24,</ref><ref type="bibr" target="#b17">22,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b32">37,</ref><ref type="bibr" target="#b22">27]</ref>. Recent bottom-up methods tend to perform best on the multi-label case, where multiple actions can be present within the same time-step.</p><p>Although these works achieve strong multi-label action localization performance, they do not explicitly model the relationships between the different action labels, which can be extremely useful for determining the presence or absence of classes within a video. Previous works have used label co-occurrence to improve performance on image classification <ref type="bibr" target="#b39">[44,</ref><ref type="bibr" target="#b43">48,</ref><ref type="bibr">9]</ref>, and video action recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">30]</ref>. However, the later works measure the video-level co-occurrence of actions, which does not differentiate between actions that occur within the same time-step and across different timesteps. This may be acceptable when the problem is videolevel single-label action recognition, but when the task is to temporally localize multiple actions (as is the case with multi-label temporal action localization) the distinction between these co-occurrences allows for more fine-grained modeling of action relationships. We define these distinct action class relationships as action dependencies.</p><p>Videos contain two types of action dependencies: i) cooccurrence dependencies, involving actions that occur at the same time (this is most analogous to object class cooccurrence within images), and ii) temporal dependencies, involving actions that precede or follow each other. To illustrate, consider <ref type="figure">Figure 1</ref> showing sample frames from pole vault and basketball videos. An example of a co-occurrence dependency is present in the first video snippet: the action "run" often occurs with the action "basketball dribble" in a basketball game, so the presence of one action gives additional prior information about the other. The second video snippet is an example of a temporal dependency. Using the available label information from the previous clips, one could infer the label following "jump" to be "fall" in the final clip even without visual or motion features corresponding to the person performing the action.</p><p>In this work, we present a method that leverages both action dependency types to improve learned feature representations for the task of multi-label temporal action detection. We propose an attention-based layer to refine classlevel features based on these dependencies. Co-occurrence dependencies are modeled by refining action features based on the presence, or absence, of other actions within a timestep; temporal dependencies are modeled by refining features based on all the time-steps of an input video sequence. In both cases, attention maps are generated which allows for improved interpretability of our model. Differing from action recognition methods that employ class co-occurrence <ref type="bibr" target="#b25">[30]</ref>, our approach does not require a ground-truth action co-occurrence matrix, but rather learns action dependencies from the training data.</p><p>To better understand how our approach models action dependencies, we present novel metrics for evaluating temporal action localization methods. Whereas previous multi-label evaluation methods, like mean average precision (mAP) and F1-score, tend to evaluate per-frame class performance independently, our proposed actionconditional precision and recall metrics explicitly measure how well pair-wise class/action dependencies are modeled both within a time-step and through different time-steps. Our proposed metrics are general -they can be applied to both images and videos by measuring performance on both co-occurrence and temporal action dependencies.</p><p>Our main contributions include the following:</p><p>? We present a novel network architecture that models both co-occurrence action dependencies and temporal action dependencies.</p><p>? We propose multi-label performance metrics to measure a method's ability to model class co-occurrence across time-steps as well as within a time-step.</p><p>? We evaluate the proposed approach on two large scale publicly available multi-label action datasets, outperforming existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, temporal action localization research has received a lot of interest. In general, approaches for temporal action localization are broadly classified into topdown, bottom-up, and end-to-end. Top-down approaches <ref type="bibr">[5,</ref><ref type="bibr" target="#b4">12,</ref><ref type="bibr" target="#b48">53,</ref><ref type="bibr">4]</ref>, start with candidate proposals and refine them to achieve the final temporal boundaries. These approaches perform well, but are often slow and suffer from over-generated proposals and rigid boundaries. Bottom-up approaches <ref type="bibr" target="#b19">[24,</ref><ref type="bibr" target="#b17">22,</ref><ref type="bibr" target="#b21">26]</ref> start with frame-level or clip-level predictions for each action class and combine the individual scores to generate the final temporal boundaries. Endto-end approaches <ref type="bibr" target="#b46">[51,</ref><ref type="bibr" target="#b18">23,</ref><ref type="bibr" target="#b2">3]</ref> integrate proposal generation and classification steps. These approaches are proposed to solve temporal action localization with non-overlapping instances and do not consider the relationships between action classes. In a multi-label setup, there are overlapping instances of different actions leading to parts of the video corresponding to multiple classes.</p><p>Multi-label classification has been studied in both images <ref type="bibr" target="#b37">[42,</ref><ref type="bibr" target="#b7">15,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b3">11]</ref> and videos <ref type="bibr" target="#b41">[46,</ref><ref type="bibr" target="#b10">18,</ref><ref type="bibr" target="#b26">31]</ref> In the image domain, it has been shown that leveraging relationships between classes help improve classifier performance. Some works [10, <ref type="bibr" target="#b15">21,</ref><ref type="bibr" target="#b14">20,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b47">52]</ref> use probabilistic graphical models to incorporate label relationships by formulating this task as a structural inference problem. Others <ref type="bibr" target="#b37">[42,</ref><ref type="bibr" target="#b20">25,</ref><ref type="bibr" target="#b9">17,</ref><ref type="bibr" target="#b38">43,</ref><ref type="bibr" target="#b43">48]</ref> use spatial attention with recurrent neural networks to model the label co-occurrence. In <ref type="bibr" target="#b44">[49,</ref><ref type="bibr" target="#b39">44,</ref><ref type="bibr" target="#b0">1]</ref> image features and label domain data are projected to a common latent space to learn the label correlations.</p><p>Videos introduce additional temporal relationships between labels which are crucial for multi-label temporal action localization. Most previous approaches <ref type="bibr" target="#b5">[13,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b24">29,</ref><ref type="bibr" target="#b28">33]</ref> for multi-label temporal action localization neither consider the label co-occurrence nor the temporal relationships between the labels. Recently, some works have explicitly modeled temporal relationships between action labels <ref type="bibr" target="#b30">[35,</ref><ref type="bibr" target="#b27">32]</ref>. The idea of learning a differentiable grammar to model high-level temporal structure and relationships between multiple action classes was introduced in <ref type="bibr" target="#b27">[32]</ref> for the first time. In <ref type="bibr" target="#b30">[35]</ref>, a framework is presented to learn temporal ordering between atomic actions to detect complex activities in videos, by using regular expressions to express the temporal composition of atomic actions. To the best of our knowledge, no existing works explicitly model both the co-occurrence and temporal dependencies between action classes. We propose a bottom-up approach that models the co-occurrence and temporal dependencies using attention to improve multi-label action localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first present the formulation of the multi-label temporal action localization problem. Then, we describe our proposed network. It consists of three main parts: (i) class-level feature extraction, (ii) feature refinement by using our MLAD layer which models both types of action dependencies, and (iii) a classification step to transform the refined features into class probabilities. The network architecture is depicted in <ref type="figure">Figure 2</ref>.</p><p>Problem Formulation The problem of multi-label temporal action localization involves classifying all activities occurring throughout a video at each time-step. Formally, in a feature sequence of length T , each time-step t = 1, ..., T contains a ground-truth action label y t,c ? {0, 1}, where c = 1, ..., C is the action class. Given a feature vector of length F , x t ? R F , for each time-step, an activity detection network predicts class probabilities? t,c ? [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-level Feature Extraction</head><p>The input to our network is a series of feature vectors x t . Since these features contain global representations (either frame-level or video-clip level, when obtained from 2D-CNN encoders and 3D-CNN encoders, respectively), we convert them to class-level representations. This nonlinear transformation is as follows:</p><formula xml:id="formula_0">f t,c = ReLU W T c x t + b c ,<label>(1)</label></formula><p>where W c and b c are learned weights for each class c. These H-dimensional vectors contain information pertinent to a given action at each time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MLAD Layer</head><p>We propose a layer that can use these class-level features and model the relationships between the various action classes across time. One approach would be to use a fullyconnected graph-based [10] or attention-based <ref type="bibr" target="#b42">[47]</ref> network to learn the relationships between the feature vectors. This, however, would lead to CT ?CT connections, which would be extremely inefficient when either the number of classes, C, or the number of time-steps, T , becomes large. Instead, we propose an efficient attention-based Multi-label Action Dependency (MLAD) layer which decomposes this operation into C ? C and T ? T sets of connections. The MLAD layer contains two branches -the Co-occurrence Dependency Branch (CB) and Temporal Dependency Branch (TB) -which model their corresponding action dependencies and refine the input class-level features. Refer to <ref type="figure" target="#fig_1">Figure 3</ref> for the architecture of the MLAD layer.</p><p>Co-occurrence Dependency Branch (CB) The CB models the relationships between actions within a given timestep. For each time-step, a self-attention operation <ref type="bibr" target="#b36">[41]</ref> is performed across all classes. At each time step, t, input features generate a set of query, key, and value tensors (Q t , K t , V t ), each with dimension R C?H . Then, a C ? C attention matrix, A (t) , is obtained as follows:</p><formula xml:id="formula_1">A (t) = softmax Q t K T t ? H .<label>(2)</label></formula><p>This attention matrix contains the relevance of each class for the classification of another class. For example, A ij should be large, otherwise, it will have a value close to 0. With this attention matrix, we obtain a refined set of class-level features that take into account the presence (or absence) of other classes within the time-step as follows:</p><formula xml:id="formula_2">f t,c = A (t) V t .<label>(3)</label></formula><p>Temporal Dependency Branch (TB) The TB models actions' temporal dependencies. For each class, c, a new set of query, key, and value tensors (Q c , K c , V c ) are created with dimension R T ?H . The self-attention operation is performed across time:</p><formula xml:id="formula_3">A (c) = softmax Q c K T c ? H . (4) Here, A (c) is a T ? T attention matrix, where A (c)</formula><p>kn denotes the importance of time-step n in the classification of the given class, c, at time-step k. The refined features are obtained as follows:</p><formula xml:id="formula_4">f t,c = A (c) V c .<label>(5)</label></formula><p>This branch incorporates information from all time-steps, producing more temporally coherent features and predictions. When the TB is used in conjunction with the CB, the MLAD layer can model both types of action dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging Branches and Classification</head><p>We merge the different sets of refined features (f t,c and f t,c ) to obtain a combined output representation. The trivial approaches for <ref type="bibr">Figure 2</ref>. Architecture of our proposed approach. Input to our model is a sequence of features (T ? F ), extracted using a pre-trained backbone. Our proposed architecture process these features in three steps. First, it learns class-specific features (T ? H) for each class C (shown in block (i)). Second, it refines these class-specific features using one or more of the attention-based Multi-Label Action Dependency (MLAD) layers (shown in block (ii)). Third, it classifies the features using individual classification layers for each class, and output class probabilities for each time step (T ? C) (shown in block (iii)). merging would be element-wise summation or concatenation followed by an MLP to reduce dimensionality. We propose to learn the amount of information which is used from each module; the module learns a value, ? ? [0, 1], that is used to merge the outputs to compute combined features, g t,c , as follows:</p><formula xml:id="formula_5">g t,c = ?f t,c + (1 ? ?) f t,c .<label>(6)</label></formula><p>We find that the use of the learned ? term leads to improvement in performance when compared to element-wise averaging. The improved class-level feature representation, g t,c , is either passed as an input to additional MLAD layers or used to produce a final classification output. This is performed by the transformation</p><formula xml:id="formula_6">y t,c = ? W T c g t,c + b c ,<label>(7)</label></formula><p>where W c ? R d k ?1 and b c ? R are learned weights and ? is the logistic sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action Dependency Metrics</head><p>The problem of multi-label temporal action localization consists of predicting the action, or actions, occurring at each time-step of a video. The standard metric for evaluating temporal action localization, f-mAP, treats each timestep as an individual sample, measures the performance of each class independently, and averages their scores; it does not explicitly measure if models learn the relationships between these classes. This issue is not unique to f-mAP. Other multi-label classification metrics <ref type="bibr" target="#b13">[19,</ref><ref type="bibr" target="#b35">40,</ref><ref type="bibr" target="#b31">36,</ref><ref type="bibr" target="#b40">45]</ref> do not consider the relationships between different classes or time-steps, which makes them unsuitable to evaluate how well action dependencies are modeled. To this end, we propose new action localization metrics that measure a method's ability to model both co-occurrence dependencies and temporal dependencies.</p><p>For a given video, k, there exist binary ground-truth labels y (k) t,c ? {0, 1}, where t is the time-step and c is the class. The network predicts class probabilities at each time-step, on which a threshold is applied to obtain binary predicted labels,? (k) t,c ? {0, 1}. Two standard metrics for multi-label classification are per-class precision and per-class recall, which are defined as:</p><formula xml:id="formula_7">P recision(c) = N correct (c) N predict (c) , Recall(c) = N correct (c) N gt (c) . (8) Here, N correct (c) = k,t 1[y (k) t,c =? (k) t,c = 1] are the num- ber of correct predictions for class c, N predict (c) = k,t 1[? (k) t,c = 1] are the total number of predictions for class c, N gt (c) = k,t 1[y (k)</formula><p>t,c = 1] are the total number of time-steps containing class c, and 1 is the indicator function. These metrics measure a model's performance on individual classes, but they do not take into account the relationships and dependencies between these classes. We propose action-conditional precision and recall to solve this issue.</p><p>We first deal with the co-occurrence relationship, where two actions occur within the same time-step. For an action class c i , we measure its precision and recall when another action, c j , is present within the same time-step. The actionconditional precision and recall of c i , given c j , are</p><formula xml:id="formula_8">P recision(c i |c j ) = N correct (c i |c j ) N predict (c i |c j ) , and Recall(c i |c j ) = N correct (c i |c j ) N gt (c i |c j ) .<label>(9)</label></formula><p>Here, the components are defined as</p><formula xml:id="formula_9">N correct (c i |c j ) = k,t 1[y (k) t,ci =? (k) t,ci = 1]1[y (k) t,cj = 1], N predict (c i |c j ) = k,t 1[? (k) t,ci = 1]1[y (k) t,cj = 1], and N gt (c i |c j ) = k,t 1[y (k) t,ci = 1]1[y (k) t,cj = 1].<label>(10)</label></formula><p>These metrics, measure the precision and recall of an action class c i when c j is present within the given time-step. Note that these metrics are not symmetric, and it may be the case that P recision(c i |c j ) = P recision(c j |c i ) and</p><p>Recall(c i |c j ) = Recall(c j |c i ). These metrics measure co-occurrence within a time-step. We extend this to measure temporal dependencies between different actions, which follow each other within some temporal window ? . We present metrics, which measure the precision and recall of action c i , given that action c j was present within the last ? time-steps and c j is not present within the current time-step (this ensures that it measures only temporal dependencies and not co-occurrence dependencies). At time-step t, this holds when the following condition is true:</p><formula xml:id="formula_10">y (k) t,cj = 0 ? ?y (k) t * ,cj = 1, t * ? [t ? ?, t).<label>(11)</label></formula><p>Therefore, the action-conditional precision and recall, denoted P recision(c i |c j , ? ) and Recall(c i |c j , ? ), are com-puted with the following components:</p><formula xml:id="formula_11">N correct (c i |c j , ? ) = k,t 1[y (k) t,ci =? (k) t,ci = 1]1[?], N predict (c i |c j , ? ) = k,t 1[? (k) t,ci = 1]1[?], and N gt (c i |c j , ? ) = k,t 1[y (k) t,ci = 1]1[?].<label>(12)</label></formula><p>Here, ? is the condition in equation 11. For ease of notation, we use ? = 0 to denote the actionconditional metrics within a time-step (equation 9), such that P recision(c i |c j , ? = 0) = P recision(c i |c j ) and</p><p>Recall(c i |c j , ? = 0) = Recall(c i |c j ).</p><p>Our proposed action-conditional metrics can be used to measure the co-occurrence dependencies and temporal dependencies between any two actions. Since some actions never co-occur or follow each other, the overall metric is computed by averaging all action pairs (c i , c j ), i = j, such that N gt (c i |c j , ? ) &gt; 0. In addition, more complex performance metrics like F1-score (the harmonic mean between precision and recall) and mAP (the area under the precision-recall curve) can also be computed using our action-conditional precision and recall metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Datasets We conduct experiments on two widely used multi-label action localization datasets: MultiTHUMOS <ref type="bibr" target="#b45">[50]</ref> and Charades <ref type="bibr" target="#b34">[39]</ref>. The MultiTHUMOS dataset is an extended version of THUMOS'14 <ref type="bibr" target="#b8">[16]</ref> dataset, containing dense, multi-label frame-level action annotations for 65 classes across the 413 sports videos from YouTube. We use the standard train/test split with 200 videos for training and 213 for testing. MultiTHUMOS contains up to 25 action labels for each video, with an average of 10.5 activity instances per video and 1.5 labels per frame. This is in contrast to other activity detection datasets such as ActivityNet [6] and HACS <ref type="bibr" target="#b49">[54]</ref>, which only have one activity per timestep. Charades <ref type="bibr" target="#b34">[39]</ref> is a large dataset with 9848 videos of daily indoor activities, collected through Amazon Mechanical Turk. The dataset consists of 66,500 temporal annotations for 157 action classes. Contrary to MultiTHUMOS, the activities tend to be performed in the home. Each video in the dataset contains an average of 6.8 activity instances. Baselines We compare our method with several baselines. The first is a linear layer which classifies individual timesteps based on the features extracted from a pre-trained I3D network (denoted I3D Baseline). A second baseline which extracts class-level features (as in equation 1) and classifies these features (as in equation 7) is also used (denoted CF Baseline). In addition, we compare with recent multi-label action localization methods Super-events (SE) <ref type="bibr" target="#b29">[34]</ref>, Temporal Gaussian Mixture (TGM) Layers <ref type="bibr" target="#b28">[33]</ref>, TGMs + SE <ref type="bibr" target="#b28">[33]</ref>, and TGMs + Differentiable Grammars (DG) <ref type="bibr" target="#b27">[32]</ref>.</p><p>Metrics To compare with previous temporal action localization works, we use the standard evaluation protocol of computing per-frame mean average precision (f-mAP).</p><p>We also present results on other multi-label metrics -Hamming Loss (HL), Zero One Loss (ZL), Ranking Loss (RL), Coverage Loss (CL), Jaccard Score (JS), and Label Ranking Average Precision (LRAP) -as well as our proposed action-conditional metrics: precision (P AC ), recall (R AC ), f1-score (F 1 AC ), and mean average precision (mAP AC ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>Our results on the MultiTHUMOS and Charades datasets are presented in <ref type="table">Table 1</ref>. Our approach achieves 51.5% f-mAP and 23.7% f-mAP on MultiTHUMOS and Charades respectively. The effectiveness of our MLAD layer is best illustrated by the comparison with CF Baseline: with only 5 MLAD layers, the class-based features are refined, leading to a 9% improvement in f-mAP for both datasets.</p><p>Comparison with state-of-the-art On MultiTHUMOS, our model outperforms the current state-of-the-art model, TGM + Differentiable Grammars, by 3.3%; on Charades, we achieve a 0.8% improvement in f-mAP. Although the absolute improvement is not as large as MultiTHUMOS (since it is a more difficult dataset with more action classes), the improvement is comparable to previous performance advancements on the dataset (e.g. 0.6% improvement for TGM + DG over TGMs + SE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action-Conditional Metric Results</head><p>We present results using other existing multi-label metrics (HL, ZL, RL, CL, JS, LRAP) alongside our proposed metrics (conditional precision, recall, f1-score, and mAP) on the MultiTHUMOS dataset in <ref type="table">Table 2</ref>. For the time-conditional metrics, we select ? = 20; results with other values of ? are presented in the Supplementary Materials. Our method achieves higher performance on all action-conditional metrics since it models different action dependencies within a video, both within a time-step (? = 0) and throughout time (? &gt; 0). Of the 1322 action pairs that co-occur within a time-step in the test set, our method improves the average precision of 961 pairs when compared to the I3D baseline.</p><p>By analyzing specific action pairs, one can better understand how various approaches model the different action dependencies. Here, we examine the dependencies described in <ref type="figure">Figure 1</ref>. To evaluate how well the method models the cooccurrence dependency between actions "Basketball Dribble" and "Run", one can compute the average precision over that pair: AP (c i = BasketballDribble|c j = Run, ? = 0). The TGM approach achieves a minor improvement over the I3D baseline (47.26% vs 45.60%), while our approach better models this relationship with an average precision of 58.85%. A similar improvement is seen for temporal dependencies. To evaluate the relationship "Fall follows Jump", we compute AP (c i = Fall|c j = Jump, ? = 20), and find that our method achieves a score of 78.12% compared to the TGM's 72.27%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablations</head><p>We evaluate the various design decisions for our method as well as its components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of MLAD Layers</head><p>Since our proposed MLAD layer can be stacked to continually refine input features, we test how performance changes as the number of MLAD layers increases. We show in <ref type="table">Table 3</ref> that increasing the number of layers tends to improve results on both Multi-THUMOS and Charades. However, this improvement has diminishing gains as the depth increases: the change from 3</p><p>Existing Metrics  <ref type="table">Table 2</ref>. Evaluation of our approach using existing multi-label classification metrics and our proposed action dependency metrics on MultiTHUMOS dataset. HL -Hamming Loss, ZL -Zero One Loss, RL -Ranking Loss, CL -Coverage Loss, JS -Jaccard Score, LRAP -Label Ranking Average Precision, PAC -Action-Conditional Precision, RAC -Action-Conditional Recall, F1AC -Action-Conditional F1-Score, mAPAC -Action-Conditional Mean Average Precision. to 5 layers leads to a smaller improvement (1.22% on Mul-tiTHUMOS) than the change from 1 to 3 layers (1.85%). An increase to L = 7 leads to no noticeable improvement, therefore all reported results have a depth of L = 5.</p><formula xml:id="formula_12">Action-Conditional Metrics ? HL ? ZL ? RL ? CE ? JS ? LRAP ? ? = 0 ? = 20 P AC R AC F1 AC mAP AC P AC R AC F1 AC mAP AC<label>I3D</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiTHUMOS Charades</head><p>Effect of Feature Sequence Length Since our approach performs computations on a feature sequence of length T , we evaluate how the sequence length affects our performance. We present two experimental setups: 1) both the training and evaluation lengths are fixed, and 2) the training length is varied, T ? {i ? 16 | i ? {1, ..., 8}}, with a fixed evaluation length. We present the results of both in <ref type="table">Table  4</ref>. We find that when the training length is fixed, the performance peaks when T = 96 at 51.31% mAP. However, when the training length is varied in experiment setup 2, we achieve the best performance with T = 128. This varying of sequence length during training can be seen as a form of data augmentation, leading to improved generalization. Effect of CB and TB Since both branches of the MLAD layer are meant to model the different action dependencies within a video, we run an ablation by removing each of these branches and present the results in <ref type="table">Table 5</ref>. We find that each branch leads to improvement over classifying with the original class-level features, but that best performance is achieved when both are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Analysis</head><p>In this section, we analyze our trained model's predictions and the learned attention maps from MLAD layers. <ref type="figure" target="#fig_2">Figure 4</ref>, we visualize the predictions of our trained model on a sample video sequence from the MultiTHUMOS test set. When compared with the outputs from the TGM <ref type="bibr" target="#b28">[33]</ref> network and the I3D baseline, our proposed method generates localizations that better overlap with the ground-truth annotations. This is most notable for the "Fall" action; the MLAD layers allow our method to model the temporal dependencies between "Fall" and "Jump", leading to the improved localization predictions. Also, our model detects every instance of "High-Jump" which co-occurs with other actions "Run", "Jump", and "Fall". In general, we find that our approach leads to a large increase in recall across most classes: of the 65 actions in the MultiTHUMOS dataset, our network improves recall for 52 classes, with an average improvement of 13.17%. Additional analysis of the results, including perclass scores, can be found in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localizaton Analysis In</head><p>Failure Cases When compared to previous approaches, our network tends to under-perform on the "Walk" and "Sit" actions on the MultiTHUMOS dataset. We find that these actions tend to occur in the background (e.g. by a referee or audience members at a sporting event) and frequently co-occur with many different foreground actions. Since these background actions are not directly dependent on those in the foreground, our method attempts to model relationships that do not exist, leading to poor performance. This suggests that the modeling of individual actors would be greatly beneficial for learning the various action dependencies within a video. We believe that this would be a promising direction for future work.</p><p>Interpretability of MLAD Layers One advantage of our approach over previous temporal action localization methods is that the network architecture (specifically the MLAD layers) allows for more interpretable results. Since the MLAD Layers consist of two attention-based branchesthe Co-occurrence Dependency Branch (CB) and the Temporal Dependency Branch (TB) -we can analyze their attention maps to better understand how the different action dependencies are modeled. These analyses are done on the <ref type="figure">Figure 6</ref>. Visualization of the T ? T (T = 128) TB attention maps from MLAD layers 1, 3 and 5 on a sample sequence for the action "Close Up Talk To Camera". The checker-board pattern shows that time-steps where the action is present (highlighted in green) tend to focus on other time-steps where it is present; and time-steps where the action is absent (in orange) tend to focus on other timesteps where it is absent. We also find that various layers model different temporal regions (e.g. layer 5 models the boundaries of the action for class "Close Up Talk To Camera").</p><p>MultiTHUMOS dataset where there are 65 action classes (C = 65) and a sequence length of 128 (T = 128) is used.</p><p>We first analyze attention maps in the CB. <ref type="figure" target="#fig_3">Figure 5</ref> contains 10 ? 10 subsets of the C ? C attention maps from different MLAD layers, which are obtained by averaging over all time-steps where a specific action is present ("Throw Discus" on the top and "Clean and Jerk" on the bottom). We find that this model successfully models the co-occurrence dependencies since the actions which are related to the present action (e.g. "Throw", "Discus Wind-Up", and "Discuss Release" often co-occur with "Throw Discus") tend to be active, whereas unrelated actions (e.g. basketball and volleyball actions) tend to have low activation. We also find that the CB focuses on actions, like "Run" and "Jump", which are most prevalent in the training set 1 -this is likely because these actions often co-occur with many different actions, so their presence (or absence) is important in determining the existence of other less common actions.</p><p>Next, we present the attention maps from the TB in <ref type="figure">Figure 6</ref>. We visualize the T ? T maps for the class "Close Up Talk To Camera" from a sample sequence. A noticeable checker-board pattern is present: Time-steps, where the action is present, tend to focus on other time-steps where it is present, while time-steps, where the action is absent, tend to focus on other time-steps where it is absent. This behavior is common across all actions. Furthermore, we find that different MLAD layers attend to different parts of an action; for example, in layer 5 the attention map is active at the action boundaries for the "Close Up Talk To Camera" action. We provide attention maps for all MLAD layers, as well as more examples, in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we propose an attention-based network architecture to learn action dependencies in videos, for solving the multi-label temporal action localization task. Our proposed MLAD layer consisting of two branches: The cooccurrence Dependency Branch and the Temporal Dependency Branch, which use attention to model dependencies between actions that occur within the same time-step, and those actions which precede/follow each other, respectively. As the existing evaluation metrics for multi-label temporal localization do not explicitly consider action dependencies, we propose a novel evaluation metric. Our method outperforms the current state-of-the-art on existing multi-label classification metrics as well as our proposed metric.</p><p>Acknowledgments This research is based upon work supported by the Office of the Director of National Intelligence(ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary 1. Overview</head><p>In this supplement, we present additional ablation experiments (Section 2) and a more detailed comparison with other methods (Section 3). We also present additional information about the parameters in our MLAD layer (Section 4) and experiments on an additional baseline (Section 5). Lastly, we present additional results using our proposed metric (Section 6) and additional visualizations of learned attention maps (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ablation Experiments</head><p>Effect of RGB and Flow We analyse how the use of features from different modalities (RGB and optical flow) effect our method in <ref type="table">Table S1</ref>. Consistent with previous action localization works <ref type="bibr" target="#b28">[33]</ref> we find that the use of Flow features outperforms using only RGB (by a margin of 6.53% on MultiTHUMOS and 1.7% on Charades), but the combination of both features leads to the best results. Notably, our method only trained using optical flow features outperforms all previous methods, even when they use both RGB and Flow features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>MultiTHUMOS  <ref type="table">Table S1</ref>. Ablation results using RGB, Flow and their combination. f-mAP is the evaluation metric. In Early Fusion, the network is trained with concatenated RGB and Flow features. In Late Fusion, two networks are trained with RGB, Flow features independently and their predictions are averaged.</p><p>Effect of Learned Feature Averaging When combining the refined features from the Co-occurrence Dependency Branch and the Temporal Dependency Branch, we use a weighted average based on a learned parameter ?. We compare the performance of our model trained with additional ? parameter and the model trained with a fixed value ? = 0.5. As shown in <ref type="table" target="#tab_4">Table S2</ref>, learning the weight parameter ? gives better results when compared to averaging (? = 0.5) the features from TB and CB branches. Although the absolute difference in score is not large, this 0.57% improvement in f-mAP is the result of only 5 learned parameters (since there are 5 MLAD layers).</p><p>Effect of initial Classification Loss Classification step in our proposed model consists of two parts: (i) classification of initial class-specific features before the MLAD layers.</p><p>Fixed Alpha Learned Alpha f-mAP 50.95 51.52  <ref type="table">Table S3</ref>. Ablation results with and without classification loss on the class-specific features. <ref type="figure">Figure S1</ref>. Comparison of performance metrics between different variants of our proposed model. "Only CB" refers to model trained with Co-occurrence Dependency Branch (CB), "Only TB" refers to model trained with Temporal Dependency Branch (TB), and "Ours" refers to model trained with both the branches.</p><p>(ii) classification of refined features after the MLAD layers.</p><p>In <ref type="table">Table S3</ref> we present the results with and without the classification loss on the initial class-specific features. We find that the use of classification loss on the initial class-specific features increases performance since it gives additional supervisory signal which improves the class-level representations given to the MLAD layers.</p><p>Effect of CB and TB In <ref type="figure">Figure S1</ref> we compare the mean Precision, Recall, F1-Score and f-mAP over all the classes in MultiTHUMOS dataset for three different models. "Only CB" refers to the model with only the Co-occurrence Dependency Branch in a MLAD layer, "Only TB" refers to the model with only the Temporal Dependency Branch and "Ours" refers to the model containing both the branches. We observe that Co-occurrence Dependency Branch helps in detecting instances of actions which co-occur within a time-step and thereby improve the overall recall of the model. The Temporal Dependency Branch, on the other hand, improves the temporal boundaries of action instances and tends to increase precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Class-level Performance</head><p>In <ref type="figure">Figure S2</ref>, we present the per-class f-mAP score comparison between the I3D baseline, TGM <ref type="bibr" target="#b28">[33]</ref>, and our proposed model on MultiTHUMOS dataset. Please refer to provided videos for a further comparison between TGM and our model. Our model outperforms previous methods in most of the classes, with one notable except "Sit". We observe that action "Sit" is associated with a background actor and is not related to the foreground action. Our proposed model, under-performs when compared to TGM in modeling the relationship between foreground action and this unrelated background action. However, our method does successfully model relationships between actions performed by the actor in the foreground. This is evident in the provided video, "video test 0000444.avi".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Parameters</head><p>Each MLAD layer contain about 200K parameters and can be easily stacked on top of any existing feature extraction backbone with a minor change to learn the action dependencies for end-to-end action classification/localization. The output features from the feature extractor need to be class specific before they can be processed by a MLAD layer. Number of parameters in a MLAD layer is insignificant when compared to the number of parameters in a typical backbone, two-stream I3D contain 25M parameters, so adding MLAD layers will not contribute to a huge increase in parameters and thereby memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Baseline -Fully Connected Attention</head><p>The trivial approach to learn relationships between actions across time-steps would be to use a fully-connected attention-based network to learn all CT ?CT relationships, where C is the number of classes and T is the number of time-steps in the input. To evaluate the efficiency of this approach, we trained a network where we replace the Cooccurrence Dependency Branch (CB) and Temporal Dependency Branch (TB) in our model with a self-attention module modeling the CT ? CT relationships.</p><p>As the memory consumption of this models increases quadratically with increase in number of classes (C) or the number of timesteps (T ), it is infeasible to train when either of these become too large. We find that on a 32GB GPU, we must reduce the batch size for both datasets to 8 (batch size = 32 is used to train our model). Furthermore, the maximum number of layers which can fit into memory are L = 3 for the MultiTHUMOS dataset with 65 classes and L = 1 for the Charades dataset with 157 classes.</p><p>In <ref type="table">Table S4</ref>, we compare the results of our model with this baseline. We observe that this trivial attention baseline with CT ? CT connections performs worse than our proposed method which decomposes the attention operation  <ref type="table">Table S4</ref>. Comparison of our results with the baseline model containing a self-attention layer modeling relationships between all the classes and timesteps. f-mAP is the evaluation metric used and the baseline results are presented only for those settings which fit in 32GB GPU used for training other models.</p><p>into C ? C and T ? T sets of connections. Furthermore, we find that increasing the number of layers in this baseline does not lead to an substantial increase in performance (only a difference of 0.11% when increasing from 1 to 3 layers). This suggests that even if more memory was available (i.e. through multiple GPUs or improvements in GPU technology), it is unlikely that it would be able to outperform our proposed MLAD layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Proposed Metric</head><p>We present additional results for our proposed metric on MultiTHUMOS with various values of ? &gt; 0 in <ref type="table">Table S5</ref>. By varying the ? parameter, we can measure short-term or long-term temporal dependencies. In general, we find that our proposed method outperforms the previous approaches on all metrics. We also evaluate our method, and our baselines, on the Charades dataset with the action-conditional metric. These results are shown in <ref type="table">Table S6</ref>.</p><p>As stated in the main text, one benefit of our proposed action-conditional metric is that we can obtain the pairwise performance between two dependant action classes. We present the pairwise scores for various action classes in the MultiTHUMOS dataset with co-occurrence dependancies (? = 0) and temporal dependencies (? = 20) in tables S7 and S8 respectively. These results show that our method successfully models the various dependencies between different actions. Furthermore, it suggests that improvements in previous methods for temporal action localization do not necessarily come from improved modeling of action dependencies. Although TGM greatly outperforms the I3D baseline on this dataset, there are several action relationships on which it performs worse than the baseline (e.g. "Cricket-Bowling" and "Throw" in table S7, and "Volleyball Spiking" and "VolleyballSet" in table S8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Analysis</head><p>In <ref type="figure">Figure 7</ref>  video, shown on the left and the right in the middle row. Attention maps from the initial layers show activation for the frequent classes in the dataset which co-occur with most of the other classes, where as active regions in the attention maps from deeper layers correspond to the classes that are present in the sequence. In this <ref type="figure">Figure 7</ref>, we show attention maps for two instances from the same video and it can be observed that the active classes in the attention maps change with the actions present in the video. In <ref type="figure" target="#fig_2">Figure S4</ref>, we present the class attention maps from all the five layers of our model. Attention maps from only layers 1, 3, and 5 are shown in the main paper. In <ref type="figure" target="#fig_3">Figure S5</ref>, we show class attention maps for more classes. Each row in the figure is for a specific class (main class, shown in black) and each column shows the attention map from a specific layer. In these attention maps, classes related to the main class (shown in green) tend to have more activation than the unrelated classes (shown in red).</p><formula xml:id="formula_13">? = 5 ? = 10 ? = 40 P AC R AC F1 AC mAP AC P AC R AC F1 AC mAP AC P AC R AC F1 AC mAP AC<label>I3D</label></formula><p>In <ref type="figure" target="#fig_6">Figure S6</ref>, we present the T ? T attention maps from each MLAD layer for a specific class, "Close Up Talk To Camera". Attention maps from only layers 1, 3, and 5 are presented in the main paper. In <ref type="figure" target="#fig_7">Figure S7</ref>, we present the T ? T attention maps from each MLAD layer for more classes in the MultiTHUMOS dataset. In all these samples, "Checkerboard" pattern is predominantly visible. This in-dicates that, timesteps where the action is present focus on other timesteps containing the action and vice-versa. We observe that role of each layer changes from one class to another.</p><p>Other patterns are also common in these attention maps. For example, a layer may focus on time-steps near the current time-step with similar features, resulting in an attention matrix with activations along the main diagonal (see layer 2 for the "Stand" action and layers 1 and 2 for the "TalkTo-Camera" action). Also, a layer may focus on all time-steps in which the action occurs, leading to vertical bars (see layer 4 for the "Long Jump" action and layer 3 for the "TalkTo-Camera" action). ? = 0 ? = 20 ? = 40 P AC R AC F1 AC mAP AC P AC R AC F1 AC mAP AC P AC R AC F1 AC mAP AC I3D 14. <ref type="bibr" target="#b29">34</ref>   <ref type="table">Table S7</ref>. Action-conditional scores for some actions pairs with co-occurrence dependencies in the MultiTHUMOS dataset. The metric reported is is aciton-conditional average precision for Action1 given Action2: AP(i = Action1|j = Action2, ? = 0).    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>denotes the relevance of class j in the classification of class i at time-step t; if these two classes co-occur within the same time-step often, then A (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>MLAD Layer. Given class-specific features (C ? H) for each time-step T , this layer refines the features by modeling action dependencies with attention. The upper Temporal Dependency Branch (TB) models dependencies across time-steps (temporal dependencies) for each class and the lower Co-occurrence Dependency Branch (CB) models dependencies between classes within each time-step (co-occurrence dependencies).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of model predictions for different time steps (x-axis) for various actions (y-axis) from a MultiTHUMOS test video. We compare our model (red), with TGM<ref type="bibr" target="#b28">[33]</ref> (blue), I3D baseline (green), and the ground-truth (black). The performance of our model in detecting temporally dependent actions "Jump" and "Fall" is higher than the baselines; between time-steps 00:30 to 02:00 our method detects all instances of "Fall" while TGM and the I3D baseline have no predictions for this class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of 10?10 subsets of the CB attention maps from MLAD layers 1, 3, and 5 obtained by averaging the maps over time-steps where action "Throw Discus" (top) and "Clean and Jerk" (bottom) are present. Columns corresponding to classes that are related to the main action (those in green) tend to be active, whereas unrelated classes (in red) tend to have low activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we present attention maps averaged over each column of the C ? C map. The resultant C ? T attention map shows the average attention received by a class from other classes for each of the time-steps in the input sequence. The top and bottom row in this figure show the C ? T attention maps for two sequences from a sample Figure S2. Per-class f-mAP comparison between our model, TGM[33] and I3D[8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S5 .</head><label>S5</label><figDesc>Visualization of 10?10 subsets of the CB attention maps from all MLAD layers obtained by averaging the maps over time-steps where a specific action (shown in black) is present. Columns corresponding to classes that are related to the main action (those in green) tend to be active, whereas unrelated classes (in red) tend to have low activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S6 .</head><label>S6</label><figDesc>Visualization of time attention maps from every MLAD layer of our network. Attention maps shown here are of dimension T ? T (T = 128). This shows the attention maps for class "Close Up Talk To Camera" from a sample sequence. Highlighted regions in green are the timesteps where the action appears in the sample and regions in orange are the timesteps without the action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S7 .</head><label>S7</label><figDesc>Visualization of time attention maps from every MLAD layer of our network. Attention maps shown here are of dimension T ? T (T = 128). Highlighted regions in green are the timesteps where the action appears in the sample. Each row is for a specific class and the corresponding action class is shown on the left of each row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Implementation DetailsIn our experiments we use RGB and Optical Flow features extracted from two-stream I3D backbone pre-trained on Kinetics-400 dataset unless otherwise stated. A 1024 dimensional feature vector is extracted per stream from the final convolutional layer of an I3D [8] network at 3 feature vectors per second from 24fps videos.Table 1. Comparison of frame-level mAP score of our approach with previous works on MultiTHUMOS and Charades datasets using features from a pre-trained two-stream I3D model. Results indicated with * are from<ref type="bibr" target="#b27">[32]</ref>.Each feature vector corresponds to 8 frames or 0.33 seconds. The input sequence length is set to T = 128 on Mul-tiTHUMOS, and T = 64 on Charades. For both datasets, our network uses L = 5 MLAD layers (See section 5.3 for discussion on other values of T and L). The dimension of the class-level feature vector, H, is set to 128 in all our experiments. During training, we classify and compute loss on both the initial class-level features and the features from the final MLAD layer (the effect of this loss computation is explored in the supplement). We train our models using Adam optimizer with an initial learning rate of 1e-4. All our models are trained on a single 32GB NVIDIA Tesla V100 GPU and implemented in PyTorch deep-learning framework.</figDesc><table><row><cell>Method</cell><cell cols="2">MultiTHUMOS Charades</cell></row><row><cell>I3D Baseline* [33]</cell><cell>29.7</cell><cell>17.2</cell></row><row><cell>CF Baseline</cell><cell>42.6</cell><cell>14.8</cell></row><row><cell>Super-events* [34]</cell><cell>36.4</cell><cell>19.4</cell></row><row><cell>TGMs* [33]</cell><cell>44.3</cell><cell>21.5</cell></row><row><cell>TGMs + SE* [33]</cell><cell>46.4</cell><cell>22.3</cell></row><row><cell>TGMs + DG* [32]</cell><cell>48.2</cell><cell>22.9</cell></row><row><cell>Our Approach</cell><cell>51.5</cell><cell>23.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>0.018 0.673 0.029 4.409 0.260 0.770 33.63 15.23 18.65 32.58 37.88 18.01 21.96 35.53 CF 0.017 0.646 0.027 4.242 0.315 0.787 36.73 21.39 23.71 35.00 41.95 23.91 27.22 38.42 TGM [33] 0.017 0.642 0.022 3.798 0.297 0.800 34.59 17.21 20.14 36.90 39.27 20.13 23.86</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.18</cell></row><row><cell>Our</cell><cell>0.017 0.635 0.017 3.276 0.373</cell><cell>0.816</cell><cell>39.22 28.33 29.37</cell><cell>40.15</cell><cell>42.89 30.27 32.18</cell><cell>43.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S2 .</head><label>S2</label><figDesc>Ablation results showing the effect of weight parameter ?. Learning the ? parameter gives better results when compared to fixed ? = 0.5.</figDesc><table><row><cell></cell><cell cols="2">W/O Initial Loss With Initial Loss</cell></row><row><cell>f-mAP</cell><cell>49.96</cell><cell>51.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table S6. Evaluation of various methods on the Charades dataset using our proposed action-conditional metric. PAC -Action-Conditional Precision, RAC -Action-Conditional Recall, F1AC -Action-Conditional F1-Score, mAPAC -Action-Conditional Mean Average Precision.</figDesc><table><row><cell></cell><cell>1.33</cell><cell>2.10</cell><cell>15.17</cell><cell cols="2">12.68 1.94</cell><cell>2.93</cell><cell>21.43</cell><cell>14.93 2.02</cell><cell>3.07</cell><cell>20.26</cell></row><row><cell>CF</cell><cell>10.27 1.04</cell><cell>1.63</cell><cell>15.77</cell><cell>9.01</cell><cell>1.50</cell><cell>2.23</cell><cell>22.23</cell><cell>10.69 1.58</cell><cell>2.36</cell><cell>21.04</cell></row><row><cell cols="2">Ours 19.33 7.23</cell><cell>8.86</cell><cell>28.94</cell><cell cols="3">18.85 8.88 10.52</cell><cell>35.74</cell><cell cols="2">19.64 9.04 10.77</cell><cell>34.78</cell></row><row><cell></cell><cell>Action1</cell><cell></cell><cell>Action2</cell><cell></cell><cell cols="5">I3D Baseline CF Baseline TGM [33] Ours</cell><cell></cell></row><row><cell></cell><cell cols="2">BasketballDunk</cell><cell>Jump</cell><cell></cell><cell>88.12</cell><cell></cell><cell>91.32</cell><cell>90.46</cell><cell>93.18</cell><cell></cell></row><row><cell></cell><cell>CliffDiving</cell><cell></cell><cell>Jump</cell><cell></cell><cell>86.18</cell><cell></cell><cell>88.20</cell><cell>90.86</cell><cell>95.01</cell><cell></cell></row><row><cell></cell><cell cols="3">VolleyballSpiking Jump</cell><cell></cell><cell>45.97</cell><cell></cell><cell>52.32</cell><cell>50.77</cell><cell>64.63</cell><cell></cell></row><row><cell></cell><cell>PickUp</cell><cell></cell><cell>Squat</cell><cell></cell><cell>62.34</cell><cell></cell><cell>63.17</cell><cell>60.79</cell><cell>65.29</cell><cell></cell></row><row><cell></cell><cell>Throw</cell><cell></cell><cell cols="2">HammerThrow</cell><cell>29.89</cell><cell></cell><cell>40.57</cell><cell>36.74</cell><cell>47.49</cell><cell></cell></row><row><cell></cell><cell>BodyBend</cell><cell></cell><cell>Diving</cell><cell></cell><cell>36.89</cell><cell></cell><cell>39.73</cell><cell>42.28</cell><cell>45.45</cell><cell></cell></row><row><cell></cell><cell cols="3">BasketballDribble Run</cell><cell></cell><cell>45.60</cell><cell></cell><cell>55.33</cell><cell>47.26</cell><cell>58.85</cell><cell></cell></row><row><cell></cell><cell cols="2">CricketBowling</cell><cell>Throw</cell><cell></cell><cell>78.23</cell><cell></cell><cell>78.18</cell><cell>77.83</cell><cell>87.16</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>poral action detection in untrimmed videos. In Procedings of the British Machine Vision Conference 2017. British Machine Vision Association, 2019. 2 [4] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, and Juan Carlos Niebles. Sst: Single-stream temporal action proposals. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2911-2920, 2017. 1, 2 [5] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Fast temporal activity proposals for efficient detection of human actions in untrimmed videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1914-1923, 2016. 1, 2 [6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961-970, 2015. 5 [7] V?ctor Campos, Brendan Jou, Xavier Gir?-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017. 2 [8] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299-6308, 2017. 5, 12 [9] Bingzhi Chen, Jinxing Li, Guangming Lu, Hongbing Yu, and David Zhang. Label co-occurrence learning with graph convolutional networks for multi-label chest x-ray image classification. IEEE Journal of Biomedical and Health Informatics, 2020. 1, 2 [10] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5177-5186, 2019. 2, 3</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Additional visualizations which illustrate this behavior can be found in the Supplementary Material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Covariance of motion and appearance featuresfor spatio temporal recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05355</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end, single-stream tem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/,2014.5" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotation order matters: Recurrent image annotator for arbitrary length image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiren</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2452" to="2457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal attention mechanism with conditional inference for large-scale multi-label video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Dong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action1 Action2 I3D</title>
		<meeting><address><addrLine>Baseline CF Baseline TGM</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Action-conditional scores for some actions pairs with temporal dependencies in the MultiTHUMOS dataset. The metric reported is is action-conditional average precision for Action1 given Action2: AP(i = Action1|j = Action2</title>
	</analytic>
	<monogr>
		<title level="m">Table S8</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Top row is for the sequence on the left in the middle row, and the bottom row is for the sequence on the right. Attention maps shown here are of dimension C ? T (T = 128 (time steps), C = 65 (classes)) and are obtained by averaging the T class attention maps, with dimensions C ? C, along the columns. This gives the average attention for a given class at each time-step. Labels of the action classes (Stand, Talk to Camera, Cricket Shot) present in these selected sequences are shown on the right at their corresponding indices. Start and end index for the sequence is shown on the x-axis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S3</forename><surname>Figure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
	<note>Visualization of class attention maps from every MLAD layer of our network for two sequences from the same video</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-class svms: From tighter data-dependent generalization bounds to novel algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunwen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2977" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">We select 10 actions out of 65 from Multi-THUMOS, and visualize a 10 ? 10 subsets of the CB attention maps from all MLAD layers obtained by averaging the maps over time-steps where action &quot;Throw Discus&quot; (top) and &quot;Clean and Jerk&quot; (bottom) are present. Columns corresponding to classes that are related to the main action (those in green) tend to be active, whereas unrelated classes (in red) tend to have low activation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S4</forename><surname>Figure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic regularisation for recurrent image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2872" to="2880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal keyless attention fusion for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video classification using semantic concept cooccurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shayan Modiri Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2529" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seil</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07960</idno>
		<title level="m">Encoding video and label priors for multi-label video classification on youtube-8m dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differentiable grammars for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning latent superevents to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5304" to="5313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inferring temporal compositions of actions using probabilistic automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="368" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ioannis Katakis, and Ioannis Vlahavas. Random k-labelsets for multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilabel image classification via feature/label co-projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified view of multilabel performance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi-Zhu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR, 2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="3780" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep determinantal point process for large-scale multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luntian</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multilabel image classification by feature attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="98005" to="98013" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Orderless recurrent models for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Vacit Oguz Yazici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13440" to="13449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning deep latent spaces for multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chieh</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jen</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00418</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8668" to="8678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
