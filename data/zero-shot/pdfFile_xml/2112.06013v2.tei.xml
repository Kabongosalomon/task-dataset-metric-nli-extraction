<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
							<email>huaibaoxing@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Yuan</surname></persName>
							<email>nicholas.jing.yuan@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@suda.edu.cnquxiaoye</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most previous studies of document-level event extraction mainly focus on building argument chains in an autoregressive way, which achieves a certain success but is inefficient in both training and inference. In contrast to the previous studies, we propose a fast and lightweight model named as PT-PCG. In our model, we design a novel strategy for event argument combination together with a non-autoregressive decoding algorithm via pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers. Compared to the previous systems, our system achieves competitive results with 19.8% of parameters and much lower resource consumption, taking only 3.8% GPU hours for training and up to 8.5 times faster for inference. Besides, our model shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements. Codes are available at https: //github.com/Spico197/DocEE . * Corresponding author This contract stipulated that [B Group] ORG pledged its [80,021,000] NUM shares to [C Ltd.] ORG . On [Dec. 16, 2016] TIME , [B Group] ORG signed a contract. Document [EntityMention] Mention Type</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event Extraction (EE) aims at filling event tables with given texts. Different from sentence-level EE (SEE) which focuses on building trigger-centered trees <ref type="bibr" target="#b2">[Chen et al., 2015;</ref><ref type="bibr" target="#b8">Nguyen et al., 2016;</ref><ref type="bibr" target="#b9">Wadden et al., 2019;</ref><ref type="bibr" target="#b5">Lin et al., 2020]</ref>, document-level EE (DEE) is to decode argument combinations from abundant entities across multiple sentences and fill these combinations into event record tables as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where annotated triggers are often not available <ref type="bibr" target="#b11">[Yang et al., 2018;</ref><ref type="bibr" target="#b11">Zheng et al., 2019;</ref><ref type="bibr" target="#b10">Xu et al., 2021]</ref>.</p><p>One of the challenges for DEE is event argument combination without triggers. Triggers in DEE datasets are either absent or in a low annotation quality since the large-scale datasets are usually generated via distantly supervised (DS) alignment with existing knowledge bases (KB) <ref type="bibr">[Mintz et al.,</ref>  2009; <ref type="bibr" target="#b2">Chen et al., 2017]</ref>. Consequently, the absence of triggers drives the development of trigger-free argument combination methods in recent studies. <ref type="bibr" target="#b11">Yang et al. [2018]</ref> first identify a key sentence and then fill the event record table by finding arguments near the sentence, while partial global features and arguments are still missing. <ref type="bibr" target="#b11">Zheng et al. [2019]</ref> and <ref type="bibr" target="#b10">Xu et al. [2021]</ref> fully utilize the global context and make significant improvements by building an directed acyclic graph <ref type="bibr">(DAG)</ref>. However, such DAG-based methods require massive computing resources as they rely on an autoregressive fashion to decode argument combinations, which is inefficient in both training and inference for long documents. Meanwhile, building DAGs consumes lots of memories to store previous paths. As a result, it takes almost one week with a minimum of four 32GB GPUs to train such DAG-based DEE models, and the inference speed is also extremely slow.</p><p>Considering the above challenge in DEE and the speed &amp; memory consuming problems in the DAG-based methods, in this paper, we aim to 1) propose a universal event argument combination strategy that works on both triggeraware and trigger-free DEE; and 2) provide a blazing fast and lightweight model for document-level event extraction.</p><p>We propose a novel non-autoregressive approach named as Pseudo-Trigger-aware Pruned Complete Graph (PTPCG). Specifically, we formulate each argument combination as a pruned complete graph, where the important arguments are identified and treated as a group of pseudo triggers with bidirectional connections to each other and other ordinary arguments are linked from these pseudo triggers in a directed manner. Based on the pruned complete graph with the pseudo triggers, we design an efficient algorithm with the non-autoregressive decoding strategy for event argument combination extraction.</p><p>The experiments results show that our PTPCG can reach competitive results with only 19.8% parameters of DAGbased SOTA models, just taking 3.8% GPU hours to train and up to 8.5 times faster in terms of inference. Besides, our PT-PCG is highly flexible and scalable that can be used as a general architecture for non-autoregressive trigger-based event extraction. If only one pseudo trigger is selected for each combination, the pruned complete graph becomes a triggercentered tree like SEE. Furthermore, pseudo triggers can be adopted as supplements to enhance the annotated-triggerbased methods.</p><p>In summary, our contributions include:</p><p>? We propose a novel non-autoregressive event argument combination paradigm based on pruned complete graph with pseudo triggers, which is compatible in documentlevel event extraction with (or without) triggers. ? Our model is fast and lightweight for end-to-end document-level event extraction and we conduct extensive experiments to show the efficiency and efficacy. ? To our best knowledge, our present approach is the first work that explores the effects of using some arguments as pseudo triggers in DEE, and we design a metric to help select a group of pseudo triggers automatically. Furthermore, such metric can also be used for measuring the quality of annotated triggers in DEE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As shown in <ref type="figure">Figure 2</ref>, our model can be divided into four components: 1) Event detection performs multi-label classification to identify all possible event types. 2) Entity extraction extracts all entities from the documents and encodes these entities into dense vectors. 3) Combination extraction builds pruned complete graphs and decodes argument combinations from such graphs. 4) Event record generation combines the results of event types and extracted argument combinations to generate the final event records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Detection</head><p>For a document D, a bidirectional long short-term memory (BiLSTM) network <ref type="bibr" target="#b3">[Hochreiter and Schmidhuber, 1997</ref>] is used to encode each sentence s i into token-wise hidden states (h</p><formula xml:id="formula_0">(1) i , h (2) i , . . . , h (|si|) i ), where h (j) i ? R d h is the concatena- tion of two direction representations ? ? ? h (j) i ? ? ? h (j)</formula><p>i and |s i | is the length of i-th sentence. The last hidden states in each direction are concatenated to get the sentence representation</p><formula xml:id="formula_1">g i ? G as g i = ? ?? ? h (|si|) i ? ? ? h (0) i .</formula><p>Subsequently, we follow Doc2EDAG <ref type="bibr" target="#b11">[Zheng et al., 2019]</ref> and use randomly initialized event queries with multi-head attention on G to make a binary classification for each event type. The loss function of event detection component L det is defined as a binary cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Extraction</head><p>The entity extraction task can be formulated as a sequence tagging task in BIO scheme <ref type="bibr" target="#b11">[Zheng et al., 2019]</ref>. To enhance the entity recognition, we add more money, date, percentage ratio and shares entities into the dataset by simple regular expression matching. To deal with the document-level entity extraction, we first split the whole document into sentences and perform a sentence-level mention extraction. Then, we use BiLSTM with <ref type="bibr">CRF [Lample et al., 2016]</ref> to extract entity mentions with the same BiLSTM used in event detection. The training objective of entity extraction is to minimize the negative log-likelihood loss L ent of CRF for each sentence.</p><p>For all tokens of a mention, a max-pooling operation is applied on token-level representations to get the mention representationm j . As mention types have been proved effective for downstream sub-modules <ref type="bibr" target="#b11">[Zheng et al., 2019;</ref><ref type="bibr" target="#b10">Xu et al., 2021]</ref>, we convert the predicted discrete mention types into vectors by looking up an embedding table. After concatenatingm j and type embeddings l j , we get the final mention representation m j =m j l j ? R da , where d a = d h + d l and d l denotes the dimension of l j . Finally, all the mentions for an entity are aggregated to get the entity representation? i via another max-pooling. For better modeling entity semantics and the latent connections for combiantion extraction, an additional BiLSTM layer is applied to get the set of entity representations E = {? i } |E| i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination Extraction</head><p>In this section, we introduce the details of selecting pseudo triggers, building pruned complete graphs, and decoding combinations from the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Trigger Selection</head><p>It is hard to annotate conventional triggers manually in documents due to the document length and dataset scale. We instead select a group of pseudo triggers for each event type in an automatic way.</p><p>Empirically, the triggers are keywords that play two roles: 1) triggers can be used to identify combinations; 2) triggers are fingerprints that can distinguish different combinations. Combinations are made of arguments and we can extract a specific combination by finding all the corresponding arguments. To this end, we design an importance metric that evaluates the possibility of whether a group of arguments can serve as the pseudo triggers. In general, we first select a group of argument roles as candidates for each event type according to the importance scores, and take the corresponding arguments as the pseudo triggers.</p><p>Formally, the importance score is obtained by the existence and the distinguishability. For a subset of predefined argument roles in type t i , R = {r j } |R| j=1 are selected as the pseudo trigger candidates. |R| is a hyper-parameter that denotes the is the number of event records that at least one corresponding argument of R is not NULL, and N (i) is the number of total records of t i . The distinguishability is defined to satisfy that the triggers can distinguish different combinations, where N (R) u is the number of records that the arguments of R do not appear in other records in the same document. With the multiplication of existence and distinguishability, pseudo triggers are selected by picking the candidate with the highest importance score.</p><formula xml:id="formula_2">Existence(R) = N (R) e N (i) , Distinguish(R) = N (R) u N (i) Importance(R) = Existence(R) ? Distinguish(R) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruned Complete Graph Construction</head><p>Based on the DEE task setting and data analysis, we propose an assumption that arguments in the same combination are close to each other in the semantic space. Following this assumption, we take the pseudo triggers as the core of argument combinations and formulate each combination as a pruned complete graph. As shown in the pruned complete graph of <ref type="figure">Figure 2</ref>, for any two arguments as pseudo triggers a (i) t and a (j) t in the same combination, they are bidirectionally connected, where the adjacent matrix y</p><formula xml:id="formula_3">(i,j) A = y (j,i) A = 1.</formula><p>For a pseudo trigger a After obtaining entity representations, a dot scaled similarity function (Eqn 2) is applied to estimate their semantic</p><p>Step 1: Find Complete Connected Cliques</p><p>Step 2: Find Shared Neighbors for Each Clique</p><p>Step 3: Make Combination Pseudo Trigger Ordinary Argument Shared Argument distances:</p><formula xml:id="formula_4">e i = e i ? W s + b s ,? j = e j ? W e + b ? A i,j = sigmoid ? i ? j / d h<label>(2)</label></formula><p>where? denotes the similarity matrix, W s , W e ? R da?da and b s , b e ? R da are trainable parameters for semantic space linear projection.</p><p>In training, we use binary cross entropy function to formulate the combination loss L comb .</p><p>To predict the binary adjacent matrix A of the pruned complete graph for further decoding, the threshold ? is used here (Eqn 3).</p><formula xml:id="formula_5">A i,j = 1? i,j ? 0 otherwise<label>(3)</label></formula><p>Non-Autoregressive Combination Decoding Event argument combinations are extracted based on the predicted adjacent matrix A with a non-autoregressive decoding algorithm. First, all the pseudo triggers are identified based on nodes' out-degree and each pseudo trigger group is recognized as a clique. If the out-degree of an entity is greater than 0 except for self-loop, then the entity is treated as a pseudo trigger. For |R| = 1, all the combinations are pseudo-trigger-centered trees, where each combination is made of a pseudo trigger with its neighbors. Otherwise, Bron-Kerbosch (BK) algorithm <ref type="bibr" target="#b1">[Bron and Kerbosch, 1973]</ref> is applied first to find all possible cliques (step 1 in <ref type="figure" target="#fig_2">Figure 3</ref>). To apply BK algorithm, the links between arguments must be undirected, thus we first extract all bidirectional links as undirected input.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the next step is to find the ordinary (non-pseudo-trigger) arguments in the combination. We further exploit all the neighbors of each pseudo trigger in the clique. After that, an intersection operation is performed to find commonly shared ordinary arguments. The combination is consist of a pseudo trigger clique and their commonly shared ordinary arguments. For those extreme records that have only one argument in the combinations, all predicted entities are aggregated together as a default combination.</p><p>The document length does not affect the event record generation speed, and the time complexity of our event records generation is polynomial</p><formula xml:id="formula_6">(O(N t ? N c )), while DAG-based model is O(N t ? N s ? N r ), where N t , N c , N s , N r are</formula><p>the number of predicted types, combinations, predicted spans and the average number of roles per type. In most cases, N s ? N r N c , so our PTPCG consistently holds the speed advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Event Records Generation</head><p>After the set of combinations C are obtained from the pruned complete graphs, the next step is to fill these combinations into event tables. First, all the combinations should match with event types. Since event detection is a multi-label classification task, there may be more than one type prediction. For all type predictions T p = {t j } |Tp| j=1 and combinations C, we perform a Cartesian product and get all type-combination pairs {&lt; t j , c k &gt; |1 j |T p |, 1 k |C|}.</p><p>For each pair &lt; t j , c k &gt;, we use an event-relevant feedforward network (FFN) as classifier to get possible role results for all arguments E k in c k , and use sigmoid function to get the probabilities p (j) role (r j |c k ). The loss L role is calculated by a binary cross entropy function.</p><p>In the role classification task, an entity can act as more than one role in a record, while a role in a table can only be filled with one entity. Following this setting, we take the entity e role (r j |c k ) &lt; 0.5, then the pair is recognized as an invalid candidate and will be dropped.</p><formula xml:id="formula_7">i * = argmax q p (j) role (r (q) j |c k )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Optimization</head><p>Our PTPCG is an end-to-end model with joint training and scheduled sampling <ref type="bibr" target="#b0">[Bengio et al., 2015]</ref> strategy. The overall loss is a weighted sum of all losses as below:</p><formula xml:id="formula_8">L = ? 1 L det + ? 2 L ent + ? 3 L comb + ? 4 L role<label>(5)</label></formula><p>where ? 1 , ? 2 , ? 3 , ? 4 are hyper-parameters to reduce the unbalanced loss effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use ChFinAnn <ref type="bibr" target="#b11">[Zheng et al., 2019]</ref> and DuEE-fin <ref type="bibr" target="#b5">[Li, 2021]</ref> datasets to make fair comparisons across all methods.</p><p>(1) ChFinAnn is by far the largest DEE dataset constructed by distantly supervised alignment without trigger annotations, which is widely used in the previous studies. This dataset contains 32k financial announcements and 48k event records, in which 29% documents have more than one record and 98% of records have arguments scattered across different sentences. On average a document contains 20 sentences and the longest document contains 6.2k Chinese characters.</p><p>(2) DuEE-fin is another dataset for DEE with trigger annotations. It contains 13 event types and 11.7k documents, where the test set is evaluated online. Each record in DuEE-fin has a labeled trigger word without specific positions, and 36% of records share the same trigger in a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Settings</head><p>We choose the hyper-parameters of our system according to the performance on the development set in ChFinAnn. In PT-PCG, we use 2 layers of shared BiLSTM for event detection and entity extraction, and another 2 layers of BiLSTM for entity encoding. We use the same vocabulary as <ref type="bibr" target="#b11">[Zheng et al., 2019]</ref> and randomly initialize all the embeddings where d h =768 and d l =32. Adam <ref type="bibr" target="#b3">[Kingma and Ba, 2015]</ref> optimizer is used with a learning rate of 5e-4 and the mini-batch size is 64. The weights in Equation 5 are 0.05, 1.0, 1.0, and 1.0, and ? in Equation 3 is 0.5. Following the setting in Zheng et al.</p><p>[2019], we train our models for 100 epochs and select the checkpoint with the best F1 score on the development set to evaluate on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines and Metrics</head><p>Baselines.</p><p>(1) DCFEE <ref type="bibr" target="#b11">[Yang et al., 2018]</ref>  Metrics. We use the same evaluation setting as in the previous studies <ref type="bibr" target="#b11">[Zheng et al., 2019;</ref><ref type="bibr" target="#b10">Xu et al., 2021]</ref>. For each predicted record, a golden record is selected without replacement by matching the record that has the same event type and the most shared arguments, and F1 scores are calculated by comparing arguments. DuEE-fin uses an online evaluation fashion and the platform only provides micro-averaged scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Main Results</head><p>As shown in ChFinAnn-Single. As for ChFinAnn-All, our model outperforms Doc2EDAG and is very close to GIT. On the DuEEfin dataset, PTPCG achieves the best F1 scores whether or not trigger words are provided, demonstrating good compatibility and universality. Our PTPCG outperforms GIT and improves the overall F1 with 7.2% w/o manually annotated triggers. For DuEE-fin w/ Tgg, we find our approach declines from 60.3% to 58.1%. After anaylzing the dataset, we find that the manually annotated triggers have less importance scores (62.9) than our pseudo triggers (83.8, |R|=1).</p><p>Although adding annotated triggers when |R|=1 can boost the importance score to 93.7, it is a trade-off between the number of triggers (annotated &amp; pseudo) and the adjacent matrix prediction. We will discuss more in Section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison on Model Parameters and Speed</head><p>Comparison on the Number of Parameters. As shown in <ref type="table" target="#tab_2">Table 1</ref>, among all the models, PTPCG is the most lightweight one and is in the same scale with DCFEE, while PTPCG outperforms DCFEE-O with 16.2% absolute gain in F1 scores on ChFinAnn-All. Without considering 16M vocabulary embedding parameters, PTPCG takes only 19.8% parameters of GIT and reaches competitive results.</p><p>Comparison on Speed. Benefiting from the lightweight architecture design and non-autoregressive decoding style, our PTPCG is fast in both training and inference. Comparing the training time as shown in <ref type="table" target="#tab_2">Table 1</ref>, it takes 633.6 GPU hours to train GIT, while PTPCG is 26.4 times faster and takes only 24.0 GPU hours. These facts indicate that PTPCG is more efficient in training and requires much lower computation resource cost than other methods, and the whole training process has been reduced from almost a week with 4 NVIDIA V100 GPUs to just one day with 1 GPU.</p><p>According to the inference speed test in <ref type="figure" target="#fig_4">Figure 4</ref>, PTPCG is more scalable than other models. With the growth of batch size, PTPCG becomes faster and finally stabilized near 125 documents per second, while Doc2EDAG and GIT are barely growing with batch size, peaking at 19 and 15 docs/s respectively. PTPCG is up to 7.0 and 8.5 times faster compared to Doc2EDAG and GIT. As an enhanced version of Doc2EDAG, GIT is 21.2% slower than Doc2EDAG on average, and raises OOM error on a 32GB memory GPU when batch size is 128. To further check the effect of different encoders, we substitute all BiLSTM layers into transformer encoders and create the TransPTPCG model with the same scale to Doc2EDAG parameters. The results in <ref type="figure" target="#fig_4">Figure 4</ref> show that TransPTPCG is up to 2.5 times faster than Doc2EDAG, validating the advantage of non-autoregressive combination decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Pseudo Triggers as Supplements</head><p>PTPCG is capable of handling with or without manually annotated triggers, and the automatically selected pseudo triggers can be supplements to enhance the performance. As shown in <ref type="table" target="#tab_4">Table 2</ref>, we find the pseudo triggers could assist the trigger words to boost the importance score from 62.9 to 93.7, and the results show that this helps identify combinations and bring 0.7% improvement in offline dev evaluation and 0.8% improvement in the online test set.</p><p>To further validate the effectiveness of importance scores for pseudo trigger selection, we select groups of pseudo triggers with the middle and the lowest importance scores instead of the highest ones and analyze the results in   importance and overall scores. Nevertheless, the highest importance score (88.3%) is not equal to 100.0%, which may limit the upper bound of decoding. We will explain more about error analysis in Section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Error Analysis</head><p>Although pruned complete graph structure is efficient for training and inference, it is not perfect in similarity calculation (adjacent matrix prediction) and combination decoding.</p><p>Here we analyze the upper bounds of the combination decoding algorithm and discuss the future directions. Results in <ref type="table" target="#tab_7">Table 4</ref> show that models with the lower number of pseudo triggers are better. However, models with more pseudo triggers have higher upper bounds for decoding and have higher importance scores. Why models with more pseudo triggers have greater importance but still result in lower performance? Results in <ref type="table" target="#tab_3">Table 3</ref> and 4 may answer this question. Models with the same |R| has a strong correlation that higher importance brings higher metric scores which validates the effectiveness of the pseudo trigger selection strategy based on importance scores. However, more pseudo triggers bring more links to connect, and the model may be not robust enough for predicting each connection correctly, leading to an adjacent accuracy decline.</p><p>Overall, it is a trade-off that more pseudo triggers improves the upper bounds and reduces combination error rates, but also brings new challenges to recover connections between entities. We believe it is the future direction to improve the similarity calculation and adjacent matrix prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Fully annotated datasets are usually small in scale <ref type="bibr" target="#b3">[Ebner et al., 2020;</ref><ref type="bibr" target="#b5">Li et al., 2021]</ref>. DS-constructed datasets are large in scale, but they can hardly match triggers to records, so triggers are likely to be absent. Unlike trigger-based methods <ref type="bibr" target="#b9">[Pouran Ben Veyseh et al., 2021;</ref><ref type="bibr" target="#b3">Du and Cardie, 2020]</ref>,  argue that event types can be detected without triggers. <ref type="bibr" target="#b11">Yang et al. [2018]</ref> extract event records using entities in a window of key sentences. It is efficient but misses lots of information from other sentences. To fully utilize all entities in the whole document, Doc2EDAG <ref type="bibr" target="#b11">[Zheng et al., 2019]</ref> formulates the argument combination to be a directed acyclic graph (DAG), making great progress in DEE. GIT <ref type="bibr" target="#b10">[Xu et al., 2021]</ref> is a variant of Doc2EDAG where a graph neural network is added to help entity encoding and further exploit the global memory mechanism during decoding. DAG extracts combinations in an auto-regressive way, which is very time consuming and needs huge space in global memory module to store all previous paths. Besides, Doc2EDAG and GIT are both large models, where Doc2EDAG and GIT use 12 and 16 layers of transformer encoders. To train such models, a minimum of four 32GB GPUs would have to be run for almost a week. Huang and Jia <ref type="bibr">[2021]</ref> utilize <ref type="bibr">BERT [Devlin et al., 2019]</ref> as sentence representations and exploit relations between sentences via graph neural networks <ref type="bibr" target="#b9">[Veli?kovi? et al., 2018]</ref> to help identify fixed number of combinations. To speed up DEE, we propose a novel non-autoregressive decoding strategy and boost the training &amp; inference speed with competitive results compared with the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Pursuing fast and general document-level event extraction (DEE), we propose a non-autoregressive model named as PT-PCG. For DEE without triggers, we first select a group of pseudo triggers to build pruned complete graphs, and then train a lightweight model to extract all possible combinations, which enables non-autoregressive decoding and is up to 8.5x faster compared to SOTA models. For DEE with annotated triggers, pseudo triggers also show the power to make improvement and are even better than annotated triggeronly method. In summary, our model costs less resources yet achieves better or comparable results, compared with the previous systems. Although PTPCG is not perfect in adjacent matrix prediction, we believe it has the ability to combine different DEE tasks and needs more explorations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of event extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>same combination, they are connected with a directional link and y (i,j) A = 1. Besides, each argument a (i) has a self-loop connection where y (i,i) A = 1. Other entries in y A are zeros, where entities that do not participate in any combination are isolated nodes in the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Combination decoding (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>* as the argument of role r (q) j if and only if it satisfies the constraint in Eqn 4. If ?j ? [1, |T p |], p (j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>e r e n c e S p e e d ( d o c s / s ) Inference speed comparison with baselines (left) and with different |R| (right) with 1 NVIDIA V100 GPU for all models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview of our PTPCG. Event types and entity mentions are first extracted with a shared LSTM encoder, then similarities between entity pairs are calculated to help recover the adjacent matrix of pruned complete graph. After combinations are decoded from the adjacent matrix, the extracted types are paired with combinations to generate final event records.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Event Detection</cell><cell></cell><cell>Event Record Generation</cell><cell>Type 1</cell></row><row><cell>Document</cell><cell>... ... ...</cell><cell>Sentence Representations ... Max-Pooling Pooling Max-... ...</cell><cell>Event Queries BiLSTM</cell><cell>Type 1: ? Type 2: ? Type 3: ?</cell><cell>Matching</cell><cell>Role 1 Role 2 Role 3 Role 4 Role 5 Role 1 Type 3 Role 2 Role 3 NULL Role 4 NULL</cell></row><row><cell></cell><cell>LSTM Encoder</cell><cell>Mention Type</cell><cell>Entities</cell><cell>Similarity Calculation</cell><cell>Pruned Complete Graph</cell><cell>Role 5</cell></row><row><cell></cell><cell></cell><cell>Entity Extraction</cell><cell></cell><cell cols="2">Combination Extraction</cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>number of selected pseudo triggers for each argument com- bination. The existence measures whether the arguments of R can identify combinations. N(R) e</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>GreedyDec is a baseline from Zheng et al. [2019] which fills one event table greedily. (4) GIT [Xu et al., 2021] is another variant of Doc2EDAG and utilizes a graph neural network to further encode entities and add more features in DAG generation.</figDesc><table><row><cell>has two variants</cell></row><row><cell>here: DCFEE-O extracts only one record from one document</cell></row><row><cell>while DCFEE-M extracts records as much as possible. (2)</cell></row><row><cell>Doc2EDAG [Zheng et al., 2019] constructs records as argu-</cell></row><row><cell>ment chains (DAG) and uses an auto-regressive way to ex-</cell></row><row><cell>tract final results. (3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 ,</head><label>1</label><figDesc>PTPCG achieves better or competitive results than the previous systems. On the ChFinAnn dataset, our PTPCG achieves the best scores evluated on 83.9 77.3 80.4 81.9 51.2 63.0 59.6 41.8 49.1 59.0 42.1 49.2 Doc2EDAG * 64M (48M) 604.8 83.2 89.3 86.2 81.1 77.0 79.0 66.7 50.0 57.2 67.1 51.3 58.1 GIT * 97M (81M) 633.6 85.0 88.7 86.8 82.4 77.6 79.9 68.2 43.4 53.1 70.3 46.0 55.6 Table 1: Main results. #Params is estimated on the ChFinAnn dataset. w/o Emb means the number of parameters without vocabulary embeddings. All models are trained with 100 epochs. -Single denotes the evaluation results on documents with only one event record. Tgg denotes the manually annotated triggers.</figDesc><table><row><cell>Model</cell><cell>#Params</cell><cell>GPU</cell><cell cols="3">ChFinAnn-Single</cell><cell cols="3">ChFinAnn-All</cell><cell cols="3">DuEE-fin w/o Tgg</cell><cell cols="3">DuEE-fin w/ Tgg</cell></row><row><cell></cell><cell>(w/o Emb)</cell><cell>Hours</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>DCFEE-O  *</cell><cell>32M (16M)</cell><cell cols="13">192.0 73.2 71.6 72.4 69.7 57.8 63.2 56.2 48.2 51.9 51.9 49.6 50.7</cell></row><row><cell>DCFEE-M  *</cell><cell>32M (16M)</cell><cell cols="13">192.0 64.9 71.7 68.1 60.1 61.3 60.7 38.7 52.3 44.5 37.3 48.6 42.2</cell></row><row><cell cols="15">GreedyDec  *  604.8 PTPCG |R|=1 32M (16M) 64M (48M) 24.0 86.3 90.1 88.2 83.7 75.4 79.4 64.5 56.6 60.3 63.6 53.4 58.1</cell></row></table><note>* We reproduce the results using their open-source codes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The results show that there is a positive correlation between the</figDesc><table><row><cell cols="2">Pseudo Impt.</cell><cell></cell><cell>Dev</cell><cell></cell><cell></cell><cell>Online Test</cell></row><row><cell>Trigger</cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>?</cell><cell>62.9</cell><cell cols="6">76.5 56.8 65.2 69.8 48.5 57.3</cell></row><row><cell></cell><cell>93.7</cell><cell cols="6">70.5 62.3 66.2 63.6 53.4 58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>PTPCG results on DuEE-fin w/ annotated triggers. Impt denotes the importance score.</figDesc><table><row><cell>Impt.</cell><cell></cell><cell>ChFinAnn</cell><cell></cell><cell>Impt.</cell><cell cols="2">DuEE-fin w/o Tgg</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>88.3</cell><cell cols="3">83.7 75.4 79.4</cell><cell>83.8</cell><cell cols="2">66.7 54.6 60.0</cell></row><row><cell>62.5</cell><cell cols="3">88.7 60.9 72.2</cell><cell>37.0</cell><cell cols="2">65.5 49.6 56.4</cell></row><row><cell>22.4</cell><cell cols="3">90.5 59.5 71.8</cell><cell>15.0</cell><cell cols="2">74.2 45.3 56.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>PTPCG results with different importance scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Analysis of combination decoding errors on ChFinAnn. SE, ME, TotE are error upper bounds of single-record documents, multiple-record documents, and overall error respectively. #links is the number of connected links among all graphs in the test set. Adj Acc is the accuracy of adjacent matrix prediction given golden entities. F1 is micro-averaged.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natural Science Foundation of China (Grant No. 61936010 and 61876115), the Priority Academic Program Development of Jiangsu Higher Education Institutions, and the joint research project of Huawei Cloud and Soochow University. We would also like to thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
	<note>Proc. of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithm 457: Finding all cliques of an undirected graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coen</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joep</forename><surname>Kerbosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="575" to="577" />
			<date type="published" when="1973-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<editor>Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</editor>
		<meeting>of ACL-IJCNLP<address><addrLine>Beijing, China; Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring sentence community for document-level event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic; Ba; San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Documentlevel event argument extraction by conditional generation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Duee-fin: A document-level event extraction dataset in the financial domain released by baidu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://aistudio.baidu.com/aistudio/competition/detail/46" />
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
	<note>A joint neural model for information extraction with global features. Online</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly multiple events extraction via attentionbased graph information aggregation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event detection without triggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling document-level context for event detection via important context selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouran</forename><surname>Ben Veyseh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document-level event extraction via heterogeneous graph-based interaction model with a tracker</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08" />
			<biblScope unit="page" from="3533" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DCFEE: A document-level Chinese financial event extraction system based on automatically labeled training data</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP<address><addrLine>Melbourne, Australia; Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
