<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BREAKING THE EXPRESSIVE BOTTLENECKS OF GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
							<email>yangmq@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BREAKING THE EXPRESSIVE BOTTLENECKS OF GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to k-WL test (k ? 1). However, the aggregators in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become expressive bottlenecks. In this paper, we improve the expressiveness by exploring powerful aggregators. We reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrix for building more powerful aggregators and even injective aggregators. It can also be viewed as the strategy for preserving the rank of hidden features, and implies that basic aggregators correspond to a special case of low-rank transformations. We also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs. Based on our theoretical analysis, we develop two GNN layers, Expand-ingConv and CombConv. Experimental results show that our models significantly boost performance, especially for large and densely connected graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are ubiquitous in the real world. Social networks, traffic networks, knowledge graphs, and molecular structures are typical graph-structured data. Graph Neural Networks (GNNs) <ref type="bibr" target="#b31">(Scarselli et al., 2008;</ref><ref type="bibr" target="#b10">Gori et al., 2005)</ref>, leveraging the power of neural networks to graph-structured data, have a rapid development recently <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017;</ref><ref type="bibr" target="#b1">Bronstein et al., 2017;</ref><ref type="bibr" target="#b9">Gilmer et al., 2017;</ref><ref type="bibr" target="#b5">Duvenaud et al., 2015)</ref>.</p><p>Expressive power of GNNs measures their abilities to represent different graph structures <ref type="bibr" target="#b30">(Sato, 2020)</ref>. It decides the performance of GNNs where the awareness of graph structures is required, especially on large graphs with complex topologies. The neighborhood aggregation scheme (or message passing) follows the same pattern with weisfiler-lehman (WL) graph isomorphism test <ref type="bibr" target="#b36">(Weisfeiler &amp; Leman, 1968)</ref> to encode graph structures, where node representations are computed iteratively by aggregating transformed representations of its neighbors with structural information learned implicitly. Therefore, the WL test is used to measure the expressiveness of GNNs. Unfortunately, general GNNs are at most as powerful as 1-order WL test <ref type="bibr" target="#b23">(Morris et al., 2019;</ref>. There is also work trying to improve the expressiveness that are beyond 1-order WL test <ref type="bibr" target="#b21">(Maron et al., 2019;</ref><ref type="bibr" target="#b23">Morris et al., 2019;</ref><ref type="bibr" target="#b18">Li et al., 2020b;</ref><ref type="bibr" target="#b34">Vignac et al., 2020)</ref>. However, the weak distinguishing strength of aggregators is the fundamental limitation. The expressiveness analysis measured by the WL test assumes that aggregators are injective, which is usually unreachable. Therefore, this motivates us to investigate the following questions: What are the key factors to limit the expressiveness of GNN? and how to break these limitations?</p><p>Aggregators are permutation invariant functions that operate on sets while preserving permutation invariance. <ref type="bibr" target="#b43">(Zaheer et al., 2017)</ref> first theoretically studied permutation invariant functions and provided a family of functions to which any permutation invariant function must belong.  extended it on multisets but only for countable space. <ref type="bibr" target="#b3">(Corso et al., 2020)</ref> further extended it to uncountable space. <ref type="bibr" target="#b24">(Murphy et al., 2018)</ref> and <ref type="bibr" target="#b25">(Murphy et al., 2019)</ref> expressed a permutation in-1 arXiv:2012.07219v1 <ref type="bibr">[cs.</ref>LG] 14 Dec 2020 Preprint variant function by approximating an average over permutation-sensitive functions with tractability strategies. <ref type="bibr" target="#b4">(Dehmamy et al., 2019)</ref> showed that a single propagation rule applied in general GNNs is rather restrictive in learning graph moments <ref type="bibr" target="#b19">(Lin &amp; Skiena, 1995)</ref>. They and <ref type="bibr" target="#b3">(Corso et al., 2020)</ref> improved the distinguishing strength of aggregation by leveraging multiple basic aggregators (SUM, MEAN, NORMALIZED MEAN, MAX/MIN, and STD). This strategy showed its effectiveness on tasks taken from classical graph theory.</p><p>In contrast to existing studies towards aggregators in GNNs, we provide a new GNN formulation, where the aggregation is represented as the multiplication of the corresponding hidden feature matrix of neighbors and the aggregation coefficient matrix. This new formulation enables us to answer the following questions: (i) when a GNN will lose its expressive power; (ii) How to build aggregators with higher distinguishing strength, even injective aggregators. Based on our theoretical analysis, we propose two GNN layers: ExpandingConv and CombConv, and evaluate them on general graph classification and graph regression tasks. Our key contributions are summarized as follows:</p><p>? We formalize the distinguishing strength of aggregators as a partial order, and theoretically show that the choice of aggregators can be bottlenecks of expressiveness. We also propose to apply nonlinear units ahead of aggregations to break the distinguishing strength limitations of aggregators as well as to achieve an implicit sampling mechanism. ? We reformulate the neighborhood aggregation with the aggregation coefficient matrix and then provide a theoretical point of view on building powerful aggregators and even injective aggregators. ? We propose ExpandingConv and CombConv layers which achieve state-of-the-art performance on a variety of graph tasks. We also show that multi-head GAT is one of the Ex-pandingConv implementations, which brings a theoretical explanation for its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NOTATIONS</head><p>For a graph GpV, Eq, we denote the set of edges, nodes and node feature vectors respectively by E G , V G and X G . N pvq represents the set of neighbors of v including itself, i.e., N pvq " tu P V G |pu, vq P E G u Y tvu. We use rns to denote the set t1, 2, ..., nu. tt...uu represents a multi-set, i.e., a set with possibly repeating elements. ? n represents the set of all permutations of the integers 1 to n. h ? , where ? P ? |h| , is a reordering of the elements of a sequence h according to ?. Given a matrix X P R a?b , X T represents the transpose of X, and vecpXq P R ab?1 represents the column stack of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH NEURAL NETWORKS</head><p>Most GNNs adopt the neighborhood aggregation scheme <ref type="bibr" target="#b9">(Gilmer et al., 2017)</ref> to learn the node representations, which utilizes both node features and graph structures. In the k-th layer, the representation of node v</p><formula xml:id="formula_0">h pkq v " Updateph pk?1q v , Aggregateptth pk?1q u |u P N pvquuqq.<label>(1)</label></formula><p>Aggregators in GNNs. An aggregator is a permutation invariant function <ref type="bibr" target="#b43">(Zaheer et al., 2017)</ref> with bounded size inputs. It satisfies: (i) size insensitive: an aggregator can take an arbitrary but finite size of inputs; (ii) permutation invariant: an aggregator is invariant to the permutation of input. There are a limited number of basic aggregators such as SUM, MEAN, NORMALIZED MEAN, MAX/MIN, STD, etc. Most proposed GNNs apply one of these aggregators. Sum-of-power mapping <ref type="bibr" target="#b43">(Zaheer et al., 2017)</ref> and normalized moments <ref type="bibr" target="#b3">(Corso et al., 2020)</ref> can also be used as aggregators and they allow for a variable number of aggregators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL</head><p>In this section, we first formalize the distinguishing strength of aggregators as a partial order, and show why basic aggregators used in popular GNNs become bottlenecks of expressiveness. Then, we analyze the requirements for building powerful aggregators and even injective aggregators. Finally, we introduce two GNN layers based on our theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISTINGUISHING STRENGTH OF AGGREGATORS</head><p>To ensure generality, our analysis of aggregators is always considered in multisets and uncountable case, where the inputs are continuous and with possibly repeating elements. We first introduce distinguishing strength under the concept of partial order <ref type="bibr" target="#b32">(Schmidt, 2011)</ref>.</p><p>Distinguishing strength. The distinguishing strength of aggregator f aggr1 is stronger than f aggr2 , denoted by f aggr1 ? f aggr2 , if and only if for any two multisets x 1 and x 2 where the number of elements can be different,</p><formula xml:id="formula_1">f aggr2 px 1 q ? f aggr2 px 2 q ? f aggr1 px 1 q ? f aggr1 px 2 q. Meanwhile, if there exist x 1 1 and x 1 2 such that f aggr1 px 1 1 q ? f aggr1 px 1 2 q but f aggr2 px 1 1 q " f aggr2 px 1 2 q, f aggr1</formula><p>is strictly stronger than f aggr2 , denoted by f aggr1 ? f aggr2 . If f aggr1 ? f aggr2 ? f aggr1 , we say these two aggregators have the same distinguishing strength, denoted by f aggr1 " f aggr2 . If there exist multisets x 1 and x 2 such that f aggr1 px 1 q ? f aggr1 px 2 q, f aggr2 px 1 q " f aggr2 px 2 q, and there also exist x 1 1 and x 1 2 such that f aggr1 px 1 1 q " f aggr1 px 1 2 q, f aggr2 px 1 1 q ? f aggr2 px 1 2 q, we say f aggr1 and f aggr2 are incomparable.</p><p>Distinguishing strength is a partial order, and the set of all aggregators form a poset. In this poset, the aggregators with the greatest distinguishing strength should be injective. With the definition of distinguishing strength, we can compare any two aggregators. The distinguishing strength of widely used aggregators SUM, MEAN, MAX/MIN is incomparable. One can easily give two multisets of elements that are distinguished by one aggregator but are not distinguished by the others as showed in <ref type="bibr" target="#b3">(Corso et al., 2020)</ref>.</p><p>Equivariant aggregator. f aggr : ttR d uu ? R d is an equivariant aggregator if and only if f aggr pttT? Widely used SUM and MEAN are equivariant aggregators but MAX/MIN is not. We denote f aggr1 b f aggr2 a new aggregator by combing f aggr1 and f aggr2 with f aggr1 b f aggr2 pXq " rf aggr1 pXq||f aggr2 pXqs, where || denotes concatenation. Lemma 1. (i) For any continuous function g, we have g?f aggr ? f aggr , and when g is injective, f aggr " g?f aggr ; (ii) f aggr1 b f aggr2 ? f aggr1 and f aggr1 b f aggr2 ? f aggr2 . If f aggr1 and f aggr2 are incomparable, f aggr1 b f aggr2 ? f aggr1 and f aggr1 b f aggr2 ? f aggr2 ; (iii) If f aggr is an equivariant aggregator, then f aggr pT?x 1 , T?x 2 ,???, T?x n q ? f aggr px 1 , x 2 ,???, x n q for any T P R m?d and ttx i P R d |i P rnsuu.</p><p>We prove Lemma 1 in Appendix B. Lemma 1 indicates that aggregators become bottlenecks of distinguishing strength. For the equivariant aggregator, any linear transformation before aggregation and any transformation after aggregation have no contribution to the distinguishing strength. For SUM and MEAN, we have gpSUMpT?x 1 , T?x 2 ,???, T?x n qq ? SUMpx 1 , x 2 ,???, x n q and gpMEANpT?x 1 , T?x 2 ,???, T?x n qq ? MEANpx 1 , x 2 ,???, x n q, where T P R m?d , and g can be any continuous function. Based on Lemma 1, we can now compare the distinguishing strength of aggregations in some popular GNNs. GIN-0 sums all hidden features of neighbors at first, and then pass them to a 2-layer MLP. Therefore, when considering in a continuous input features space, the distinguishing strength of GIN-0 is at most as powerful as the SUM aggregator. GCN uses a NORMALIZED MEAN (denoted by nMEAN) aggregator. Given a node v and its neighbors, nMEANpv, u 1 ,???, u n q "</p><formula xml:id="formula_2">1 ? |N pvq|?p hv ? |N pvq|`h u 1 ? |N pu1q|`???`h un ? |N punq|?1</formula><p>q. nMEAN is also an equivariant aggregator, and the distinguishing strength of aggregation in GCN is at most as powerful as nMEAN. GAT corresponds to the weighted SUM aggregation, where the weight coefficients are the functions of hidden features. This makes the distinguishing strength of GAT and SUM incomparable. Based on these observations, a potential approach to breaking the distinguishing strength limitation is to apply a nonlinear processing on inputs before aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BUILDING POWERFUL AGGREGATORS</head><p>In this section, we analyze the requirements for building more powerful aggregators and further injective aggregators. We first introduce a new representation of GNN layers which unifies several popular GNN layers. Given a node v and its neighbors N pvq, our new formulation represents the GNN operation as follows: (2)</p><formula xml:id="formula_3">$ ' &amp; ' % m v " f</formula><p>Here, m v P R |N pvq| is the aggregation coefficients vector of node v. Note that m v should be the mapping of local structures such as node degrees, node or edge features of the k-hop neighbors assigned on node v to ensure the same encoding of isomorphic graphs.h</p><formula xml:id="formula_4">pt?1qT v? " ph pt?1q v , h pt?1q u1 ,???, h pt?1q u | N pvq| q T P R |N pvq|?d is the matrix representation of v's neighbors accord- ing to a permutation ?. f NN : R d ? R d 1</formula><p>is a neural network that extracts task-relevant information from the aggregated representation r ptq v , and is used to update hidden feature h ptq v of node v. According to Equation 2, the aggregation should be with high distinguishing strength to avoid indistinguishability among neighbors. Meanwhile, the extraction should be powerful enough to efficiently extract task-relevant structural patterns from the aggregated representation of neighbors. Based on these observations, we reformulate GCN, GIN0 and GAT with their corresponding threestage representations as follows:</p><formula xml:id="formula_5">GCN : $ ' ' &amp; ' ' % mv " 1 ? |N pvq| p 1 ? |N pvq| , 1 ? |N pu 1 q| ,???, 1 ? |N pu |N pvq|?1 q| q r ptq v " mv?h pt?1qT v? h ptq v " ?pW r ptq v`bq, GIN0 : $ ' &amp; ' % mv " 1 1?|N pvq| r ptq v " mv?h pt?1qT v? h ptq v " MLPpr ptq v q, GAT : $ ' &amp; ' % m ptq v " pattph pt?1q v , h pt?1q v q, attph pt?1q v , h pt?1q u 1 q,???, attph pt?1q v , h pt?1q u |N pvq?1| qq r ptq v " m ptq v?h pt?1qT v? h ptq v " ?pW r ptq v`bq.</formula><p>Their default formulations are given in Appendix A. In the aggregation step, GCN's m v is the mapping of neighbors' degrees; GIN0's m v is the mapping of node v's degree which is equivalent to SUM aggregator; GAT's m v is the mapping of neighbors' features. All of them are the mappings of local structures as given in Equation 2.</p><p>In this three-stage representation, the aggregation is reformulated as the multiplication of the aggregation coefficients vector and the feature matrix of neighbors. It provides insights on improving the distinguishing strength of aggregations. First, we show how to characterize the permutation invariance in this formulation. Let M P R s?n denote an aggregation coefficient matrix where s ? 1. Note that in GCN, GIN and GAT, s is restricted to be 1. h ? P R n?d is the matrix representation of n input elements according to ?. The aggregation computation in the second step of Equation 2 is f aggr pM , h ? q " M P ? h ? " M ? h ? , where P ? is the permutation matrix according to ?. P ? h ? ensures the same output for all h ? , ? P ? |h| . M ? " M P ? is the reordering of columns of M according to ?. For any ? 1 , ? 2 P ? n , f aggr pM , h ?1 q " f aggr pM , h ?2 q, thus permutation invariance holds. Once M is decided, we obtain a unique aggregator denoted by f M . For any sequence of input elements h, f M phq " f aggr pM , h ? q, where ? P ? n can be any ordering of neighbors. Next, we analyze the distinguishing strength of f M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1. For any two matrices</head><formula xml:id="formula_6">M P R s?n and M 1 P R s 1?n with s, s 1 ? n, we have (i) f`M M 1?? f M , where`M M 1?means stacking these two matrices; (ii) f`M M 1?? f M if and only if rankp`M M 1?q ? rankpM q; (iii) Any multiset of size n is distinguishable with f M if and only if rankpM q " n.</formula><p>We prove Proposition 1 in Appendix C. Proposition 1 shows that the distinguishing strength of f M is decided by the rank of the corresponding M . Yet, the distinguishing strength analysis in Proposition 1 is only suitable for multisets aggregated with shared f M . Next, we extend the analysis for the case of different aggregators.</p><p>Let Respf M q denote the set of all outputs of f M . Our proposed three-stage representation also provides useful insight on the constraints among different aggregators. That is, in order to fully distinguish different local structures, for any two different f M1 and f M2 , Respf M1 q X Respf M2 q " H. This is because to fully distinguish different local structures, we should ensure their aggregated representations are different. Since M is restricted to be the mapping of local structures (such as khop neighbors), different M means that the corresponding local structures are different. Therefore, the aggregation results of different f M must be different. However, it is not satisfied by existing GNNs, and there are few studies on distinguishing multisets aggregated by different aggregators. In Proposition 2, we present a detailed analysis of it.</p><formula xml:id="formula_7">Proposition 2. For any M 1 , M 2 P R s?n1 and M 1 1 , M 1 2 P R s 1?n 2 , (i) Respf`M 1 M 1 1?q X Respf`M 2 M 1 2?q ? Respf M1 q X Respf M2 q; (ii) If Respf`M 1 M 1 1?q X Respf`M 2 M 1 2?q ? Respf M1 q X Respf M2 q, then rankp`M 1 M2 M 1 1 M 1 2?q ? rankp`M1 M2?q;</formula><p>We prove Proposition 2 in Appendix D. Proposition 2 shows the necessity of preserving the rank of aggregation coefficient matrix when considering the distinguishing strength among different aggregators. Next, we provide a sufficient condition for building desired multiple injective aggregators with the outputs having no intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3. For any two aggregators</head><formula xml:id="formula_8">f M1 and f M2 with M 1 P R s?n1 and M 2 P R s?n2 , if rankp`M1 M2?q " n 1`n2 , then f M1 and f M2 are injective and Respf M1 q X Respf M2 q " H.</formula><p>We prove Proposition 3 in Appendix E. Proposition 1, 2 and 3 provide a new perspective for building powerful aggregators and even injective aggregators. Compared with the distinguishing strength studies in  and <ref type="bibr" target="#b3">(Corso et al., 2020)</ref>, as well as existing strategies for building injective aggregators, e.g., sum-of-power mapping <ref type="bibr" target="#b43">(Zaheer et al., 2017)</ref> and normalized moments <ref type="bibr" target="#b3">(Corso et al., 2020)</ref>, we reformulate the aggregation with aggregation coefficients matrix and show the relations of the distinguishing strength of aggregators and the rank of the corresponding aggregation coefficients matrices. Besides, the aggregation of this method is controlled by aggregation coefficients which can be learned from graph data to better leverage structural information. In this paper, to simplify the analysis, we only consider the aggregations within one-hop neighbors. The results can be easily extended to more sophisticated aggregators with the overall framework unchanged</p><p>In the perspective of preserving the rank of hidden features among neighbors, r " M h indicates that rankprq ? minprankpM q, rankphqq. To preserve the rank of hidden features in aggregations such that rankprq " rankphq, we need rankpM q ? rankphq. This builds a connection between improving the distinguishing strength of aggregators and preserving the rank of hidden features among neighbors, both of which have the requirements on the rank of M . General aggregators such as ones in GCN, GIN-0 and GAT have rankpM q " 1. Thus, rankprq is always fixed to 1 no matter what the rank of the input features is. Correspondingly, they have a weak distinguishing strength.</p><p>Equation 2 splits the aggregation and feature/structure extraction into two independent steps, which helps to figure out that the expressive power loss happens in the aggregation step, and then the model extracts feature/structure information on the distorted encodings of neighbors. From Equation 2, the aggregation can be considered as a representation regularization step, which unifies different multisets of neighbors into the same representation style while holding permutation invariance. Then, the model can extract structural information on this regulated data with a shared trainable matrix as the third step in Equation 2. Based on this observation, we propose two novel GNN layers: ExpandingConv and CombConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EXPANDINGCONV</head><p>In this section, we first present ExpandingConv framework. Then we provide one of its implementations and analyze how ExpandingConv achieves more powerful aggregations. The ExpandingConv framework is # m </p><formula xml:id="formula_9">P R d?1 .</formula><p>Then a GNN layer f aggr learns structural information on this expanded representations. We introduce an implelentation as follows:</p><formula xml:id="formula_10"># m ptq uv " TanhpW rh pt?1q v ||h pt?1q u s`bq h ptq v " ? uPN pvq MLPpvecpm ptq uv h pt?1qT u qq.</formula><p>(3)</p><p>In Equation <ref type="formula">3</ref>, we implement f local pu, vq as the function of hidden features of nodes u and v. There can be other implementations, and we leave them for future work. W P R s?2d and b P R s?1 are trainable matrices. <ref type="bibr" target="#b20">(Luan et al., 2019)</ref> empirically showed that different nonlinear activatoin functions have different contributions in preserving the rank of matrices. We use the recommended Tanh as the activation function in the computation of m ptq uv to better preserve the rank of aggregation coefficient matrices. MLP denotes a 2-layer perceptron.</p><p>Next, we represent Equation 3 with the corresponding three-stage representation as given in Section 3.2 to obtain its aggregation coefficient matrix and analyze its distinguishing strength. To simplify this process, we only consider 1-layer MLP with W 1 P R d?sd and b 1 P R d?1 . </p><formula xml:id="formula_11">h ptq v " ? uPN pvq ReLUpW 1 vecpm ptq uv h pt?1qT u q`b 1 q " ? uPN pvq ReLUpW 1 r1,:s vecpm ptq uv h pt?1qT u q`b 1 r1s q ? uPN pvq ReLUpW 1 r2,:s vecpm ptq uv h pt?1qT u q`b 1 r2s q . . . ? uPN pvq ReLUpW 1 rd,:s vecpm ptq uv h pt?1qT u q`b 1 rds q? ? ? ? ? ? ? "?? uPN 1 pvq pW 1 r1,:s vecpm ptq uv h pt?1qT u q`b 1 r1s q ? uPN 2 pvq pW 1 r2,:s vecpm ptq uv h pt?1qT u q`b 1 r2s q . . . ? uPN d pvq pW 1 rd,:s vecpm ptq uv h pt?1qT u q`b 1 rds q? ? ? ? ? ? ? "?W 1 r1,:s W 1 r2,:s . . . W 1 rd,:s? ? ? ? ? ? ??v ecp ? uPN 1 pvq m ptq uv h pt?1qT u q vecp ? uPN 2 pvq m ptq uv h pt?1qT u q . . . vecp ? uPN d pvq m ptq uv h pt?1qT u q? ? ? ? ? ? ?`?| N1pvq|?b</formula><formula xml:id="formula_12">? ? ? ? ? ??v ecpM ptq v 1 ? 1h pt?1qT v 1 ? 1 q vecpM ptq v 2 ? 2h pt?1qT v 2 ? 2 q . . . vecpM ptq v d ? dh pt?1qT v d ? d q? ? ? ? ? ? ?`?| N1pvq|?b 1 r1s |N2pvq|?b 1 r2s . . . |N d pvq|?b 1 rds? ? ? ? ? ? ? ,<label>(4)</label></formula><p>where N i pvq ? N pvq|i P rds are sampled subsets of neighbors in each dimension. ,???, h pt?1q u |N i pvq| q P R d?|Nipvq| are aggregation coefficients matrix and hidden feature matrix corresponding to the subset of neighbors N i pvq ? N pvq according to ? i . We denote rh</p><formula xml:id="formula_13">pt?1q v ||h pt?1q vi s " ph pt?1q v ||h pt?1q u1 , h pt?1q v ||h pt?1q u2 ,???, h pt?1q v ||h pt?1q u |N i pvq| q P R 2d?|Nipvq| , then M ptq vi " TanhpW rh pt?1q v ||h pt?1q vi</formula><p>s`bq P R s?|Nipvq| . According to Equation 4, we finally obtain the three-stage representation equivalent to Equation 3.</p><formula xml:id="formula_14">$ ' ' ' ' &amp; ' ' ' ' % M ptq v i " TanhpW rh pt?1q v ||h pt?1q v i s`bq r ptq v i " M ptq v i ? ih pt?1qT v i ? i , i P rds h ptq v " diagpW 1 r1,:s , W 1 r2,:s ,???, W 1 rd,:s qpvecpr ptq v 1 q, vecpr ptq v 2 q,???, vecpr ptq v d qq T p|N1pvq|?b 1 r1s , |N2pvq|?b 1 r2s ,???, |N d pvq|?b 1 rds q T .<label>(5)</label></formula><p>According to the computation ofr ptq vi , rankpr ptq vi q ? minps, d, |N i pvq|q. By configuring a larger s, we have rankpr ptq vi q ? 1 with a high probability, which is different from general GNNs with a rank of 1. As analyzed in Section 3.2, this achieves more powerful aggregators as well as preserves the rank of hidden features among neighbors. The obtainedr ptq vi after aggregation is the unified representations of neighbors. We then use the trainable matrix W 1 P R sd?d to extracts feature/structure information. Unlike the aggregation step, the dimensions reduction here (from sd to d) would not cause information loss. This can be explained by the fact that only task-relevant structural information needs to be preserved and passed to the next layer, and it can be embedded in lower dimensions.</p><p>Comparisons with multi-head GAT. Proposition 4. Multi-head GAT is an implementation of ExpandingConv as follows:</p><formula xml:id="formula_15">$ ' &amp; ' % ? vu " softmaxpLeakyReLUprdiagp? 1T ,? 2T ,???,? KT q ||diagp? 11T ,? 12T ,???,? 1KT qsrW h pt?1q v ||W h pt?1q u sqq h ptq v " ??1 K W ? jPNv vecp? vu h pt?1qT u q?,<label>(6)</label></formula><p>where W " || K k"1 W k P R kd?d is the concatenation of the trainable matrix in all K heads.</p><p>We prove Proposition 4 in Appendix F. Although multi-head GAT is based on attention mechanism, ExpandingConv provides a new perspective to explain its effectiveness. Applying multi-head attention mechanism helps to preserve the rank of hidden features as well as achieve more powerful aggregators. However, the usage of LeakyReLU may be harmful to preserving the rank of the aggregation coefficient matrix <ref type="bibr" target="#b20">(Luan et al., 2019)</ref>.</p><p>GAT as well as most other GNNs (such as GCN, GIN, etc) follows the same pattern that applies nonlinear units after aggregation. According to the analysis in Section 3.1, Equation 3 applies MLP on vecpm ptq uv h pt?1qT u q before SUM to break the distinguishing strength limitation of SUM. It also produces other interesting results. By reformulating Equation 3 with its three-stage representation as Equation 5, each dimension of hidden features aggregates on a subset of neighbors independently, which corresponds to a kind of dimension-wise neighbor sampling mechanism. We call the modification of applying ReLU ahead of SUM aggregator as Re-SUM mechanism. <ref type="bibr" target="#b22">(Mishra et al., 2020)</ref> and <ref type="bibr" target="#b28">(Rong et al., 2019)</ref> studied dropedge and node masking mechanism on node-level predictions, both of which can be considered as neighbor sampling strategies that have shown their effectiveness in improving the generalization ability of aggregation-based GNNs and are also used as unbiased data augmentation technique for training. Compared with dropedge and node masking, Re-SUM realizes a dimension-wise neighbor sampling, and it does not need to manually set the sampling ratio since this mechanism takes effects implicitly. Re-SUM shows that the neural network itself can perform sampling by properly combining nonlinear units and aggregators, without explicitly modifying the network architecture. Our experimental results verified the effectiveness of the Re-SUM on a variety of graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COMBCONV</head><p>The CombConv framework is # m </p><p>where W P R d?2d and b P R d?1 . Similar to ExpandingConv, CombConv also applies Re-SUM aggregation. The difference is that each dimension of hidden features is aggregated with an independent weighted aggregator. ExpandingConv with s " 1 corresponds to a special case of CombConv where all dimensions share the same aggregator. Therefore, the distinguishing strength of Comb-Conv is stronger than ExpandingConv with s " 1. Meanwhile, CombConv does not expand the hidden features of nodes in aggregation. Hence, it requires fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate ExpandingConv and CombConv on graph-level prediction tasks on OGB (Weihua Hu, 2020), TU <ref type="bibr" target="#b14">(Kersting et al., 2016;</ref><ref type="bibr" target="#b41">Yanardag &amp; Vishwanathan, 2015)</ref> and QM9 <ref type="bibr" target="#b27">(Ramakrishnan et al., 2014;</ref><ref type="bibr" target="#b37">Wu et al., 2018;</ref><ref type="bibr" target="#b29">Ruddigkeit et al., 2012)</ref>. The code is available at https://github.com/qslim/epcb-gnns.</p><p>Configurations. We use the default dataset splits for OGB. The QM9 dataset is randomly split into 80% train, 10% validation and 10% test as given in <ref type="bibr" target="#b23">(Morris et al., 2019;</ref><ref type="bibr" target="#b21">Maron et al., 2019)</ref>. For TU dataset, we follow the standard 10-fold cross validation protocol and splits from <ref type="bibr" target="#b44">(Zhang et al., 2018)</ref> and report our results following the protocol described in <ref type="bibr" target="#b42">Ying et al., 2018)</ref>. We use the concatenation of hidden features from all layers to compute the entire graph representations <ref type="bibr" target="#b39">(Xu et al., 2018)</ref>. In our tests, all models are equipped with batch normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy, 2015)</ref> on each hidden layer when evaluating on OGB and TU, and are not when evaluating on QM9. All datasets' descriptions and detailed hyperparameter settings are given in Appendix H.</p><p>We first conduct comprehensive ablation studies to evaluate the effectiveness of powerful aggregators and Re-SUM mechanism on OGB and QM9 as given in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. Then, we compare the performance of ExpandingConv and CombConv with competitive baselines on all three datasets as given in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref> to show their improvements. ExpC-s denotes ExpandingConv with W P R s?? . We use ExpC* and CombC* to denote the ExpandingConv and CombConv without Re-SUM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ABLATION STUDIES</head><p>Effect of powerful aggregators. For complex graph structures with dense connections or with abundant node/edge features, they would benefit from a higher expressive model to maximumly distinguish different structures and extract relevant structural patterns as the model goes deeper to leverage large receptive fields. This is validated on both QM9 and OGB. We configure s " 1, 4, 8, 16, 32 of ExpC-s for all 12 targets of QM9. As we apply a larger s, the model continuously achieves better performance on most targets. We randomly select s " 1, 4 for ogbg-ppa, ogbgmolhiv and s " 1, 5 for ogbg-code. The results show that applying larger s gains performance improvements, especially on ogbg-ppa which involves large graphs with dense connections.</p><p>Effect of Re-SUM mechanism. In <ref type="table" target="#tab_1">Table 1 and Table 2</ref>, the performance differences between ExpC*-1 (CombC*) and ExpC-1 (CombC) show the effectiveness of Re-SUM. In our tests, the Re-SUM can be extremely powerful on graphs with dense connections such as ogbg-ppa, which is validated on both ExpandingConv (with 6.85% improvements) and CombConv (with 4% improvements). On most targets of QM9, this mechanism also gains improvements. For small graphs with sparse connections such as ogbg-hiv and ogbg-molpcba, the improvements are not very significant.   <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref> show the performance comparisons of our models with baselines on QM9, TU and OGB respectively. All datasets in QM9 and OGB graph-level predictions are used for evaluations. For TU, we use 3 widely used datasets: COLLAB includes graphs with dense connections; REDDIT-BINARY (RDT-B) and REDDIT-MULTI-12K (RDT-M12) include large and sparse graphs with one center node having dense connections with other nodes. All results of baselines are taken from the original papers except for the results of GraphSAGE on TU, multi-head GAT on OGB and GIN0* on QM9 which were not reported by the original papers. We report the results of Graph-SAGE provided by <ref type="bibr" target="#b42">(Ying et al., 2018)</ref> and evaluate multi-head GAT and GIN0* by ourselves. To ensure a fair comparison, for OGB and TU, we configure the number of heads in multi-head GAT and s in ExpC-s to be the same which is selected in t3, 4, 5u. For QM9, the number of heads is 8 and s P t4, 8, 16, 32u. GIN0* in QM9 denotes GIN0 without batch normalization.</p><p>Compared with baselines, our models achieve the best performance on 7 out of all 12 targets of QM9, 3 out of all 4 graph-level prediction datasets of OGB and all 3 selected TU datasets. Our models get 1.9% improvements on COLLAB and 2.83% improvements on REDDIT-MULTI-12K compared with SOTA baselines. On ogbg-ppa, our models achieve 2.6% higher classification accuracies compared with SOTA baselines. On ogbg-code, they achieve 1.5% improvements. Multi-head GAT can also be considered as an implementation of ExpandingConv. However, its performance on graph-level predictions is not competitive. According to its three-stage representation, the usage of LeakyReLU in the aggregation step is harmful to preserving the rank, and the usage of softmax makes it harder to analyze the rank. In the extraction step, the 1-layer MLP may have a limited representation power to represent the desired extraction functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We show how basic aggregators used in general GNNs become expressive bottlenecks. To address this limitation, we develop theoretical foundations of building powerful aggregators. We also propose the Re-SUM mechanism which achieves dimension-wise sampling. To evaluate their effectiveness, we develop two novel GNN layers, and conduct extensive experiments on public graph benchmarks. The results are consistent with our analysis, and our proposed models achieve SOTA performance on a variety of graph-level prediction benchmarks.</p><p>Proof. Since M 1 P R s?n1 , M 2 P R s?n2 and rankp`M1 M2?q " n 1`n2 , we have rankpM 1 q " n 1 and rankpM 2 q " n 2 . According to Proposition 1, f M1 and f M2 are injective.</p><p>We build the system of linear equations y " Ax, where x P R n1`n2 , and A " M1 M2?`I n 1 0 0?I n 2?P R s?pn1`n2q . Then, rankpAq " rankp`M1 M2?`I n 1 0 0?I n 2?q " rankp`M1 M2?q " n 1`n2 , which means Ax " 0 has no non-zero solutions. Let x 1 " pxr1s, xr2s,???, xrn 1 sq and x 2 " pxrn 1`1 s, xrn 1`2 s,???, xrn 1`n2 sq such that x "`x 1 x 2?.</p><p>For any x ? 0,</p><formula xml:id="formula_17">Ax " pM 1 M 2 q?I n1 0 0?I n2??x 1 x 2?" M 1 x 1?M 2 x 2 ? 0.</formula><p>Therefore, for any x 1 P R n1 , x 2 P R n2 and x 1 , x 2 ? 0, M 1 x 1 ? M 2 x 2 , hence M 1 pP ? x 1 ? q ? M 2 pP ? x 2 ? q for any P ? x 1 ? and P ? x 2 ? . As a result, Respf M1 q X Respf M2 q " H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F PROOF OF PROPOSITION 4</head><p>Proof. For Multi-head GAT, there are two types of implementations on aggregating each head, concatenation and average. Here, we only consider the average aggregation implementation.</p><formula xml:id="formula_18">h ptq v " ??1 K K ? k"1 ? jPNv ? k vu W k h pt?1q u" ??1 K ? jPNv K ? k"1 W k p? k vu h pt?1q u q" ??1 K ? jPNv p|| K k"1 W k qp|| K k"1 ? k vu h pt?1q u q" ??1 K ? jPNv W vecp? vu h pt?1qT u q" ??1 K W ? jPNv vecp? vu h pt?1qT u q?,</formula><p>where W k P R d?d is the trainable matrix for the k-th head, and W " || K k"1 W k P R kd?d is the concatenation of the trainable matrix in all K heads;</p><formula xml:id="formula_19">? vu "softmaxpLeakyReLU?a 1T rW 1 h pt?1q v ||W 1 h pt?1q u s a 2T rW 2 h pt?1q v ||W 2 h pt?1q u s . . . a KT rW 1 h pt?1q v ||W K h pt?1q u s? ? ? ? ? q. Let? ? " pa ? 1 , a ? 2 ,???, a ? K q and? 1? " pa ? K`1 , a ? K`2 ,???, a ? 2K q. Then, a 1T rW 1 h pt?1q v ||W 1 h pt?1q u s a 2T rW 2 h pt?1q v ||W 2 h pt?1q u s . . . a KT rW 1 h pt?1q v ||W K h pt?1q u s? ? ? ? ? "?r? 1 ||? 11 s T rW 1 h pt?1q v ||W 1 h pt?1q u s r? 2 ||? 12 s T rW 2 h pt?1q v ||W 2 h pt?1q u s . . . r? K ||? 1K s T rW 1 h pt?1q v ||W K h pt?1q u s? ? ? ? ? "?? 1T W 1 h pt?1q v`? 11T W 1 h pt?1q ? a 2T W 2 h pt?1q v`? 12T W 2 h pt?1q u . . . a KT W K h pt?1q v`? 1KT W K h pt?1q u? ? ? ? ? "?? 1T W 1 h pt?1q ? a 2T W 2 h pt?1q v . . . a KT W K h pt?1q v? ? ? ? ?`?? 11T W 1 h pt?1q ? a 12T W 2 h pt?1q u . . . a 1KT W K h pt?1q u? ? ? ? ? "?? 1T? 2T . . .? KT? ? ? ??W 1 W 2 . . . W K? ? ? h pt?1q v`?? 11T? 12T . . .? 1KT? ? ? ??W 1 W 2 . . . W K? ? ? h pt?1q u " diagp? 1T ,? 2T ,???,? KT qW h pt?1q v`d iagp? 11T ,? 12T ,???,? 1KT qW h pt?1q u " rdiagp? 1T ,? 2T ,???,? KT q||diagp? 11T ,? 12T ,???,? 1KT qsrW h pt?1q v ||W h pt?1q u s.</formula><p>Therefore, multi-head GAT is an implementation of ExpandingConv as follows:</p><formula xml:id="formula_20">$ ' &amp; ' % ? vu " softmaxpLeakyReLUprdiagp? 1T ,? 2T ,???,? KT q ||diagp? 11T ,? 12T ,???,? 1KT qsrW h pt?1q v ||W h pt?1q u sqq h ptq v " ??1 K W ? jPNv vecp? vu h pt?1qT u q?.</formula><p>G COMPARISONS WITH MULTI-AGGREGATOR IMPLEMENTATIONS ExpandingConv can also be considered as a kind of multi-aggregator scheme. In Equation 5, each row of M vi can be viewed as a weighted aggregator where the weight coefficients are learned from data. Proposition 1 shows that to obtain higher distinguishing strength by utilizing more aggregators, the weight coefficients of newly added aggregators should be linearly independent to all existing aggregators. The distinguishing strength of weighted aggregators is incomparable with basic aggregators. However, since each row of M vi is equivalent to an independent aggregator, one can simply modify the implementation of f local pu, vq to obtain the variant whose distinguishing strength is strict stronger than basic aggregators as follows:</p><formula xml:id="formula_21">ExpandingConv| m 1ptq uv "rm ptq uv ||1s ? SUM, ExpandingConv| m 1ptq uv "rm ptq uv ||1|| 1 |N pvq| s ? SUM b MEAN.</formula><p>Compared with lerveraging multiple basic aggregators in <ref type="bibr" target="#b3">(Corso et al., 2020)</ref> and <ref type="bibr" target="#b4">(Dehmamy et al., 2019)</ref>, lerveraging weighted aggregator allows for variable numbers of aggregators. Meanwhile, the weighted coefficients are learned from data, which can better capture relevant structural patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H DETAILS OF EXPERIMENTAL SETUP</head><p>Datasets. Benchmark datasets for graph kernels provided by TU <ref type="bibr" target="#b14">(Kersting et al., 2016)</ref> suffer from their small scales of data, making them not sufficient to evaluate the performance of models <ref type="bibr" target="#b6">(Dwivedi et al., 2020)</ref>. Our evaluations are conducted on graph property predictions datasets ogbg-ppa, ogbg-code, ogbg-molhiv in OGB (Weihua Hu, 2020) and QM9 <ref type="bibr" target="#b27">(Ramakrishnan et al., 2014;</ref><ref type="bibr" target="#b37">Wu et al., 2018;</ref><ref type="bibr" target="#b29">Ruddigkeit et al., 2012)</ref> which are large-scale graph datasets including graph classification and graph regression tasks. ogbg-ppa is extracted from the protein-protein association networks with large and densely connected graphs. ogbg-code is a collection of Abstract Syntax Trees (ASTs) obtained from Python method definitions with large and sparse graphs. ogbg-molhiv is molecular property prediction datasets with relative small graphs. QM9 consists 134K small organic molecules with the task to predict 12 targets for each molecule. All data is obtained from pytorch-geometric library .      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ptq uv " f local pu, vq| uPN pvq h ptq v " f aggr pttvecpm ptq uv h pt?1qT u q|u P N pvquuq, where m ptq uv P R s?1 with s ? 1 and f local pu, vq is the mapping of local structures between nodes u and v. The implementation of f local pu, vq is very flexible with the only restriction of ensuring the same encoding of isomorphic graphs. vecpm ptq uv h pt?1qT u q P R sd?1 is the expanded representation of hidden features h pt?1q u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>u</head><label></label><figDesc>|N i pvq| v q P R s?|Nipvq| andh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ptq uv " f local pu, vq| uPN pvq h ptq v " f aggr pttvecpm ptq uv d h pt?1q u q|u P N pvquuq,where m ptq uv P R d?1 and d denotes element-wise product. An implementation of CombConv is given as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Learning curves on ogbg-ppa, ogbg-molhiv, ogbg-molpcba and ogbg-code. Effectiveness of Re-SUM on QM9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Effectiveness of powerful aggregators on QM9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on OGB and QM9. Higher is better.</figDesc><table><row><cell></cell><cell>ogbg-ppa</cell><cell>ogbg-molhiv</cell><cell>ogbg-molpcba</cell><cell>ogbg-code</cell></row><row><cell>ExpC*-1</cell><cell>70.65</cell><cell>77.63</cell><cell>22.65</cell><cell>32.2</cell></row><row><cell>ExpC-1</cell><cell>77.50</cell><cell>76.79</cell><cell>23.39</cell><cell>32.6</cell></row><row><cell>ExpC-3,4,5</cell><cell>80.11</cell><cell>77.89</cell><cell>23.44</cell><cell>33.2</cell></row><row><cell>CombC*</cell><cell>73.61</cell><cell>76.47</cell><cell>23.45</cell><cell>32.29</cell></row><row><cell>CombC</cell><cell>77.64</cell><cell>76.63</cell><cell>23.73</cell><cell>32.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on QM9. Lower is better.</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell>homo</cell><cell>lumo</cell><cell>?</cell><cell>xR 2 y</cell><cell>ZP V E</cell><cell>U0</cell><cell>U</cell><cell>H</cell><cell>G</cell><cell>Cv</cell></row><row><cell>ExpC*-1</cell><cell>0.467</cell><cell>0.283</cell><cell>0.00337</cell><cell>0.00340</cell><cell>0.00467</cell><cell>22.9</cell><cell>0.000205</cell><cell>0.0255</cell><cell>0.0263</cell><cell>0.0242</cell><cell>0.0261</cell><cell>0.1189</cell></row><row><cell>ExpC-1</cell><cell>0.469</cell><cell>0.268</cell><cell>0.00326</cell><cell>0.00329</cell><cell>0.00466</cell><cell>20.8</cell><cell>0.000186</cell><cell>0.0202</cell><cell>0.0199</cell><cell>0.0202</cell><cell>0.0201</cell><cell>0.1039</cell></row><row><cell>ExpC-4</cell><cell>0.413</cell><cell>0.255</cell><cell>0.00273</cell><cell>0.00300</cell><cell>0.00420</cell><cell>19.4</cell><cell>0.000168</cell><cell>0.0184</cell><cell>0.0183</cell><cell>0.0178</cell><cell>0.0182</cell><cell>0.1115</cell></row><row><cell>ExpC-8</cell><cell>0.400</cell><cell>0.257</cell><cell>0.00259</cell><cell>0.00286</cell><cell>0.00395</cell><cell>18.1</cell><cell>0.000172</cell><cell>0.0158</cell><cell>0.0170</cell><cell>0.0177</cell><cell>0.0184</cell><cell>0.1060</cell></row><row><cell>ExpC-16</cell><cell>0.382</cell><cell>0.255</cell><cell>0.00248</cell><cell>0.00268</cell><cell>0.00373</cell><cell>17.2</cell><cell>0.000170</cell><cell>0.0170</cell><cell>0.0174</cell><cell>0.0193</cell><cell>0.0165</cell><cell>0.1043</cell></row><row><cell>ExpC-32</cell><cell>0.368</cell><cell>0.244</cell><cell>0.00248</cell><cell>0.00257</cell><cell>0.00364</cell><cell>16.3</cell><cell>0.000174</cell><cell>0.0151</cell><cell>0.0167</cell><cell>0.0165</cell><cell>0.0198</cell><cell>0.0962</cell></row><row><cell>CombC*</cell><cell>0.4062</cell><cell>0.248</cell><cell>0.00259</cell><cell>0.00273</cell><cell>0.00387</cell><cell>17.1</cell><cell>0.000170</cell><cell>0.0185</cell><cell>0.0181</cell><cell>0.0164</cell><cell>0.0174</cell><cell>0.1022</cell></row><row><cell>CombC</cell><cell>0.399</cell><cell>0.241</cell><cell>0.00261</cell><cell>0.00278</cell><cell>0.00386</cell><cell>15.9</cell><cell>0.000160</cell><cell>0.0144</cell><cell>0.0145</cell><cell>0.0147</cell><cell>0.0140</cell><cell>0.0858</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with baselines on OGB and TU. Higher is better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">OGB</cell><cell></cell><cell></cell><cell>TU</cell><cell></cell></row><row><cell></cell><cell>ogbg-ppa</cell><cell>ogbg-molhiv</cell><cell>ogbg-molpcba</cell><cell>ogbg-code</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M12</cell></row><row><cell>DGK (Yanardag &amp; Vishwanathan, 2015)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>73.09?0.25</cell><cell>78.04?0.39</cell><cell>32.22?0.1</cell></row><row><cell>PSCN (Niepert et al., 2016)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>73.76?0.50</cell><cell>86.30?1.58</cell><cell>41.32?0.42</cell></row><row><cell>AWE (Ivanov &amp; Burnaev, 2018)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>73.93?1.94</cell><cell>87.89?2.53</cell><cell>39.20?2.09</cell></row><row><cell>GCN (Kipf &amp; Welling, 2016)</cell><cell>68.39?0.84</cell><cell>76.06?0.97</cell><cell>20.20?0.24</cell><cell>31.63?0.18</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>GIN (Xu et al., 2019)</cell><cell>68.92?1.0</cell><cell>75.58?1.40</cell><cell>22.66?0.28</cell><cell>31.63?0.20</cell><cell>80.2?1.9</cell><cell>92.4?2.5</cell><cell>NA</cell></row><row><cell>GraphSAG(Hamilton et al., 2017)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>68.25</cell><cell>NA</cell><cell>42.24</cell></row><row><cell>DiffPool (Ying et al., 2018)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>75.48</cell><cell>NA</cell><cell>47.08</cell></row><row><cell>CapsGNN (Xinyi &amp; Chen, 2019)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>79.62?0.91</cell><cell>NA</cell><cell>46.62?1.9</cell></row><row><cell>PPGN (Maron et al., 2019)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>80.16?1.1</cell><cell>NA</cell><cell>NA</cell></row><row><cell>DeeperGCN (Li et al., 2020a)</cell><cell>77.12?0.71</cell><cell>78.58?1.17</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>HIMP (Fey et al., 2020)</cell><cell>NA</cell><cell>78.80?0.82</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>WEGL (Kolouri et al., 2020)</cell><cell>NA</cell><cell>77.57?1.11</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>multi-head GAT(Veli?kovi? et al., 2017)</cell><cell>NA</cell><cell>75.81</cell><cell>20.10</cell><cell>31.10</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>ExpC-s</cell><cell>79.76?0.72</cell><cell>77.99?0.82</cell><cell>23.42?0.29</cell><cell>33.18?0.17</cell><cell>82.10?1.60</cell><cell>92.2?1.87</cell><cell>49.91?1.75</cell></row><row><cell>CombC</cell><cell>77.81?0.76</cell><cell>77.15?1.32</cell><cell>23.63?0.23</cell><cell>32.76?0.15</cell><cell>81.90?1.75</cell><cell>92.5?1.69</cell><cell>49.02?1.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with baselines on QM9. Lower is better.</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell>homo</cell><cell>lumo</cell><cell>?</cell><cell>xR 2 y</cell><cell>ZP V E</cell><cell>U0</cell><cell>U</cell><cell>H</cell><cell>G</cell><cell>Cv</cell></row><row><cell>DTNN (Wu et al., 2018)</cell><cell>0.244</cell><cell>0.95</cell><cell>0.00388</cell><cell>0.00512</cell><cell>0.0112</cell><cell>17</cell><cell>0.00172</cell><cell>2.43</cell><cell>2.43</cell><cell>2.43</cell><cell>2.43</cell><cell>0.27</cell></row><row><cell>MPNN (Gilmer et al., 2017)</cell><cell>0.358</cell><cell>0.89</cell><cell>0.00541</cell><cell>0.00623</cell><cell>0.0066</cell><cell>28.5</cell><cell>0.00216</cell><cell>2.05</cell><cell>2</cell><cell>2.02</cell><cell>2.02</cell><cell>0.42</cell></row><row><cell>k-GNN (Morris et al., 2019)</cell><cell>0.476</cell><cell>0.27</cell><cell>0.00337</cell><cell>0.00351</cell><cell>0.0048</cell><cell>22.9</cell><cell>0.00019</cell><cell>0.0427</cell><cell>0.111</cell><cell>0.0419</cell><cell>0.0469</cell><cell>0.0944</cell></row><row><cell>PPGN (Maron et al., 2019)</cell><cell>0.0934</cell><cell>0.318</cell><cell>0.00174</cell><cell>0.0021</cell><cell>0.0029</cell><cell>3.78</cell><cell>0.000399</cell><cell>0.022</cell><cell>0.0504</cell><cell>0.0294</cell><cell>0.024</cell><cell>0.144</cell></row><row><cell>GIN0* (Xu et al., 2019)</cell><cell>0.471</cell><cell>0.281</cell><cell>0.00327</cell><cell>0.00340</cell><cell>0.00473</cell><cell>22.9</cell><cell>0.000202</cell><cell>0.0244</cell><cell>0.0245</cell><cell>0.0233</cell><cell>0.0255</cell><cell>0.1283</cell></row><row><cell>GAT-s(Veli?kovi? et al., 2017)</cell><cell>0.452</cell><cell>0.286</cell><cell>0.00322</cell><cell>0.00327</cell><cell>0.00460</cell><cell>22.7</cell><cell>0.000228</cell><cell>0.0212</cell><cell>0.0223</cell><cell>0.0223</cell><cell>0.0219</cell><cell>0.1247</cell></row><row><cell>ExpC-s</cell><cell>0.368</cell><cell>0.244</cell><cell>0.00248</cell><cell>0.00257</cell><cell>0.00364</cell><cell>16.3</cell><cell>0.000168</cell><cell>0.0151</cell><cell>0.0167</cell><cell>0.0165</cell><cell>0.0165</cell><cell>0.0962</cell></row><row><cell>CombC</cell><cell>0.399</cell><cell>0.241</cell><cell>0.00261</cell><cell>0.00278</cell><cell>0.00386</cell><cell>15.9</cell><cell>0.000160</cell><cell>0.0144</cell><cell>0.0145</cell><cell>0.0147</cell><cell>0.0140</cell><cell>0.0858</cell></row><row><cell cols="5">4.2 COMPARISONS WITH BASELINES</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter settings for OGB.</figDesc><table><row><cell></cell><cell cols="2">ogbg-ppa</cell><cell cols="2">ogbg-molhiv</cell><cell cols="2">ogbg-molpcba</cell><cell cols="2">ogbg-code</cell></row><row><cell></cell><cell>ExpC*-1, ExpC-s</cell><cell>CombC*, CombC</cell><cell>ExpC*-1, ExpC-s</cell><cell>CombC*, CombC</cell><cell>ExpC*-1, ExpC-s</cell><cell>CombC*, CombC</cell><cell>ExpC*-1, ExpC-s</cell><cell>CombC*, CombC</cell></row><row><cell>batch size</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>64</cell></row><row><cell>layers</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>4</cell></row><row><cell>hidden</cell><cell>256</cell><cell>256</cell><cell>64</cell><cell>64</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>lr</cell><cell>0.0005</cell><cell>0.0002</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>step size</cell><cell>20</cell><cell>20</cell><cell>5</cell><cell>5</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>5</cell></row><row><cell>lr decay</cell><cell>0.8</cell><cell>0.7</cell><cell>0.7</cell><cell>0.7</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell></row><row><cell>dropout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>readout</cell><cell>SUM</cell><cell>SUM</cell><cell>MEAN</cell><cell>MEAN</cell><cell>MEAN</cell><cell>MEAN</cell><cell>MEAN</cell><cell>MEAN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter settings for TU. ExpC-s; hidden = 512 for CombC* and CombC.Table 7gives the individual hyperparameter settings of each model on each target, including the number of layers.</figDesc><table><row><cell></cell><cell cols="2">COLLAB</cell><cell cols="2">REDDIT-BINARY</cell><cell cols="2">REDDIT-MULTI-12K</cell></row><row><cell></cell><cell>ExpC-s</cell><cell>CombC</cell><cell>ExpC-s</cell><cell>CombC</cell><cell>ExpC-s</cell><cell>CombC</cell></row><row><cell>batch size</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell>layers</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>hidden</cell><cell>180</cell><cell>180</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>lr</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>step size</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>lr decay</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>dropout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>readout</cell><cell>SUM</cell><cell>SUM</cell><cell>SUM</cell><cell>SUM</cell><cell>SUM</cell><cell>SUM</cell></row><row><cell cols="7">The shared hyperparameter settings of ExpC*-1, ExpC-s, CombC* and CombC on all 12 targets</cell></row><row><cell cols="7">of QM9: batch sizes = 64; lr = 0.0001; step size = 30; lr decay = 0.85; readout = SUM. hidden =</cell></row><row><cell>256 for ExpC*-1 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Number of layers for QM9.</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell>homo</cell><cell>lumo</cell><cell>?</cell><cell>xR 2 y</cell><cell>ZP V E</cell><cell>U 0</cell><cell>U</cell><cell>H</cell><cell>G</cell><cell>C v</cell></row><row><cell>ExpC*-1,ExpC-s</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>CombC*,CombC</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell cols="5">I MORE EXPERIMENTAL RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">We present more results of ablation studies on OGB and QM9, which demonstrate the effectiveness</cell></row><row><cell cols="5">of ExpandingConv, CombConv and Re-SUM.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, T?x 2 ,???, T?x n uuq " T?f aggr pttx 1 , x 2 ,???, x n uuq for any T P R m?d and ttx i P R d |i P rnsuu.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A GCN, GAT AND GIN Here, we present implementations of GCN, GAT and GIN for the usage of our analysis.</p><p>Graph Convolution Networks (GCN) <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2016)</ref>.</p><p>Graph Attention Networks (GAT) <ref type="bibr" target="#b33">(Veli?kovi? et al., 2017)</ref>.</p><p>Graph Isomorphism Networks (GIN-0) .</p><p>B PROOF OF LEMMA 1</p><p>Proof. (i) For any two multisets x 1 and x 2 , if gpf aggr px 1 qq ? gpf aggr px 2 qq, then f aggr px 1 q ? f aggr px 2 q. Therefore, we have f aggr ? g?f aggr . If g is injective, then f aggr px 1 q ? f aggr px 2 q ? gpf aggr px 1 qq ? gpf aggr px 2 qq. We have f aggr ? g?f aggr ? f aggr , therefore f aggr " g?f aggr .</p><p>(ii) For any two multisets x 1 and</p><p>Therefore, there exist x 3 and x 4 such that rf aggr1 px 3 q||f aggr2 px 3 qs ? rf aggr1 px 4 q||f aggr2 px 4 qs but f aggr1 px 3 q " f aggr1 px 4 q. f aggr1 b f aggr2 ? f aggr1 .</p><p>(iii) Since f aggr is an equivariant aggregator, then f aggr pT?x 1 , T?x 2 ,???, T?x n q " Tf aggr px 1 , x 2 ,???, x n q ? f aggr px 1 , x 2 ,???, x n q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF PROPOSITION 1</head><p>Proof</p><p>then for any x 1 and x 2 , we have</p><p>We prove the claim by contradiction. Assume that rankp`M M 1?q ? rankpM q and f`M</p><p>where ? is the ordering of input elements. Let s " x 1??x2? . For any i P rns, sris " x 1? risx 2? ris P R. Then for any s P R n , m ? s " 0 ?`M M 1?? s " 0. The system of linear equations m ? x " 0 and`M M 1?? x " 0 share the same solution space. Let R S denote the rank of this solution space, then rankpm ? q`R S " rankp`M M 1?? q`R S " n. Therefore, rankpm ? q " rankp`M M 1?? q, then we have rankpM q " rankp`M M 1?q. Since we assumed that rankp`M M 1?q ? rankpM q, we reach a contradiction.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>{flag}</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=mj7WsaHYxj.underreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15413" to="15423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Graph Representation Learning and Beyond (GRL+) Workhop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/ivanov18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09430</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distance encoding-design provably more powerful gnns for structural representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00142</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for square roots of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaw-Ling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multiscale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10945" to="10955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11136</idno>
		<title level="m">Provably powerful graph networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Goossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07524</idno>
		<title level="m">Node masking: Making graph neural networks generalize and scale better</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01900</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Relational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunther</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Building powerful and equivariant graph neural networks with message-passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15107</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<editor>Marinka Zitnik Yuxiao Dong Hongyu Ren Bowen Liu Michele Catasta Jure Leskovec Weihua Hu, Matthias Fey</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
