<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
							<email>andreiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
							<email>egbazavan@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
							<email>hongyixu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<email>wfreeman@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human sensing</term>
					<term>normalizing flows</term>
					<term>semantic alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D human pose and shape estimation is challenging due to the many degrees of freedom of the human body and the difficulty to acquire training data for large-scale supervised learning in complex visual scenes. In this paper we present practical semi-supervised and self-supervised models that support training and good generalization in real-world images and video. Our formulation is based on kinematic latent normalizing flow representations and dynamics, as well as differentiable, semantic body part alignment loss functions that support selfsupervised learning. In extensive experiments using 3D motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, as well as image repositories like COCO, we show that the proposed methods outperform the state of the art, supporting the practical construction of an accurate family of models based on large-scale training with diverse and incompletely labeled image and video data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recovering 3D human pose and shape from monocular RGB images is important for motion and behavioral analysis, robotics, self-driving cars, computer graphics, and the gaming industry. Considerable progress has been made recently in increasing the size of datasets, in the level of detail of human body modeling, and the use of deep learning. A difficulty is the somewhat limited diversity of supervision available in the 3D domain. Many datasets offer 2D human body joint annotations or semantic body part segmentation masks for images collected in the wild, but lack 3D annotations. Motion capture datasets in turn offer large and diverse 3D annotations but their image backgrounds, clothing or body shape variation is not as high. Multi-task models, or models able to learn using limited forms of supervision, represent a potential solution to the current 3D supervision limitations. However, the number of human body shapes and poses observed in images collected in the wild is large, so strong pose, shape priors and expressive loss functions appear necessary in order to make learning feasible. In this paper we address some of these challenges by designing a family of normalizing flow based kinematic priors, together with semantic alignment losses that make large scale weakly and self-supervised learning more accurate and efficient. The introduction and integration of these components, new in the framework of human sensing, with strong results, is one of the main contributions of this work. An evaluation (with ablation studies) on large scale datasets like Human3.6M, COCO, 3DPW, indicates good weakly supervised performance for 3D reconstruction. Our proposed priors and loss functions are amenable to both integration into deep learning losses and to direct non-linear state optimization (refinement) of a model given a random seed or initialization from a learnt predictor. Mindset. Our use of different data sources is practically minded, as we aim towards large scale operation in the wild. Hence we rely on all types of supervision and data sources available. We often start with models trained in the lab, e.g. using Human3.6M and those are supervised. We also use 3D motion capture repositories like CMU in order to construct kinematic (output) priors and that component alone would make our approach semi-supervised. Finally, we make use of large scale predict-and-reproject losses for unlabeled datasets like MS COCO, which makes our approach, at least to an extent self-supervised. Whatever model curriculum used, we aim, long-term, to converge on self-supervised operation. We work with a semi-supervised output prior and model ignition is based on supervision in the lab. By convention, we call this regime weakly-supervised.</p><p>Related Work. There is considerable work in 3D human pose estimation based on 2D keypoints, semantic segmentation of body parts, and 3D joint positions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9]</ref>. More recently, there has been significant interest in 3D human pose and shape estimation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44]</ref>, with some in the form of a reduced parametric model <ref type="bibr" target="#b31">[32]</ref> decoded by 2D predictions, volumetric variants <ref type="bibr" target="#b40">[41]</ref> or direct vertex prediction combined with 3D model fitting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref>. Learning under weak supervision represents the next frontier, considered in this work as well. <ref type="bibr" target="#b44">[45]</ref> learns a discriminator in order to transfer knowledge gained on a 3D dataset to a 2D one. <ref type="bibr" target="#b47">[48]</ref> train a shared representation for both 2D and 3D pose estimation, with a regularizer operating on body segments in order to preserve statistics. <ref type="bibr" target="#b11">[12]</ref> use a discriminator as prior, with adversarial training, and mixes 3D supervision and image labels. <ref type="bibr" target="#b28">[29]</ref> uses segmentations as an intermediate layer, defines a loss on 2D and 3D joints, and rely on rotation matrices instead of angle-axiss representation. <ref type="bibr" target="#b32">[33]</ref> uses a differentiable renderer (OpenDR) to compute a silhouette loss with a limited basin of attraction. This is only used for finetuning the network, but the authors report not having observed significant gains. <ref type="bibr" target="#b39">[40]</ref> rely on a segmentation loss defined on silhouettes, not on the body parts, and rely on multiple views and temporal constraints for learning.</p><p>A variety of methods rely on priors for 3D optimization starting from an initial estimate provided by a neural network and/or by relying on image features like keypoints or silhouettes. <ref type="bibr" target="#b1">[2]</ref> fit a Gaussian mixture model to motion capture data from CMU <ref type="bibr" target="#b10">[11]</ref> and use it during optimization. We will evaluate this prior in our work. SPIN <ref type="bibr" target="#b18">[19]</ref> alternates rounds of training with estimation of new targets using optimization (we will compare in ?3).</p><p>Multiple differentiable rendering models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed recently, in more general settings. Such models are elegant and offer the promise of optimizing photo-realistic losses in the long run. The challenge is in defining an end-to-end model that embeds the difficult assignment problem between the model predictions (rasterized or not) and the image features in ways that are both differentiable and amenable to larger basins of attraction. Our semantic alignment loss is not technically a rendering model, but is differentiable and offers large basins of attraction, supported by explicit, long-range semantic body part correspondences. Gradients can be propagated for points that are not rendered (i.e. points that fail the z-test) and the operation is parallelizable and easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>3D Pose and Shape Representations. We use a statistical body model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref> to represent the pose and the shape of the human body. Given a monocular RGB image, our objective is to infer the pose state variables ? ? R Nj ?3 and shape ? ? R Ns . A posed mesh M(?, ?) has N v associated 3D vertices V = {v i , i = 1 . . . N v }. By dropping dependency on parameters we sometimes denote M(V, k) the subset of vertices associated with body part index k (e.g. torso or head).</p><p>For prediction and optimization tasks we experiment with several kinematic representations. The angle-axis gives good results in connection with deep learning architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref>. The representation consists of a set of N j angle-axis variables ? = {? 1 , ? 2 , . . . , ? Nj }, ? i ? R 3 , where the norm of ? i is the rotation angle in radians and ?i ?i is the unit length 3D axis of rotation. We also explore a new 6D over-parameterization of rotations <ref type="bibr" target="#b48">[49]</ref>, given by the first two columns of the rotation matrix. We test this parameterization in the context of optimization, by building a prior and minimizing a cost function over the compound space of 6D kinematic rotations. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Normalizing Flow-based Representations</head><p>Existing Work on 3D Human Priors. The method of <ref type="bibr" target="#b1">[2]</ref> builds a density model to favor more probable poses over improbable ones. They use a mixture with 8 Gaussian modes N (? j , ? j ), fitted to 1 million CMU poses. During optimization, the prior is evaluated to produce the log-likelihood of the pose. For numerical stability and to avoid excessive averaging effects, an approximation based on choosing the closest mode is used, which is not smooth, and may still lead to instability during mode switching.</p><p>For neural network models, <ref type="bibr" target="#b11">[12]</ref> proposed a factorized adversarial network to learn the admissible rotation manifold of 3D poses, by relying on N j + 1 discriminators, one for each joint, and one for the whole pose. The rotation limits for each joint are expected to be learned implicitly by each of the N j discriminators, while the last one measures the probability of the combined pose. Learning rotation matrices (as opposed to angle-axis based) discriminators, is beneficial in avoiding the non-continuous nature of the angle-axis representation, but trades off increasing representational redundancy and consequently dimensionality.</p><p>Another approach has been pursued by <ref type="bibr" target="#b30">[31]</ref>, where the authors use a variational auto-encoder for 3D poses. The reconstruction loss is the mean per-vertex error between the input posed mesh and the reconstructed one. The latent representation can be used as a prior, by querying the log-likelihood of a given pose. Our experiments with VAEs constructed on top of kinematic representations (joint angles, rotations) showed that those have poor performance compared to our proposed models. The more sophisticated approaches used in VPoser <ref type="bibr" target="#b30">[31]</ref> rely on losses defined on meshes rather than kinematics, but meshes inevitably introduce artefacts due to e.g. imperfect skinning. Moreover, VAEs need to balance two terms -the reconstruction loss and a KL divergence, which leads to a compromise: either the latent space is not close to Gaussian or/and decoding is imperfect. Our normalizing flow approach ensures that reconstruction loss is perfect (by the bijectivity of NFlow's construction) and during training we only optimize against the simpler Gaussian latent space objective. Normalizing Flow Priors. In this paper we propose different normalizing flow-based prior representations, to our knowledge used for the first time in modeling 3D human pose. A normalizing flow <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16</ref>] is a sequence of invertible transformations applied to the original distribution. The end-result is a warped (latent) space with a potentially simple and tractable density function, e.g. z ? N (0; I)). We consider ? ? p * (?) sampled from an unknown distribution. One way to learn it is to use a dataset D (e.g. from CMU or Human3.6M) and maximize data log-likelihood with respect to a parametric model p ? (?)</p><formula xml:id="formula_0">max ? ??D log p ? (?)<label>(1)</label></formula><p>where ? are the parameters of the generative model. If we choose z = f ? (?) where f ? is a component-wise invertible transformation, one can rewrite the log-probability under a change of variables</p><formula xml:id="formula_1">log p ? (?) = log p ? (z) + log |det(dz/d?)| (2) Dropping the subscript ?, if f is the composition of multiple bijections f i , with intermediate output h i , (2) becomes log p ? (?) = log p ? (z) + K i=1 log |det(dh i /dh i?1 )|<label>(3)</label></formula><p>where h 0 = ? and h K = z, and p ? (z) = N (z; 0, I) is chosen as a spherical multivariate Gaussian distribution. State-of-the-art flow architectures are based on auto-regressive versions, such as the Masked Autoregressive Flow (MAF) <ref type="bibr" target="#b29">[30]</ref>, Inverse Autoregressive Flow (IAF) <ref type="bibr" target="#b16">[17]</ref>, NICE <ref type="bibr" target="#b3">[4]</ref>, MADE <ref type="bibr" target="#b6">[7]</ref> or Real-NVP <ref type="bibr" target="#b4">[5]</ref>. In our experiments, we found MAF/IAF/MADE to be too slow given our representation and dataset size, with no measurable improvement over a Real-NVP. A Real-NVP step takes as input a variable x and outputs the transformed variable y, under the following rules</p><formula xml:id="formula_2">y 1:d = x 1:d , y d+1:D = x d+1:D exp s + t,<label>(4)</label></formula><p>where s and t are shift-and-scale vectors that can be modelled as neural network outputs, i.e. (s, t) = NN(x 1:d ), and d is the splitting location of the current Ddimensional variable. The ' ' operator represents the pointwise product, while 'exp' is the exponential function. In order to chain multiple Real-NVP steps, one has to ensure that order is not constant, otherwise the first d-dimensions would not be transformed. Typically, x is permuted before the operation. Because, in our case, ? has moderate although sufficiently large size, we introduce a trainable, fully-connected layer before each NVP step. This is fast and results in better models. We also experiment with a lower capacity model, which replaces the Real-NVP with a simple parametric ReLU, as activation function. We do not use batch normalization. We found that we can trade a bit of accuracy (given by RealNVP) for a standard MLP that is faster and requires less memory. For the same network depth, the Real-NVP variant had 2x the number of parameters, and had marginal performance benefits (2%). More details can be found in the Sup. Mat. For optimization-based inference or neural network training, we can parameterize the problem either in the latent (warped) space, or in the ambient (original) kinematic space, given the exact connection between them. Our empirical studies show that directly predicting (or optimizing) the latent representation always yields better results over working in the ambient space (see <ref type="table">table 1</ref>).</p><p>In <ref type="figure" target="#fig_0">fig. 1</ref> we show a sample pose interpolation in latent space. Optimization. To optimize normalizing flow representations, we assume normalization variable ?, Gaussian variable z = f (?), and ? = f ?1 (z), given that f is bijective. We define the normalizing flow prior as the negative loglikelihood in ambient ? nf (f (?)) = ? log p ? (f (?)) or, equivalently in latent space, ? nf (z) = ? log p ? (z). Then, for any objective function or loss defined as L(?, ?),</p><p>we have either the option of working (i.e. predicting or optimizing) in the ambient space and back-projecting in the latent space at each step arg min</p><formula xml:id="formula_3">?,? L(?, ?) + ? nf (f (?))<label>(5)</label></formula><p>or the option to operate in the latent space directly arg min</p><formula xml:id="formula_4">z,? L(f ?1 (z), ?) + ? nf (z)<label>(6)</label></formula><p>Both approaches are differentiable and we will evaluate them in ?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differentiable Semantic Alignment Loss</head><p>In order to be able to efficiently learn using weak supervision (e.g. just images of people), one needs a measure of prediction quality during the different phases of model training. In this work we explore forms of structured feedback by considering detailed correspondences between the different body part vertices of our 3D human body mesh (projected in the image), and the semantic human body part segmentation produced by another neural network. As presented by <ref type="bibr" target="#b45">[46]</ref>, an Iterated Closest Point (ICP)-style cost for body part alignment can be designed in 2D (for 3D this is quite common e.g. <ref type="bibr" target="#b46">[47]</ref>). Given a set of N b body parts, their semantic image segmentation {S i ? R 2 } and associated mesh vertices of similar type {M(V, k) ? R 3 } (i.e. the 3D vertex set of body part k), a distance between the set of semantic segmentation regions and the 3D mesh vertex projections (using an operator ?) can be defined as the first term of <ref type="bibr" target="#b6">(7)</ref>. This term encourages pixels of a particular semantic body type (e.g. torso, head or left lower arm) to attract projected model vertices with the same body part label. Depending on the sizes of the image regions with particular labels, and the corresponding number of vertices, the minimum of this function is not necessarily achieved only when all vertices are inside the body part. Consequently, we add a complementary loss, encouraging good overlap between model projections and image regions of corresponding semantics</p><formula xml:id="formula_5">L BA (S, V) = N b k=1 p?S k min v?M(V,k) p ? ?(v) + N b k=1 v?M(V,k) min p?S k p ? ?(v)<label>(7)</label></formula><p>We will refer to the two terms as the forward semantic segmentation loss and the backward loss, respectively. Compared to state-of-the-art differentiable rendering techniques like <ref type="bibr" target="#b14">[15]</ref>, this loss has exact gradients, because we express it as an explicit objective connecting semantic image masks and mesh vertex projections. Furthermore, our method is designed for categorical masks and only defined for regions explained by the vertex projections of our model, rather than all the image pixels. The process is naturally parallelizable, and we offer a GPU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Architecture</head><p>Our architecture is based on a multistage deep convolutional neural network to predict human body joints, semantic segmentation of body parts, as well as 3D body pose and shape. The network consists of multiple modules, and has multiple losses, each corresponding to a different prediction task, but it can be run with a subset of the losses under different levels of supervision ranging from full to none. The first module takes as input the image and outputs keypoint (body joint) heatmaps <ref type="bibr" target="#b2">[3]</ref>. We extract the joint positions from the heatmaps and obtain J 2d = {J i , i = 1 . . . N j }. The next module computes semantic body part segmentations by processing images and the keypoint heatmaps obtained by the keypoint prediction module. The outputs are semantic segmentation heatmaps for each body part (see <ref type="figure" target="#fig_1">fig. 2</ref></p><formula xml:id="formula_6">), S = {S i , i = 1 . . . N b }.</formula><p>The last module predicts pose and shape parameters. It takes as input the outputs from previous modules and produces {?, ?}. For the camera, we adopt a perspective projection model. We fix the intrinsics and estimate translation by means of fitting the predicted 3D skeleton to 2D joint detections (that step alone requires a weak perspective approximation, see Sup. Mat.). 3D Pose and Shape. The goal of the 3D pose layers is to predict the pose and shape parameters {?, ?}. The associated network is similar to the ones of the previous two modules. A stack of convolutional stages is created with losses on each stage to reinforce the weights and avoid vanishing gradients <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b2">3]</ref>. The architecture of each 3D regressor stage is composed of a stack of 5 x 2D convolutional layers with 128 feature maps, 7x7 kernels, relu activations, followed by another 2D convolutional module with 128 layers and 1x1 kernels. The last layer is a 2D convolutional layer, has no activation function and the number of heatmaps is equal to the number of predicted parameters. Two separate dense layers are used to output {?, ?}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised and Weakly Supervised Losses</head><p>We train our network by using a combination of fully and weakly supervised losses. The fully supervised training regime assumes complete ground truth on pose, shape. A predicted posed mesh</p><formula xml:id="formula_7">M(?, ?) with N v associated vertices V = {v i , i = 1 . . . N v } has ground truth M( ?, ?) with vertices { v i }.</formula><p>We define the following MSE losses, respectively, on the mesh</p><formula xml:id="formula_8">L V = 1 N v Nv i=1 v i ? v i 2 2<label>(8)</label></formula><p>pose and shape parameters</p><formula xml:id="formula_9">L ? = 1 N j Nj i=1 ? i ? ? i 2 2 , L ? = 1 N s Ns i=1 ? i ? ? i 2 2<label>(9)</label></formula><p>The supervised loss combines previously defined losses</p><formula xml:id="formula_10">L f s = L V + L ? + L ?<label>(10)</label></formula><p>For the weakly supervised case, the predicted mesh M(?, ?) is projected into the image. Denote the projected skeleton joints by J 2d = {J i }, the estimated (or ground truth) 2D joint positions by J 2d = { J i }, and the semantic body part segmentation maps by S = { S i , i = 1 . . . N b }. The weakly supervised regime assumes access to large 3D mocap datasets, e.g. CMU -in order to construct kinematic priors -but without the corresponding images. Additionally we also rely on images in the wild, with only 2D body joint or semantic segmentation maps ground truth. Our weakly-supervised model relies on all practically useful data in order to bootstrap a self-supervised system at later stages. Hence we do not discard 3D data when we have it, and aim to use it to circumvent the missing link: images in the wild with 3D pose and shape ground truth. Then, one can define weakly supervised losses for keypoint alignment:</p><formula xml:id="formula_11">L KA = 1 Nj Nj i=1 J i ? J i 2 2 ,</formula><p>semantic body-part alignment: L BA S, V , and the prior: L ? = ? nf (f (?)) (or ? nf (z) when working in the latent space). The weakly supervised loss is a combination of multiple losses, plus a term that regularizes the shape parameters</p><formula xml:id="formula_12">L ws = L KA + L BA + L ? + ? 2 2<label>(11)</label></formula><p>The total loss will be L total = L f s + L ws . For a graceful transition between supervision regimes, during fully supervised training we use L total , then switch to L ws in the weakly supervised phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We run our fully supervised experiments on the Human80K (H80K) -a representative subset sampled from Human3.6M (H3.6M) <ref type="bibr" target="#b7">[8]</ref>. We also use H80K in order to train pose priors and for optimization experiments. We report errors in the form of MPJPE (mean per-joint position error) and MPVPE (mean per-vertex position error) all in 3D. We split the training set of H80K (composed of ? 54, 000 images) into train, eval and test. As there are no publicly available statistical body model fittings <ref type="figure">Fig. 3</ref>. Reconstruction results of models trained weakly-supervised using COCO (best seen in color). Starting from a network fully supervised on H80K (red), we fine-tune with a weakly-supervised loss (green) and a normalizing flow kinematic prior. Notice considerable improvement in both alignment and the perceptual 3D estimates. Last column shows a different view angle for the WS estimate.</p><p>for H80K data, we had to build them ourselves. Based on the ground truth 3D joint positions J 3D (this is used to retrieve pose ?) and the available 3D subject scans (used to retrieve shape ?) provided with the dataset, we optimize a fitting objective (solved using BFGS). We then project the 3D meshes associated to motion captured body configurations in each frame, to obtain ground truth 2D annotations, J 2d and S 2d . We thus have full supervision on H80K in the form of ?, ?, J 2d , S 2d for each image in the training set.</p><p>We also use CMU <ref type="bibr" target="#b10">[11]</ref> and AMASS <ref type="bibr" target="#b24">[25]</ref> to train 3D pose priors. Both datasets have publicly available kinematic model fittings and we used the ? values to train our normalizing pose model p ? (?). This results in priors over ambient and latent spaces, ? nf (f (?)) and ? nf (z), respectively. Similar models were trained for H80K.</p><p>For weakly supervised learning 'in the wild', we use a subset of 15,000 images from COCO 2014 <ref type="bibr" target="#b21">[22]</ref>. The dataset has no 3D ground truth, but offers 2D annotations for human body joints J 2d , as well semantic segmentation of body parts S 2d . We split the data in 14,000 examples for training and 1,000 for testing, and use it for building the weakly supervised models. We refer to models trained using 2D body joints and semantic body part losses as KA and BA, respectively. Optimization with Different Priors and Losses. In order to analyze the impact of priors and semantic segmentation losses on optimization, we choose H80K where ground-truth is available for all components including 3D camera, pose and shape. We perform non-linear optimization with the objective function as defined in <ref type="formula" target="#formula_0">(11)</ref>, where L ? is changed to accommodate all the various priors, and L KA and L BA are studied both together and independently.</p><p>We evaluate different prior types: i) ? gmm (?) -GMM <ref type="bibr" target="#b1">[2]</ref>, ii) ? nf (f (?)) and ? nf (z) -normalizing flow in ambient and latent space, as given by <ref type="bibr" target="#b4">(5)</ref>, and (6), using either the angle-axis or the 6D representation, iii) ? V P oser (z) -the variational auto-encoder VPoser of <ref type="bibr" target="#b30">[31]</ref>, iv) ? hmr (?) -the discriminator of <ref type="bibr" target="#b11">[12]</ref>.</p><p>We also evaluate different datasets (CMU, H80K, AMASS) for prior construction, different loss functions based on either body joints/keypoints or semantic segmentation of body parts (KA and BA). To directly compare with VPoser, we train a light-weight normalizing flow prior (? 93, 000 parameters compared to ? 344, 000 for VPoser), with the same operating speed, and constructed on the same dataset (AMASS) and train/test splits.</p><p>To isolate confounding factors, optimization is performed using the groundtruth 2D joints (KA) and body part labels (BA), under a perspective projection model, by using the loss defined at <ref type="bibr" target="#b10">(11)</ref>. Optimization relies on BFGS with analytical Jacobians, obtained through automatic differentiation. We start with four different initializations and report the solution with the smaller loss (N.B. this does not require observing the ground truth). We consider four different global rotations, and initialize parameters with 0, for pose (either in ambient or latent space) and shape. We test the model on 500 images and report results in table 1. The best results are achieved by normalizing flow priors when optimization is performed in latent space. By using both keypoint and body part alignment-based self-supervision, the results improve. The 6D rotation representation has a slight edge over the angle-axis. The light-weight normalizing flow trained on AMASS is the best performer, surpassing VPoser even at a third of its capacity. Note that we do not have to balance two terms (the reconstruction loss and the KL-divergence), as normalizing flows support exact latent-variable inference. Additionally, VPoser requires posing meshes during training, whereas normalizing flow models do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error <ref type="formula">(</ref>  <ref type="table">Table 1</ref>. Optimization-based pose and shape estimation experiments with evaluation on the ground truth of H80K dataset. Priors are learned on the training sets of CMU, AMASS or H80K. The HMR discriminator has the largest errors, as it was arguably designed for use with deep neural network losses, and not for model fitting. Optimizing in latent space (using normalizing flows) and semantic alignment always helps. The 6D representation performs slightly better than angle-axis. The best performers are objective functions that include normalizing flow priors trained on H80K or AMASS. VPoser performs slightly worse than our normalizing flow prior, even though it also encodes and decodes 6D rotations. Notation: AA = angle-axis representation, 6D = 6 dimensions rotation representation, RM = rotation matrices, KA = keypoint alignment, BA = body alignment.</p><p>Fully to Weakly Supervised Transfer Learning. We present experiments and ablation studies showing how the weakly supervised training of shape and pose parameters (?, ?) can be successful in conjunction with the proposed normalizing flow priors and self-supervised losses.  <ref type="table">Table 2</ref>. Ablations on H80K, reported as MPJPE/MPVPE metrics in millimeters. Notice the impact of weakly supervised losses (WS), especially in the fully supervised (FS) regime with small training sets, as well as for the model initialized randomly (column two, 0% supervision).</p><p>For this study, we split H80K into two parts where we keep 5 subjects for training (S1, S5, S6, S7 and S8) and two subjects (S9, S11) for testing.</p><p>We further split the training set into partitions of 20%, 40%, 60%, 80%, 100%. We initially train the network fully supervised (FS) on the specific partition of the data using L f s loss. We train the fully supervised model for 30 epochs, then continue in a weakly supervised (WS) regime based on L ws on all the data. In table 2 we report MPJPE/MPVPE for the ablation study. Notice that in all cases weak supervision improves performance whenever additional image data is available.</p><p>We also check that our methodology compares favorably to a similar method HMR <ref type="bibr" target="#b11">[12]</ref> which we retrained on H80K. In this case our model achieves 84mm MPJE whereas HMR has 88mm. <ref type="bibr" target="#b1">2</ref> We were not able to train on their split and retargeting of H3.6M, as their training data was not available. Weakly Supervised Transfer for Images in the Wild. In order to validate our network predictions beyond a motion capture laboratory, 'in the wild', we refined the network on the subset of COCO which has body part labelling available. We started with a network pre-trained on H80K, then continued training on COCO using the complete loss. As ground truth 3D is not available for COCO, we monitor errors between ground truth and estimated 2D projections of the 3D model joints, and the IoU semantic body part alignment metrics. As shown in <ref type="figure" target="#fig_3">fig. 4</ref>, in all cases the pixel error of the projected 2D joints decreased consistently, as a result of weakly supervised fine tuning. A similar trend can be seen for the IoU metric computed for body part alignment, illustrating the importance of a segmentation loss. We explicitly run two configurations, one in which we only use the keypoints alignment (KA) and another based on body part alignment (KA+BA).</p><p>A potentially interesting question is whether the 3D prediction is affected by a self-supervised refinement. We run experiments on 3DPW <ref type="bibr" target="#b25">[26]</ref> which consists of ? 60, 000 images containing one or more humans performing various actions in the wild. The subjects were recorded using IMUs so shape and pose parameters were recovered. We used the training data as supervision, and evaluate on the test set. We report results for a model trained only with full supervision, as well as results of refining the fully supervised (feed-forward) estimate by fur-  <ref type="table">Table 3</ref>. Results on the 3DPW test set for two regimes: static and video. FS is fully supervised, FS+OPT are predictions from FS with optimization. FS+WS are results for self-supervised refinement of the FS model on MS COCO. S stands for smoothing in the video regime. MPJPE is the mean per joint position error, whereas MPJPE-PA is the error after Procrustes alignment. Static: we observe that the self-supervised training did not affect the performance of the 3D predictions. The semantic alignment loss reduces error more than only keypoints alignment. Perceptually, image alignment is also much better for BA than KA, even when it does not immediately produce significant 3D quantitative improvements. Video: the best performer is our FS+WS (KA+BA) model, further optimized over 16 frames with the temporal smoothing term.</p><p>ther optimizing the KA, and KA+BA losses against the predicted 2D outputs (keypoint and body part alignment). After training the network in the weakly supervised regime we obtain better accuracy, showing that 3D prediction quality is preserved. We show the results in table 3. To the best of our knowledge, these are the lowest errors reported so far on the 3DPW test set in a static setting. Temporal optimization. We also experiment in the temporal setting, on batches of 4, 8 and 16 consecutive frames drawn from the 3DPW dataset. Starting from the best results obtained per frame in the static setting, we do a whole batch optimization. Different from the L ws objective, now the shape parameters are tied across frames, with an additional term that enforces smoothness between adjacent temporal pose parameters (in latent space):</p><formula xml:id="formula_13">L smooth = N f t=2 z t ? z t?1 2 2<label>(12)</label></formula><p>The weight for this term is set to be 50? the weight of the prior, as we expect a lower variance for pose dynamics. We compare our method with the recent work of <ref type="bibr" target="#b17">[18]</ref>, showing the results in table 3. As in the static setting, these are also the lowest errors reported so far. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have presented large scale weakly supervised deep learning-based models for 3D human pose and shape estimation from monocular images and video. Key to scalability is unlocking the ability to exploit human statistics implicitly available in large, diverse image repositories, which however do not come with detailed 3D pose or shape supervision. Key to making such approaches feasible, in terms of identifying model parameters with good generalization performance, is the ability to design training losses that are tightly controlled by both the existing prior knowledge on human pose and shape, and by the image and video evidence.</p><p>We introduce latent normalizing flow representations and dynamical models, as well as fully differentiable, structured, semantic body party alignment (reprojection) loss functions which provide informative feedback for self-supervised learning. In extensive, large-scale experiments, using both motion capture datasets like CMU, Human3.6M, AMASS, or 3DPW, as well as 'in the wild' repositories like COCO, we show that our proposed methodology achieves state-of-the-art results in both images and video, supporting the claim that constructing accurate models based on large-scale weak supervision 'in the wild' is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architectures</head><p>We describe the architectures used to construct a normalizing flow prior. We illustrate the case where the input is represented in terms of 6D rotation variables. We assume 23 joints (corresponding to the SMPL kinematic hierarchy), each with 6 dimensions representing each rotation. Hence, the total dimension of the body pose representation is 138. For other rotation representations (e.g. angle-axis, rotation matrix, etc.) the same procedure applies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-capacity version</head><p>We test a low-capacity normalizing flow architecture with the following structure: FC138-PReLU-FC138-PreLU-FC138-PreLU-FC138-PReLU-FC138, with a total of 95, 914 trainable parameters. In comparison, VPoser <ref type="bibr" target="#b30">[31]</ref> uses 344, 190 parameters. Note that in the backward pass from latent to ambient space we do not use matrix inversions -the fully connected layers are applied in a standard way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-NVP version</head><p>We also use a more complex normalizing flow architecture, which replaces the PreLU activation unit with a Real-NVP step. The structure is then FC138-RNVP-FC138-RNVP-FC138-RNVP-FC138-RNVP-FC138-RNVP-FC138, with a total of 331, 462 trainable parameters. For the Real-NVP unit, we use a simple FC128-Tanh-FC128-Tanh-FC69 architecture.</p><p>Training We use a custom TensorFlow implementation for all architectures. In training, the batch size is set to 64, and we use ADAM optimization with an initial learning rate of 1e?4 and an exponential decay rate of 0.99 at every 10, 000 steps. The training is stopped after 200, 000 steps. For the AMASS dataset, this corresponds to ? 4 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation Estimation from 2d Keypoints</head><p>For all of our experiments, we assume a perspective camera model. In this case, one unknown is the global model translation, T, which has to be either predicted or estimated. Unfortunately, predicting a 3d translation directly is difficult with neural networks. In order to circumvent this, we propose the following solution: given a posed mesh M(?, ?), with skeleton joints J 3d = {J 3d i , i = 1 . . . N j }, projected skeleton joints J 2d = {J i , i = 1 . . . N j } and detected 2d joint locations { J i }, we rewrite the keypoint alignment error as:</p><formula xml:id="formula_14">L KA = 1 N j i J i ? J i 2 = 1 N j i ?(J 3d i + T) ? J i 2<label>(13)</label></formula><p>where ? is the perspective projection operator. By relaxing the operator to a weak-perspective one, ? W , we can solve for translation directly, by using least-squares:</p><formula xml:id="formula_15">T * = arg min T 1 N j i ? W (J 3d i + T) ? J i 2 2<label>(14)</label></formula><p>Note that <ref type="formula" target="#formula_0">(14)</ref> is used only to predict the global translation, whereas <ref type="formula" target="#formula_0">(13)</ref> is used afterwards to compute the keypoint alignment loss, based on the estimated T * . Gradients will flow to all the variables of the network, through both layers implementing the above operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Normalizing Flows and VPoser on 3DPW</head><p>In this experiment, we compare our light-version normalizing flow prior (trained on AMASS) with the prior of <ref type="bibr" target="#b30">[31]</ref>, on 500 random images sampled from the 3DPW dataset. In this study, the 2d keypoints and semantic segmentation are predictions from a deep-neural network we trained, and images have groundtruth 3d meshes which permits evaluation. We fit the SMPL model in the same conditions (starting from 4 globally rotated 0-mean latent space kinematic initializations, using both KA and KA+BA losses), for both priors, and report errors in <ref type="figure" target="#fig_4">fig. 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Self-supervised Learning on COCO and OpenImages</head><p>In order to further explore the effect of additional self-supervision to our training process, we extended the set of in-the-wild images with a subset of OpenImages <ref type="bibr" target="#b20">[21]</ref>. OpenImages contains various annotations from which we used the ones related to people (bounding boxes) in various shapes and poses. We kept the images on which the keypoint detector component of our network predicted enough keypoints with high confidence. Using the 2D keypoints and the semantic segmentation predictions from the network we extended the training data with up to 70,000 samples, including the initial COCO data mentioned in the paper. We gradually increased the amount of data from 10% up to 100% used to further train the network and we show results on the 3DPW test set. As can be seen in table 1, the 2D joint error is decreasing and the overlap mIOU metric is increasing showing that with more self supervision the predictions of the network get better. As can be seen in the paper this also leads to better 3D predictions.  <ref type="table">Table 4</ref>. Self supervised experiments on the 3DPW test set using COCO and Open-Images data for additional training. FS identifies the model trained only using full supervision. WS is the model trained weakly supervised. KA and BA denote the keypoint, respectively the body part alignment losses. We gradually increased the amount of self supervised data used for refining the network, which was initially trained fully supervised. We observe that the 2D joint error (measured in pixels) is decreasing as we add more data. As expected, the mIOU metric is increasing-more so in the case where the KA+BA loss is used denoting better alignment. Usually decreases in KA correlate with increases in BA, although this is not always the case-one can expect that a certain lack of calibration between the 2d detected skeletons and the 3d SMPL counterpart, or an aggressive maximization of overlap when clothing makes it difficult to correctly segment body parts, could lead to potential inconsistencies between the trends of the two losses. In practice we find the model image alignment to be much better for BA than for KA, with 3d reconstructions that are perceptually good.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>From left to right: interpolation in latent space for normalizing flow, for two (begin and end) normal random codes. Notice smooth results, plausible human poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>From left to right: Original image, ground truth semantic body part segmentation mask from MSCOCO 2014, predicted segmentation mask, projected semantic mask of our 3D mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>cm) prior, dataset, representation, features MPJPE/MPVPE ?gmm(?), CMU, AA, KA 7.9/10.4 ?gmm(?), CMU, AA, KA + BA 6.9/9.6 ?V P oser (z), AMASS, 6D, KA 4.6/6.7 ? nf (z), AMASS, 6D, KA 4.3/6.0 ? hmr (?), H3.6M, RM, KA 11.9/15.3 ? nf (f (?)), CMU, AA, KA 6.2/8.4 ? nf (f (?)), CMU, AA, KA + BA 6.0/8.1 ? nf (z), CMU, AA, KA 5.0/7.1 ? nf (z), CMU, AA, KA + BA 4.9/6.9 ? nf (f (?)), CMU, 6D, KA 6.1/8.4 ? nf (f (?)), CMU, 6D, KA + BA 5.8/8.0 ? nf (z), CMU, 6D, KA 5.1/6.8 ? nf (z), CMU, 6D, KA + BA 4.8/6.6 ? nf (f (?)), H80K, AA, KA 5.4/7.5 ? nf (z), H80K, AA, KA 4.4/6.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Left and Right: Weakly supervised experiments on COCO with different loss combinations (KA, KA+BA) and different amounts of training data. The baseline is obtained by running the network trained fully supervised on H80K. WS Only is trained only on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Reconstruction errors, MPJPE (left plot) and MPVPE (right), for different priors and loss functions. Notice that normalizing flow priors reduce the reconstruction error in all cases.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We have also considered quaternions, but our experiments showed these to be inferior even to angle-axis (AA), by at least 10%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Based on HMR's Github repository, we identify a total of ?27M trainable parameters. Our model has 6 stages, each with 5 ? 7 ? 7 ? 128 ? 128 parameters resulting in ?24M trainable parameters.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12929" to="12941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manafas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03278</idno>
		<title level="m">Amass: Archive of motion capture as surface shapes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A versatile scene model with differentiable visibility applied to generative pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ghum &amp; ghuml: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6184" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Detailed, accurate, human shape estimation from clothed 3d scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07035</idno>
		<title level="m">On the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
