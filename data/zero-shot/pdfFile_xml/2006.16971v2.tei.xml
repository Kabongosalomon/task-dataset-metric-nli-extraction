<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Eck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen &amp; IMPRS-IS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of T?bingen &amp; IMPRS-IS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many realworld applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to 76.7% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6% mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings. * Equal contribution. ? Equal contribution.; Online version and code: domainadaptation.org/batchnorm 34th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) are known to perform well in the independent and identically distributed (i.i.d.) setting when the test and training data are sampled from the same distribution. However, for many applications this assumption does not hold. In medical imaging, X-ray images or histology slides will differ from the training data if different acquisition systems are being used. In quality assessment, the images might differ from the training data if lighting conditions change or if dirt particles accumulate on the camera. Autonomous cars may face rare weather conditions like sandstorms or big hailstones. While human vision is quite robust to those deviations <ref type="bibr" target="#b0">[1]</ref>, modern machine vision models are often sensitive to such image corruptions.</p><p>We argue that current evaluations of model robustness underestimate performance in many (but not all) real-world scenarios. So far, popular image corruption benchmarks like ImageNet-C [IN-C; 2] focus only on ad hoc scenarios in which the tested model has zero prior knowledge about the corruptions it encounters during test time, even if it encounters the same corruption multiple times. In the example of medical images or quality assurance, the image corruptions do not change from sample to sample but are continuously present over a potentially large number of samples. Similarly, autonomous cars will face the same weather condition over a continuous stream of inputs during the same sand-or hailstorm. These (unlabeled) observations can allow recognition models to adapt to the change in the input distribution.</p><p>Such unsupervised adaptation mechanisms are studied in the field of domain adaptation (DA), which is concerned with adapting models trained on one domain (the source, here clean images) to another for which only unlabeled samples exist (the target, here the corrupted images). Tools and methods from domain adaptation are thus directly applicable to increase model robustness against common corruptions, but so far no results on popular benchmarks have been reported. The overall goal of this work is to encourage stronger interactions between the currently disjoint fields of domain adaptation and robustness towards common corruptions.</p><p>We here focus on one popular technique in DA, namely adapting batch normalization [BN <ref type="bibr">; 3]</ref> statistics <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. In computer vision, BN is a popular technique for speeding up training and is present in almost all current state-of-the-art image recognition models. BN estimates the statistics of activations for the training dataset and uses them to normalize intermediate activations in the network.</p><p>By design, activation statistics obtained during training time do not reflect the statistics of the test distribution when testing in out-of-distribution settings like corrupted images. We investigate and corroborate the hypothesis that high-level distributional shifts from clean to corrupted images largely manifest themselves in a difference of first and second order moments in the internal representations of a deep network, which can be mitigated by adapting BN statistics, i.e. by estimating the BN statistics on the corrupted images. We demonstrate that this simple adaptation alone can greatly increase recognition performance on corrupted images.</p><p>Our contributions can be summarized as follows:</p><p>? We suggest to augment current benchmarks for common corruptions with two additional performance metrics that measure robustness after partial and full unsupervised adaptation to the corrupted images. ? We draw connections to domain adaptation and show that even adapting to a single corrupted sample improves the baseline performance of a ResNet-50 model trained on IN from 76.7% mCE to 71.4%. Robustness increases with more samples for adaptation and converges to a mCE of 62.2%. ? We show that the robustness of a variety of vanilla models trained on ImageNet [IN; <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> substantially increases after adaptation, sometimes approaching the current state-of-the-art performance on IN-C without adaptation. ? Similarly, we show that the robustness of state-of-the-art ResNet-50 models on IN-C consistently increases when adapted statistics are used. We surpass the best non-adapted model (52.3% mCE) by almost 7% points. ? We show results on several popular image datasets and discuss both the generality and limitations of our approach. ? We demonstrate that the performance degradation of a non-adapted model can be well predicted from the Wasserstein distance between the source and target statistics. We propose a simple theoretical model for bounding the Wasserstein distance based on the adaptation parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Measuring robustness against common corruptions</head><p>The ImageNet-C benchmark <ref type="bibr" target="#b1">[2]</ref> consists of 15 test corruptions and four hold-out corruptions which are applied with five different severity levels to the 50 000 test images of the ILSVRC2012 subset of ImageNet <ref type="bibr" target="#b7">[8]</ref>. During evaluation, model responses are assumed to be conditioned only on single samples, and are not allowed to adapt to e.g. a batch of samples from the same corruption. We call this the ad hoc or non-adaptive scenario. The main performance metric on IN-C is the mean corruption error (mCE), which is obtained by normalizing the model's top-1 errors with the top-1 errors of AlexNet <ref type="bibr" target="#b8">[9]</ref> across the C = 15 test corruptions and S = 5 severities (cf. (1)</p><p>Note that mCE reflects only one possible averaging scheme over the IN-C corruption types. We additionally report the overall top-1 accuracies and report results for all individual corruptions in the supplementary material and the project repository.</p><p>In many application scenarios, this ad hoc evaluation is too restrictive. Instead, often many unlabeled samples with similar corruptions are available, which can allow models to adapt to the shifted data distribution. To reflect such scenarios, we propose to also benchmark the robustness of adapted models. To this end, we split the 50 000 validation samples with the same corruption and severity into batches with n samples each and allow the model to condition its responses on the complete batch of images. We then compute mCE and top-1 accuracy in the usual way.</p><p>We consider three scenarios: In the ad hoc scenario, we set n = 1 which is the typically considered setting. In the full adaptation scenario, we set n = 50 000, meaning the model may adapt to the full set of unlabeled samples with the same corruption type before evaluation. In the partial adaptation scenario, we set n = 8 to test how efficiently models can adapt to a relatively small number of unlabeled samples.</p><p>3 Correcting Batch Normalization statistics as a strong baseline for reducing covariate shift induced by common corruptions</p><p>We propose to use a well-known tool from domain adaptation-adapting batch normalization statistics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>-as a simple baseline to increase robustness against image corruptions in the adaptive evaluation scenarios. IN trained models typically make use of batch normalization [BN; 3] for faster convergence and improved stability during training. Within a BN layer, first and second order statistics ? c , ? 2 c of the activation tensors z c are estimated across the spatial dimensions and samples for each feature map c. The activations are then normalized by subtracting the mean ? c and dividing by ? 2 c . During training, ? c and ? 2 c are estimated per batch. During evaluation, ? c and ? 2 c are estimated over the whole training dataset, typically using exponential averaging <ref type="bibr" target="#b9">[10]</ref>.</p><p>Using the BN statistics obtained during training for testing makes the model decisions deterministic but is also problematic if the input distribution changes. If the activation statistics ? c , ? 2 c change for samples from the test domain, then the activations of feature map c are no longer normalized to zero mean and unit variance, breaking a crucial assumption that all downstream layers depend on. Mathematically, this covariate shift 2 can be formalized as follows: Definition 1 (Covariate Shift, cf. <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref>. There exists covariate shift between a source distribution with density p s : X ? Y ? R + and a target distribution with density p t : X ? Y ? R + , written as p s (x, y) = p s (x)p s (y|x) and p t (x, y) = p t (x)p t (y|x), if p s (y|x) = p t (y|x) and p s (x) = p t (x) where y ? Y denotes the class label.</p><p>Removal of covariate shift. If covariate shift (Def. 1) only causes differences in the first and second order moments of the feature activations z = f (x), it can be removed by applying normalization:</p><formula xml:id="formula_0">p f (x) ? E s [f (x)] V s [f (x)] x p s (x) ? p f (x) ? E t [f (x)] V t [f (x)] x p t (x).<label>(2)</label></formula><p>Reducing the covariate shift in models with batch normalization is particularly straightforward: it suffices to estimate the BN statistics ? t , ? 2 t on (unlabeled) samples from the test data available for adaptation. If the number of available samples n is too small, the estimated statistics would be too unreliable. We therefore leverage the statistics ? s , ? 2 s already computed on the training dataset as a prior and infer the test statistics for each test batch as follows,  The hyperparameter N controls the trade-off between source and estimated target statistics and has the intuitive interpretation of a pseudo sample size (p. 117, 14) for samples from the training set. The case N ? ? ignores the test set statistics and is equivalent to the standard ad hoc scenario while N = 0 ignores the training statistics. Supported by empirical and theoretical results (see results section and appendix), we suggest using N ? <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">128]</ref> for practical applications with small n &lt; 32.</p><formula xml:id="formula_1">? = N N + n ? s + n N + n ? t ,? 2 = N N + n ? 2 s + n N + n ? 2 t .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Models. We consider a large range of models (cf. <ref type="table" target="#tab_3">Table 2</ref>, ?B,E) and evaluate pre-trained variants of DenseNet <ref type="bibr" target="#b15">[15]</ref>, GoogLeNet <ref type="bibr" target="#b16">[16]</ref>, Inception and GoogLeNet <ref type="bibr" target="#b17">[17]</ref>, MNASnet <ref type="bibr" target="#b18">[18]</ref>, MobileNet <ref type="bibr" target="#b19">[19]</ref>, ResNet <ref type="bibr" target="#b20">[20]</ref>, ResNeXt <ref type="bibr" target="#b21">[21]</ref>, ShuffleNet <ref type="bibr" target="#b22">[22]</ref>, VGG <ref type="bibr" target="#b23">[23]</ref> and Wide Residual Network [WRN, 24] from the torchvision library <ref type="bibr" target="#b25">[25]</ref>. All models are trained on the ILSVRC2012 subset of IN comprised of 1.2 million images in the training and a total of 1000 classes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. We also consider a ResNeXt-101 variant pre-trained on a 3.5 billion image dataset and then fine-tuned on the IN training set <ref type="bibr" target="#b26">[26]</ref>. We evaluate 3 models from the SimCLRv2 framework <ref type="bibr" target="#b27">[27]</ref>. We additionally evaluate the four leading methods from the ImageNet-C leaderboard, namely Stylized ImageNet training [SIN; 28], adversarial noise training [ANT; 29] as well as a combination of ANT and SIN <ref type="bibr" target="#b29">[29]</ref>, optimized data augmentation using AutoAugment [AugMix; <ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref> and Assemble Net <ref type="bibr" target="#b32">[32]</ref>. For partial adaptation, we choose N ? {2 0 , ? ? ? , 2 10 } and select the optimal value on the holdout corruption mCE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Adaptation boosts robustness of a vanilla trained ResNet-50 model. We consider the pre-trained ResNet-50 architecture from the torchvision library and adapt the running mean and variance on all corruptions and severities of IN-C for different batch sizes. The results are displayed in <ref type="figure" target="#fig_1">Fig. 1</ref> where different line styles of the green lines show the number of pseudo-samples N indicating the influence of the prior given by the training statistics. With N = 16, we see that even adapting to a single sample can suffice to increase robustness, suggesting that even the ad hoc evaluation scenario can benefit from adaptation. If the training statistics are not used as a prior (N = 0), then it takes around 8 samples to surpass the performance of the non-adapted baseline model (76.7% mCE). After around 16 to 32 samples, the performance quickly converges to 62.2% mCE, considerably improving the baseline result. These results highlight the practical applicability of batch norm adaptation in basically all application scenarios, independent of the number of available test samples.</p><p>Adaptation consistently improves corruption robustness across IN trained models. To evaluate the interaction between architecture and BN adaptation, we evaluate all 25 pre-trained models in the torchvision package and visualize the results in <ref type="figure" target="#fig_2">Fig. 2</ref>. All models are evaluated with N = 0 and n = 2000. We group models into different families based on their architecture and observe consistent improvements in mCE for all of these families, typically on the order of 10% points. We observe that in both evaluation modes, DenseNets <ref type="bibr" target="#b15">[15]</ref> exhibit higher corruption robustness despite having a comparable or even smaller number of trainable parameters than ResNets which are usually considered as the relevant baseline architecture. A take-away from this study is thus that model architecture alone plays a significant role for corruption robustness and the ResNet architecture might not be the optimal choice for practical applications.</p><p>Adaptation yields new state of the art on IN-C for robust models. We now investigate if BN adaptation also improves the most robust models on IN-C. The results are displayed in <ref type="table" target="#tab_1">Table 1</ref>. All models are adapted using n = 50 000 (vanilla) or n = 4096 (all other models) and N = 0. The performance of all models is considerably higher whenever the BN statistics are adapted. The DeepAugment+AugMix reaches a new state of the art on IN-C for a ResNet-50 architecture of 45.4% mCE. Evaluating the performance of AugMix over the number of samples for adaptation ( <ref type="figure" target="#fig_1">Fig. 1</ref>, we find that as little as eight samples are sufficient to improve over AssembleNet <ref type="bibr" target="#b32">[32]</ref>, the current state-of-the-art ResNet-50 model on IN-C without adaptation. We have included additional results in ?C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Ablation Studies</head><p>Severity of covariate shift correlates with performance degradation. The relationship between the performance degradation on IN-C and the covariate shift suggests an unsupervised way of estimating the classification performance of a model on a new corruption. Taking the normalized Wasserstein distance (cf. ?A) between the statistics of the source and target domains 3 computed on all samples with the same corruption and severity and averaged across all network layers, we find a correlation with the top-1 error ( <ref type="figure">Fig. 3</ref>  (v) <ref type="figure">Figure 3</ref>: The Wasserstein metric between optimal source (IN) and target (IN-C) statistics correlates well with top-1 errors (i) of non-adapted models on IN-C, (ii) of adapted models on IN-C, indicating that even after reducing covariate shift, the metric is predictive of the remaining source-target mismatch (iii) IN-C adapted models on IN, the reverse case of (i). Holdout corruptions can be used to get a linear estimate on the prediction error of test corruptions (tables). We depict input and downsample (iv) as well as bottlneck layers (v) and notice the largest shift in early and late downsampling layers. The metric is either averaged across layers (i-iii) or across corruptions (iv-v).  regression, the top-1 accuracy of hold-out corruptions can be estimated with around 1-2% absolute mean deviation (cf. ?C.5) within a corruption, and with around 5-15% absolute mean deviation when the estimate is computed on the holdout corruption of each category (see <ref type="figure">Fig. 3</ref>, typically, a systematic offset remains). In <ref type="figure">Fig. 3</ref>(iv-v), we display the Wasserstein distance across individual layers and observe that the covariate shift is particularly present in early and late downsampling layers of the ResNet-50.</p><p>Large scale pre-training alleviates the need for adaptation. Computer vision models based on the ResNeXt architecture <ref type="bibr" target="#b21">[21]</ref> pretrained on a much larger dataset comprised of 3.5 ? 10 9 Instagram images (IG-3.5B) achieve a 45.7% mCE on IN-C <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b37">37]</ref>. We re-evaluate these models with our proposed paradigm and summarize the results in <ref type="table" target="#tab_3">Table 2</ref>. While we see improvements for the small model pre-trained on IN, these improvements vanish once the model is trained on the full IG-3.5B dataset. This observation also holds for the largest model, suggesting that training on very large datasets might alleviate the need for covariate shift adaptation.</p><p>Group Normalization and Fixup Initialization performs better than non-adapted batch norm models, but worse than batch norm with covariate shift adaptation. So far, we considered image classification models with BN layers and concluded that using training dataset statistics in BN generally degrades model performance in out-of-distribution evaluation settings. We now consider models trained without BN and study the impact on corruption robustness, similar to Galloway et al. <ref type="bibr" target="#b38">[38]</ref>.     <ref type="figure" target="#fig_3">Fig. 4</ref>). For ON, the performance increases slightly when computing statistics on more than 64 samples. In <ref type="table" target="#tab_6">Table 4</ref> (first and second column), we observe that the GroupNorm and Fixup models perform better than our BN adaptation scheme: while there is a dataset shift in ON compared to IN, BN adaptation is only helpful for systematic shifts across multiple inputs and this assumption is violated on ON.</p><p>As a control experiment, we sample a dataset "Mixed IN-C" where we shuffle the corruptions and severities. In <ref type="table" target="#tab_6">Table 4</ref> (third and fourth column), we now observe that BN adaptation expectedly no longer improves performance. On IN-R, we achieve better results for the adapted model compared to the non-adapted model as well as the GroupNorm and Fixup models, see <ref type="table" target="#tab_6">Table 4 (last column)</ref>. Additionally, on IN-R, we decrease the top-1 error for a wide range of models through adaptation (see <ref type="table" target="#tab_7">Table 5</ref>). For IN-R, we observe performance improvements for the vanilla trained ResNet50 when using a sample size of larger than 32 samples for calculating the statistics ( <ref type="figure" target="#fig_3">Fig. 4</ref>, right-most plot). A model for correcting covariate shift effects. We evaluate how the batch size for estimating the statistics at test time affects the performance on IN, IN-V2, ON and IN-R in <ref type="figure" target="#fig_3">Fig. 4</ref>. As expected, for IN the adaptation to test time statistics converges to the performance of the train time statistics in the limit of large batch sizes, see <ref type="figure" target="#fig_3">Fig. 4</ref> middle. For IN-V2, we find similar results, see <ref type="figure" target="#fig_3">Fig. 4</ref> left. This observation shows that (i) there is no systematic covariate shift between the IN train set and the IN-V2 validation set that could be corrected by using the correct statistics and (ii) is further evidence for the i.i.d. setting pursued by the authors of IN-V2. In case of ON ( <ref type="figure" target="#fig_3">Fig. 4 right)</ref>, we see slight improvements when using a batch size bigger than 128.</p><formula xml:id="formula_2">0.0 5.0 ? 2 t /? 2 s 0.0 1.0 ?t ? ?s minN L (n = 8) 0.0 5.0 ? 2 t /? 2 s minN U (n = 8)</formula><p>Choosing the number of pseudo-samples N offers an intuitive tradeoff between estimating accurate target statistics (low N ) and relying on the source statistics (large N ). We propose a simple model to investigate optimal choices for N , disregarding all special structure of DNNs, and focusing on the statistical error introduced by estimating? t and? 2 t from a limited number of samples n. To this end, we estimate upper (U ) and lower (L) bounds of the expected squared Wasserstein distance W 2 2 as a function of N and the covariate shift which provides good empirical fits between the estimated W and empirical performance for ResNet-50 for different N <ref type="figure">(Fig. 5</ref>; bottom row). Choosing N such that L or U are minimized ( <ref type="figure">Fig. 5</ref>; example in top row) qualitatively matches the values we find, see ?D for all details. Proposition 1 (Bounds on the expected value of the Wasserstein distance between target and combined estimated target and source statistics). We denote the source statistics as ? s , ? 2 s , the true target statistics as ? t , ? 2 t and the biased estimates of the target statistics as? t ,? 2 t . For normalization, we take a convex combination of the source statistics and estimated target statistics as discussed in Eq. 3. At a confidence level 1 ? ?, the expectation value of the Wasserstein distance W 2 2 (?,?, ? t , ? t ) between ideal and estimated target statistics w.r.t. to the distribution of sample mean? t and sample variance? 2 t is bounded from above and below with</p><formula xml:id="formula_3">L ? E[W 2 2 ] ? U , where L = ? t ? N N + n ? 2 s + n ? 1 N + n ? 2 t 2 + N 2 (N + n) 2 (? t ? ? s ) 2 + n (N + n) 2 ? 2 t U = L + ? 5 t (n ? 1) 2(N + n) 2 N N + n ? 2 s + 1 N + n ? 2 1??/2,n?1 ? 2 t ?3/2</formula><p>The quantity ? 2 1??/2,n?1 denotes the left tail value of a chi square distribution with n ? 1 degrees of freedom, defined as P X ?</p><formula xml:id="formula_4">? 2 1??/2,n?1 = ?/2 for X ? ? 2 n?1 . Proof: See Appendix ?D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The IN-C benchmark <ref type="bibr" target="#b1">[2]</ref> has been extended to MNIST <ref type="bibr" target="#b41">[41]</ref>, several object detection datasets <ref type="bibr" target="#b42">[42]</ref> and image segmentation <ref type="bibr" target="#b43">[43]</ref> reflecting the interest of the robustness community. Most proposals for improving robustness involve special training protocols, requiring time and additional resources. This includes data augmentation like Gaussian noise <ref type="bibr" target="#b44">[44]</ref>, optimized mixtures of data augmentations in conjunction with a consistency loss <ref type="bibr" target="#b30">[30]</ref>, training on stylized images <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45]</ref> or against adversarial noise distributions <ref type="bibr" target="#b29">[29]</ref>. Other approaches tweak the architecture, e.g. by adding shift-equivariance with an anti-aliasing module, <ref type="bibr" target="#b46">[46]</ref> or assemble different training techniques <ref type="bibr" target="#b32">[32]</ref>.</p><p>Unsupervised domain adaptation (DA) is a form of transductive inference where additional information about the test dataset is used to adapt a model to the test distribution. Adapting feature statistics was proposed by Sun et al. <ref type="bibr" target="#b47">[47]</ref> and follow up work evaluated the performance of adapting BN parameters in unsupervised <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and supervised DA settings <ref type="bibr" target="#b3">[4]</ref>. As an application example in medical imaging, Bug et al. <ref type="bibr" target="#b48">[48]</ref> show that adaptive normalization is useful for removing domain shifts on histopathological data. More involved methods for DA include self-supervised domain adaptation on single examples <ref type="bibr" target="#b49">[49]</ref> and pseudo-labeling French et al. <ref type="bibr" target="#b50">[50]</ref>. Xie et al. <ref type="bibr" target="#b51">[51]</ref> achieve the state of the art on IN-C with pseudo-labeling. In work concurrent to ours, Wang et al. <ref type="bibr" target="#b52">[52]</ref> also show BN adaptation results on IN-C. They also perform experiments on CIFAR10-C and CIFAR100-C and explore other domain adaptation techniques.</p><p>Robustness scores obtained by adversarial training can be improved when separate BN or GroupNorm layers are used for clean and adversarial images <ref type="bibr" target="#b53">[53]</ref>. The expressive power of adapting only affine BN parameters BN parameters was shown in multi-task <ref type="bibr" target="#b54">[54]</ref> and DA contexts <ref type="bibr" target="#b3">[4]</ref> and holds even for fine-tuning randomly initialized ResNets <ref type="bibr" target="#b55">[55]</ref>. Concurrent work shows additional evidence that BN adaptation yields increased performance on ImageNet-C <ref type="bibr" target="#b56">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Conclusion</head><p>We showed that reducing covariate shift induced by common image corruptions improves the robustness of computer vision models trained with BN layers, typically by 10-15% points (mCE) on IN-C. Current state-of-the-art models on IN-C can benefit from adaptation, sometimes drastically like AugMix (?14% points mCE). This observation underlines that current benchmark results on IN-C underestimate the corruption robustness that can be reached in many application scenarios where additional (unlabeled) samples are available for adaptation.</p><p>Robustness against common corruptions improves even if models are adapted only to a single sample, suggesting that BN adaptation should always be used whenever we expect machine vision algorithms to encounter out-of-domain samples. Most further improvements can be reaped by adapting to 32 to 64 samples, after which additional improvements are minor.</p><p>Our empirical results suggest that the performance degradation on corrupted images can mostly be explained by the difference in feature-wise first and second order moments. While this might sound trivial, the performance could also degrade because models mostly extract features susceptible to common corruptions <ref type="bibr" target="#b57">[57]</ref>, which could not be fixed without substantially adapting the model weights. The fact that model robustness increases after correcting the BN statistics suggests that the features upon which the models rely on are still present in the corrupted images. The opposite is true in other out-of-domain datasets like IN-A or ObjectNet where our simple adaptation scheme does not substantially improve performance, suggesting that here the main problem is in the features that models have learned to use for prediction.</p><p>Batch Norm itself is not the reason why models are susceptible to common corruptions. While alternatives like Group Normalization and Fixup initialization slightly increase robustness, the adapted BN models are still substantially more robust. This suggests that non-BN models still experience an internal covariate shift on corrupted images, but one that is now absorbed by the model parameters instead of being exposed in the BN layers, making it harder to fix.</p><p>Large-scale pre-training on orders of magnitude more data (like IG-3.5B) can remove the first-and second-order covariate shift between clean and corrupted image samples, at least partially explaining why models trained with weakly supervised training <ref type="bibr" target="#b26">[26]</ref> generalize so well to IN-C.</p><p>Current corruption benchmarks emphasize ad hoc scenarios and thus focus and bias future research efforts on these constraints. Unfortunately, the ad hoc scenario does not accurately reflect the information available in many machine vision applications like classifiers in medical computer vision or visual quality inspection algorithms, which typically encounter a similar corruption continuously and could benefit from adaptation. This work is meant to spark more research in this direction by suggesting two suitable evaluation metrics-which we strongly suggest to include in all future evaluations on IN-C-as well as by highlighting the potential that even a fairly simple adaptation mechanism can have for increasing model robustness. We envision future work to also adopt and evaluate more powerful domain adaptation methods on IN-C and to develop new adaptation methods specifically designed to increase robustness against common corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The primary goal of this paper is to increase the robustness of machine vision models against common corruptions and to spur further progress in this area. Increasing the robustness of machine vision systems can enhance their reliability and safety, which can potentially contribute to a large range of use cases including autonomous driving, manufacturing automation, surveillance systems, health care and others. Each of these uses may have a broad range of societal implications: autonomous driving can increase mobility of the elderly and enhance safety, but could also enable more autonomous weapon systems. Manufacturing automation can increase resource efficiency and reduce costs for goods, but may also increase societal tension through job losses or increase consumption and thus waste. Of particular concern (besides surveillance) is the use of generative vision models for spreading misinformation or for creating an information environment of uncertainty and mistrust.</p><p>We encourage further work to understand the limitations of machine vision models in out-ofdistribution generalization settings. More robust models carry the potential risk of automation bias, i.e., an undue trust in vision models. However, even if models are robust to common corruptions, they might still quickly fail on slightly different Besides analyzing the performance drop when evaluating a model using source statistics on a target dataset, we consider the mismatch in model statistics directly. We first take an ImageNet trained model and adapt it to each of the 95 conditions in IN-C. To obtain a more exact estimate of the true statistics, we split the model into multiple stages with only few BN layers per stage and apply the following simple algorithm 4 :</p><p>? Start with image inputs z 0 n ? x n from the validation set to adapt to, for each n ? [50000]. ? Split the model into multiple stages, h(x) = (f m ? ? ? ? ? f 1 )(x), where each module f i can potentially contain one or multiple BN layers. We denote the number of BN layers in the i-th module as b i .</p><p>? For each stage i ? [m], repeat b i times: z i n ? f i (z i?1 n ) for each n, and update the BN statistics in module f i (z i?1 n ). ? Return h with adapted statistics.</p><p>Using this scheme, we get source statistics ? s and ? s for each layer and ? t and ? t for each layer and corruption. In total, we get 96 different collections of statistics across network layers (for IN and the 95 conditions in IN-C). For simplicity, we will not further index the statistics. Note that all covariance matrices considered here are diagonal, which is a further simplification. We expect that our domain shift estimates could be improved by considering the full covariance matrices.</p><p>In the following, we will introduce three possible distances and divergences which can be applied between source and target statistics to quantify the effect of common corruptions induced covariate shift. We consider the Wasserstein distance, a normalized version of the Wasserstein distance, and the Jeffrey divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Wasserstein distance</head><p>Given a baseline ResNet-50 model with source statistics ? s , ? s on IN, the Wasserstein distance (cf. 58) between the train and test distribution with statistics ? t , ? t is given as</p><formula xml:id="formula_5">W 2 (p s , p t ) 2 = ? s ? ? t 2 2 + tr ? s + ? t ? 2 ? 1/2 t ? s ? 1/2 t 1/2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The source-normalized Wasserstein distance</head><p>When estimated for multiple layers across the network, the Wasserstein distance between source and target depends on the overall magnitude of the statistics. Practically, this means the metric is dominated by features with large magnitude (e.g. in the first layer of a neural network, which receives larger inputs).</p><p>To mitigate this issue, we normalize both statistics with the source statistics and define the normalized Wasserstein distance as</p><formula xml:id="formula_6">W 2 2 = W 2 2 ? ?1/2 s ? s , I, ? ?1/2 s ? t , ? ?1 s ? t (5) = Tr I + ? t ? ?1 s ? 2? 1/2 t ? ?1/2 s + (? t ? ? s ) T ? ?1 s (? t ? ? s ).<label>(6)</label></formula><p>In the uni-variate case, the normalized Wasserstein distance W 2 2 is equal to the Wasserstein distance W 2 2 between source and target statistics divided by ? 2 s :</p><formula xml:id="formula_7">W 2 2 = W 2 2 ? s ? s , 1, ? t ? s , ? 2 t ? 2 s = 1 + ? 2 t ? 2 s ? 2 ? t ? s + (? t ? ? s ) 2 ? 2 s = 1 ? 2 s W 2 2 (? s , ? 2 s , ? t , ? 2 t ).<label>(7)</label></formula><p>A. <ref type="bibr" target="#b2">3</ref> The Jeffrey divergence</p><p>The Jeffrey divergence J(p s , p t ) between source distribution p s and target distribution p t is the symmetrized version of the Kullback-Leibler divergence D KL :</p><formula xml:id="formula_8">J(p s , p t ) = 1 2 (D KL (p s p t ) + D KL (p t p s ))<label>(8)</label></formula><p>The Kullback-Leibler divergence between the D-dimensional multivariate normal source and target distributions is defined as</p><formula xml:id="formula_9">D KL (N t N s ) = 1 2 Tr ? ?1 s ? t + (? s ? ? t ) ? ?1 s (? s ? ? t ) ? D + ln det ? s det ? t .<label>(9)</label></formula><p>The Jeffrey divergence between the D-dimensional multivariate normal source and target distributions then follows as</p><formula xml:id="formula_10">J(N t , N s ) = 1 4 Tr ? ?1 s ? t + Tr ? ?1 t ? s + (? s ? ? t ) ? ?1 s + ? ?1 t (? s ? ? t ) ? 2D .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Summary statistics and quantification of covariate shift between different IN-C conditions</head><p>Given the 95 distances/divergences between the baseline (IN) statistics and 95 IN-C conditions, we first perform a layer-wise analysis of the statistics and depict the results in <ref type="figure" target="#fig_6">Figure 6</ref>. The unnormalized Wasserstein distance is sensitive to the magnitude of the source statistics and hence differs qualitatively from the results on the normalized Wasserstein distance and Jeffrey Divergence. We appreciate that the most notable difference between source and target domains is visible in the ResNet-50 downsampling layers. All three metrics suggest that the shift is mainly present in the first and final layers of the network, supporting the hypothesis that within the common corruption dataset, we have both superficial covariate shift which can be corrected by simple means (such as brightness or contrast variations) in the first layers, and also more "high-level" domain shifts which can only be corrected in the later layers of the network.</p><p>In <ref type="figure" target="#fig_8">Figure 7</ref>, we more closely analyze this relationship for different common corruptions. We can generally appreciate the increased measures as the corruption severity increases.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Notes on the experimental setup B.1 Practical considerations for implementing the method</head><p>Our method is conceptually very easy to implement. We generally recommend to first explore the easier variant of the algorithm where N = 0, i.e., no source statistics are used. As shown in our experiments, this setting works well if 100 or more target samples are available.</p><p>In this case, implementing the method boils down to enabling the training mode for all BN layers across the network. We will discuss this option along with two variants important for application to practical problems: Using exponential moving averaging (EMA) to collect target statistics across multiple batches, and using the source statistics as a prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example implementation in PyTorch and caveats</head><p>We encourage authors of robust models to always evaluate their models, and in particular baseline algorithms on both the train and test set statistics. Implementation in both PyTorch, Tensorflow and other machine learning libraries is straightforward and adds only minimal overhead. For PyTorch, adaptation is possible by simply adding def use_test_statistics(module):</p><p>if isisinstance(module, nn._BatchNorm): module.train() model.eval() model.apply(use_test_statistics) before starting a model evaluation. For the adaptation to a full dataset, we provide a reference implementation with the source code release of this paper. Also, in contrast to the convention of not shuffling examples during test time, make sure to enable dataset shuffling also during test time in order to compute the correct statistics marginalized over class assignment.</p><p>Exponential moving averaging In practice, it might be beneficial to keep track of samples already encountered and use a running mean and variance on the test set to normalize new samples. We can confirm that this technique closely matches the full-dataset adaptation case even when evaluating with batch size 1 and is well suited for settings with less powerful hardware, or in general settings where access to the full batch of samples is not possible. Variants of this technique include the adaptation of the decay factor to discard statistics of samples encountered in the past (e.g. when the data domain slowly drifts over time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Notes on models</head><p>Note that we only re-evaluate existing model checkpoints, and hence do not perform any hyperparameter tuning or adaptations to model training except for selecting the pseudo batchsize N for the source domain. Depending on the batch size and the architecture, model evaluations are done on one to eight Nvidia RTX 2080 GPUs (i.e., using 12 to 96 GB of memory) or up to four Nvidia V100 GPUs (128 GB of memory). Since we merely re-evaluate trained models, it is also possible to work on less powerful hardware with less memory. In these cases, the aggregation of batch normalization statistics has to be done across several batches using a variant of EMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Hyperparameter tuning</head><p>Our method is generally parameter-free if only target statistics should be considered for normalization. This approach is generally preferred for larger batch sizes n and should also be adapted in practice when a sufficient amount of samples is available. For tuning N , we consider the pre-defined holdout corruptions in IN-C, including speckle noise, saturation, Gaussian blur and spatter using a grid search across different values for N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Notes on datasets</head><p>In the main paper, we have used several datasets and provide more relevant information here:</p><p>ImageNet-C (IN-C) For the evaluation on IN-C, we use the JPEG compressed images from github.com/hendrycks/robustness as is advised by the authors to ensure reproducibility. We note that Ford et al. <ref type="bibr" target="#b44">[44]</ref> report a decrease in performance when the compressed JPEG files are used as opposed to applying the corruptions directly in memory without compression artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ObjectNet (ON)</head><p>We find that there are 9 classes with multiple possible mappings from ON to IN (see the list in <ref type="table" target="#tab_12">Table 6</ref>); we discard these classes in our evaluation. Models trained on IN experience a large performance drop on the order of 40-45% when tested on ON. ON is an interesting test case for unsupervised domain adaptation since IN and ON are likely sampled from different distributions. ON intentionally shows objects from new viewpoints on new backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-V2 (IN-V2)</head><p>There are three test sets in IN-V2 that differ in selection frequencies of the MTurk workers. The selection frequency is given by the fraction of MTurk workers who selected an image for its target class. For the "MatchedFrequency" dataset, images were sampled according to the estimated selection frequency of sampling of the original IN validation dataset. For the "Threshold0.7" variant of IN-V2, images were sampled with a selection frequency of at least 0.7. The "TopImages" was sampled from images with the highest selection frequency. Although all three test sets were sampled from the same Flickr candidate pool and were labeled correctly and selected by more than 70% of MTurk workers, the model accuracies on these datasets vary by 14%.  <ref type="bibr" target="#b59">[59]</ref>. After correcting the bias, <ref type="bibr" target="#b59">[59]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Overview of models in torchvision</head><p>In <ref type="table" target="#tab_14">Table 7</ref>, we provide a list of the models we evaluate in the main paper, along with numbers of trainable parameters and BN parameters. Note that the fraction of BN parameters is at most at 1% compared to all trainable parameters in all considered models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Baseline corruption errors</head><p>In <ref type="table">Table 8</ref>, we report the scores used for converting top-1 error into the mean corruption error (mCE) metric proposed by Hendrycks and Dietterich <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Software stack</head><p>We use various open source software packages for our experiments, most notably Docker <ref type="bibr" target="#b60">[60]</ref>, scipy and numpy <ref type="bibr" target="#b61">[61]</ref>, GNU parallel <ref type="bibr" target="#b62">[62]</ref>, Tensorflow <ref type="bibr" target="#b63">[63]</ref>, PyTorch <ref type="bibr" target="#b9">[10]</ref> and torchvision <ref type="bibr" target="#b25">[25]</ref>.   <ref type="table">Table 8</ref>: AlexNet top1 errors on ImageNet-C   C Additional results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Performance of SimCLRv2 models</head><p>We evaluate the performance of 3 models from the SimCLRv2 framework with and without batchnorm adaptation. We test a ResNet50, a ResNet101 and a ResNet152, finetuned on 100% of IN training data. Since our code-base is in PyTorch, we use the Pytorch-SimCLR-Converter <ref type="bibr" target="#b64">[64]</ref> to convert the provided checkpoints from Tensorflow to PyTorch. We notice a slight decline in performance when comparing the top-1 accuracy on the IN validation set, see <ref type="table" target="#tab_15">Table 9</ref>. For preprocessing, we disable the usual PyTorch normalization and use the PIL.Image.BICUBIC interpolation for resizing because this interpolation is used in the TensorFlow code (instead of the default PIL.Image.BILINEAR in PyTorch).</p><p>The BN adaptation results for the converted models are shown in <ref type="table" target="#tab_1">Table 10</ref>. Adaptation improves the performance of the ResNet50 and the ResNet101 model, but hurts the performance of the ResNet152 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Relationship between parameter count and IN-C improvements</head><p>In addition to <ref type="figure">Fig. 3</ref> in the main paper, we show the relationship between parameter count and IN-C mCE. In general, we see that the parameter counts correlates with corruption robustness since larger models have smaller mCE values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Per-corruption results on IN-C</head><p>We provide more detailed results on the individual corruptions of IN-C for the most important models considered in our study in <ref type="figure" target="#fig_11">Fig. 9</ref>. The results are shown for models where the BN parameters are adapted on the full test sets. The adaptation consistently improves the error rates on all corruptions for both vanilla and AugMix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Qualitative analysis of similarities between common corruptions</head><p>In this analysis, we compute a t-SNE embedding of the Wasserstein distances between the adapted models and the non-adapted model from Section 5, <ref type="figure" target="#fig_3">Fig. 4</ref>(i) of the main paper. The results are displayed in <ref type="figure" target="#fig_1">Fig. 10</ref>. We observe that the different corruption categories indicated by the different colors are grouped together except for the 'digital' category (pink). This visualization shows that corruption categories mostly induce similar shifts in the BN parameters. This might be an explanation why training a model on Gaussian noise generalizes so well to other noise types as has been observed by Rusak et al. <ref type="bibr" target="#b29">[29]</ref>: By training on Gaussian noise, the BN statistics are adapted to the Gaussian noise corruption and from <ref type="figure" target="#fig_1">Fig. 10</ref>, we observe that these statistics are similar to the BN statistics of other noises. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Error prediction based on the Wasserstein distance</head><p>In Section 5, <ref type="figure" target="#fig_3">Fig. 4</ref>(i), we observe that the relationship between the Wasserstein distance and the top-1 error on IN-C is strikingly linear in the considered range of the Wasserstein distance. Similar corruptions and corruption types (indicated by color) exhibit similar slope, allowing to approximate the expected top-1 error rate without any information about the test domain itself. Using the split of the 19 corruptions into 15 test and 4 holdout corruptions <ref type="bibr" target="#b1">[2]</ref>, we compute a linear regression model on the five data points we get for each of the holdout corruptions (corresponding to the five severity levels), and use this model to predict the expected top-1 error rates for the remaining corruptions within the corruption family. This scheme works particularly for the "well defined" corruption types such as noise and digital (4.1% points absolute mean deviation from the real error. The full results are depicted in <ref type="table" target="#tab_1">Table 11</ref>.  <ref type="table" target="#tab_1">Table 11</ref>: Estimating top-1 error of unseen corruptions within the different corruption classes. We note that especially for well defined corruptions (like noise or digital corruptions), the estimation scheme works well. We follow the categorization originally proposed by Hendrycks and Dietterich <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Training details on the models trained with Fixup initialization and GroupNorm</head><p>In Section 5 of the main paper, we consider IN models trained with GroupNorm and Fixup initialization. For these models, we consider the original reference implementations provided by the authors. We train ResNet-50, ResNet-101 and ResNet-152 models with stochastic gradient descent with momentum (learning rate 0.1, momentum 0.9), with batch size 256 and weight decay 1 ? 10 ?4 for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Effect of Pseudo Batchsize</head><p>We show the full results for considering different choices of N for ResNet-50, Augmix, ANT, ANT+SIN and SIN models and display the result in <ref type="figure" target="#fig_1">Fig. 12</ref>. We observe a characteristic shape which we believe can be attributed to the way statistics are estimated. We provide evidence for this view by proposing an analytical model which we discuss in ?D. Batch size Pseudo Batch Size Best Pseudo Batchsize N <ref type="figure" target="#fig_1">Figure 11</ref>: Left: Performance for all the considered ResNet-50 variants based on the sample batch size. The optimal N is chosen according to the mCE on the holdout corruptions. Right: Best choice for N depending on the input batchsize n. Note that in general for high values n, the model is generally more robust to the choice of N .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analytical error model</head><p>We consider a univariate model in ?D.1-D.3 and discuss a simple extension to the multivariate diagonal case in ?D.4. As highlighted in the main text, the model qualitatively explains the overall characteristics of our experimental data. Note that we assume a linear relationship between the Wasserstein distance and the error under domain shift, as suggested by our empirical findings.</p><p>Univariate model. We denote the source statistics as ? s , ? 2 s , the true target statistics as ? t , ? 2 t and the estimated target statistics as? t ,? 2 t . For normalization, we take a convex combination of the source statistics and estimated target statistics:</p><formula xml:id="formula_11">? = N N + n ? s + n N + n? t ,? 2 = N N + n ? 2 s + n N + n? 2 t .<label>(11)</label></formula><p>We now analyze the trade-off between using an estimate closer to the source or closer to the estimated target statistics. In the former case, the model will suffer under the covariate shift present between target and source distribution. In the latter case, small batch sizes n will yield unreliable estimates for the true target statistics, which might hurt the performance even more than the source-target mismatch. Hence, we aim to gain understanding in the trade-off between both options, and potential optimal choices of N for a given sample size n.</p><p>As a metric of domain shift with good properties for our following derivation, we leverage the Wasserstein distance. In ?5 and ?C.5, we already established an empirical link between domain shift measured in terms of the top-1 performance vs. the Wasserstein distance between model statistics and observed a linear relationship for case of common corruptions.</p><p>Proposition 1 (Bounds on the expected value of the Wasserstein distance between target and combined estimated target and source statistics). We denote the source statistics as ? s , ? 2 s , the true target statistics as ? t , ? 2 t and the biased estimates of the target statistics as? t ,? 2 t . For normalization, we take a convex combination of the source statistics and estimated target statistics as discussed in Eq. 11. At a confidence level 1 ? ?, the expectation value of the squared Wasserstein distance W 2 2 (?,?, ? t , ? t ) between ideal and estimated target statistics w.r.t. to the distribution of sample mean? t and sample variance? 2 t is bounded from above and below with L ? E[W 2 2 ] ? U , where</p><formula xml:id="formula_12">L = ? t ? N N + n ? 2 s + n ? 1 N + n ? 2 t 2 + N 2 (N + n) 2 (? t ? ? s ) 2 + n (N + n) 2 ? 2 t U = L + ? 5 t (n ? 1) 2(N + n) 2 N N + n ? 2 s + 1 N + n ? 2 1??/2,n?1 ? 2 t ?3/2<label>(12)</label></formula><p>The quantity ? 2 1??/2,n?1 denotes the left tail value of a chi square distribution with n ? 1 degrees of freedom, defined as P X ? ? 2 1??/2,n?1 = ?/2 for X ? ? 2 n?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Proof sketch</head><p>We are interested in the expected value of the Wasserstein distance defined in (A.1) between the target statistics ? t , ? 2 t and the mixed statistics?,? 2 introduced above in equation <ref type="formula" target="#formula_11">(11)</ref>, taken with respect to the distribution of the sample moments? t ,? 2 t . The expectation value itself cannot be evaluated in closed form because the Wasserstein distance contains a term proportional to? being the square root of the convex combination of target and source variance.</p><p>In Lemma 3, the square root term is bounded from above and below using Jensen's inequality and Holder's defect formula which is reviewed in Lemma 2. After having bounded the problematic square root term, the proof of Proposition 1 reduces to inserting the expectation values of sample mean and sample variance reviewed in Lemma 1.</p><p>is bounded from above and below at a confidence level 1 ? ? by</p><formula xml:id="formula_13">E [? 2 ] ? 1 2 M V[? 2 ] ? E ?? 2 ? E [? 2 ]<label>(19)</label></formula><formula xml:id="formula_14">E [? 2 ] = N N + n ? 2 s + n ? 1 N + n ? 2 t ,<label>(20)</label></formula><formula xml:id="formula_15">1 2 M V[? 2 ] = (n ? 1) 4(N + n) 2 ? 4 t N N + n ? 2 s + 1 N + n ? 2 1??/2,n?1 ? 2 t .<label>(21)</label></formula><p>The quantity ? 2 1??/2,n?1 denotes the left tail value of a chi square distribution with n ? 1 degrees of freedom, defined as P X ? ? 2 1??/2,n?1 = ?/2 for X ? ? 2 n?1 .</p><p>Proof. The square root function is concave, therefore Jensen's inequality implies the upper bound</p><formula xml:id="formula_16">E ?? 2 ? E[? 2 ].<label>(22)</label></formula><p>The square root of the expectation value of? 2 is computed using the expectation value of the sample variance as given in Lemma 1.</p><formula xml:id="formula_17">E[? 2 ] = N N + n ? 2 s + n N + n n ? 1 n ? 2 t = N N + n ? 2 s + n ? 1 N + n ? 2 t .<label>(23)</label></formula><p>To state a lower bound, we use Holder's defect formula in probabilistic notation stated in Lemma 2. Holder's formula for concave functions requires that the random variable? 2 can take values in the compact interval [a, b] and that the second derivative of the square root function f (? 2 ) = ?? 2 , exists and is strictly smaller than zero in <ref type="bibr">[a, b]</ref>. Regarding the interval of? 2 , we provide probabilistic upper and lower bounds. The ratio of sample variance and true variance divided by n follows a chi square distribution with n ? 1 degrees of freedom. At confidence level 1 ? ?, this ratio lies between ? 2 1??/2,n?1 and ? 2 ?/2,n?1 which are defined as follows:</p><formula xml:id="formula_18">? 2 1??/2,n?1 ?? 2 t ? 2 t /n ? ? 2 ?/2,n?1 ,<label>(24)</label></formula><formula xml:id="formula_19">P r(X ? ? 2 1??/2,n?1 ) = ? 2 , P r(X ? ? 2 ?/2,n?1 ) = ? 2 .<label>(25)</label></formula><p>Then at the same confidence level, the sample variance itself lies between the two quantiles multiplied by ? 2 t /n,</p><formula xml:id="formula_20">? 2 1??/2,n?1 ? 2 t n ?? 2 t ? ? 2 ?/2,n?1 ? 2 t n ,<label>(26)</label></formula><p>and the random variable? 2 lies in the interval</p><formula xml:id="formula_21">? 2 ? [a, b] with a = N N + n ? 2 s + 1 N + n ? 2 1??/2,n?1 ? 2 t ,<label>(27)</label></formula><formula xml:id="formula_22">and b = N N + n ? 2 s + 1 N + n ? 2 ?/2,n?1 ? 2 t .<label>(28)</label></formula><p>The variances and chi square values are all positive and therefore both a and b are positive as well, implying that the second derivative of the square root is strictly negative in the interval [a, b].</p><formula xml:id="formula_23">f (? 2 ) = ?? 2 , f (? 2 ) = 1 2 (? 2 ) ?1/2 , f (? 2 ) = ? 1 4 (? 2 ) ?3/2 &lt; 0 ? [a, b].<label>(29)</label></formula><p>Consequently the second derivative is in the interval [M, m] at the given confidence level:</p><formula xml:id="formula_24">?M ? f (? 2 ) ? ?m ? 0 for? 2 ? [a, b] with M = 1 4 a ?3/2 , m = 1 4 b ?3/2 .<label>(30)</label></formula><p>The defect formula 2 states that the defect is bounded by</p><formula xml:id="formula_25">E[? 2 ] ? E[ ?? 2 ] ? 1 2 M V[? 2 ].<label>(31)</label></formula><p>The constant M was computed above in <ref type="bibr" target="#b30">(30)</ref>, and the variance of? 2 is calculated in the next lines, using the first and second moment of the sample variance as stated in 1.</p><formula xml:id="formula_26">V[? 2 ] = E[(? 2 ? E[? 2 ]) 2 ] = E n N + n? 2 t ? n N + n n ? 1 n ? 2 t 2 = n 2 (N + n) 2 E ? 2 t ? E[? 2 t 2 = n 2 (N + n) 2 V ? 2 t = n 2 (N + n) 2 2(n ? 1) n 2 ? 4 t = 2(n ? 1) (N + n) 2 ? 4 t .<label>(32)</label></formula><p>Inserting V[? 2 ] computed in <ref type="bibr" target="#b32">(32)</ref> and M defined in (30) with a as defined in <ref type="formula" target="#formula_0">(27)</ref> into the defect formula (31) yields the lower bound: </p><formula xml:id="formula_27">E[? 2 ] ? 1 2 M V[? 2 ] ? E[ ?? 2 ] E[? 2 ] ? 1 2 M V[? 2 ] = E[? 2 ] ? 1 2 ? 1 4 a ?3/2 2(n ? 1) (N + n) 2 ? 4 t = E[? 2 ] ? (n<label>?</label></formula><p>Assuming that source and target variance are of the same order of magnitude ?, the defect will be of order of magnitude ?: The factor V[X] scales with ? 4 and M with ? ?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Proof of Proposition 1</head><p>Proof. For two univariate normal distributions with moments ? t , ? 2 t and?,? 2 , the Wasserstein distance as defined in (A.1) reduces to</p><formula xml:id="formula_29">W 2 2 = ? 2 t +? 2 ? 2?? t + (? ? ?) 2 .<label>(34)</label></formula><p>The expected value of the Wasserstein distance across many batches is given as</p><formula xml:id="formula_30">E[W 2 2 ] = ? 2 t + E[? 2 ] ? 2E[?]? t + E[(? t ??) 2 ] = ? 2 t + N N + n ? 2 s + n N + n n ? 1 n ? 2 t ? 2? t E N N + n ? 2 s + n N + n? 2 t + E ? t ? N N + n ? s ? n N + n? t 2<label>(35)</label></formula><p>which can already serve as the basis for our numerical simulations. To arrive at a closed form analytical solution, we invoke Lemma 3 to bound the expectation value E [?] in equation <ref type="bibr" target="#b35">(35)</ref>.</p><formula xml:id="formula_31">?2? t E [? 2 ] ? ?2? t E ?? 2 ? ?2? t E [? 2 ] ? 2? t ? 1 2 M V[? 2 ]<label>(36)</label></formula><p>Apart from the square root term bounded in equation <ref type="formula" target="#formula_1">(36)</ref> above, the expectation value of the Wasserstein distance can be computed exactly. Hence the bounds on E [?] multiplied by a factor of (?2? 2 t ) coming from equation <ref type="formula" target="#formula_1">(35)</ref> determine lower and upper bounds L and U on the expected value of W 2 2 :</p><formula xml:id="formula_32">L ? E W 2 2 ? U = L + ? t M V[? 2 ]<label>(37)</label></formula><p>In the next lines, the lower bound is calculated: </p><formula xml:id="formula_33">L = ? 2 t + N N + n ? 2 s + n ? 1 N + n ? 2 t ? 2? t E N N + n ? 2 s + n ? 1 N + n ? 2 t + ? t ? N N + n ?</formula><p>Based on choices of the model parameters, the model qualitatively matches our experimental results. We plot different choices in <ref type="figure" target="#fig_1">Fig. 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Extension to multivariate distributions.</head><p>We now derive a multivariate variant that can be fit to data from a DNN. Due to the estimation of running statistics in the network, we have access to a diagonal approximation of the true covariance matrix.</p><p>We denote the diagonal covariance matrices with matrix elements ? 2 i as</p><formula xml:id="formula_35">(? t ) ii = (? 2 t ) i , (? t ) ii = (? 2 t ) i , (? s ) ii = (? 2 s ) i<label>(40)</label></formula><p>and extend our definition of the statistics used for normalization to? and?:</p><formula xml:id="formula_36">? = N N + n ? s + n N + n? t ,? = N N + n ? s + n N + n? t .<label>(41)</label></formula><p>The Wasserstein distance between?,? and ? t , ? t is then defined as</p><formula xml:id="formula_37">W 2 2 = Tr ? t +? ? 2? 1/2 t? 1/2 + (? t ??) T (? t ??) = D i=1 (? 2 t ) i + (? 2 ) i ? 2(?) i (? t ) i + ((? t ) i ? (? t ) i ) 2 = D i=1 (W 2 2 ) i<label>(42)</label></formula><p>Every component (W 2 2 ) i in the sum above is bounded by the univariate bound discussed above. The multivariate Wasserstein distance which sums over the diagonal covariance matrix entries is then bounded by the sums over the individual bounds L i and U i given in <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_38">L i ? (W 2 2 ) i ? U i ? D i=1 L i ? W 2 2 ? D i=1 U i .<label>(43)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Limits of Proposition 1</head><p>Limit n ? ? In the limit of infinite batch size n ? ?, upper and lower bounds on the expected Wasserstein distance between?,? 2 and ? t , ? 2 t both go to zero. (44) The intuition behind this limit is that if a large number of samples from the target domain is given,? and? 2 approximate the true target statistics very well. As? and? 2 dominate? and? 2 for large n, the expected Wasserstein distance has to vanish.</p><p>Limit N ? ? In the opposite limit N ? ?, the expected value of the Wasserstein distance reduces to the Wasserstein distance between source and target statistics. </p><formula xml:id="formula_39">? lim N ?? E[W 2 2 ] = ? 2 t + ? 2 s ? 2? t ? s + (? t ? ? s ) 2 = W 2 2 ? s , ? 2 s , ? t , ? 2 t .<label>(45)</label></formula><p>Limiting case ? t = ? s and ? 2 t = ? 2 s When source and target domain coincide, and the statistics ? 2 s = ? 2 t and ? s = ? t are known, then the source target mismatch is not an error source. However, one might assume that source and target domain are different even though they actually coincide. In this case, proceeding with our proposed strategy and using the statistics? and? 2 , the bounds on the expected Wasserstein distance follow from setting ? 2 t to ? 2 s and ? t to ? s in </p><p>It could also be the case that the equality of source and target statistics is known but the concrete values of the statistics are unknown. In our model, this amounts to setting the number of pseudo samples N to zero and assuming that source and target statistics are equal. Setting N = 0 in equation <ref type="formula" target="#formula_5">(47)</ref>  </p><p>D.6 Bounds on the normalized Wasserstein distance</p><p>The Wasserstein distance (cf. ?A.1) between the interpolating statistics?,? 2 and the target statistics can also be normalized by a factor of ? ?2 s . Because ? ?2 s is constant, the bounds on the expectation value of the unnormalized Wasserstein distance discussed in the previous subsections just have to be multiplied by ? ?2 s to obtain bounds on the normalized Wasserstein distance (cf. ?A.2):</p><formula xml:id="formula_43">L ? 2 s ? W 2 2 = W 2 2 ? ? s , ,? 2 ? 2 s , ? t ? s , ? 2 t ? 2 s = 1 ? 2 s W 2 2 (?,? 2 , ? t , ? 2 t ) ? U ? 2 s .<label>(49)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Sample size vs. performance tradeoff in terms of the mean corruption error (mCE) on IN-C for ResNet-50 and AugMix (AM). Black line corresponds to (non-adapted) ResNet50 stateof-the-art performance of DeepAug+AugMix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Across 25 model architectures in the torchvision library, the baseline mCE (?) improves with adaptation (?), often on the order of 10 points. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Batch size vs. performance trade-off for different natural image datasets with no covariate shift (IN, IN-V2), complex and shuffled covariate shift (ObjectNet), complex and systematic covariate shift (ImageNet-R). Straight black lines show baseline performance (no adaptation). ImageNet plotted for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Figure 5 :</head><label>25</label><figDesc>The bound suggests small optimal N for most parameters (i) and qualitatively explains our empirical observation (ii).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>16 B 24 D 33 E</head><label>162433</label><figDesc>perturbations like surface reflections. Understanding under what conditions model decisions can be deemed reliable or not is still an open research question that deserves further attention. Supplementary Material A Distances and divergences for quantifying domain shift 15 A.1 The Wasserstein distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 The source-normalized Wasserstein distance . . . . . . . . . . . . . . . . . . . 15 A.3 The Jeffrey divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.4 Summary statistics and quantification of covariate shift between different IN-C conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Notes on the experimental setup 19 B.1 Practical considerations for implementing the method . . . . . . . . . . . . . . 19 B.2 Notes on models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3 Hyperparameter tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.4 Notes on datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.5 Overview of models in torchvision . . . . . . . . . . . . . . . . . . . . . . .. 20 B.6 Baseline corruption errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.7 Software stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C Additional results 22 C.1 Performance of SimCLRv2 models . . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 Relationship between parameter count and IN-C improvements . . . . . . . . . 22 C.3 Per-corruption results on IN-C . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.4 Qualitative analysis of similarities between common corruptions . . . . . . . . . 23 C.5 Error prediction based on the Wasserstein distance . . . . . . . . . . . . . . . . 24 C.6 Training details on the models trained with Fixup initialization and GroupNorm . 24 C.7 Effect of Pseudo Batchsize . . . . . . . . . . . . . . . . . . . . . . . . . . . . Analytical error model 27 D.1 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.2 Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.3 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 D.4 Extension to multivariate distributions. . . . . . . . . . . . . . . . . . . . . . . 32 D.5 Limits of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.6 Bounds on the normalized Wasserstein distance . . . . . . . . . . . . . . . . . . Full list of models evaluated on IN 34 E.1 Torchvision models trained on IN . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.2 Robust ResNet50 models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.3 SimCLRv2 models [27] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.4 Robust ResNext models [21] . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.5 ResNet50 with Group Normalization [40] . . . . . . . . . . . . . . . . . . . . . 35 E.6 ResNet50 with Fixup initialization [39] . . . . . . . . . . . . . . . . . . . . . . 35</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :s a tu ra te s h o t n o is e s n o w s p a tt e r s p e c k le n o is e z o o m b lu r</head><label>6</label><figDesc>Wasserstein distance, normalized Wasserstein distance and Jeffrey divergence estimated among source and target statistics between different network layers. We report the respective metric w.r.t. to the difference between baseline (IN) and target (IN-C) statistics and show the value averaged across all corruptions. We note that for a ResNet-50 model, downsampling layers contribute most to the overall error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>g h tn e s s c o n tr a s t d e fo c u s b lu r e la s ti c tr a n s fo rm fo g fr o s t g a u s s ia n b lu r g a u s s ia n n o is e g la s s b lu r im p u ls e n o is e jp e g c o m p re s s io n m o ti o n b lu r p ix e la te s a tu ra te s h o t n o is e s n o w s p a tt e r s p e c k le n o is e z o o m b lu r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Normalized Wasserstein distance and Jeffrey divergence across corruptions and layers in a ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The authors observe a systematic accuracy drop when comparing model performance on the original IN validation set and IN-V2 and attribute it to the distribution gap between their datasets and the original IN dataset. They quantify the distribution gap by how much the change from the original distribution to the new distribution affects the considered model. Engstrom et al. analyze the creation process of IN-V2 and identify statistical bias resulting from noisy readings of the selection frequency statistic as a main source of dropping performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Adaptation (?) improves baseline (?) mCE across all 25 model architectures in the torchvision library, often on the order of 10% points. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>b r i gFigure 9 :</head><label>9</label><figDesc>h t n e s s c o n t r a s t d e f o c u s -b l u r e l a s t i c -t r a n s f o r m f o g f r o s t g a u s s i a n -b l u r g a u s s i a n -n o i s e g l a s s -b l u r i m p u l s e -n o i s e j p e g -c o m p r e s s i o n m o t i o n -b l u r p i x e l a t e s a t u r a t e s h o t -n o i s e s n o w s p a t t e r s p e c k l e -n o i s e z g h t n e s s c o n t r a s t d e f o c u s -b l u r e l a s t i c -t r a n s f o r m f o g f r o s t g a u s s i a n -b l u r g a u s s i a n -n o i s e g l a s s -b l u r i m p u l s e -n o i s e j p e g -c o m p r e s s i o n m o t i o n -b l u r p i x e l a t e s a t u r a t e s h o t -n o i s e s n o w s p a t t e r s p e c k l e -n o i s e z Results on the individual corruptions of IN-C for the vanilla trained ResNet-50 and the AugMix model with and without adaptation. Adaptation reduces the error on all corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10</head><label>10</label><figDesc>: t-SNE embeddings of the Wasserstein distances between BN statistics adapted on the different corruptions. This plot shows evidence on the similarities between different corruption types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 t? 3 / 2 .</head><label>232</label><figDesc>/2,n?1 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>s 2 ? 2 2 t ( 38 )= L + ? t 1 4 N 2 t? 3 / 2 (n ? 1 ) 2 (</head><label>22238423212</label><figDesc>n) 2 V[? t ] + (E[? t ]) n) 2 (? t ? ? s ) 2 +n (N + n) 2 ? After having derived the lower bound, the upper bound is the sum of the lower bound and the defect term as computed in Lemma 3.E[W 2 ] ? U = L + ? t M V[? 2 ] /2,n?1 ? N + n) 2 ? 5 t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>n) 2 (? t ? ? s ) 2 + n (N + n) 2 ? 2 t =(? t ? ? t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>L = ? 2 t2N 2 + 2 ? 2</head><label>2222</label><figDesc>4N n ? N + 2n 2 (N + n)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>and keeping n finite yields L = 2? 2 t 1</head><label>21</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>30 000 images with various artistic renditions of 200 classes of the original IN dataset. Additional information on the used models and datasets can be found in ?B. For IN, we resize all images to 256 ? 256px and take the center 224 ? 224px crop. For IN-C, images are already cropped. We also center and re-scale the color values with ? RGB = [0.485, 0.456, 0.406] and ? = [0.229, 0.224, 0.225].</figDesc><table /><note>Datasets. ImageNet-C [IN-C; 2] is comprised of corrupted versions of the 50 000 images in the IN validation set. The dataset offers five severities per corruption type, for a total of 15 "test" and 4 "holdout" corruptions. ImageNet-A [IN-A; 33] consists of unmodified real-world images which yield chance level classification performance in IN trained ResNet-50 models. ImageNet-V2 [IN- V2; 34] aims to mimic the test distribution of IN, with slight differences in image selection strategies. ObjectNet [ON; 35] is a test set containing 50 000 images like IN organized in 313 object classes with 109 unambiguously overlapping IN classes. ImageNet-R [IN-R; 36] contains</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Adaptation improves mCE (lower is better) and Top1 accuracy (higher is better) on IN-C for different models and surpasses the previous state of the art without adaptation. We consider n = 8 for partial adaptation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">IN-C mCE ( )</cell><cell></cell><cell></cell><cell cols="2">Top1 accuracy ( )</cell><cell></cell></row><row><cell></cell><cell>w/o</cell><cell>partial</cell><cell>full</cell><cell></cell><cell>w/o</cell><cell>partial</cell><cell>full</cell><cell></cell></row><row><cell>Model</cell><cell>adapt</cell><cell>adapt</cell><cell>adapt</cell><cell>?</cell><cell>adapt</cell><cell>adapt</cell><cell>adapt</cell><cell>?</cell></row><row><cell>Vanilla ResNet-50</cell><cell>76.7</cell><cell>65.0</cell><cell>62.2</cell><cell>(?14.5)</cell><cell>39.2</cell><cell>48.6</cell><cell>50.7</cell><cell>(+11.5)</cell></row><row><cell>SIN [28]</cell><cell>69.3</cell><cell>61.5</cell><cell>59.5</cell><cell>(?9.8)</cell><cell>45.2</cell><cell>51.6</cell><cell>53.1</cell><cell>(+7.9)</cell></row><row><cell>ANT [29]</cell><cell>63.4</cell><cell>56.1</cell><cell>53.6</cell><cell>(?9.8)</cell><cell>50.4</cell><cell>56.1</cell><cell>58.0</cell><cell>(+7.6)</cell></row><row><cell>ANT+SIN [29]</cell><cell>60.7</cell><cell>55.3</cell><cell>53.6</cell><cell>(?7.0)</cell><cell>52.6</cell><cell>56.8</cell><cell>58.0</cell><cell>(+5.4)</cell></row><row><cell>AugMix [AM; 30]</cell><cell>65.3</cell><cell>55.4</cell><cell>51.0</cell><cell>(?14.3)</cell><cell>48.3</cell><cell>56.3</cell><cell>59.8</cell><cell>(+11.4)</cell></row><row><cell>Assemble Net [32]</cell><cell>52.3</cell><cell>-</cell><cell>50.1</cell><cell>(?1.2)</cell><cell>59.2</cell><cell>-</cell><cell>60.8</cell><cell>(+1.5)</cell></row><row><cell>DeepAug [36]</cell><cell>60.4</cell><cell>52.3</cell><cell>49.4</cell><cell>(?10.9)</cell><cell>52.6</cell><cell>59.0</cell><cell>61.2</cell><cell>(+8.6)</cell></row><row><cell>DeepAug+AM [36]</cell><cell>53.6</cell><cell>48.4</cell><cell>45.4</cell><cell>(?8.2)</cell><cell>58.1</cell><cell>62.2</cell><cell>64.5</cell><cell>(+6.4)</cell></row><row><cell>DeepAug+AM+RNXt101 [36]</cell><cell>44.5</cell><cell>40.7</cell><cell>38.0</cell><cell>(?6.6)</cell><cell>65.2</cell><cell>68.2</cell><cell>70.3</cell><cell>(+5.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Improvements from adapting the</cell></row><row><cell cols="3">BN parameters vanish for models trained</cell></row><row><cell cols="3">with weakly supervised pre-training.</cell></row><row><cell></cell><cell cols="2">IN-C mCE ( )</cell></row><row><cell>ResNeXt101</cell><cell>BN</cell><cell>BN+adapt</cell></row><row><cell>32x8d, IN</cell><cell cols="2">66.6 56.7 (?9.9)</cell></row><row><cell>32x8d, IG-3.5B</cell><cell cols="2">51.7 51.6 (?0.1)</cell></row><row><cell cols="3">32x48d, IG-3.5B 45.7 47.3 (+1.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Fixup and GN trained models perform better</cell></row><row><cell cols="4">than non-adapted BN models but worse than adapted</cell></row><row><cell>BN models.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">IN-C mCE ( )</cell></row><row><cell>Model</cell><cell>Fixup GN</cell><cell cols="2">BN BN+adapt</cell></row><row><cell>ResNet-50</cell><cell cols="2">72.0 72.4 76.7</cell><cell>62.2</cell></row><row><cell cols="3">ResNet-101 68.2 67.6 69.0</cell><cell>59.1</cell></row><row><cell cols="3">ResNet-152 67.6 65.4 69.3</cell><cell>58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>GN and Fixup achieve the best results on ObjectNet (ON). After shuffling IN-C corruptions, BN adaptation does no longer decrease the error. Adaptation improves the performance of a vanilla ResNet50 on IN-R.</figDesc><table /><note>ON Mixed IN-C IN-R ResNet50 top-1 top-5 top-1 top-5 top-1 BN w/o adapt 78.2 60.9 61.1 40.8 63.8 BN w/ adapt 76.0 58.9 60.9 40.3 59.9 GroupNorm 70.8 49.8 57.3 36.0 61.2 Fixup 71.5 51.4 56.8 35.4 65.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">base adapt</cell><cell>?</cell></row><row><cell>ResNet50</cell><cell>63.8</cell><cell>59.9</cell><cell>-3.9</cell></row><row><cell>SIN</cell><cell>58.6</cell><cell>54.2</cell><cell>-4.4</cell></row><row><cell>ANT</cell><cell>61.0</cell><cell>58.0</cell><cell>-3.0</cell></row><row><cell>ANT+SIN</cell><cell>53.8</cell><cell>52.0</cell><cell>-1.8</cell></row><row><cell>AugMix (AM)</cell><cell>59.0</cell><cell>55.8</cell><cell>-3.2</cell></row><row><cell>DeepAug (DAug)</cell><cell>57.8</cell><cell>52.5</cell><cell>-5.3</cell></row><row><cell>DAug+AM</cell><cell>53.2</cell><cell>48.9</cell><cell>-4.3</cell></row><row><cell cols="2">DAug+AM+RNXt101 47.9</cell><cell>44.0</cell><cell>-3.9</cell></row></table><note>Adaptation improves the performance (top-1 error) of robust models on IN-R (n=2048).First, using Fixup initialization [39] alleviates the need for BN layers. We train a ResNet-50 model on IN for 100 epochs to obtain a top-1 error of 24.2% and top-5 error of 7.6% (compared to 27.6% reported by Zhang et al. [39] with shorter training, and the 23.9% obtained by our ResNet-50 baseline trained with BN). The model obtains an IN-C mCE of 72.0% compared to 76.7% mCE of the vanilla ResNet-50 model and 62.2% mCE of our adapted ResNet-50 model (cf. Table 3). Additionally, we train a ResNet-101 and a ResNet-152 with Fixup initialization with similar results. Second, GroupNorm [GN; 40] has been proposed as a batch-size independent normalization technique. We train a ResNet-50, a ResNet-101 and a ResNet-152 architecture for 100 epochs and evaluate them on IN-C and find results very similar to Fixup. Results on other datasets: IN-A, IN-V2, ObjectNet, IN-R We use N = 0 and vary n in all ablation studies in this subsection. The technique does not work for the case of "natural adversarial examples" of IN-A [33] and the error rate stays above 99%, suggesting that the covariate shift introduced in IN-A by design is more severe compared to the covariate shift of IN-C and can not be corrected by merely calculating the correct BN statistics. We are not able to increase performance neither on IN nor on IN-V2, since in these datasets, no domain shift is present by design (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>find that the accuracy drop between IN and IN-V2 measures only 3.6% ? 1.5% of the original 11.7% ? 1.0%.</figDesc><table><row><cell>ON class</cell><cell>IN classes</cell></row><row><cell>wheel</cell><cell>wheel; paddlewheel, paddle wheel</cell></row><row><cell>helmet</cell><cell>football helmet; crash helmet</cell></row><row><cell>chair</cell><cell>barber chair; folding chair; rocking chair, rocker</cell></row><row><cell cols="2">still_camera Polaroid camera, Polaroid Land camera; reflex camera</cell></row><row><cell cols="2">alarm_clock analog clock; digital clock</cell></row><row><cell>tie</cell><cell>bow tie, bow-tie, bowtie; Windsor tie</cell></row><row><cell>pen</cell><cell>ballpoint, ballpoint pen, ballpen, Biro; quill, quill pen; fountain pen</cell></row><row><cell>bicycle</cell><cell>mountain bike, all-terrain bike, off-roader; bicycle-built-for-two, tandem bicycle, tandem</cell></row><row><cell>skirt</cell><cell>hoopskirt, crinoline; miniskirt, mini; overskirt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Mapping between 9 ambiguous ON classes and the possible correspondences in IN. Different IN classes are separated with a semicolon.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Overview of different models with parameter counts. We show the total number of BN parameters, which is a sum of affine parameters.</figDesc><table><row><cell>Category</cell><cell>Corruption</cell><cell>top1 error</cell></row><row><cell></cell><cell>Gaussian Noise</cell><cell>0.886428</cell></row><row><cell>Noise</cell><cell>Shot Noise</cell><cell>0.894468</cell></row><row><cell></cell><cell>Impulse Noise</cell><cell>0.922640</cell></row><row><cell></cell><cell>Defocus Blur</cell><cell>0.819880</cell></row><row><cell>Blur</cell><cell>Glass Blur Motion Blur</cell><cell>0.826268 0.785948</cell></row><row><cell></cell><cell>Zoom Blur</cell><cell>0.798360</cell></row><row><cell></cell><cell>Snow</cell><cell>0.866816</cell></row><row><cell>Weather</cell><cell>Frost Fog</cell><cell>0.826572 0.819324</cell></row><row><cell></cell><cell>Brightness</cell><cell>0.564592</cell></row><row><cell></cell><cell>Contrast</cell><cell>0.853204</cell></row><row><cell></cell><cell>Elastic Transform</cell><cell>0.646056</cell></row><row><cell>Digital</cell><cell cols="2">Pixelate JPEG Compression 0.606500 0.717840</cell></row><row><cell>Hold-out Noise</cell><cell>Speckle Noise</cell><cell>0.845388</cell></row><row><cell>Hold-out Digital</cell><cell>Saturate</cell><cell>0.658248</cell></row><row><cell>Hold-out Blur</cell><cell>Gaussian Blur</cell><cell>0.787108</cell></row><row><cell cols="2">Hold-out Weather Spatter</cell><cell>0.717512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>After converting the checkpoints from TensorFlow to Pytorch, we notice a slight degradation in performance on the IN val set.</figDesc><table><row><cell cols="2">IN val top-1 accuracy in %.</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">TF PyTorch</cell></row><row><cell>SimCLRv2 ResNet50</cell><cell>76.3</cell><cell>75.6</cell></row><row><cell cols="2">SimCLRv2 ResNet101 78.2</cell><cell>77.5</cell></row><row><cell cols="2">SimCLRv2 ResNet152 79.3</cell><cell>78.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Adaptation improves the performance of the ResNet50 and the ResNet101 model but hurts the performance of the ResNet152 model.</figDesc><table><row><cell cols="2">ImageNet-C (n=4096), mCE.</cell><cell></cell><cell></cell></row><row><cell>Model, adaptation:</cell><cell cols="2">base adapt</cell><cell>?</cell></row><row><cell>SimCLRv2 ResNet50</cell><cell>72.4</cell><cell>68.0</cell><cell>-4.2</cell></row><row><cell cols="2">SimCLRv2 ResNet101 66.6</cell><cell>65.1</cell><cell>-0.9</cell></row><row><cell cols="2">SimCLRv2 ResNet152 63.7</cell><cell>64.2</cell><cell>+0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Effects of batch size n and pseudo batch size N for the various considered models. We report mCE averaged across 15 test corruptions.</figDesc><table><row><cell>1 1 1 DeepAugment 50 60 70 80 90 100 mCE 50 60 70 80 90 100 mCE 50 60 70 80 90 100 mCE ANT 1 2 4 8 16 32 64 128 256 512 ANT+SIN 1 2 4 8 16 32 64 128 256 512 SIN 8 DeepAugment+AugMix 8 ResNext+DeepAugment+Augmix 8 Batch size 64 resnet 8 64 Batch size sin 8 64 Batch size antsin 116.10 1 1 93.88 74.51 63.65 58.37 55.78 54.51 53.92 53.66 53.53 1 108.24 87.60 71.12 62.23 57.83 55.62 54.57 54.02 53.76 53.64 1 1 65.37 1 52.59 1 Figure 12: ResNet-50 8 42.09</cell><cell>512 512 512 2 2 93.58 83.74 71.06 62.50 57.87 55.54 54.41 53.85 53.61 53.49 2 84.75 78.40 68.32 61.38 57.51 55.51 54.49 53.98 53.74 53.63 2 2 63.87 2 51.98 2 41.74</cell><cell>N 128 1 2 4 8 16 32 64 256 mCE 100 50 60 70 80 90 50 60 70 80 90 100 mCE 4 8 4 8 72.31 62.28 72.01 62.69 66.34 61.15 60.74 58.43 57.14 56.11 55.20 54.66 54.21 53.88 53.71 53.53 53.50 53.37 53.41 53.33 4 8 67.42 59.91 68.32 60.63 64.31 59.78 59.98 57.93 57.00 56.17 55.33 54.96 54.40 54.25 53.95 53.85 53.71 53.67 53.60 53.57 4 8 4 8 61.37 58.11 4 8 51.05 49.83 4 8 41.29 40.67</cell><cell>1 1 16 16 60.07 58.97 57.55 56.04 54.77 53.91 53.42 53.28 53.20 53.21 16 58.15 57.54 56.63 55.69 54.96 54.38 53.98 53.72 53.59 53.51 16 16 54.48 16 48.5 16 39.96</cell><cell>32 32 60.73 59.10 57.03 55.02 53.67 53.06 52.84 52.85 52.96 53.02 32 58.49 57.47 56.06 54.59 53.76 53.55 53.51 53.49 53.47 53.45 32 32 52.17 32 47.81 32 39.69</cell><cell>8 Batch size 64 augmix 8 64 Batch size ant 64 128 64 128 61.75 62.48 60.44 61.67 58.51 60.29 56.10 58.22 53.76 55.61 52.50 53.18 52.23 51.94 52.29 51.80 52.54 52.04 52.78 52.38 64 128 59.24 59.85 58.33 59.23 57.01 58.24 55.30 56.79 53.61 54.92 52.80 53.13 52.84 52.36 53.07 52.53 53.23 52.85 53.35 53.12 64 128 64 128 52.33 54.18 64 128 48.36 49.72 64 128 40.35 41.55</cell><cell>512 512 256 256 62.90 62.44 61.64 60.20 58.06 55.35 52.87 51.65 51.60 51.90 256 60.23 59.87 59.23 58.21 56.68 54.73 52.89 52.12 52.33 52.75 256 256 56.36 256 51.12 256 42.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table /><note>Test mCE for various batch sizes (rows) vs. pseudo batch sizes (columns)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that our notion of internal covariate shift differs from previous work<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>: In i.i.d. training settings, Ioffe and Szegedy<ref type="bibr" target="#b2">[3]</ref> hypothesized that covariate shift introduced by changing lower layers in the network is reduced by BN, explaining the empirical success of the method. We do not provide evidence for this line of research in this work: Instead, we focus on the covariate shift introduced (by design) in datasets such as IN-C, and provide evidence for the hypothesis that high-level domain shifts in the input partly manifests in shifts and scaling of internal activations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For computing the Wasserstein metric we make the simplifying assumption that the empirical mean and covariances fully parametrize the respective distributions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that for simplicity, we do not reset the statistics of the remaining (bi ? i) BN layers. This could potentially be adapted in future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">t ,(18)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Prerequisites</head><p>Lemma 1 (Mean and variance of sample moments, following <ref type="bibr" target="#b65">[65]</ref>). The sample moments? t ,? 2 t are random variables depending on the sample size n.</p><p>For brevity, we use the shorthand E[?] for all expectation values with respect to the distribution of p(? t ,? 2 t |n). In particular, our computation uses mean and variance of? t and? 2 t which are well known for a normal target distribution:</p><p>The derivation of the variance V[? 2 t ] in the last line uses the fact that the variance of a chi square distributed variable with (n ? 1) degrees of freedom is equal to 2(n ? 1). Lemma 2 (Holder's defect formula for concave functions in probabilistic notation, following Becker <ref type="bibr" target="#b66">[66]</ref> ). If the concave function f : [a, b] ? R is twice continuously differentiable and there are finite bounds m and M such that</p><p>then the defect between Jensen's inequality estimate f (E[X]) for a random variable X taking values x ? [a, b] and the true expectation value E[f (X)] is bounded from above by a term proportional to the variance of X:</p><p>Lemma 3 (Upper and lower bounds on the expectation value of?). The expectation value of the square root of the random variable? 2 defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full list of models evaluated on IN</head><p>The following lists contains all models we evaluated on various datasets with references and links to the corresponding source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Torchvision models trained on IN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weights</head><p>were taken from https://github.com/pytorch/vision/tree/master/ torchvision/models</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalisation in humans and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7538" to="7550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task generalization and adaptation between noisy digit datasets: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS), Workshop on Continual Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Cariucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Machine learning in non-stationary environments: Introduction to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML&apos;12</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning, ICML&apos;12<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">9781450312851</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer-Verlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. In Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Increasing the robustness of dnns against image corruptions by playing the game of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno>abs/2001.06057</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Natural adversarial examples. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robustness properties of facebook&apos;s resnext wsl models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>A Emin Orhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Batch normalization is a cause of adversarial vulnerability. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Galloway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Golubeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medhat</forename><surname>Moussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fixup initialization: Residual learning without normalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">MNIST-C: A robustness benchmark for computer vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Benchmarking robustness in object detection: Autonomous driving when winter is coming. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Benchmarking the robustness of semantic segmentation models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial examples are a natural consequence of test error in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nic</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogus</forename><surname>Cubuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Miko?ajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Grochowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Interdisciplinary PhD Workshop (IIPhDW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Correlation alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context-based normalization of histological stains using deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Grote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Feuerhake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Sch?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Test-time training for out-of-distribution generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Self-ensembling for domain adaptation. CoRR, abs/1706.05208</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">H</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fully test-time adaptation by entropy minimization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Intriguing properties of adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morcos</surname></persName>
		</author>
		<idno>abs/2003.00152</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Evaluating prediction-time batch normalization for robustness under covariate shift. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno>abs/2004.07780</idno>
		<title level="m">Shortcut learning in deep neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Identifying statistical bias in dataset replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno>abs/2005.09619</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Docker: Lightweight linux containers for consistent development and deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Merkel</surname></persName>
		</author>
		<idno>1075-3583</idno>
	</analytic>
	<monogr>
		<title level="j">Linux J</title>
		<imprint>
			<biblScope unit="issue">239</biblScope>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Paul van Mulbregt, and SciPy 1. 0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Vand Erplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
		<ptr target="https://doi.org/10.1038/s41592-019-0686-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Gnu parallel -the command-line power tool. ;login: The USENIX Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tange</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/s/parallel" />
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A PyTorch Converter for SimCLR Checkpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://github.com/tonylins/simclr-converter.CommitID" />
		<imprint>
			<date type="published" when="2020-10-21" />
			<biblScope unit="page" from="139" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Standard deviation distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="https://mathworld.wolfram.com/StandardDeviationDistribution.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The variance drain and jensen&apos;s inequality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">For training, we keep all hyperparameters at their default values and note</title>
		<ptr target="https://github.com/hongyi-zhang/Fixup/tree/master/imagenet" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
