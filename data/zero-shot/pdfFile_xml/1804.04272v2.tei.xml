<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Neural Networks Motivated by Partial Differential Equations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-12">December 12, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
							<email>lruthotto@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
							<email>ehaber@eoas.ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Earth and Ocean Science</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Neural Networks Motivated by Partial Differential Equations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-12">December 12, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Deep Neural Networks</term>
					<term>Partial Dif- ferential Equations</term>
					<term>PDE-Constrained Optimization</term>
					<term>Image Classifi- cation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite dimensional setting provides powerful tools for their analysis and solution. Over the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction.</p><p>In this paper, we establish a new PDE-interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning.</p><p>Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last three decades, algorithms inspired by partial differential equations (PDE) have had a profound impact on many processing tasks that involve speech, image, and video data. Adapting PDE models that were traditionally used in physics to perform image processing tasks has led to ground-breaking contributions. An incomplete list of seminal works includes optical flow models for motion estimation <ref type="bibr" target="#b25">[26]</ref>, nonlinear diffusion models for filtering of images <ref type="bibr" target="#b37">[38]</ref>, variational methods for image segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>, and nonlinear edge-preserving denoising <ref type="bibr" target="#b41">[42]</ref>.</p><p>A standard step in PDE-based data processing is interpreting the involved data as discretizations of multivariate functions. Consequently, many operations on the data can be modeled as discretizations of PDE operators acting on the underlying functions. This continuous data model has led to solid mathematical theories for classical data processing tasks obtained by leveraging the rich results from PDEs and variational calculus (e.g., <ref type="bibr" target="#b42">[43]</ref>). The continuous perspective has also enabled more abstract formulations that are independent of the actual resolution, which has been exploited to obtain efficient multiscale and multilevel algorithms (e.g., <ref type="bibr" target="#b33">[34]</ref>). In this paper, we establish a new PDE-interpretation of deep learning tasks that involve speech, image, and video data as features. Deep learning is a form of machine learning that uses neural networks with many hidden layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>. Although neural networks date back at least to the 1950s <ref type="bibr" target="#b40">[41]</ref>, their popularity soared a few years ago when deep neural networks (DNNs) outperformed other machine learning methods in speech recognition <ref type="bibr" target="#b38">[39]</ref> and image classification <ref type="bibr" target="#b24">[25]</ref>. Deep learning also led to dramatic improvements in computer vision, e.g., surpassing human performance in image recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. These results ignited the recent flare of research in the field. To obtain a PDE-interpretation, we use a continuous representation of the images and extend recent works by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref>, which relate deep learning problems for general data types to ordinary differential equations (ODE).</p><p>Deep neural networks filter input features using several layers whose operations consist of element-wise nonlinearities and affine transformations. The main idea of convolutional neural networks (CNN) <ref type="bibr" target="#b29">[30]</ref> is to base the affine transformations on convolution operators with compactly supported filters. Supervised learning aims at learning the filters and other parameters, which are also called weights, from training data. CNNs are widely used for solving large-scale learning tasks involving data that represent a discretization of a continuous function, e.g., voice, images, and videos <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. By design, each CNN layer exploits the local relation between image information, which simplifies computation <ref type="bibr" target="#b38">[39]</ref>.</p><p>Despite their enormous success, deep CNNs still face critical challenges including designing a CNN architecture that is effective for a practical learning task, which requires many choices. In addition to the number of layers, also called depth of the network, important aspects are the number of convolution filters at each layer, also called the width of the layers, and the connections between those filters. A recent trend is to favor deep over wide networks, aiming at improving generalization (i.e., the performance of the CNN on new examples that were not used during the training) <ref type="bibr" target="#b30">[31]</ref>. Another key challenge is designing the layer, i.e., choosing the combination of affine transformations and nonlinearities. A practical but costly approach is to consider depth, width, and other properties of the architecture as hyper-parameters and jointly infer them with the network weights <ref type="bibr" target="#b22">[23]</ref>. Our interpretation of CNN architectures as discretized PDEs provides new mathematical theories to guide the design process. In short, we obtain architectures by discretizing the underlying PDE through adequate time integration methods.</p><p>In addition to substantial training costs, deep CNNs face fundamental challenges when it comes to their interpretability and robustness. In particular, CNNs that are used in mission-critical tasks (such as driverless cars) face the challenge of being "explainable." Casting the learning task within nonlinear PDE theory allows us to understand the properties of such networks better. We believe that further research into the mathematical structures presented here will result in a more solid understanding of the networks and will close the gap between deep learning and more mature fields that rely on nonlinear PDEs such as fluid dynamics. A direct impact of our approach can be observed when studying, e.g., adversarial examples. Recent works <ref type="bibr" target="#b36">[37]</ref> indicate that the predictions obtained by deep networks can be very sensitive to perturbations of the input images. These findings motivate us to favor networks that are stable, i.e., networks whose output are robust to small perturbations of the input features, similar to what PDE analysis suggests.</p><p>In this paper, we consider residual neural networks (ResNet) <ref type="bibr" target="#b21">[22]</ref>, a very effective type of neural networks. We show that residual CNNs can be interpreted as a discretization of a space-time differential equation. We use this link for analyzing the stability of a network and for motivating new network models that bear similarities with well-known PDEs. Using our framework, we present three new architectures. First, we introduce parabolic CNNs that restrict the forward propagation to dynamics that smooth image features and bear similarities with anisotropic filtering <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref>. Second, we propose hyperbolic CNNs that are inspired by Hamiltonian systems and finally, a third, second-order hyperbolic CNN. As to be expected, those networks have different properties. For example, hyperbolic CNNs approximately preserve the energy in the system, which sets them apart from parabolic networks that smooth the image data, reducing the energy. Computationally, the structure of a hyperbolic forward propagation can be exploited to alleviate the memory burden because hyperbolic dynamics can be made reversible on the continuous and discrete levels. The methods suggested here are closely related to reversible ResNets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The remainder of this paper is organized as follows. In Section 2, we provide a brief introduction into residual networks and their relation to ordinary and, in the case of convolutional neural networks, partial differential equations. In Section 3, we present three novel CNN architectures motivated by PDE theory. Based on our continuous interpretation we present regularization functionals that enforce the smoothness of the dynamical systems, in Section 4. In Section 5, we present numerical results for image classification that indicate the competitiveness of our PDE-based architectures. Finally, we highlight some directions for future research in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Residual Networks and Differential Equations</head><p>The abstract goal of machine learning is to find a function f :  <ref type="figure">Figure 1</ref>: Classification results of the three proposed CNN architecture for four randomly selected test images from the STL-10 dataset <ref type="bibr" target="#b12">[13]</ref>. The predicted and actual class probabilities are visualized using bar plots on the right of each image. While all networks reach a competitive prediction accuracy between around 74% and 78% across the whole dataset, predictions for individual images vary in some cases.</p><formula xml:id="formula_0">R n ? R p ? R m such that f (?,</formula><p>a spoken word, etc.). The function is parameterized by the weight vector ? ? R p that is trained using examples. In supervised learning, a set of input features y 1 , . . . , ys ? R n and output labels c1, . . . , cs ? R m is available and used to train the model f (?, ?). The output labels are vectors whose components correspond to the estimated probability of a particular example belonging to a given class. As an example, consider the image classification results in <ref type="figure">Fig. 1</ref> where the predicted and actual labels are visualized using bar plots. For brevity, we denote the training data by Y = [y1, y2, . . . , ys] ? R n?s and C = [c1, c2, . . . , cs] ? R m?s . In deep learning, the function f consists of a concatenation of nonlinear functions called hidden layers. Each layer is composed of affine linear transformations and pointwise nonlinearities and aims at filtering the input features in a way that enables learning. As a fairly general formulation, we consider an extended version of the layer used in <ref type="bibr" target="#b21">[22]</ref>, which filters the features Y as follows</p><formula xml:id="formula_1">F(?, Y) = K2(? (3) )? N (K1(? (1) )Y, ? (2) ) .<label>(1)</label></formula><p>Here, the parameter vector, ?, is partitioned into three parts where ? <ref type="bibr" target="#b0">(1)</ref> and ? (3) parameterize the linear operators K1(?) ? R k?n and K2(?) ? R k out ?k , respectively, and ? <ref type="bibr" target="#b1">(2)</ref> are the parameters of the normalization layer N . The activation function ? : R ? R is applied component-wise. Common examples are ?(x) = tanh(x) or the rectified linear unit (ReLU) defined as ?(x) = max(0, x). A deep neural network can be written by concatenating many of the layers given in <ref type="bibr" target="#b0">(1)</ref>. When dealing with image data, it is common to group the features into different channels (e.g., for RGB image data there are three channels) and define the operators K1 and K2 as block matrices consisting of spatial convolutions. Typically each channel of the output image is computed as a weighted sum of each of the convolved input channels. To give an example, assume that K1 has three input and two output channels and denote by K (?,?) 1 (?) a standard convolution operator <ref type="bibr" target="#b20">[21]</ref>. In this case, we can write K1 as</p><formula xml:id="formula_2">K1(?) = K (1,1) 1 (? (1,1) ) K (1,2) 1 (? (1,2) ) K (1,3) 1 (? (1,3) ) K (2,1) 1 (? (1,2) ) K (2,2) 1 (? (2,2) ) K (2,3) 1 (? (2,3) ) ,</formula><p>(2) where ? (i,j) denotes the parameters of the stencil of the (i, j)-th convolution operator.</p><p>A common choice for N in <ref type="formula" target="#formula_1">(1)</ref> is the batch normalization layer <ref type="bibr" target="#b26">[27]</ref>. This layer computes the empirical mean and standard deviation of each channel in the input images across the spatial dimensions and examples and uses this information to normalize the statistics of the output images. While the coupling of different examples is counter-intuitive, its use is wide-spread and motivated by empirical evidence showing a faster convergence of training algorithms. The weights ? <ref type="bibr" target="#b1">(2)</ref> represent scaling factors and biases (i.e., constant shifts applied to all pixels in the channel) for each output channel that are applied after the normalization.</p><p>ResNets have recently improved the state-of-the-art in several benchmarks including computer vision contests on image classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. Given the input features Y0 = Y, a ResNet unit with N layers produces a filtered version YN as follows</p><formula xml:id="formula_3">Yj+1 = Yj + F(? (j) , Yj), for j = 0, 1, . . . , N ? 1,<label>(3)</label></formula><p>where ? (j) are the weights (convolution stencils and biases) of the jth layer. To emphasize the dependency of this process on the weights, we denote YN (?). Note that the dimension of the feature vectors (i.e., the image resolution and the number of channels) is the same across all layers of a ResNets unit, which is limiting in many practical applications. Therefore, implementations of deep CNNs contain a concatenation of ResNet units with other layers that can change, e.g., the number of channels and the image resolution (see, e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>).</p><p>In image recognition, the goal is to classify the output of (3), YN (?), using, e.g., a linear classifier modeled by a fullyconnected layer, i.e., an affine transformation with a dense matrix. To avoid confusion with the ResNet units we denote these transformations as WYN (?) + (BW ?)e s , where the columns of BW represent a distributed bias and es ? R s is a vector of all ones. The parameters of the network and the classifier are unknown and have to be learned. Thus, the goal of learning is to estimate the network parameters, ?, and the weights of the classifier, W, ?, by approximately solving the optimization problem min ?,W,?</p><formula xml:id="formula_4">1 2 S(WYN (?) + (BW ?)e s , C) + R(?, W, ?), (4)</formula><p>where S is a loss function, which is convex in its first argument, and R is a convex regularizer discussed below. Typical examples of loss functions are the least-squares function in regression and logistic regression or cross entropy functions in classification <ref type="bibr" target="#b16">[17]</ref>.</p><p>The optimization problem in (4) is challenging for several reasons. First, it is a high-dimensional and non-convex optimization problem. Therefore one has to be content with local minima. Second, the computational cost per example is high, and the number of examples is large. Third, very deep architectures are prone to problems such as vanishing and exploding gradients <ref type="bibr" target="#b4">[5]</ref> that may occur when the discrete forward propagation is unstable <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Residual Networks and ODEs</head><p>We derived a continuous interpretation of the filtering provided by ResNets in <ref type="bibr" target="#b18">[19]</ref>. Similar observations were made in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>. The ResNet in (3) can be seen as a forward Euler discretization (with a fixed step size of ?t = 1) of the initial value problem</p><formula xml:id="formula_5">?tY(?, t) = F(?(t), Y(t)), for t ? (0, T ] Y(?, 0) = Y0.<label>(5)</label></formula><p>Here, we introduce an artificial time t ? [0, T ]. The depth of the network is related to the arbitrary final time T and the magnitude of the matrices K1 and K2 in <ref type="formula" target="#formula_1">(1)</ref>. This observation shows the relation between the learning problem (4) and parameter estimation of a system of nonlinear ordinary differential equations. Note that this interpretation does not assume any particular structure of the layer F. The continuous interpretation of ResNets can be exploited in several ways. One idea is to accelerate training by solving a hierarchy of optimization problems that gradually introduce new time discretization points for the weights, ? <ref type="bibr" target="#b19">[20]</ref>. Also, new numerical solvers based on optimal control theory have been proposed in <ref type="bibr" target="#b32">[33]</ref>. Another recent work <ref type="bibr" target="#b10">[11]</ref> uses more sophisticated time integrators to solve the forward propagation and the adjoint problem (in this context commonly called back-propagation), which is needed to compute derivatives of the objective function with respect to the network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional ResNets and PDEs</head><p>In the following, we consider learning tasks involving features given by speech, image, or video data. For these problems, the input features, Y, can be seen as a discretization of a continuous function Y (x). We assume that the matrices K1 ? Rw ?w in and K2 ? R w out ?w in (1) represent convolution operators <ref type="bibr" target="#b20">[21]</ref>. The parameters win,w, and wout denote the width of the layer, i.e., they correspond to the number of input, intermediate, and output features of this layer.</p><p>We now show that a particular class of deep residual CNNs can be interpreted as nonlinear systems of PDEs. For ease of notation, we first consider a one-dimensional convolution of a feature with one channel and then outline how the result extends to higher space dimensions and multiple channels.</p><p>Assume that the vector y ? R n represents a one-dimensional grid function obtained by discretizing y : [0, 1] ? R at the cellcenters of a regular grid with n cells and a mesh size h = 1/n, i.e., for i = 1, 2, . . . , n</p><formula xml:id="formula_6">y = [y(x1), . . . , y(xn)] with xi = i ? 1 2 h.</formula><p>Assume, e.g., that the operator K1 = K1(?) ? R n?n in <ref type="formula" target="#formula_1">(1)</ref> is parameterized by the stencil ? ? R 3 . Applying a coordinate change, we see that</p><formula xml:id="formula_7">K1(?)y = [?1 ?2 ?3] * y = ? 1 4 [1 2 1] + ? 2 2h [?1 0 1] + ? 3 h 2 [?1 2 ? 1] * y.</formula><p>Here, the weights ? ? R 3 are given by</p><formula xml:id="formula_8">? ? 1 4 ? 1 2h ? 1 h 2 1 2 0 2 h 2 1 4 1 2h ? 1 h 2 ? ? ? ? ? 1 ? 2 ? 3 ? ? = ? ? ?1 ?2 ?3 ? ? ,</formula><p>which is a non-singular linear system for any h &gt; 0. We denote by ?(?) the unique solution of this linear system. Upon taking the limit, h ? 0, this observation motivates one to parameterize the convolution operator as</p><formula xml:id="formula_9">K1(?) = ? 1 (?) + ? 2 (?)?x + ? 3 (?)? 2 x .</formula><p>The individual terms in the transformation matrix correspond to reaction, convection, diffusion and the bias term in (1) is a source/sink term, respectively. Note that higher-order derivatives can be generated by multiplying different convolution operators or increasing the stencil size. This simple observation exposes the dependence of learned weights on the image resolution, which can be exploited in practice, e.g., by multiscale training strategies <ref type="bibr" target="#b19">[20]</ref>. Here, the idea is to train a sequence of network using a coarse-to-fine hierarchy of image resolutions (often called image pyramid). Since both the number of operations and the memory required in training is proportional to the image size, this leads to immediate savings during training but also allows one to coarsen already trained networks to enable efficient evaluation. In addition to computational benefits, ignoring fine-scale features when training on the coarse grid can also reduce the risk of being trapped in an undesirable local minimum, which is an observation also made in other image processing applications.</p><p>Our argument extends to higher spatial dimensions. In 2D, e.g., we can relate the 3 ? 3 stencil parametrized by ? ? R 9 to</p><formula xml:id="formula_10">K1(?) =? 1 (?) + ? 2 (?)?x + ? 3 (?)?y + ? 4 (?)? 2 x + ? 5 (?)? 2 y + ? 6 (?)?x?y + ? 7 (?)? 2 x ?y + ? 8 (?)?x? 2 y + ? 9 (?)? 2 x ? 2 y .</formula><p>To obtain a fully continuous model for the layer in (1), we proceed the same way with K2. In view of(2), we note that when the number of input and output channels is larger than one, K1 and K2 lead to a system of coupled partial differential operators. Given the continuous space-time interpretation of CNN we view the optimization problem (4) as an optimal control problem and, similarly, see learning as a parameter estimation problem for the time-dependent nonlinear PDE <ref type="bibr" target="#b4">(5)</ref>. Developing efficient numerical methods for solving PDE-constrained optimization problems arising in optimal control and parameter estimation has been a fruitful research endeavor and led to many advances in science and engineering (for recent overviews see, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref>). Using the theoretical and algorithmic framework of optimal control in machine learning applications has gained some traction only recently (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Neural Networks motivated by PDEs</head><p>It is well-known that not every time-dependent PDE is stable with respect to perturbations of the initial conditions <ref type="bibr" target="#b1">[2]</ref>. Here, we say that the forward propagation in <ref type="bibr" target="#b4">(5)</ref> </p><formula xml:id="formula_11">is stable if there is a constant M &gt; 0 independent of T such that Y(?, T ) ??(?, T ) F ? M Y(0) ??(0) F ,<label>(6)</label></formula><p>where Y and? are solutions of (5) for different initial values and ? F is the Frobenius norm. The stability of the forward propagation depends on the values of the weights ? that are chosen by solving <ref type="bibr" target="#b3">(4)</ref>. In the context of learning, the stability of the network is critical to provide robustness to small perturbations of the input images. In addition to image noise, perturbations could also be added deliberately to mislead the network's prediction by an adversary. There is some recent evidence showing the existence of such perturbations that reliably mislead deep networks by being barely noticeable to a human observer (e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>).</p><p>To ensure the stability of the network for all possible weights, we propose to restrict the space of CNNs. As examples of this general idea, we present three new types of residual CNNs that are motivated by parabolic and first-and second-order hyperbolic PDEs, respectively. The construction of our networks guarantees that the networks are stable forward and, for the hyperbolic network, stable backward in time.</p><p>Though it is common practice to model K1 and K2 in (1) independently, we note that it is, in general, hard to show the stability of the resulting network. This is because, the Jacobian of F(?, Y) with respect to the features has the form</p><formula xml:id="formula_12">J Y F = K2(?) diag(? (K1(?Y))) K1(?),</formula><p>where ? denotes the derivatives of the pointwise nonlinearity and for simplicity we assume N (Y) = Y. Even in this simplified setting, the spectral properties of J Y , which impact the stability, are unknown for arbitrary choices of K1 and K2.</p><p>As one way to obtain a stable network, we introduce a symmetric version of the layer in (1) by choosing K2 = ?K 1 in (1). To simplify our notation, we drop the subscript of the operator and define the symmetric layer</p><formula xml:id="formula_13">Fsym(?, Y) = ?K(?) ? (N (K(?)Y, ?)) .<label>(7)</label></formula><p>It is straightforward to verify that this choice leads to a negative semi-definite Jacobian for any non-decreasing activation function. As we see next, this choice also allows us to link the discrete network to different types of PDEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parabolic CNN</head><p>We define the parabolic CNN by using the symmetric layer from <ref type="bibr" target="#b6">(7)</ref> in the forward propagation, i.e., in the standard ResNet we replace the dynamic in (5) by ?tY(?, t) = Fsym(?(t), Y(t)), for t ? (0, T ].</p><p>Note that <ref type="formula" target="#formula_14">(8)</ref> is equivalent to the heat equation if ?(x) = x, N (Y) = Y and K(t) = ?. This motivates us to refer to this network as a parabolic CNN. Nonlinear parabolic PDEs are widely used, e.g., to filter images <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref> and our interpretation implies that the networks can be viewed as an extension of such methods. The similarity to the heat equation motivates us to introduce a new normalization layer motivated by total variation denoising. For a single example y ? R n that can be grouped into c channels, we define</p><formula xml:id="formula_15">Ntv(y) = diag 1 A A(y 2 ) + y,<label>(9)</label></formula><p>where the operator A ? R n/c?n computes the sum over all c channels for each pixel, the square, square root, and the division are defined component-wise, 0 &lt; 1 is fixed. As for the batch norm layer, we implement Ntv with trainable weights corresponding to global scaling factors and biases for each channel. In the case that the convolution is reduced to a discrete gradient, Ntv leads to the regular dynamics in TV denoising.</p><p>Stability. Parabolic PDEs have a well-known decay property that renders them robust to perturbations of the initial conditions. For the parabolic CNN in (8) we can show the following stability result.</p><p>Theorem 1 If the activation function ? is monotonically nondecreasing, then the forward propagation through a parabolic CNN satisfies <ref type="bibr" target="#b5">(6)</ref>.</p><p>Proof 1 For ease of notation, we assume that no normalization layer is used, i.e., N (Y) = Y in <ref type="bibr" target="#b7">(8)</ref>. We then show that Fsym(?(t), Y) is a monotone operator. Note that for all t ? [0, T ]</p><formula xml:id="formula_16">? (?(K(t)Y) ? ?(K(t)Y ), K(t)(Y ? Y )) ? 0.</formula><p>Where (?, ?) is the standard inner product and the inequality follows from the monotonicity of the activation function, which shows that ?t Y(t) ? Y (t) 2 F ? 0. Integrating this inequality over [0, T ] yields stability as in <ref type="bibr" target="#b5">(6)</ref>. The proof extends straightforwardly to cases when a normalization layer with scaling and bias is included.</p><p>One way to discretize the parabolic forward propagation <ref type="formula" target="#formula_14">(8)</ref> is using the forward Euler method. Denoting the time step size by ?t &gt; 0 this reads</p><formula xml:id="formula_17">Yj+1 = Yj + ?tFsym(?(tj), Yj), j = 0, 1, . . . , N ? 1,</formula><p>where tj = j?t. The discrete forward propagation of a given example y0 is stable if ?t satisfies max i=1,2,...,n |1 + ?t?i(J(tj))| ? 1, j = 0, 1, . . . , N ? 1, and accurate if ?t is chosen small enough to capture the dynamics of the system. Here, ?i(J(tj)) denotes the ith eigenvalue of the Jacobian of Fsym with respect to the features at a time point tj. If we assume, for simplicity, that no normalization layer is used, the Jacobian is J(tj) = ?K (? (1) (tj)) D(tj)K(? (1) (tj)), with D(t) = diag ? K(? (1) (t))y(t) .</p><p>If the activation function is monotonically nondecreasing, then ? (?) ? 0 everywhere. In this case, all eigenvalues of J(tj) are real and bounded above by zero since J(tj) is also symmetric. Thus, there is an appropriate ?t that renders the discrete forward propagation stable. In our numerical experiments, we aim at ensuring the stability of the discrete forward propagation by limiting the magnitude of elements in K by adding bound constraints to the optimization problem (4).  <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b8">[9]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperbolic CNNs</head><p>Hamiltonian CNNs. Introducing an auxiliary variable Z (i.e., by partitioning the channels of the original features), we consider the dynamics</p><formula xml:id="formula_18">?tY(t) = ?Fsym(? (1) (t), Z(t)), Y(0) = Y0 ?tZ(t) = Fsym(? (2) (t), Y(t)), Z(0) = Z0.</formula><p>We showed in <ref type="bibr" target="#b8">[9]</ref> that the eigenvalues of the associated Jacobian are imaginary. When assuming that ? <ref type="bibr" target="#b0">(1)</ref> and ? <ref type="bibr" target="#b1">(2)</ref> change sufficiently slow in time stability as defined in <ref type="formula" target="#formula_11">(6)</ref> is obtained. A more precise stability result can be established by analyzing the kinematic eigenvalues of the forward propagation <ref type="bibr" target="#b2">[3]</ref>. We discretize the dynamic using the symplectic Verlet integration (see, e.g., <ref type="bibr" target="#b1">[2]</ref> for details)</p><formula xml:id="formula_19">Yj+1 = Yj + ?tFsym(? (1) (t), Zj), Zj+1 = Zj ? ?tFsym(? (2) (t), Yj+1),<label>(10)</label></formula><p>for j = 0, 1, . . . , N ? 1 using a fixed step size ?t &gt; 0. This dynamic is reversible, i.e., given YN , YN?1 and ZN , ZN?1 it can also be computed backwards</p><formula xml:id="formula_20">Zj = Zj+1 + ?tFsym(? (2) (t), Yj+1) Yj = Yj+1 ? ?tFsym(? (1) (t), Yj+1),</formula><p>for j = N ? 1, N ? 2, . . . , 0. These operations are numerically stable for the Hamiltonian CNN (see <ref type="bibr" target="#b8">[9]</ref> for details).</p><p>Second-order CNNs. An alternative way to obtain hyperbolic CNNs is by using a second-order dynamics</p><formula xml:id="formula_21">? 2 t Y(t) = Fsym(?(t), Y(t)), Y(0) = Y0, ?tY(0) = 0.<label>(11)</label></formula><p>The resulting forward propagation is associated with a nonlinear version of the telegraph equation <ref type="bibr" target="#b39">[40]</ref>, which describes the propagation of signals through networks. Hence, one could claim that second-order networks better mimic biological networks and are therefore more appropriate than first-order networks for approaches that aim at imitating the propagation through biological networks. We discretize the second-order network using the Leapfrog method. For j = 0, 1, . . . , N ? 1 and ?t &gt; 0 fixed this reads</p><formula xml:id="formula_22">Yj+1 = 2Yj ? Yj?1 + ? 2 t Fsym(?(tj), Yj).</formula><p>We set Y?1 = Y0 to denote the initial condition. Similar to the symplectic integration in <ref type="bibr" target="#b9">(10)</ref>, this scheme is reversible. We show that the second-order network is stable in the sense of (6) when we assume stationary weights. Weaker results for the time-dependent dynamic are possible assuming ?t?(t) to be bounded.</p><p>Theorem 2 Let ?(t) be constant in time and assume that the activation function satisfies |?(x)| ? |x| for all x. Then, the forward propagation through the second-order network satisfies <ref type="bibr" target="#b5">(6)</ref>.</p><p>Proof 2 For brevity, we denote K = K(?(t)) and consider the forward propagation of a single example. Let y : [0, T ] ? R n be a solution to <ref type="bibr" target="#b10">(11)</ref> and consider the energy E(t) = 1 2 (?ty(t)) ?ty(t) + (Ky(t)) ?(Ky(t)) .</p><p>Given that |?(x)| ? |x| for all x by assumption, this energy can be bounded as follows</p><formula xml:id="formula_24">E(t) ? E lin (t) = 1 2 (?tu(t)) ?tu(t) + (Ku(t)) (Ku(t)) ,</formula><p>where E lin is the energy associated with the linear wave-like hyperbolic equation</p><formula xml:id="formula_25">? 2 t u(t) = ?K Ku(t), u(0) = y0, ?tu(0) = 0.</formula><p>Since by assumption K is constant in time, we have that</p><formula xml:id="formula_26">?tE lin (t) = ?tu(t) ? 2 t u(t) + K Ku(t) = 0.</formula><p>Thus, the energy of the hyperbolic network in (12) is positive and bounded from above by the energy of the linear wave equation. Applying this argument to the initial condition y0 ? y we derive (6) and thus the forward propagation is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Regularization</head><p>The proposed continuous interpretation of the CNNs also provides new perspectives on regularization. To enforce stability of the forward propagation, the linear operator K in <ref type="formula" target="#formula_13">(7)</ref> should not change drastically in time. This suggests adding a smoothness regularizer in time. In <ref type="bibr" target="#b18">[19]</ref> a H 1 -seminorm was used to smooth kernels over time with the goal to avoid overfitting. A theoretically more appropriate function space consists of all kernels that are piecewise smooth in time. To this end, we introduce the regularizer</p><formula xml:id="formula_27">R(?, W, ?) =?1 T 0 ?? (?t?(t))dt + ?2 2 T 0 ?(t) 2 dt + W 2 F + ? 2 ,<label>(13)</label></formula><p>where the function ? (x) = ? x 2 + ? is a smoothed 1-norm with conditioning parameter ? &gt; 0. The first term of R can be seen as a total variation <ref type="bibr" target="#b41">[42]</ref> penalty in time that favors piecewise constant dynamics. Here, ?1, ?2 ? 0 are regularization parameters that are assumed to be fixed.</p><p>A second important aspect of stability is to keep the time step sufficiently small. Since ?t can be absorbed in K we use the box constraint ?1 ? ? (1) (tj) ? 1 for all j, and fix the time step size to ?t = 1 in our numerical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Experiments</head><p>We demonstrate the potential of the proposed architectures using the common image classification benchmarks STL-10 <ref type="bibr" target="#b12">[13]</ref>, CIFAR-10, and CIFAR-100 <ref type="bibr" target="#b27">[28]</ref>. Our central goal is to show that, despite their modeling restrictions, our new network types achieve competitive results. We use our basic architecture for all experiments, do not excessively tune hyperparameters individually for each case, and employ a simple data augmentation technique consisting of random flipping and cropping.</p><p>Network Architecture. Our architecture is similar to the ones in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> and contains an opening layer, followed by several blocks each containing a few time steps of a ResNet and a connector that increases the width of the CNN and coarsens the images. Our focus is on the different options for defining the ResNet block using parabolic and hyperbolic networks. To this end, we choose the same basic components for the opening and connecting layers. The opening layer increases the number of channels from three (for RGB image data) to the number of channels of the first ResNet using convolution operators with 3 ? 3 stencils, a batch normalization layer and a ReLU activation function. We build the connecting layers using 1 ? 1 convolution operators that increase the number of channels, a batch normalization layer, a ReLU activation, and an average pooling operator that coarsens the images by a factor of two. Finally, we obtain the output features Y(?) by averaging the features of each channel to ensure translation-invariance. The ResNet blocks use the symmetric layer <ref type="bibr" target="#b6">(7)</ref> including the total variation normalization (9) with = 10 ?3 . The classifier is modeled using a fully-connected layer, a softmax transformation, and a crossentropy loss.</p><p>Training Algorithm.</p><p>In order to estimate the weights, we use a standard stochastic gradient descent (SGD) method with momentum of 0.9. We use a piecewise constant step size (in this context also called learning rate), starting with 0.1, which is decreased by a factor of 0.2 at a-priori specified epochs. For STL-10 and CIFAR-10 examples, we perform 60, 20, and 20 epochs with step sizes of 0.1, 0.02, 0.004, respectively. For the more challenging CIFAR-100 data set, we use 60, <ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40</ref>, and 20 epochs with step sizes of 0.1, 0.02, 0.004, 0.0008, 0.00016, respectively.</p><p>In all examples, the SGD steps are computed using mini-batches consisting of 125 randomly chosen examples. For data augmentation, we apply a random horizontal flip (50% probability), pad the images by a factor of 1/16 with zeros into all directions and randomly crop the image by 1/8 of the pixels, counting from the lower-left corner. The training is performed using the opensource software Meganet on a workstation running Ubuntu 16.04 and MATLAB 2018b with two Intel(R) Xeon(R) CPU E5-2620 v4 and 64 GB of RAM. We use NVIDIA Titan X GPU for accelerating the computation through the frameworks CUDA 9.1 and CuDNN 7.0.</p><p>Results for STL-10.</p><p>The STL-10 dataset <ref type="bibr" target="#b12">[13]</ref> contains 13,000 digital color images of size 96 ? 96 that are evenly divided into ten categories, which can be inferred from <ref type="figure">Fig. 1</ref>. The dataset is split into 5,000 training and 8,000 test images. The STL-10 data is a popular benchmark test for image classification algorithms and challenging due to the relatively small number of training images.</p><p>For each dynamic, the network uses four ResNet blocks with 16, 32, 64, and 128 channels and image sizes of 96 ? 96, 48 ? 48, 24 ? 24, 12 ? 12, respectively. Within the ResNet blocks, we perform three time steps with a step size of ?t = 1 and include a total variation normalization layer and ReLU activation. This architecture leads to 324,794 trainable weights for the Hamiltonian network and 618,554 weights for the parabolic and secondorder network, respectively. We note that our network are substantially smaller than commonly used ResNets. For example, the architectures in <ref type="bibr" target="#b8">[9]</ref> contain about 2 million parameters. Reducing the number of parameters is important during training and, e.g., when trained networks have to be deployed on devices with limited memory. The regularization parameters are ?1 = 4 ? 10 ?4 and ?2 = 1 ? 10 ?4 .</p><p>To show how the generalization improves as more training data becomes available, we train the network with an increasing number of examples that we choose randomly from the training dataset. We also randomly sample 1,000 examples from the remaining training data to build a validation set, which we use to monitor the performance after each full epoch. We use no data augmentation in this experiment. In all cases, the training accuracy was close to 100%. After the training, we compute the accuracy of the networks parameterized by the weights that performed best on the validation data for all the 8,000 test images; see <ref type="figure">Fig. 2</ref>. The predictions of the three networks may vary for single examples without any apparent pattern (see also <ref type="figure">Fig. 1</ref>). However, overall their performance and convergence are comparable which leads to similarities in the confusion matrices; see <ref type="figure">Fig. 3</ref>.</p><p>To show the overall performance of the networks, we train the networks using a random partition of the examples into 4,000 These results are competitive with the results reported, e.g., in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14]</ref>. Fine-tuning of hyperparameters such as step size, number of time steps, and width of the network may achieve additional improvements for each dynamic. Using these means and performing training on all 5,000 images we achieved a test accuracy of around 85% in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Results for CIFAR 10/100. For an additional comparison of the proposed architectures we use the CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b27">[28]</ref>. Each of these datasets consists of 60,000 labeled RGB images of size 32 ? 32 that are chosen from the 80 million tiny images dataset. In both cases, we use 50,000 images for training and validation and keep the remaining 10,000 to test the generalization of the trained weights. While CIFAR-10 consists of 10 categories the CIFAR-100 dataset contains 100 categories and, thus, classification is more challenging.</p><p>Our architectures contain three blocks of parabolic or hyperbolic networks between which the image size is reduced from 32 ? 32 to 8 ? 8. For the simpler CIFAR-10 problem, we use a narrower network with 32, 64, 112 channels while for the CIFAR-100 challenge we use more channels (32, 64, and 128) and add a final connecting layer that increases the number of channels to 256. This leads to networks whose number of trainable weights vary between 264,106 and 652,484; see also <ref type="table">Table 1</ref>. As regularization parameters we use ?1 = 2 ? 10 ?4 and ?2 = 2 ? 10 ?4 , which is similar to <ref type="bibr" target="#b8">[9]</ref>.</p><p>As for the STL-10 data set, the three proposed architectures achieved comparable results on these benchmarks; see convergence plots in <ref type="figure" target="#fig_0">Figure 4</ref> and test accuracies in <ref type="table">Table 1</ref>. In all cases, the training loss is near zero after the training. For these datasets, the second-order network slightly outperforms the other networks. Additional tuning of the learning rate, regularization parameter, and other hyperparameters may further improve the results shown here. Using those techniques architectures with more time-steps and the entire training dataset we achieved about 5% higher accuracy on CIFAR-10 and 9% higher accuracy on CIFAR-100 in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Outlook</head><p>In this paper, we establish a link between deep residual convolutional neural networks and PDEs. The relation provides a general framework for designing, analyzing, and training those CNNs. It also exposes the dependence of learned weights on the image resolution used in training. Exemplarily, we derive three PDE-based network architectures that are forward stable (the parabolic network) and forward-backward stable (the hyperbolic networks).</p><p>It is well-known that different types of PDEs have different properties. For example, linear parabolic PDEs have decay properties while linear hyperbolic PDEs conserve energy. Hence, it is common to choose different numerical techniques for solving and optimizing different kinds of PDEs. The type of the underlying PDE is not known a-priori for a standard convolutional ResNet as it depends on the trained weights. This renders ensuring the stability of the trained network and the choice of adequate time-integration methods difficult. These considerations motivate us to restrict the convolutional ResNet architecture a-priori to discretizations of nonlinear PDEs that are stable.</p><p>In our numerical examples, our new architectures lead to an adequate performance despite the constraints on the networks. In fact, using only networks of relatively modest size, we obtain results that are close to those of state-of-the-art networks with a considerably larger number of weights. This may not hold in general, and future research will show which types of architectures are best suited for a learning task at hand. Our intuition is that, e.g., hyperbolic networks may be preferable over parabolic ones for image extrapolation tasks to ensure the preservation of edge information in the images. In contrast to that, we anticipate parabolic networks to perform superior for tasks that require filtering, e.g., image denoising.</p><p>We note that our view of CNNs mirrors the developments in PDE-based image processing in the 1990s. PDE-based methods have since significantly enhanced our mathematical understanding of image processing tasks and opened the door to many popular algorithms and techniques. We hope that continuous models of CNNs will result in similar breakthroughs and, e.g., help streamline the design of network architectures and improve training outcomes with less trial and error.  <ref type="figure">Figure 3</ref>: Confusion matrices for classifiers obtained using the three proposed architectures (row-wise) for an increasing number of training data from the STL-10 dataset (column-wise). The (i, j)th element of the 10 ? 10 confusion matrix counts the number of images of class i for which the predicted class is j. We use the entire test data set, which contains 800 images per class. <ref type="table">Table 1</ref>: Summary of numerical results for the STL-10, CIFAR-10, and CIFAR-100 datasets. In each experiment, we randomly split the training data into 80% used to train the weights and 20% used to validate the performance after each epoch. After training, we compute and report the classification accuracy and the value of cross entropy loss (in brackets) for the test data. We evaluate the performance using the weights with the best classification accuracy on the validation set. We also report the number of trainable weights for each network. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>number of test data (8,000) number of test data (10,000) number of test data (10Performance of the three proposed architectures for the CIFAR-10 (top) and CIFAR-100 (bottom) datasets. Validation accuracy computed on 10,000 randomly chosen images is shown at every epoch of the stochastic gradient descent method. In this example, all architectures perform comparably with the second-order network slightly outperforming the parabolic and first-order hyperbolic architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>?) accurately predicts the result of an observed phenomenon (e.g., the class of an image,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Parabolic CNN</cell><cell>Hamiltonian CNN</cell><cell>second-order CNN</cell><cell>true label</cell><cell></cell><cell>Parabolic CNN</cell><cell>Hamiltonian CNN</cell><cell>second-order CNN</cell><cell>true label</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>airplane</cell></row><row><cell>Parabolic CNN</cell><cell>Hamiltonian CNN Parabolic CNN</cell><cell>second-order CNN Hamiltonian CNN</cell><cell>Parabolic CNN true label second-order CNN</cell><cell>Hamiltonian CNN</cell><cell>true label Parabolic CNN</cell><cell>second-order CNN airplane bird car cat deer dog horse monkey ship truck Hamiltonian CNN</cell><cell cols="4">airplane bird car cat deer dog horse monkey ship truck true label Hamiltonian CNN second-order CNN true label Parabolic CNN Hamiltonian CNN airplane bird car cat deer dog horse monkey ship truck second-order CNN Parabolic CNN true label second-order CNN</cell><cell>airplane bird car cat deer dog horse monkey ship truck true label</cell><cell>Hamiltonian CNN Hamiltonian CNN horse Parabolic CNN Parabolic CNN bird car cat deer dog monkey second-order CNN ship truck</cell><cell>true label second-order CNN</cell><cell>true label</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Different types of networks can be obtained by considering hyperbolic PDEs. In this section, we present two CNN architectures that are inspired by hyperbolic systems. A favorable feature of hyperbolic equations is their reversibility. Reversibility allows us to avoid storage of intermediate network states, thus achieving higher memory efficiency. This is particularly important for very deep networks where memory limitation can hinder training (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>,000 validation data. For data augmentation, we use horizontal flipping and random cropping. The performance of the networks on the validation data after each epoch can be seen in the bottom plot ofFig. 2. As before, the optimization found weights that almost perfectly fit the training data. After the training, we compute the loss and classification accuracy for all the test images. For this example, the parabolic and Hamiltonian network perform slightly superior to the secondorder network 77.0% and 78.3% vs. 74.3% test classification accuracy, respectively. It is important to emphasize that the Hamiltonian network achieves the best test accuracy using only about half as many trainable weights as the other two networks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">A. Generalization for partial data</cell></row><row><cell></cell><cell>80</cell><cell></cell></row><row><cell>% test accuracy</cell><cell>40 50 60 70</cell><cell></cell></row><row><cell></cell><cell cols="3">30 10 20 30 40 50 60 70 80 % training data</cell></row><row><cell></cell><cell></cell><cell cols="2">B. Convergence for full data</cell></row><row><cell></cell><cell>80</cell><cell></cell></row><row><cell>% test accuracy</cell><cell>50 60 70 40</cell><cell></cell><cell>Parabolic CNN Hamiltonian CNN</cell></row><row><cell></cell><cell>30</cell><cell>20</cell><cell>40 epochs 60 Second-order CNN 80 100</cell></row><row><cell cols="4">Figure 2: Performance of the three proposed architectures for the</cell></row><row><cell cols="4">STL-10 dataset. Top: Improvement of test accuracy when</cell></row><row><cell cols="4">increasing the number of training images (10% to 80% in</cell></row><row><cell cols="4">increments of 10%). Bottom: Validation accuracy on re-</cell></row><row><cell cols="4">maining 20% of training examples at every epoch of the</cell></row><row><cell cols="4">stochastic gradient descent method. In this example, the</cell></row><row><cell cols="4">parabolic and first-order hyperbolic architectures outper-</cell></row><row><cell cols="4">form the second-order network.</cell></row><row><cell>training and 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements L.R. is supported by the U.S. National Science Foundation (NSF) through awards DMS 1522599 and DMS 1751636 and by the NVIDIA Corporation's GPU grant program. We thank Martin Burger for outlining how to show stability using monotone operator theory and Eran Treister and other contributors of the Meganet package. We also thank the Isaac Newton Institute (INI) for Mathematical Sciences for support and hospitality during the programme on Generative Models, Parameter Learning and Sparsity (VMVW02) when work on this paper was undertaken. INI was supported by EPSRC Grant Number: LNAG/036, RG91310.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximation of Functionals Depending on Jumps by Elliptic Functionals via Gamma-Convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Tortorelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="999" to="1036" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Numerical methods for Evolutionary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>SIAM, Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Numerical Solution of Boundary Value Problems for Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mattheij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>SIAM, Philadelphia, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI. Found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Long-Term Dependencies with Gradient Descent Is Difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bloemen Waanders, editors. Real-time PDE-constrained Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ghattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinkenschloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="issue">SIAM</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computational optimization of systems governed by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borz?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on AI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlier</surname></persName>
		</author>
		<idno>1704.04932</idno>
		<title level="m">Deep Relaxation: Partial Differential Equations for Optimizing Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07366</idno>
		<title level="m">Neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Analysis of Single-Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Clustering for Unsupervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Proposal on Machine Learning via Dynamical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv Neural Inf Process Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2211" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Explaining and Harnessing Adversarial Examples. arXiv.org</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Probl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning across scales -A multiscale method for convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
		<idno>abs/1703.02009</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on AI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deblurring Images: Matrices, Spectra and Filtering. Matrices, Spectra, and Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>O&amp;apos;leary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>SIAM, Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A general framework for constrained bayesian optimization using information-based search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2" to="51" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Algorithms for PDE-constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="163" to="176" />
		</imprint>
	</monogr>
	<note type="report_type">GAMM-Mitteilungen</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Determining optical flow. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">10971105</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page">255258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">253256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum principle based algorithms for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5998" to="6026" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">FAIR: Flexible Algorithms for Image Registration. Fundamentals of Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modersitzki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>SIAM, Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>openaccess.thecvf.com</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal Approximations by Piecewise Smooth Functions and Associated Variational-Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="685" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepxplore: Automated whitebox testing of deep learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Symposium on Oper. Sys. Princ</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual International Conference</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moodie</surname></persName>
		</author>
		<title level="m">Wave Phenomena: Modern Theory and Applications. North-Holland Mathematics Studies</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The perceptron: A probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="386" to="408" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonlinear Total Variation Based Noise Removal Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Variational methods in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Scherzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grasmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grossauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haltmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lenzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Visual Representation Learning with Target Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L C C L W K S X T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on AI</title>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="3848" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Anisotropic Diffusion in Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
