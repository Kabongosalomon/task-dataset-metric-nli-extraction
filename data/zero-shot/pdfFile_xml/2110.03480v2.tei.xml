<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Regress Bodies from Images using Differentiable Semantic Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Kumar Dwivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
							<email>nathanasiou@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
							<email>mkocabas@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Regress Bodies from Images using Differentiable Semantic Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Differentiable Semantic Rendering (DSR). A state-of-the-art approach [14] (purple) fails to estimate accurate 3D pose and shape for in-the-wild scenarios. We address this by exploiting the clothing semantics of the human body. Our approach, DSR, (blue) captures more accurate 3D pose and shape compared to previous work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or partsegmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel "Differentiable Semantic Rendering (DSR)" loss. For Minimally-Clothed (MC) regions, we define the DSR-MC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D human pose and shape from in-the-wild images has received great research interest <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b52">54]</ref> because of its varied applications in animation, games, and the fashion industry. One aspect that makes this problem challenging is the difficulty of obtaining accurate 3D ground-truth annotations, as they require either specialized -mostly indoors-MoCap systems or careful calibration and setup of IMU sensors <ref type="bibr" target="#b44">[46]</ref>. Such data would facilitate training robust regressors paving the way for estimating human-scene interaction with greater granularity.</p><p>Given the lack of in-the-wild 3D ground-truth, the vast majority of previous methods focus on 2D keypoints <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b12">14]</ref> with some learned 3D priors. Even though sparse 2D keypoints give useful constrained, relying only on these leads to unrealistic poses because of depth ambiguities and occlu-sion. They also do not provide reliable information about body shape. On the other hand, relying too strongly on 3D priors introduces bias. To circumvent this problem, recent approaches <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b48">50]</ref> propose to use part-segmentations or silhouettes. However, there is a mismatch between partsegmentations/silhouettes and projected SMPL bodies since segmentation covers clothed bodies while the common 3D body models are minimally clothed. We propose an alternative approach to compensate for limited 3D supervision that leverages high-level 2D image cues.</p><p>Specifically, we propose more detailed clothing segmentation labels to supervise a neural network. Traditional multi-class clothing segmentation approaches cannot be directly applied as the segmentation loss tries to exactly match the rendered SMPL body. Hence, to make use of such labels, we need to reason about which parts of the SMPL body model correspond to which clothing label. This is non-trivial to obtain because a body part can be covered by many clothing types. Therefore, we learn a semantic clothing prior from a large-scale clothed human scan dataset, which has varied subjects, poses and camera views to which the SMPL body is fitted <ref type="bibr" target="#b29">[31]</ref>. This prior encodes the likelihood of clothing types given a vertex on the SMPL body model, which gives the correspondence between segmentation labels and the SMPL body surface. Then, we use this prior to calculate a loss between the SMPL body and observed clothing labels in images. To achieve this we introduce Differentiable Semantic Rendering (DSR), a novel loss that supervises the training of 3D body regression with clothing semantics using weak supervision <ref type="bibr" target="#b6">[8]</ref>.</p><p>Our novel loss has two components: DSR-C for supervising the clothing region and DSR-MC for the minimallyclothed region. A high-level illustration of our idea is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. While the former ensures that the rendered SMPL mesh stays inside the observed clothing label, the latter tries to tightly match the rendered SMPL mesh to the 2D minimal-clothing mask. The loss between the rendered output and the target mask is back-propagated using a differentiable renderer. Specifically, for the DSR-MC term, we apply pixel-level supervision for tight-fitting with the minimal-clothing regions, while for DSR-C, we minimize the negative log probability of a SMPL semantic part label being inside the respective segmentation mask. For example, there will be a high penalty if the rendered vertices with a high probability of being "shirt" fall in the "pants" segmentation pixels. To ensure that our method is fast and differentiable, we render the semantic class probabilities computed from 3D scans as textures of the SMPL mesh.</p><p>While training, DSR can be used as an additional loss in any neural network-based human body estimator that predicts SMPL parameters. First, we examine the effect of our approach over a baseline full-body mask supervision and 3D joint only supervision which verifies our hypothe- sis about the value of clothing semantics. Then, we perform extensive comparisons and show that DSR outperforms previous state-of-the-art methods as shown in <ref type="figure" target="#fig_2">Fig. 1</ref>. In summary:</p><p>1. We explore the importance of clothing semantics for 3D human body estimation by introducing a novel differentiable semantic rendering loss that distinguishes between clothed and minimally-clothed regions. 2. We estimate a semantic clothing prior for SMPL from 3D scans of clothed people for our method which can be used also for other cases when a vertex clothing probability for a 3D SMPL body is required. 3. We outperform all state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP, suggesting the value of using human parsing and semantics for more accurate human body estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Estimating human pose and shape is a vastly growing field using different sources of supervision and input (image, video, keypoints, etc.). Here, we focus on different works that estimate 3D human pose and shape from an RGB image. We refer to recent surveys <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b36">38]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image cues and 2D/3D joints</head><p>Towards estimating 3D human pose and shape, initial attempts focus on estimating the coordinates of 3D joints or heatmaps <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">[39]</ref><ref type="bibr" target="#b38">[40]</ref><ref type="bibr" target="#b39">[41]</ref><ref type="bibr" target="#b41">43]</ref> from images using geometric assumptions for the human body and 3D training data. However, those approaches require 3D ground-truth data, which are limited in terms of pose variation, quantity and background, and lack generalization to in-the-wild images. The vast progress in 2D pose detection <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b47">49]</ref>, along with the introduction of parametric body models of pose and shape <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b22">24]</ref> lead to significant progress and highquality in-the-wild 3D humans. In <ref type="bibr" target="#b3">[5]</ref> the authors use 2D keypoints to obtain SMPL parameters with an optimizationbased approach, while this process improves via human annotations on predicted fits <ref type="bibr" target="#b18">[20]</ref>. Martinez et al. <ref type="bibr" target="#b25">[27]</ref> show that lifting the predictions of a 2D keypoint detector provides a reasonable baseline for the 3D pose. Pavlakos et al. <ref type="bibr" target="#b30">[32]</ref> use additional ordinal depth annotations for weak 3D supervision. Kolotouros et al. <ref type="bibr" target="#b17">[19]</ref> regress vertex locations using a sub-sampled SMPL mesh and Graph-CNNs. Xiang et al. <ref type="bibr" target="#b45">[47]</ref> extract joint confidence maps and 3D orientation information via CNNs and pair them with a deformable body model. Furthermore, in HMR <ref type="bibr" target="#b13">[15]</ref>, a regressor from 2D joints to SMPL parameters is trained, using a discriminator with unpaired 3D data <ref type="bibr" target="#b24">[26]</ref> to encourage plausible poses. Along these lines, some recent approaches using video as input, have applied similar methods to predict temporal kinematics of 3D bodies <ref type="bibr" target="#b14">[16]</ref> and estimate the body using temporal features and a motion discriminator <ref type="bibr" target="#b15">[17]</ref>. Another approach <ref type="bibr" target="#b49">[51]</ref>, uses a disentanglement of the skeleton from the 3D human mesh paired with a selfattention network to ensure temporal coherence. SPIN <ref type="bibr" target="#b16">[18]</ref>, revisits optimizations methods in collaboration with neural networks as it uses a network <ref type="bibr" target="#b13">[15]</ref> that provides an initial estimate to the optimization process (SMPLify). Moreover, a regressor-based alternative suggests the use of the 3D neural regressor as a pose prior <ref type="bibr" target="#b12">[14]</ref>. Although such methods produce promising results, they typically estimate average body shapes, are not robust to occlusion, and produce poses that are only approximate. Without 3D training data the problem is hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image alignment and pixel-level supervision</head><p>Concurrently, there is a line of research that uses additional constraints, in addition to image features and 2D/3D joints, to better align the body with the image such as dense body landmarks, silhouettes, body part segmentation or pixel aligned implicit functions. Initial seminal lines of work, use a few keypoints along with the SCAPE body model and optimize 3D body shape with silhouettes and smooth shading <ref type="bibr" target="#b7">[9]</ref>. Along these lines, Balan et al. <ref type="bibr" target="#b2">[4]</ref> propose a distance function for the connected silhouette to ensure the rendered 3D model falls inside the mask. Later work uses 2D keypoints, background segmentation, and SMPL to extract 3D bodies from images <ref type="bibr" target="#b42">[44]</ref>, similar to <ref type="bibr" target="#b32">[34]</ref> who use silhouettes for supervision. Even silhouettes, although they provide supervision when keypoints fail, are often ambiguous in the case of self-occlusion. Towards a detailed alignment of the 3D human body surface and pixels the authors in <ref type="bibr" target="#b8">[10]</ref> introduce a dataset with image-tosurface correspondence from MS-COCO <ref type="bibr" target="#b20">[22]</ref> and a variant of Mask-RCNN that regresses UV coordinates from images. Part-segmentation masks and IUV are also used in <ref type="bibr" target="#b28">[30]</ref> and <ref type="bibr" target="#b52">[54]</ref>, respectively, as dense supervision. A continuous UV map of SMPL for direct pixel correspondence of the image and the 3D mesh is introduced in <ref type="bibr" target="#b52">[54]</ref>. In a similar approach <ref type="bibr" target="#b48">[50]</ref>, exploits IUV maps as a proxy representation. It estimates SMPL parameters by minimising dense body landmarks and human part masks and also by using motion discriminator. While a large majority of the aforementioned work leverages a parametric 3D body model, there is some recent work that uses voxel representations along with 2D pose and part segmentation supervision <ref type="bibr" target="#b43">[45]</ref> or employs implicit functions with surface reconstruction techniques to reconstruct clothed humans. Although these approaches output fine-grain details, they are unable to capture the shape under clothing and are prone to occlusion <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37</ref>]. An interesting approach is proposed in <ref type="bibr" target="#b53">[55]</ref> where a partial UV map of the person is used and the human pose estimation is formulated as an image inpainting problem. Another work that explores scene semantics <ref type="bibr" target="#b33">[35]</ref>, predicts the label of an occluding object and employs this information to detect invisible joints. Finally, Zanfir et al. <ref type="bibr" target="#b50">[52]</ref> represent the body with a normalizing flows-based latent space and use body part segmentation supervision to estimate 3D human body pose from videos and images, unifying different previous approaches. Clothing segmentation is used in <ref type="bibr" target="#b46">[48]</ref> for clothing deformation to penalize the vertex offset of the clothed body if the rendered vertex falls outside the clothing boundary.</p><p>Most of these approaches are based on joints, silhouettes and part-segmentation masks using approximate supervision for the pose of a person in clothing. We claim that there is more that an image can tell us about human pose. Our key insight is that clothing for different parts of the body conveys important information for detailed fitting. We employ an off-the-shelf 2D semantic segmentation method <ref type="bibr" target="#b6">[8]</ref> and a semantic clothing prior to apply these labels in 3D. Given those, we supervise clothed and minimal-clothed regions separately, yielding more aligned fits of 3D humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>DSR uses high-level semantic information for more accurate pose and shape estimation using two additional loss terms DSR-MC and DSR-C, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. DSR takes an image I as input which passes through a CNN. Then, the image features ?(I) are fed to an iterative regressor, similar to HMR <ref type="bibr" target="#b13">[15]</ref>, to estimate the parameters of SMPL body model,?. Given the rendered SMPL mesh, we apply our novel DSR-MC and DSR-C losses in addition to standard loss terms used in EFT <ref type="bibr" target="#b12">[14]</ref>. SMPL is a parametric body model that represents body pose and shape by ? = [? ? R 72 , ? ? R 10 ]. The pose parameters ? include the global rotation and rotations of 23 body joints in axisangle format and the shape parameters ? consist of the first 10 coefficients of a PCA shape space. SMPL model is a differentiable function M(?, ?) ? R 6890?3 that outputs a 3D mesh according to the pose and shape parameters. Clothing Semantic Information. Ground-truth clothing segmentations are expensive to obtain for in-the-wild datasets, which limits the scalability of such an approach. Hence, to analyze the importance of clothing semantics for human pose and shape estimation, we employ an off-theshelf segmentation model to generate pseudo ground-truth clothing semantics. Graphonomy <ref type="bibr" target="#b6">[8]</ref> is a state-of-the-art clothing segmentation model that uses inter and intra graph transfer learning for unifying different clothing datasets and produces 20 clothing labels and body part segmentations. As DSR-MC reasons about the minimal-clothing region, we use a binary mask comprised of 5 labels -LeftArm, RightArm, LeftShoe, RightShoe and Face from Graphonomy as ground-truth (whenever available). For DSR-C, we use 4 labels -UpperClothes, LowerClothes, Minimal-Clothing and Background. We run the Universal Model of Graphonomy on all the datasets to generate pseudo-truth clothing segmentations. For more details on the generation of pseudo-ground truth, cleaning of obtained masks and mapping of graphonomy labels for DSR-C and DSR-MC, please refer to the Sup. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Render People</head><p>Semantic Prior for SMPL. To use the semantic information obtained from Graphonomy as pseudo ground-truth training labels, we need a semantic prior of clothing for SMPL 3D bodies. To achieve this, we use thousands of scans from Renderpeople [1] with varied clothing, subject, pose and 10 camera views for which we have ground-truth SMPL fits from AGORA <ref type="bibr" target="#b29">[31]</ref>. We run the universal model of Graphonomy on the rendered images of the scan with 20 clothing and body part segmentation labels. Next, we use the ground-truth SMPL mesh to compute the visible face triangles given the mesh and camera parameters. Then, each visible triangular face is assigned the corresponding segmentation label. We repeat this process for all the available scans. We compute the probability of each vertex being a particular label out of the 20 labels from Graphonomy. This probabilistic label for each vertex is referred to as semantic prior. For more details refer to Sup. Mat.</p><p>Differentiable Semantic Rendering. We use Soft-Ras <ref type="bibr" target="#b21">[23]</ref> as the differentiable renderer to supervise the estimation of the 3D parametric model using semantic information. It uses a differentiable aggregation process for rendering, which fuses the probabilistic contributions of all mesh triangles with respect to rendered pixels. The semantic prior obtained from AGORA <ref type="bibr" target="#b29">[31]</ref> is used as a texture. Specifically, for each semantic label, we render the probability of that label for each visible vertex. Once the semantic probability is rendered as images by SoftRas, the loss is computed on the 2D image output by comparing with the semantic image segmentation and this is backpropagated to change the vertices, in turn, changing the network to give more accurate SMPL parameters.</p><p>Standard Losses. As we use the EFT <ref type="bibr" target="#b12">[14]</ref> data for training, we use the standard supervision loss L SD similar to EFT which is defined as:</p><formula xml:id="formula_0">L 2D (?(M(?)), j) + L 3D (M(?), J) + L ? (?,?) (1)</formula><p>where,? are the estimated SMPL parameters, L 2D is the joint reprojection loss, L 3D and L ? are losses on 3D joints and SMPL parameters, respectively. Ground truth 2D joints are represented by j, 3D joints by J, SMPL parameters by ? and the camera projection function by ?.</p><p>DSR -Minimal-Clothing. For minimal-clothing, we choose five labels from Graphonomy namely, LeftArm, RightArm, LeftShoe, RightShoe and Face, which often appear similar in shape to the rendered SMPL body; i.e. look roughly "naked." For a particular image, we take the clothing segmentation mask given by Graphonomy and create a binary mask G comprising of the valid labels for that image from the available five labels. This forms the ground-truth for DSR-MC denoted by GT -DSR-MC in <ref type="figure" target="#fig_0">Fig. 3</ref>. We render the probability distribution of vertex labels for SMPL precomputed from RenderPeople as textures; these are shown as Rendered Semantics in <ref type="figure" target="#fig_0">Fig. 3</ref>. We only take the probability distribution of vertices that are visible and set the others as zero. Thus, we define the DSR-MC loss to tightly match the corresponding rendered minimal-clothing region of SMPL to the available semantic binary mask as shown in <ref type="figure" target="#fig_0">Fig. 3 (bottom)</ref>.</p><p>We study two variants of the loss for DSR-MC: soft-DistM and soft-IOU. Soft-DistM is inspired by the DistM loss of Naked Truth <ref type="bibr" target="#b2">[4]</ref> which was originally proposed for estimating body shape under-clothing. Since we render the semantic probability instead of silhouettes, we call it soft-DistM. It is a distance measure function that takes the rendered image R and target binary Graphonomy mask G and is defined as:</p><formula xml:id="formula_1">L M C?sDistM = i,j (R i,j ? d i,j (G))/( i,j R i,j ) 3/2<label>(2)</label></formula><p>where R i,j are the pixels inside rendered human body and d i,j is a distance function which is zero if pixel (i, j) is inside G. For points outside, it is defined as the Euclidean distance to the closest point on the boundary of G. Soft-DistM can pull the output inside the target because of the sharp difference in penalization between pixels inside the mask and pixels outside. Given a good initial estimate, the Soft-DistM loss ignores spurious and scattered labels outside the region of interest because the loss is high for pixels far away. This is particularly helpful, when using an off-the-shelf segmentation model without instance segmentation, which can give the wrong output for hard examples.</p><p>However, soft-DistM cannot fully ensure that the rendered output exactly matches the target as it gives the same penalty for outputs with different percentages of overlap when it is inside the boundary. Hence, we studied soft-IoU, which ensures tight fitting and is calculated as:</p><formula xml:id="formula_2">L M C?sIOU = 1 N (i,j) P i,j ? G i,j (i,j) P i,j + G i,j ? P i,j ? G i,j<label>(3)</label></formula><p>where P i,j is the rendered vertex probability at pixel (i, j), G i,j is the graphonomy label for that pixel. Soft-IoU suffers from spurious and scattered labels outside the region of interest and also suffers from the lack of instance segmentation in the off-the-shelf model. However, we choose soft-IoU for the metric for DSR-MC due to better quantitative results in the baseline experiments in <ref type="table" target="#tab_2">Table 1</ref>. DSR -Clothing. The rendered SMPL body mesh cannot exactly match all the target pixels for the clothing region. Hence, for a more accurate estimate of the 3D body model, we want to encourage the rendered SMPL mesh to stay inside the clothing mask. Previous methods <ref type="bibr" target="#b2">[4]</ref> define a distance function to deal with such scenarios. However, we have higher-level semantic information than a silhouette to better address this. We have additional boundaries other than the body outline to enforce that a particular semantic part of the SMPL mesh should fall inside the corresponding semantic part of the segmentation mask. Clothing segmentation provides additional boundaries, such as between the upper and lower body or between clothing and skin.</p><p>Specifically, we define four labels, UpperClothes, Low-erClothes, MinimalClothing and Background, shown as four color masks in <ref type="figure" target="#fig_0">Fig. 3 (top)</ref>. We introduce a Minimal-Clothing label for DSR-C to avoid confusion between the background and minimal-clothing region. Without it, the DSR-C loss would give the same penalty when the minimalclothing region falls on the corresponding target region or the background. As the semantic prior learned from Ren-derPeople has 20 probability labels per vertex, we add all the probabilities of upper body clothing labels for Upperclothes, lower body clothing labels for LowerClothes and body part segmentation labels for MinimalClothing. We define DSR-C loss as the negative log-likelihood (NLL) of the rendered probability distribution of each vertex belonging to one of the four labels. The rendered probability distribution is first sent through log softmax before applying NLL loss for numerical stability. So, L DSR?C is defined as</p><formula xml:id="formula_3">L DSR?C = W i=1 H j=1 ?log(y i,j )<label>(4)</label></formula><p>where y i,j is the probability output for the vertex at pixel (i, j), H is the height and W is the width of the image. Hence, the total loss L total = L SD + L M C?sIOU + L DSR?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joints</head><p>Joints + DSR <ref type="figure" target="#fig_2">Figure 4</ref>: Are 3D joints enough? We over-fit a batch of H36M samples on ground-truth (GT) joints (green) and joints with DSR (blue). The weak supervision with semantic information improves accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Training Procedure. Following EFT <ref type="bibr" target="#b12">[14]</ref>, we train a regressor similar to HMR <ref type="bibr" target="#b13">[15]</ref> with mixed 3D and 2D datasets. We use the pseudo-ground 3D annotations for 2D datasets from EFT. For 2D data, we only use COCO <ref type="bibr" target="#b20">[22]</ref> as including other in-the-wild datasets did not give a performance gain and for 3D datasets, we use Human3.6M <ref type="bibr" target="#b11">[13]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b26">[28]</ref>. We also use the 3DPW <ref type="bibr" target="#b44">[46]</ref> training set for fair comparisons and the same data ratio for mixed 2D and 3D datasets as EFT. For baseline and ablation experiments, we train only on COCO-EFT <ref type="bibr" target="#b12">[14]</ref>. For faster training, we initialize the network with SPIN pre-trained weights and use the same hyper-parameters as SPIN <ref type="bibr" target="#b16">[18]</ref> and train the model for 100K iterations.</p><p>Evaluation Procedure. For state-of-the-art comparisons, we use 3DPW <ref type="bibr" target="#b19">[21]</ref>, Human3.6M <ref type="bibr" target="#b11">[13]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b26">[28]</ref>. As in prior work <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b16">18]</ref>, we use the gender information for ground truth meshes on 3DPW. We report results with and without 3DPW training on Procrustesaligned mean per joint position error (PA-MPJPE), mean per joint position error (MPJPE) and Per Vertex Error (PVE).</p><p>Differentiable Semantic Rendering. We use Soft-Ras <ref type="bibr" target="#b21">[23]</ref> to render the probability distribution for DSR-C and DSR-MC. For SoftRas, we use a higher gamma value of 1.0 ? 10 ?1 to ensure the loss affects the occluded part of the body and a lower sigma value of 1.0 ? 10 ?5 to ensure the error does not significantly affect the spatial region. For more details, refer to SoftRas <ref type="bibr" target="#b21">[23]</ref>. We render the probability distribution of each triangle face as textures and compute the loss on the RGB channel of the rendered output. We render 5 images for each sample in a batched manner: 1 for DSR-MC and 4 for DSR-C. However, the loss is calculated per individual sample to avoid calculating for samples that do not have a valid segmentation mask. In such cases,  the loss is set to zero. After using the heuristics to clean the mask, a valid label set is created for DSR-C and DSR-MC. The weighting parameters for both the components are set to 0.01. As DSR depends on weak supervision from off the shelf clothing segmentation model and hence not robust for hard examples, we enable the loss after 10K iterations into our training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline Comparison and Ablation Studies</head><p>We perform baseline experiments to (1) motivate the use of semantic rendering and (2) study how the different terms and design choices contribute to the final result as shown in <ref type="table" target="#tab_2">Table 1</ref>. As a baseline, we use an HMR <ref type="bibr" target="#b13">[15]</ref> based regressor trained on EFT-COCO <ref type="bibr" target="#b12">[14]</ref> data and report results on 3DPW (C-EFT). Then, we supervise the baseline with an additional full-body silhouette (DSR-FB) which is a per pixel binary classification loss guided by differentiable rendering. The results deteriorate as the rendered SMPL body does not match the full body. We further train DSR-FB with the Dist-M loss in contrast to per-pixel classification to ensure all body parts (irrespective of clothing) stay inside the silhouette. The result in <ref type="table" target="#tab_2">Table 1</ref> shows that explicit supervision with clothing semantics (Ours) outperforms the naive cloth-agnostic approach. We study the importance of estimating clothing semantics from scans in contrast to manual vertex painting (MVP) of semantic labels as the former gives a distribution over possible clothing labels <ref type="bibr" target="#b18">(20)</ref> for each vertex whereas the latter would give only 1. To quantitatively verify the benefit of the probabilistic clothing semantic prior, we take the most likely label per vertex <ref type="figure" target="#fig_2">(Fig. 2)</ref> as a proxy for MVP. Since we have 1 label per vertex, we use IoU instead of s-IoU. <ref type="table" target="#tab_2">Table 1</ref> shows low performance of a fixed semantic prior (MVP) compared to a probabilistic one (Ours). We also study the individual contribution of DSR-C and DSR-MC on the overall performance and find that the clothing term helps more than the minimal-clothing   term. One possible explanation could be that the off-theshelf segmentation model is not robust for hands and feet hence causing less gain. Empirically, we observe that soft-IoU performs better than soft-DistM and hence use it as the metric for DSR-MC for all subsequent experiments. Overall, the best accuracy is reached when both terms are used showing that supervising minimally-clothed and clothed regions differently helps improve 3D body estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">State-of-the-art comparison</head><p>We compare our approach with state-of-the-art methods in <ref type="table" target="#tab_4">Table 2</ref>. We use two variants of our model, with and without the 3DPW training set, to be aligned with the training data of other methods. In 3DPW, an in-the-wild challenging 3D dataset, we outperform previous work when using 3DPW training data, while performing on par with EFT <ref type="bibr" target="#b12">[14]</ref> when they are not used. Moreover, we clearly improve accuracy on Human3.6M <ref type="bibr" target="#b11">[13]</ref>, a standard indoor benchmark, over state-of-the-art SPIN <ref type="bibr" target="#b16">[18]</ref> and EFT <ref type="bibr" target="#b12">[14]</ref> methods. We also report on par results in MPI-INF-3DHP <ref type="bibr" target="#b0">[2]</ref>. We perform significantly better than previous approaches that use ground-truth part-segmentation or silhouettes <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b50">52]</ref> compared to our weak supervision. Overall, we consistently perform better than previous approaches across different datasets, both indoors and outdoors. In <ref type="figure" target="#fig_2">Fig. 5</ref> we can see different comparisons of DSR with the previous state-of-  <ref type="table">Table 4</ref>: Per joint error for Human3.6M subset. SD refers to standard joint loss used in 3D body estimation.</p><p>the-art and observe that the estimated mesh is more aligned with image evidence. These observations validate our hypothesis that clothing semantics, even when used as weak supervision, provides additional information for estimating more accurate 3D bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Potential of DSR</head><p>To test the significance of high-level semantics on shape and pose estimation, we use an off-the-shelf segmentation model <ref type="bibr" target="#b6">[8]</ref>. However, such models are not robust to in-thewild examples. Because we use the output of the model as pseudo ground-truth for supervision, it is hard to determine the full potential of our approach. Hence, we experiment on the Human3.6M dataset to test the DSR loss in a more controlled setting. Human3.6M is an indoor dataset with significantly less background complexity as compared to outdoor datasets. Hence, it is ideal for testing the limit of DSR. We study two different cases. First, we split the training set of Human3.6M, with SMPL parameters computed by MoSh <ref type="bibr" target="#b23">[25]</ref>, into training and validation sets with S8 in the validation set. This is done to evaluate the per-vertexerror (PVE) using the MoSh ground truth SMPL parameters, thus, giving insight into the shape estimation efficacy of our method. As shown in <ref type="table">Table.</ref> 3, the performance gain with the DSR loss is significantly higher compared to the standard joint loss. This emphasizes the importance of semantic information. We also analyze the per joint error to understand the source of a performance gain as shown in SPIN EFT DSR SPIN EFT DSR <ref type="figure" target="#fig_2">Figure 5</ref>: Qualitative Results on COCO. From left to right -Input image, SPIN <ref type="bibr" target="#b16">[18]</ref>, EFT <ref type="bibr" target="#b12">[14]</ref> and DSR results.</p><p>the <ref type="table">Table.</ref> 4. Using the DSR, the maximum performance gains are from Ankle, Knee, Wrist which are common failures in 3D pose estimation. Second, we take a step further to examine whether ground truth 3D joints are enough for accurate and pixel aligned body estimation. To this end, we take a random batch of 64 samples from Human3.6M and over-fit on only joints and joints with the DSR loss for 100 iterations with the same hyper-parameters used for other experiments. The qualitative results are depicted in <ref type="figure" target="#fig_2">Fig. 4</ref>. As we can see, supervision with ground 3D joints cannot always reason about all the pixels. Using DSR produces more pixel-aligned fits, especially for hands and feet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>While huge progress has been made in estimating 3D human bodies, we are still far from estimating highly accurate 3D humans. We hypothesize that clothing semantics is an under-explored feature that can benefit 3D body estimation methods. Therefore, we introduce a novel method to exploit clothing semantics as weak supervision. Namely, we: (1) Introduce a novel differentiable loss that supervises clothed and minimally-clothed regions differently to ensure that the body lies inside the clothes for the former while tightly fitting for the latter. (2) Learn a semantic clothing prior, i.e. a probability distribution over clothing labels for SMPL vertices, to apply our method efficiently. (3) Thoroughly evaluate our approach qualitatively and quantitatively, outperforming the state-of-the-art. (4) Analyze our method's components and show that clothing semantics, even as weak supervision, is a valuable complementary cue to 3D joints that improves the estimation of 3D bodies. Our experiments show the importance of such semantics, providing new insight into 3D human body estimation.</p><p>DSR uses clothing as weak supervision, which can be limited in complex scenes with multiple people and occlusion. Our method can be easily extended to pipelines that account for multiple people in the scene <ref type="bibr" target="#b51">[53]</ref>. In the future, we should explore methods that model 3D clothing semantics, build a better prior for SMPL bodies or incorporate additional constraints to disambiguate scene semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Regress Bodies from Images using Differentiable Semantic Rendering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Clothing Semantic Information</head><p>It is difficult to obtain ground-truth clothing segmentation masks for in-the-wild datasets. Hence, we use Graphonomy <ref type="bibr" target="#b6">[8]</ref>, which is an off-the-shelf human clothing segmentation model that provides reasonably reliable pseudo-ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Clothing Segmentation Masks</head><p>Graphonomy has three different models depending on the granularity of the segmentation mask and we choose the one with 20 labels, also known as the Universal Model. This model provides the best clothing segmentation performance compared to other Graphonomy variants. The different labels are: Background, Hat, Hair, Glove, Sunglasses, Upper-Clothes, Dress, Coat, Socks, Pants, Jumpsuits, Scarf, Skirt, Face, LeftArm, RightArm, LeftLeg, RightLeg, LeftShoe and RightShoe.</p><p>During inference, to get more accurate predictions -as suggested in the original implementation -we use 4 different scaling factors for the input image -0.5, 0.75, 1.0, 1.5to account for different image resolutions. Then, we merge the outputs for different scaling factors using appropriate upsample and downsample functions (bilinear) to produce an output size the same as the original image. For images more than 1080?1080, we use a single scaling factor of 1.0. We also flip the image horizontally and average the output predictions of the flipped image with the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Processing Pseudo Ground-Truth Masks</head><p>The generated pseudo ground-truth cannot be directly used for supervising existing human body estimator networks because of incompatibility between Graphonomy's output and 3D pose regressor's training procedure <ref type="bibr" target="#b13">[15]</ref>.</p><p>Graphonomy is not an instance segmentation model, which means it is hard to differentiate between people in the image. However, standard human body estimators <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18]</ref> use a single person during training. To circumvent this problem, we use 2D keypoints to get a rough estimate of the region of the person in the image. Furthermore, we add/subtract an offset of 30 pixels in both x and y direction according to the maximum/minimum keypoint location.</p><p>Due to occlusion or inaccuracies in the prediction, the spread of pixels for a particular label of Graphonomy may  cover an extremely small part of the image. As DSR-MC tries to tightly supervise the rendered SMPL body with the target binary mask, it is important to ensure the target masks are reliable. Hence, we remove labels that cover less than 60 pixels from the predefined set of five labels (LeftArm, RightArm, LeftShoe, RightShoe, Face).</p><p>There is a one to one mapping from the DSR-MC labels to Graphonomy labels. The same is not true for DSR-C as there are several clothing labels. Consequently, for DSR-C, we define a coarse mapping as per <ref type="table" target="#tab_8">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Prior for SMPL</head><p>To supervise the human body regressor network with semantic information, we need a term that captures the a priori probability that describes what parts of the SMPL body correspond to a particular semantic label.</p><p>To this end, we use 2500 clothed human scans from the AGORA dataset <ref type="bibr" target="#b29">[31]</ref> with varied clothing, pose and identity. AGORA contains clothed 3D people with ground truth SMPL-X bodies fit to the scans. We convert SMPL-X fits to SMPL. For each scan, we render it from 10 different camera views to cover different angles and generate scan images. We run Graphonomy on each of these images to obtain 10 2D clothing segmentation images for each scan. An illustration of the output from this process is depicted in <ref type="figure" target="#fig_2">Fig. .</ref> We also render the fitted SMPL model with the known camera parameters to obtain the correspondences between the vertices of the SMPL body and the pixels in the image.</p><p>Given this training data, we can very simply compute the prior probability of a SMPL vertex having one of the 20 Graphonomy labels. We estimate this by calculating the occurrences of a particular label being present at the vertex divided by the total occurrences of other labels-excluding the Background label. Finally, this gives us the prior per-vertex probability that a SMPL vertex has given a Graphonomy label. We also assign a small probability of a vertex being assigned the background label; this increases robustness to occlusion. As an additional step, we use the SMPL body part segmentation to clean the semantic prior. Graphonomy gives incorrect predictions for some clothed body scan images and this will affect downstream tasks. Hence, if the semantic label probability of a "leg" vertex (denoted by SMPL part segmentation) has a higher probability of being hand, we set it to zero. This approach helps to avoid obvious failures when Graphonomy produces incorrect predictions. Note that a more sophisticated prior model could also capture spatial correlations of clothing but we did not find this necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure Case Analysis</head><p>We qualitatively analyse the failure cases using our method and broadly categorise them into two types: occlusion failures as shown in <ref type="figure" target="#fig_2">Fig. I</ref> and multi-person failures as shown in <ref type="figure" target="#fig_2">Fig. I</ref>. Note that these are also cases where stan-dard 3D pose estimation methods commonly fail.</p><p>First, we observe failures in case of either self-occlusion or scene occlusion producing unreasonable pose. Hence, we tried to analyse the training samples with occlusion. As we can see in <ref type="figure" target="#fig_2">Fig. I</ref>, Graphonomy outputs a black patch (Background class) when an object or the scene is occluding the person. As DSR-C tries to minimise the negative log probability of a rendered vertex being a particular label, and the background label has a low probability, occlusions can cause the pose to be incorrect. More complete labeling of things like backpacks or training with synthetic occlusion could improve this. Moreover, it can also hinder detailed fitting of the body where the labels associated with DSR-MC are occluded. Additional occlusion handling techniques could help our approach in such cases.</p><p>Furthermore, another failure case occurs when multiple people are present in a scene. As Graphonomy is not an instance segmentation network, the pseudo ground-truth data may still contain other people even after using the heuristics to clean them, as described in Section A.2. This confuses training, resulting in misaligned bodies at inference time. <ref type="figure" target="#fig_2">Figure I</ref> shows common cases where all the upper body clothing of multiple people are merged into one segment and clothing masks of partially visible people in the background, which affect the quality of the obtained masks. Our entire method could be improved by better instancelevel clothing segmentation.</p><p>Higher quality of Graphonomy masks leads to increased performance gains in the case of DSR. We demonstrate it by doing an ablation study using Human3.6M <ref type="bibr" target="#b11">[13]</ref> dataset where the Graphonomy predictions are more reliable because of the simpler background and single subject. The quantitative results of this experiment are reported in the main paper.</p><p>Overall, our performance is affected by the off-the-self model we use to supervise the clothing semantics of the person. However, improvements over the state-of-the-art show that even weak supervision of clothing semantics is crucial for detailed 3D body fits. The success of our approach suggests that more accurate human parsing and clothing segmentation are a good investment for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Qualitative Results</head><p>We show additional qualitative results comparing our method with other state-of-the-art methods <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b16">18]</ref> for 3DPW <ref type="bibr" target="#b19">[21]</ref> and COCO <ref type="bibr" target="#b20">[22]</ref> which are challenging in-thewild benchmarks for 3D human pose and shape estimation. The results are depicted in Figures I and I. Next to each example, we show the corresponding side view. We observe that our approach produces more accurate pose and shape that are better aligned with the human in the image than current SOTA approaches. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of DSR -SMPL is rendered with the semantic prior learned from RenderPeople scans. The two novel loss terms are calculated based on different semantic regions of the clothed person. DSR-MC tightly fits the minimal-clothed region, while DSR-C ensures that the rendered body lies within the clothing boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure :</head><label>:</label><figDesc>Clothed Human Scans. Examples of clothed human scans in different clothing, pose and camera views (Columns 1,3,5) along with the corresponding SMPL bodies where each vertex is colored based on the output of the clothing segmentation model [8] (Columns 2,4,6) applied on the respective scan images. We only show 3 camera views here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure I :</head><label>I</label><figDesc>Occlusion Failure Analysis Qualitative failure results in case of occlusion. We show outputs from COCO and 3DPW in Rows 1-2 respectively. Rows 3-4: Similar occlusion cases present in the training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :</head><label>4</label><figDesc>Multi-Person Failure CasesSimilar Examples from TrainingFigure I: Multi-Person Failure Analysis Qualitative failure results in case multiple people are present. We show outputs from COCO and 3DPW in Rows 1-2 respectively. Rows 3-Similar multi-person failure cases present in the training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure I :</head><label>I</label><figDesc>Additional Qualitative Results of 3DPW. From left to right -Input image, SPIN [18], SPIN Sideview, EFT [14], EFT Sideview, DSR and DSR Sideview results Image SPIN Sideview EFT Sideview DSR Sideview Figure I: Additional Qualitative Results of COCO. From left to right -Input image, SPIN [18], SPIN Sideview, EFT [14], EFT Sideview, DSR and DSR Sideview results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Baseline Comparisons for DSR on 3DPW.</figDesc><table><row><cell>C-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MPJPE ? MPJPE ? PVE ? PA-MPJPE ? MPJPE ? PA-MPJPE ? MPJPE ?</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell cols="2">Human3.6M</cell><cell cols="2">MPI-INF-3DHP</cell></row><row><cell>Models PA-HMR [15]</cell><cell>76.7</cell><cell>130.0</cell><cell>-</cell><cell>56.8</cell><cell>88</cell><cell>89.8</cell><cell>124.2</cell></row><row><cell>NBF [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos et al. [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMR [19]</cell><cell>70.2</cell><cell>-</cell><cell>-</cell><cell>50.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPIN [18]</cell><cell>59.2</cell><cell>96.9</cell><cell>116.4</cell><cell>41.1</cell><cell>62.5</cell><cell>67.5</cell><cell>105.2</cell></row><row><cell>EFT [14]</cell><cell>54.2</cell><cell>-</cell><cell>-</cell><cell>43.7</cell><cell>-</cell><cell>68.0</cell><cell>-</cell></row><row><cell>Zanfir et. al [52] (w/ 3DPW train)</cell><cell>57.1</cell><cell>90.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EFT [14] (w/ 3DPW train)</cell><cell>52.2</cell><cell>-</cell><cell>-</cell><cell>43.8</cell><cell>-</cell><cell>67.0</cell><cell>-</cell></row><row><cell>DSR</cell><cell>54.1</cell><cell>91.7</cell><cell>105.8</cell><cell>40.3</cell><cell>60.9</cell><cell>66.7</cell><cell>105.3</cell></row><row><cell>DSR (w/ 3DPW train)</cell><cell>51.7</cell><cell>85.7</cell><cell>99.5</cell><cell>41.4</cell><cell>62.0</cell><cell>67.0</cell><cell>104.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of state-of-the-art models on 3DPW, Human3.6M, and MPI-INF-3DHP datasets. DSR is our proposed model trained on monocular images similar to<ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18]</ref>. DSR outperforms all state-of-the-art models, including EFT<ref type="bibr" target="#b12">[14]</ref> on the challenging datasets. "?" shows the results that are not available.</figDesc><table><row><cell>Method</cell><cell cols="3">PAMPJPE ? MPJPE ? PVE ?</cell></row><row><cell>Standard Loss (SD)</cell><cell>47.5</cell><cell>73.9</cell><cell>99.2</cell></row><row><cell>SD + DSR</cell><cell>45.1</cell><cell>71.3</cell><cell>96.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Potential of DSR. We train and test on a subset of Human3.6M to evaluate the full potential of DSR loss. SD refers to standard joint loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table I :</head><label>I</label><figDesc>Mapping of DSR-C labels to Graphonomy labels.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragomir Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The naked truth: Estimating body shape under clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular human pose estimation: A survey of deep learning-based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphonomy: Universal human parsing via graph transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geopifu: Geometry and pixel aligned implicit functions for single-view human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">J</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based vision: a program to see a walking person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Image Vis. Comput</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards inthe-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graphics (Proc. SIG-GRAPH Asia)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naureen Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao Paul</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for singleimage 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tracking loose-limbed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A joint model for 2D and 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast articulated motion tracking using a sums of gaussians body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monoclothcap: Towards temporally coherent clothing capture from monocular rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabi?n</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Wenpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Yi-Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weakly supervised 3D human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes: The importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
