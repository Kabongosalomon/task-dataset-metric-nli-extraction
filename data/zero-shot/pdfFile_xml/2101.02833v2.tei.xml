<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
							<email>mengdebin16@mails.ucas.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many state-of-the-art few-shot learners focus on developing effective training procedures for feature representations, before using simple (e.g., nearest centroid) classifiers. We take an approach that is agnostic to the features used, and focus exclusively on meta-learning the final classifier layer. Specifically, we introduce MetaQDA, a Bayesian meta-learning generalisation of the classic quadratic discriminant analysis. This approach has several benefits of interest to practitioners: meta-learning is fast and memory efficient, without the need to fine-tune features. It is agnostic to the off-the-shelf features chosen, and thus will continue to benefit from future advances in feature representations. Empirically, it leads to excellent performance in cross-domain few-shot learning, class-incremental few-shot learning, and crucially for real-world applications, the Bayesian formulation leads to state-of-the-art uncertainty calibration in predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot recognition methods aim to solve classification problems with limited labelled training data, motivating a large body of work <ref type="bibr" target="#b61">[62]</ref>. Contemporary approaches to few-shot recognition are characterized by a focus on deep meta-learning <ref type="bibr" target="#b22">[23]</ref> methods that provide data efficient learning of new categories by using auxiliary data to train a model designed for rapid adaptation to new categories <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b68">69]</ref>, or for synthesizing a classifier for new categories in a feed-forward manner <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref>. Most of these meta-learning methods have been intimately interwoven with the training algorithm and/or architecture of the deep network that they build upon. For example, many have relied on episodic training schemes <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b58">59]</ref>, where few-shot learning problems are simulated at each iteration of training; differentiable optimisers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>, or new neural network modules <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b14">15]</ref> * Xueting and Debin contributed equally to this research, code is available https://github.com/Open-Debin/Bayesian_MQDA to facilitate data efficient learning and recognition.</p><p>Against this backdrop, a handful of recent studies <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b59">60]</ref> have pushed back against deep metalearning. They have observed, for example, that a well tuned convolutional network pre-trained for multi-class recognition and combined with a simple linear or nearest centroid classifier can match or outperform state-of-the-art meta-learners. Even self-supervised pre-training <ref type="bibr" target="#b36">[37]</ref> has led to feature extractors that outperform many meta-learners. These analyses raise the question: is meta-learning indeed beneficial, or is focusing on improving conventional pre-training sufficient?</p><p>We take a position in defense of meta-learning for few-shot recognition. To disentangle the influences of meta-learning per-se and feature learning discussed above, we restrict ourselves to fixed pre-trained features and conduct no feature learning in this study. It shows that meta-learning, even in its shallowest form, can boost few-shot learning above and beyond whatever is provided by the pre-trained features alone.</p><p>We take an amortized Bayesian inference approach <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref> to shallow meta-learning. During meta-testing, we infer a distribution over classifier parameters given the support set; and during meta-training we learn a feed-forward inference procedure for these parameters. While the limited recent work in Bayesian meta-learning is underpinned by amortized Variational Inference <ref type="bibr" target="#b14">[15]</ref>, our approach relies instead on conjugacy <ref type="bibr" target="#b11">[12]</ref>. Specifically, we build upon the classic Quadratic Discriminant Analysis (QDA) <ref type="bibr" target="#b8">[9]</ref> classifier and extended it with a Bayesian prior, an inference pipeline for the QDA parameter posterior given the support set, and gradient-based meta-training. We term the overall framework MetaQDA.</p><p>MetaQDA has several practical benefits for real-world deployments. Firstly, MetaQDA allows meta-learning to be conducted in the resource constrained scenario without endto-end training <ref type="bibr" target="#b23">[24]</ref>, while providing superior performance to fixed-feature approaches <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37]</ref>. Furthermore by decomposing representation learning from classifier meta-learning, MetaQDA is expected to benefit from continued progress in CNN architectures and training strategies. Indeed our empirical results show our feature-agnostic strategy benefits a diverse range of classic and recent feature representations.</p><p>Secondly, as computer vision systems begin to be deployed in high-consequence applications where safety <ref type="bibr" target="#b27">[28]</ref> or fair societal outcomes <ref type="bibr" target="#b4">[5]</ref> are at stake, their calibration becomes as equally, or more, important as their actual accuracy. E.g., Models must reliably report low-certainty in those cases where they do make mistakes, thus allowing their decisions in those cases to be reviewed. Indeed, proper calibration is a hard requirement for deployment in many high importance applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref>. Crucially, we show that our Bayesian MetaQDA leads to significantly better calibrated models than the standard classifiers in the literature.</p><p>Finally, we show that MetaQDA has particularly good performance in cross-domain scenarios where existing methods are weak <ref type="bibr" target="#b3">[4]</ref>, but which are ubiquitious in practical applications, where there is invariably insufficient domain-specific data to conduct in-domain meta-learning <ref type="bibr" target="#b17">[18]</ref>. Furthermore, as a Bayesian formulation, MetaQDA is inherently suited to the highly practical, but otherwise hard to achieve setting of incremental <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b44">45]</ref> few-shot learning, where it achieves state of the art performance 'out of the box'.</p><p>To summarize our contributions: (i) We present MetaQDA, a novel and efficient Bayesian approach to classifier metalearning based on conjugacy. (ii) We empirically demonstrate that MetaQDA's efficient fixed feature learning provides excellent performance across a variety of settings and metrics including conventional, cross-domain, class-incremental, and probability calibrated few-shot learning. (iii) We shed light on the meta-learning vs vanilla pre-training debate by disentangling the two and showing a clear benefit from meta-learning, across a variety of fixed feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-Shot and Meta-Learning Overview Few-shot and meta-learning are now a widely studied area that is too broad to review here. We refer the reader to comprehensive recent surveys for an introduction and review <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b22">23]</ref>. In general they proceed in two stages: meta-training the strategy for few-shot learning based on one or more auxiliary datasets; and meta-testing (learning new categories) on a target dataset, which should be done data-efficiently given the knowledge from meta-training. A high level categorization of common approaches groups them into methods that (1) meta-learn how to perform rapid gradient-based adaptation during meta-test <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b68">69]</ref>; and (2) meta-learn a feed-forward procedure to synthesize a classifier for novel categories given an embedding of the support set <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref>, where metric-based learners are included in the latter category <ref type="bibr" target="#b22">[23]</ref>.</p><p>Is Meta-Learning Necessary? Many recent papers have questioned whether elaborate meta-learning procedures are necessary. SimpleShot <ref type="bibr" target="#b60">[61]</ref> observes vanilla CNN features pre-trained for recognition achieve near SotA performance when appropriately normalized and used in a trivial nearest centroid classifier (NCC). Chen et al. <ref type="bibr" target="#b3">[4]</ref> present the simple but high-performance Baseline++, based on fixing a pre-trained feature extractor and then building a linear classifier during meta-test. <ref type="bibr" target="#b13">[14]</ref> observe that although SotA meta-learned deep features do exhibit strong performance in few-shot learning, this feature quality can be replicated by adding simple compactness regularisers to vanilla classifier pre-training. S2M2 <ref type="bibr" target="#b36">[37]</ref> demonstrates that after pre-training a network with self-supervised learning and/or manifold-regularised vanilla classification, excellent few-shot recognition is achieved by simply training a linear classifier on the resulting representation. <ref type="bibr" target="#b63">[64]</ref> analyzes whether the famous MAML algorithm is truly meta-learning, or simply pre-training a strong feature.</p><p>We show that for fixed features pre-trained by several of the aforementioned "off-the-shelf" non-meta techniques <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b36">37]</ref>, meta-learning solely in classifier-space further improves performance. This allows us to conclude that metalearning does add value, since alternative vanilla (i.e., nonmeta) pre-training approaches do not influence the final classifier. We leave conclusive analysis of the relative merits of metalearning vs vanilla pre-training of feature representation space to future work. In terms of empirical performance, we surpass all existing strategies based on fixed pre-trained features, and most alternatives based on deep feature meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixed Feature Meta-Learning</head><p>A minority of metalearning studies such as <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b33">34]</ref> have also built on fixed features. LEO <ref type="bibr" target="#b49">[50]</ref> synthesizes a classifier layer for a fixed feature extractor using a hybrid gradient-and feedforward-strategy. The concurrent URT <ref type="bibr" target="#b33">[34]</ref> addresses multi-domain few-shot learning by meta-training a module that fuses an array of fixed features and dynamically produces a new feature encoding for a new domain. Ultimately, URT uses a ProtoNet <ref type="bibr" target="#b51">[52]</ref> classifier, and thus our contribution is orthogonal to URT's, as MetaQDA aims to replace the classifier (ie, ProtoNet), not produce a new feature. Indeed we show empirically that MetaQDA can use URT's feature and improve their performance, further demonstrating the flexibility of our feature-agnostic approach.</p><p>Bayesian Few-Shot Meta-Learning Relatively few methods in the literature take Bayesian approaches to few-shot learning. A few studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b64">65]</ref> focus on understanding MAML <ref type="bibr" target="#b7">[8]</ref> as a hierarchical Bayesian model. Versa <ref type="bibr" target="#b14">[15]</ref> treats the weights of the final linear classifier layer as the quantity to infer given the support set during meta-test. It takes an amortized variational inference (VI) approach, training an inference neural network to predict the classifier parameters given the support set. However, unlike us, it then performs end-to-end representation learning, and is not fully Bayesian as it does not ultimately integrate the classifier parameters, as we achieve here. Neural Processes <ref type="bibr" target="#b10">[11]</ref> takes a Gaussian Process (GP) inspired approach to neural network design, but ultimately does not provide a clear Bayesian model. The recent DKT <ref type="bibr" target="#b41">[42]</ref> achieves true Bayesian meta-learning via GPs with end-to-end feature learning. However, despite performing feature learning, these Bayesian approaches have generally not provided SotA benchmark performance compared to the broader landscape of competitors at the time of their publication. A classic study <ref type="bibr" target="#b20">[21]</ref> explored shallow learning-to-learn of linear regression by conjugacy. We also exploit conjugacy but for classifier learning, and demonstrate SotA results on heavily benchmarked tasks for the first time with Bayesian meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier Layer Design</head><p>The vast majority of few-shot studies use either linear <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>, cosine similarity <ref type="bibr" target="#b42">[43]</ref>, or nearest centroid classifiers <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b51">52]</ref> under some distance metric. We differ in: (i) using a quadratic classifier, and (ii) taking a "generative" approach to fitting the model <ref type="bibr" target="#b18">[19]</ref>. While a quadratic classifier potentially provides a stronger fit than a linear classifier, its larger number of parameters will overfit catastrophically in a few-shot/high-dimension regime. This is why few studies have applied them, with the exception of <ref type="bibr" target="#b0">[1]</ref> who had to carefully hand-craft regularisers for them.</p><p>Our key insight is to use conjugacy to enable the quadratic classifier prior to be efficiently meta-learned, thus gaining improved fitting strength, while avoiding overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Probabilistic Meta-Learning</head><p>One can formalise a conventional classification problem as consisting of an input space ?, an output space ?, and a distribution over ? ? ? that defines the task to be solved. Few-shot recognition is the problem of training a classifier to distinguish between different classes in a sparse data regime, where only labelled training instances are available for each class. Meta-learning aims to distill relevant knowledge from multiple related few-shot learning problems into a set of shared parameters that boost the learning of subsequent novel fewshot tasks. The simplest way to extend the standard formalisation of classification problems to a meta-learning context is to instead consider the set, ? of all distributions over ? ? ?, each of which represents a possible classification task. One can then assume the existence of a distribution, over ? <ref type="bibr" target="#b1">[2]</ref>.</p><p>From a probabilistic perspective, the parameters inferred by the meta-learner that are shared across tasks, which we denote by , can be seen as specifying or inducing a prior distribution over the task-specific parameters for each few-shot problem. As such, meta-learning can be thought of as learning a procedure to induce a prior over models for future tasks by meta-training on a collection of related tasks. Representing task-specific parameters for task by , the few-shot training (aka support) and testing (aka query) sets as and , a Bayesian few-shot learner should use the learned prior to determine the posterior distribution over model parameters,</p><formula xml:id="formula_0">( | , ) = ( | ) ( | ) ? ( | ) ( | )d .<label>(1)</label></formula><p>Once the distribution obtained, one can model query samples, (? , ) ? , using the posterior predictive distribution,</p><formula xml:id="formula_1">( | , ) = | | ? =1 ? (? , | ) ( | , )d . (2)</formula><p>A natural measure for the goodness of fit for is the expected log likelihood of the few-shot models that make use of the shared prior,</p><formula xml:id="formula_2">, ? , ? [ ( | , )],<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">( | , ) = | | ? =1 log (? , | , ).<label>(4)</label></formula><p>The process of meta-learning the prior parameters can then be formalised as an risk minimisation problem,</p><formula xml:id="formula_4">* = arg min , ? , ? [? ( | , )].<label>(5)</label></formula><p>Discussion A prior probabilistic meta-learner <ref type="bibr" target="#b14">[15]</ref> focused on the term ( | , ), taking an amortized variational inference perspective that treats as the parameters of a neural network that predicts a distribution over the parameters of a linear classifier given support set . In contrast, our framework will use a QDA rather than linear classifier, and then exploit conjugacy to efficiently compute a distribution over the QDA mean and covariance parameters given the support set. This is both efficient and probabilistically cleaner, as our model contains a proper prior, while <ref type="bibr" target="#b14">[15]</ref> does not.</p><p>The integrals in Eqs. 1 and 2 are key to Bayesian metalearning, but can be computationally intractable and <ref type="bibr" target="#b14">[15]</ref> relies on sampling. Our conjugate setup allows the integrals to be computed exactly in closed form, without relying on sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Meta-Quadratic Discriminant Analysis</head><p>Our MetaQDA provides a meta-learning generalization of the classic QDA classifier <ref type="bibr" target="#b18">[19]</ref>. QDA works by constructing a multivariate Gaussian distribution corresponding to each class by maximum likelihood. At test time, predictions are made by computing the likelihood of the query instance under each of these distributions, and using Bayes theorem to obtain the posterior ( | , ). Rather than using maximum likelihood fitting for meta-testing, we introduce a Bayesian version of QDA that will enable us to exploit a meta-learned prior over the parameters of the multivariate Gaussian distributions. Two Bayesian strategies for inference using such a prior are explored: 1) using the maximum a posterior (MAP) estimate of the Gaussian parameters; and 2) the fully Bayesian approach that propagates the parameter uncertainty through to the class predictions. The first of these is conceptually simpler, while the second allows for better handling of uncertainty due to the fully Bayesian nature of the parameter inference. For both cases we make use of Normal-Inverse-Wishart priors <ref type="bibr" target="#b11">[12]</ref>, as their conjugacy with multivariate Gaussians leads to an efficient implementation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MAP-Based QDA</head><p>We begin by describing a MAP variant of QDA. In conventional QDA the likelihood of an instance, ? ? ? , belonging to class ? ? is given by ? (? | ? , ? ) and the parameters are found via maximum likelihood estimation (MLE) on the subset of the support set associated with class ,</p><formula xml:id="formula_5">? , ? = arg max ? ,? ? =1 ? (? , | ? , ?).<label>(6)</label></formula><p>This optimisation problem has a convenient closed form solution: the sample mean and covariance of the relevant subset of the support set. In order to incorporate prior knowledge learned from related few-shot learning tasks, we define a Normal-inverse-Wishart (NIW) prior <ref type="bibr" target="#b38">[39]</ref> over the parameters and therefore obtain a posterior for the parameters,</p><formula xml:id="formula_6">( ? , ? |? , ? , , , ) = ? =1 ? (? , | ? , ? )? ??( ? , ? | ? , , , ) ? ? ? =1 ? (? , | ? , ?)? ??( ? ? , ? ? | ? , , , ) ? ? ? ? .<label>(7)</label></formula><p>Training This enables us to take advantage of prior knowledge learned from related tasks when inferring the model parameters by MAP inference,</p><formula xml:id="formula_7">? , ? = arg max ? ,? ? =1 ( ? , ? |? , , ? , , , ).<label>(8)</label></formula><p>Because NIW is the conjugate prior of multivariate Gaussians, we know that the posterior distribution over the parameters takes the form of</p><formula xml:id="formula_8">( ? , ? |? , ? , , , ) = ? ??( ? , ? | ? , , , ), (9) where ? = ? + ? + , = + , = + , = + ? =1 (? , ?? )(? , ?? ) + + (? ? ? )(? ? ? ) ,<label>(10)</label></formula><p>and we have used? = 1 ? =1 ? , . The posterior is maximised at the mode, which occurs at</p><formula xml:id="formula_9">? = ? , ? = 1 + + 1 .<label>(11)</label></formula><p>Testing After computing point estimates of the parameters, one can make predictions on instances from the query set according to the usual QDA model,</p><formula xml:id="formula_10">( = |? , ? , , , ) = ? (? | ? , ? ) ( = ) ? =1 ? (? | ? , ? ) ( = ) .<label>(12)</label></formula><p>Note the prior over the classes ( ) can be dropped in the standard few-shot benchmarks that assume a uniform distribution over classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fully Bayesian QDA</head><p>Computing point estimates of the parameters throws away potentially useful uncertainty information that can help to better calibrate the predictions of the model. Instead, we can marginalise the parameters out when making a prediction,</p><formula xml:id="formula_11">( = |? ) = ? ? ? (? | , ? )? ??( ? , ? | ? , , , ) ? ? ? =1 ? ? ? (? | , ? )? ??( ? , ? | ? , , , ) ? ? .</formula><p>(13) Each of the double integrals has the form of a multivariate -distribution <ref type="bibr" target="#b38">[39]</ref>, yielding</p><formula xml:id="formula_12">( = |? , ? , , , ) = ? ? | ? , +1 ( ? +1) , ? + 1 ? =1 ? ? | ? , +1 ( ? +1) , ? + 1 .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Meta-Learning the Prior</head><p>Letting = ( ? , , , ), our objective is to minimise the negative expected log likelihood of models constructed with the shared prior on the parameters, as given in Eq. 5. For MAP-based QDA, the log likelihood function is given by</p><formula xml:id="formula_13">( | , ) = ? =1 ? =1 log ? (? , | ? , ? ),<label>(15)</label></formula><p>where ? and ? are the point estimates computed via the closed-form solution to the MAP inference problem given in Equation <ref type="bibr" target="#b10">11</ref>. When using the fully Bayesian variant of QDA, we have the following log likelihood function:</p><formula xml:id="formula_14">( | , ) = ? =1 ? =1 log ? ? , | ? , + 1 ( ? + 1) , ? + 1 .<label>(16)</label></formula><p>Meta-Training We approximate the optimization in Equation 5 by performing empirical risk minimisation on a training dataset using episodic training. In particular, we choose ? to be the set of uniform distributions over all possible -way classification problems, as the uniform distribution over ?, and the process of sampling from each ? ? results in balanced datasets containing instances from each of the classes. Episodic training then consists of sampling a few-shot learning problem, building a Bayesian QDA classifier using the support set, computing the negative log likelihood on the query set, and finally updating using stochastic gradient descent. Crucially, the use of conjugate priors means that no iterative optimisation procedure must be carried out when constructing the classifier in each episode. Instead, we are able to backpropagate through the conjugacy update rules and directly modify the prior parameters with stochastic gradient descent. The overall learning procedure is given in Algorithm 1. Some of the prior parameters must be constrained in order to learn a valid NIW distribution. In particular, must be positive definite, must be positive, and must be strictly greater than ? 1. The constraints can be enforced for and by clipping any values that are outside the valid range back to the minimum allowable value. We parameterise the scale matrix in terms of its Cholesky factors,</p><formula xml:id="formula_15">= ,<label>(17)</label></formula><p>where is a lower triangular matrix. During optimisation we ensure remains lower triangular by setting all elements above the diagonal to zero after each weight update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We measure the efficacy of our model in standard, crossdomain and multi-domain few-shot learning and few-shot class-incremental problem settings. MetaQDA is a shallow classifier-layer meta-learner that is agnostic to the choice of fixed extracted features. Unless otherwise stated, we report results for the FB-based variant of MetaQDA. During metatraining, we learn the priors = ( ? , , , ) over episodes drawn from the training set, keeping the feature extractor fixed. We use the meta-validation datasets for model selection and hyperparameter tuning. During meta-testing, the support set is used to obtain the parameter posterior, and then a QDA classifier is established according to either Eq 12 or Eq 14. All algorithms are evaluated on -way -shot learning <ref type="bibr" target="#b51">[52]</ref>, with a batch of 15 query images per class in a testing episode. All accuracies are calculated by averaging over 600 randomly generated testing tasks with 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Standard Few-Shot Learning</head><p>Datasets miniImageNet <ref type="bibr" target="#b43">[44]</ref> is split into 64/16/20 for meta-train/val/test, respectively, containing 100 classes and 600 examples per class, drawn from ILSVRC-12 <ref type="bibr" target="#b48">[49]</ref>. Images are resized to 84?84 <ref type="bibr" target="#b19">[20]</ref>. tieredImageNet is a more challenging benchmark <ref type="bibr" target="#b46">[47]</ref> consisting of 608 classes (779,165 images) and 391/97/160 classes for meta-train/val/test folds, respectively. Images are resized to 84?84. CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> was created by randomly sampling from CIFAR-100 <ref type="bibr" target="#b26">[27]</ref> by using the same criteria as miniImageNet (100 classes with 600 images per class, split into folds of 64/16/20 for meta-train/val/test). Image are resized to 32?32.</p><p>Feature Extractors Conv-4 (64-64-64-64) as in <ref type="bibr" target="#b51">[52]</ref>. See Appendix for details. ResNet-18 is standard 18-layer 8-block architecture with pre-trained weights in <ref type="bibr" target="#b60">[61]</ref>. WRN-28-10 is standard architecture with 28 convolutional layers and width factor 10, and the pre-trained weights from <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competitors</head><p>We group competitors into two categories: 1) direct competitors that also make use of "off-the-shelf" fixed pre-trained networks and only update the classifier to learn novel classes; and 2) non-direct competitors that specifically meta-learn a feature optimised for few-shot learning and/or update features during meta-testing. Baseline++ <ref type="bibr" target="#b3">[4]</ref> fixes the feature encoder and only tunes the (cosine similarity) classifier during the meta-test stage. SimpleShot <ref type="bibr" target="#b60">[61]</ref> uses an NCC classifier with different feature encoders and studies different feature normalizations. We use their best reported variant, CL2N. S2M2 <ref type="bibr" target="#b36">[37]</ref> uses a linear classifier after self-supervised and/or regularized classifier pre-training. SUR <ref type="bibr" target="#b6">[7]</ref> also uses pretrained feature extractors, but focuses on weighting multiple features extracted from different backbones or multiple layers of the same backbone. We compare their reported results of a single ResNet backbone trained for multi-class classification as per ours, but they have the advantage of fusing features extracted from multiple layers. Unravelling <ref type="bibr" target="#b13">[14]</ref> proposes some new regularizers for vanilla backbone training that improve feature quality for few-shot learning without meta-learning.</p><p>Results <ref type="table" target="#tab_2">Table 1</ref>-3 summarize the results on miniImageNet, tieredImageNet and CIFAR-FS. MetaQDA performs better than all the previous methods that rely on off-the-shelf feature extractors, and also the majority of methods that meta-learn representations specialised for few-shot problems. We do not make efforts to carefully fine-tune the hyperparameters, but focus on showing that our model has robust advantages in different few-shot learning benchmarks with various backbones. A key benefit of fixed feature approaches (grey) is small compute cost, e.g., under 1-hour training. In contrast, SotA end-to-end competitors (white) such as <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b67">68]</ref> require over 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cross-Domain Few-Shot Learning</head><p>Dataset CUB <ref type="bibr" target="#b21">[22]</ref> contains 11,788 images across 200 fine-grained classes, split into folds of 100, 50, and 50 Cars <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b57">58]</ref> contains 196 classes randomly split into folds of 98, 49, and 49 classes for meta-train/val/test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Backbone</head><p>1-shot 5-shot METALSTM <ref type="bibr" target="#b43">[44]</ref> Conv-4 43.44 ? 0.77% 60.60 ? 0.71% MAML <ref type="bibr" target="#b7">[8]</ref> Conv-4 48.70 ? 1.84% 63.11 ? 0.92% PROTONET <ref type="bibr" target="#b51">[52]</ref> Conv-4 49.42 ? 0.78% 68.20 ? 0.66% GNN <ref type="bibr" target="#b9">[10]</ref> Conv-4 50.33 ? 0.36% 66.41 ? 0.63%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METASSL[47]</head><p>Conv-4 50.41 ? 0.31% 64.39 ? 0.24% RELATIONNET <ref type="bibr" target="#b54">[55]</ref> Conv-4 50.44 ? 0.82% 65.32 ? 0.70% METASGD <ref type="bibr" target="#b32">[33]</ref> Conv-4 50.47 ? 1.87% 64.03 ? 0.94% CAVIA <ref type="bibr" target="#b68">[69]</ref> Conv-4 51.82 ? 0.65% 65.85 ? 0.55% TPN <ref type="bibr" target="#b34">[35]</ref> Conv-4 52.78 ? 0.27% 66.59 ? 0.28% R2D2 <ref type="bibr" target="#b2">[3]</ref> Conv  <ref type="bibr" target="#b12">[13]</ref> ResNet-12 55.45 ? 0.89% 70.13 ? 0.68% TADAM <ref type="bibr" target="#b40">[41]</ref> ResNet-12 58.50 ? 0.30% 76.70 ? 0.30% CAML <ref type="bibr" target="#b24">[25]</ref> ResNet-12 59.23 ? 0.99% 72.35 ? 0.18% AM3 <ref type="bibr" target="#b62">[63]</ref> ResNet-12 65.21 ? 0.49% 75.20 ? 0.36% MTL <ref type="bibr" target="#b53">[54]</ref> ResNet-12 * 61.20 ? 1.80% 75.50 ? 0.80% TAP NET <ref type="bibr" target="#b65">[66]</ref> ResNet-12 61.65 ? 0.15% 76.36 ? 0.10% RELATIONNET2[68] ResNet- <ref type="bibr" target="#b11">12</ref> 63.92 ? 0.98% 77.15 ? 0.59%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2D2[3]</head><p>ResNet-12 59.38 ? 0.31% 78.15 ? 0.24% METAOPT <ref type="bibr" target="#b29">[30]</ref> ResNet-12 * 64.09 ? 0.62% 80.00 ? 0.45% RELATIONNET <ref type="bibr" target="#b3">[4]</ref> ResNet-18 52.48 ? 0.86% 69.83 ? 0.68% PROTONET <ref type="bibr" target="#b3">[4]</ref> ResNet-18 54.16 ? 0.82% 73.68 ? 0.65% DCEM <ref type="bibr" target="#b5">[6]</ref> ResNet-18 58.71 ? 0.62% 77.28 ? 0.46% AFHN <ref type="bibr" target="#b31">[32]</ref> ResNet-18 62.38 ? 0.72% 78.16 ? 0.56%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUR[7]</head><p>ResNet-12 60.79 ? 0.62% 79.25 ? 0.41%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNRAVELLING[14]</head><p>ResNet-12 * 59.37 ? 0.32% 77.05 ? 0.25% BASELINE++ <ref type="bibr" target="#b3">[4]</ref> ResNet-18 51.87 ? 0.77% 75.68 ? 0.63%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMPLESHOT[61]</head><p>ResNet-18 62.85 ? 0.20% 80.02 ? 0.14% S2M2 <ref type="bibr" target="#b36">[37]</ref> ResNet-18 <ref type="bibr" target="#b63">64</ref>  Competitors Better few-shot learning methods should degrade less when transferring to new domains <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58]</ref>. We are specifically interested in comparing MetaQDA with other methods using off-the-shelf features. In particular, we consider Baseline++ <ref type="bibr" target="#b3">[4]</ref> and S2M2 <ref type="bibr" target="#b36">[37]</ref> who use linear classifiers, and the nearest centroid method of SimpleShot <ref type="bibr" target="#b60">[61]</ref>.</p><p>Results <ref type="table" target="#tab_9">Table 4</ref> demonstrates that MetaQDA exhibits good robustness to domain shift. Specifically, our method outperforms other approaches by at least 2% ? 4% across all dataset, support set size, and feature combinations.  PROTONET <ref type="bibr" target="#b36">[37]</ref> ResNet-12 72.20 ? 0.70% 83.50 ? 0.50% METAOPT <ref type="bibr" target="#b29">[30]</ref> ResNet-12 * 72.00 ? 0.70% 84.20 ? 0.50% UNRAVELLING <ref type="bibr" target="#b13">[14]</ref> ResNet-12 * 72.30 ? 0.40% 86.30 ? 0.20% BASELINE++ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref> ResNet- <ref type="bibr" target="#b17">18</ref> 59.67 ? 0.90% 71.40 ? 0.69% S2M2 <ref type="bibr" target="#b36">[37]</ref> ResNet-18 <ref type="bibr" target="#b62">63</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multi-Domain Few-Shot Learning</head><p>Dataset Meta-Dataset <ref type="bibr" target="#b56">[57]</ref> is a challenging large-scale benchmark spanning 10 image datasets. Following <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b0">1]</ref>, we report results using the first 8 datasets for meta training (some classes are reserved for "in-domain" testing performance evaluation), and hold out entirely the remaining 2 (Traffic Signs and MSCOCO) plus an additional 3 datasets (MNIST <ref type="bibr" target="#b28">[29]</ref>, CIFAR10, CIFAR100 <ref type="bibr" target="#b26">[27]</ref>) for an unseen "out-of-domain" performance evaluation. Note that the meta-dataset protocol is random way and shot.</p><p>Competitors CNAP <ref type="bibr" target="#b47">[48]</ref> and SCNAP <ref type="bibr" target="#b0">[1]</ref> meta-learn an adaptive feature extractor whose parameters are modulated by an adaptation network that takes the current task's dataset Model Backbone 1-shot 5-shot miniImageNet?CUB MAML <ref type="bibr" target="#b41">[42]</ref> Conv-4 34.01 ? 1.25% -RELATIONNET <ref type="bibr" target="#b41">[42]</ref> Conv-4 37.13 ? 0.20% -DKT <ref type="bibr" target="#b41">[42]</ref> Conv-4 40.22 ? 0.54% -PROTONET <ref type="bibr" target="#b41">[42]</ref> Conv-4 33.27 ? 1.09% -BASELINE++ <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4]</ref>   <ref type="bibr" target="#b3">[4]</ref> ResNet-18 -57.71 ? 0.73% LRP (CAN) <ref type="bibr" target="#b52">[53]</ref> ResNet-12 46.23 ? 0.42% 66.58 ? 0.39% LRP (GNN) <ref type="bibr" target="#b52">[53]</ref> ResNet-10 48.29 ? 0.51% 64.44 ? 0.48% LFWT <ref type="bibr" target="#b57">[58]</ref> ResNet-10 47.47 ? 0.75% 66.98 ? 0.68% PROTONET <ref type="bibr" target="#b3">[4]</ref> ResNet-18 -62.02 ? 0.70% BASELINE++ <ref type="bibr" target="#b3">[4]</ref> ResNet- <ref type="bibr" target="#b17">18</ref>   as input. SUR <ref type="bibr" target="#b6">[7]</ref> performs feature selection among a suite of meta-train domain-specific features. The concurrent URT <ref type="bibr" target="#b33">[34]</ref> meta-learns a transformer to dynamically meta-train dataset features before nearest-centroid classification with ProtoNet. We apply MetaQDA upon the fixed fused features learned by URT, replacing ProtoNet.</p><p>Results <ref type="table">Table 5</ref> reports the average rank and accuracy of each model across all 13 datasets. We also break accuracy down among the 'in-domain' and 'out-of-domain' datasets (i.e., seen/unseen during meta-training). MetaQDA has the best average rank and overall accuracy. In particular it achieves strong out-of-domain performance, which is in line with our good cross-domain results above. Detailed results broken down by dataset are in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Few-Shot Class-Incremental Learning</head><p>Problem Setup Few-Shot Class-Incremental Learning (FSCIL) requires to incrementally learn novel classes <ref type="bibr" target="#b44">[45]</ref> from few labelled samples <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56]</ref> ideally without forgetting. Our fixed feature assumption provides both an advantage and  <ref type="table">Table 5</ref>: Few-shot classification results on Meta-Dataset.</p><p>Average accuracy and rank across episodes and datasets. a disadvantage in this regard. But our MetaQDA is naturally suited to incremental learning, and more than makes up for any disadvantage. Following <ref type="bibr" target="#b55">[56]</ref>, miniImageNet is split into 60/40 base/novel classes, each with 500 training and 100 testing images. Each meta-test episode starts from a base classifier and proceeds in 8 learning sessions adding a 5-way-5-shot support set per session. After each session, models are evaluated on the full set of classes seen so far, leading to a 100-way generalized few-shot problem in the 9th session.</p><p>MetaQDA As per <ref type="bibr" target="#b55">[56]</ref>, we pre-train a ResNet18 backbone (see Appendix for details) and then meta-train MetaQDA on 60 base classes before performing incremental meta-testing. The MetaQDA prior is not updated during meta-testing.</p><p>Results <ref type="table" target="#tab_12">Table 6</ref> reports the average results of 10 meta-test episodes with random 5-shot episodes. Clearly MetaQDA significantly outperforms both NCC and the previous SotA <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Model Calibration</head><p>In real world scenarios, where high-importance decisions are being made, the probability calibration of a machine learning model is critical <ref type="bibr" target="#b16">[17]</ref>. Any errors they make should be accompanied with associated low-confidence scores, e.g., so they can be checked by another process.</p><p>Metrics Following <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17]</ref>, we compute Expected Calibration Error (ECE) with and without temperature scaling (TS). ECE assigns each prediction to a bin that indicates how confident the prediction is, which should reflect its probability of correctness. IE:</p><formula xml:id="formula_16">= ? =1 |acc( ) ? conf ( )|, where</formula><p>is the number of predictions in bin , is the number of instances, and acc( ) and conf ( ) are the accuracy and confidence of bin . We use = 20. Temperature scaling uses validation episodes to calibrate a softmax temperature for best ECE. Please see <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17]</ref> for full details.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion: Why QDA but not other classifiers?</head><p>In principle, one could attempt an analogous Bayesian metalearning approach to other classifiers, but we build on discriminant analysis. This is because most classifiers do not admit a tractable Bayesian treatment, besides logistic regression (LR) and discriminant analysis. While LR has a Bayesian generalization <ref type="bibr" target="#b35">[36]</ref>, it requires approximate inference and is significantly more complicated to implement, making it difficult to extend to meta-learning. In contrast our generative discriminant analysis approach admits an exact closed form solution, and is easy to extend to meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Discriminant Analysis Methods</head><p>We compare how moving from LDA to QDA changes perfor-mance; and study the impact of changing from (i) no prior, (ii) hand-crafted NIW prior, and (iii) meta-learned prior. We set the hard-crafted NIW prior to ? = 0, = 1, = , and = which worked well in practice. <ref type="table" target="#tab_14">Table 8</ref> demonstrates that classic unregularized discriminant analysis methods (LDA and QDA without priors) have very poor performance in the fewshot setting, due to extreme overfitting. This can be seen because: 1) the higher capacity QDA exhibits worse performance than the lower capacity LDA; and 2) incorporating a prior into LDA and QDA, thereby reducing model capacity and overfitting, results in an improvement in performance. Finally, by meta-learning the prior, we are able to optimize inductive bias for few-shot learning performance. Both LDA and QDA benefit from meta-learning, but QDA performs better overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Bayesian Meta-Learning?</head><p>To disentangle the impact of Bayesian modeling from our classifier architecture and episodic meta-learning procedure, we evaluate a non-Bayesian MetaQDA as implemented by performing MAML learning on the initialization of the QDA covariance factor (Eq 17). From <ref type="table" target="#tab_16">Table 9</ref> we can see that MAML is worse than MetaQDA in both accuracy and calibration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose an efficient shallow meta-learner for few-shot learning. MetaQDA provides a fast exact inference strategy for amortized Bayesian meta-learning through conjugacy, and highlights a distinct avenue of meta-learning research in contrast to meta representation learning. The empirical performance of our model exceeds that of others that rely on off-the-shelf feature extractors, and often outperforms those that train extractors specialised for few-shot learning. In particular it excels in a number of challenging but highly practically important metrics including cross-domain few-shot learning, class incremental few-shot learning, and providing accurate probability calibration-a vital property for many applications where safety or reliability is of paramount concern.</p><p>A. Illustrative Schematic of MetaQDA To illustrate the mechanism of MetaQDA, we compare it schematically to conventional linear classifier used in many studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b33">34]</ref>, and vanilla QDA in <ref type="figure" target="#fig_1">Figure 1</ref>. In the figure, the colored circles indicate 3-way-5-shot support datasets, and the "x" data points with are the query set of the corresponding color. The dashed line is the decision boundary of different classifiers. <ref type="figure" target="#fig_1">Figure 1(a)</ref> shows Nearest Centre Classifier (NCC) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b33">34]</ref>, where the stars represents the mean of the support set class distributions, and these induce linear decision boundaries. <ref type="figure" target="#fig_1">Figure 1(b)</ref> depicts the Quadratic Discriminant Analysis (QDA) classifier, where the dashed ellipses represents the class covariance models, estimated from the support set. These induce a non-linear decision boundary. <ref type="figure" target="#fig_1">Figure 1(c)</ref> illustrates our MetaQDA, where the meta-training process learns a shared NIW prior (the shadow ellipse) from many few-shot training tasks. Then MetaQDA uses conjugacy to update the class covariances (solid line) using the support set and prior, and so induces a better non-linear decision boundary.</p><p>This illustrates how the MetaQDA setup allows us to exploit the benefit of a non-linear classifier, without the associated overfitting risk that would normally undermine such an attempt (as illustrated by the poor results of vanilla MetaQDA in Tab 7, 8 of the main manuscript).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experimental Setting Details: Standard Few-shot Learning</head><p>Parameters for training the Conv-4 extractor Following <ref type="bibr" target="#b60">[61]</ref>, we use stochastic gradient descent (SGD) with a multi-step learning rate schedule, momentum of 0.9, and the initial learning rate is set to 0.01 for both miniImageNet and CIFAR-FS, and 0.001 for tieredImageNet. At epochs 70 and 100 we reduce the learning rate by a factor of 0.1. Weight decay is set as 0.0001 through out training.</p><p>Parameters for training the ResNet-18 extractor Following <ref type="bibr" target="#b60">[61]</ref>, we use stochastic gradient descent (SGD) with a multi-step learning rate schedule, momentum of 0.9, and the initial learning rate is set to 0.001 for tieredImageNet. At epochs 70 and 100 we reduce the learning rate by a factor of 0.1. Weight decay is set as 0.0001 throughout training. Batch size is 256 images.</p><p>Parameters for training the WRN-28-10 extractor Following <ref type="bibr" target="#b36">[37]</ref>, as for 1-shot classification on miniImageNet, we use stochastic gradient descent (SGD) with a multi-step learning rate schedule, momentum of 0.9, and the initial learning rate is set to 0.001. For 5-shot classification on miniImageNetand 1-shot classification on tieredImageNet, we use ADAM optimiser. For CIFAR-FS, we use the pre-trained WRN backbone of S2M2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Setting Details: Few-Shot Class Incremental Learning</head><p>Training setup We follow the experimental setup of <ref type="bibr" target="#b55">[56]</ref>. Specifically, we use the same 60 base classes to pre-train an initial ResNet-18 backbone using mini-batch size as 128 and use stochastic gradient descent (SGD) with the initial learning rate of 0.1, decreasing the learning rate to 0.01/0.001 after 30/40 epochs, respectively. Meta-Training: The MetaQDA prior is then trained using Algorithm 1 (main manuscript) by generating episodes from the 60 base class set, using the feature extractor trained as above.</p><p>Meta-Testing: Due to our Bayesian class-conditional modeling, meta-testing decomposes over classes. Classincremental learning is thus trivially realized by running MetaQDA's update step for each new category, and adding the final mean and covariance to the set used by the final QDA classifier. We apply MetaQDA both for the many-shot base classes, and 5-shot incrementally added classes.</p><p>The results in Tab 6 of the main manuscript are averages generated by independently repeating both meta-train and meta-test (8 incremental sessions each) phases 10 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Full Meta-Dataset Results</head><p>Implementation Details We use the same backbone as SUR <ref type="bibr" target="#b6">[7]</ref> and URT <ref type="bibr" target="#b33">[34]</ref>, and take the trained fused features by URT <ref type="bibr" target="#b33">[34]</ref>. We use ADAM optimizer and cosine learning rate scheduler, and the initial learning rate is set to 0.0003, beta is set as 0.9 and 0.999. Weight decay is set as 0.0001 throughout training. The number of training episodes is 10000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Following <ref type="bibr" target="#b56">[57]</ref>, few-shot tasks are sampled with varing number of classes , varying number of shots and class imbalance. <ref type="table" target="#tab_2">Table 10</ref> reports performance in accuracy over over 600 sampled meta-test tasks. Because most of the results have very similar confidence interval, we omit this part to make the table more readable. The results of other SotA algorithms are taken from URT <ref type="bibr" target="#b33">[34]</ref> and SCNAP <ref type="bibr" target="#b0">[1]</ref>. From the results we can see that MetaQDA performs well in both seen domains (left) and out-of-distribution unseen (right) domains. It achieves highest performance in 8 of 13 domains within the meta-dataset benchmark.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Set Query Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Centre Boundary</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :; 7 Build Bayesian QDA Model 8 If// Eq 11 9 10 Update Prior 11 ?</head><label>178111011</label><figDesc>Pseudocode for epsiodic meta-learning of hyper-parameters in MetaQDA. 1 Require: Distribution over tasks , number of iterations , learning rate 2 Result: prior parameters 3 Init: 0 = { ? = ? 0, = , = 1, = } 4 for = 1 to do 5 Sample task, ? ; 6 Sample support and query set, , ? MAP: ? {( ? , ? )} =1 ; If Fully Bayes: ? {( ? , , , )} =1 ; // Eq 10 ?1 ? ? ( ?1 | , ) ; // Eq 15 or 16 12 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustrative Schematic of MetaQDA. (a) NCC classifier uses the class mean to induce linear decision boundaries. (b) QDA uses both the support class mean and covariance to induce a curved decision boundary, but easily overfits in a few-shot regime due. (c) MetaQDA meta-learns the QDA parameter prior to provide stable estimation of a non-linear decision boundary without overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>0.64% 84.28 ? 0.69%</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>.06 ? 0.18% 80.58 ? 0.12%</cell></row><row><cell>METAQDA</cell><cell>ResNet-18</cell><cell>65.12 ? 0.66% 80.98 ? 0.75%</cell></row><row><cell>LEO [50]</cell><cell>WRN</cell><cell>61.78 ? 0.05% 77.59 ? 0.12%</cell></row><row><cell>PPA [43]</cell><cell>WRN</cell><cell>59.60 ? 0.41% 73.74 ? 0.19%</cell></row><row><cell>SIMPLESHOT[61]</cell><cell>WRN</cell><cell>63.50 ? 0.20% 80.33 ? 0.14%</cell></row><row><cell>S2M2 [37]</cell><cell>WRN</cell><cell>64.93 ? 0.18% 83.18 ? 0.22%</cell></row><row><cell>METAQDA</cell><cell>WRN</cell><cell>67.83 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Few-shot classification results on miniImageNet.</head><label>1</label><figDesc></figDesc><table /><note>? : two-step optimization with attention. : requires gradient-based optimisation at meta-test time.* : Use a wider CNN than standard and higher dimensional embedding. Grey: Fixed feature methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MAML [37]</cell><cell>Conv-4</cell><cell cols="2">58.90 ? 1.90% 71.50 ? 1.00%</cell></row><row><cell>RELATIONNET [37]</cell><cell>Conv-4</cell><cell cols="2">55.50 ? 1.00% 69.30 ? 0.80%</cell></row><row><cell>PROTONET [37]</cell><cell>Conv-4</cell><cell cols="2">55.50 ? 0.70% 72.02 ? 0.60%</cell></row><row><cell>R2D2 [3]</cell><cell>Conv-4</cell><cell cols="2">62.30 ? 0.20% 77.40 ? 0.10%</cell></row><row><cell>SIMPLESHOT + [61]</cell><cell>Conv-4</cell><cell cols="2">59.35 ? 0.89% 74.76 ? 0.72%</cell></row><row><cell>METAQDA</cell><cell>Conv-4</cell><cell></cell><cell></cell></row></table><note>Few-shot classification results on tieredImageNet.? : Make use of additional unlabeled data for semi-supervised learning or transductive inference. Gray: Use fixed pre-trained backbones.60.52 ? 0.88% 77.33 ? 0.73%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>0.88% 88.79 ? 0.75%</head><label></label><figDesc>.66 ? 0.17% 76.07 ? 0.19%</figDesc><table><row><cell>METAOPTNET [30]</cell><cell>WRN</cell><cell>72.00 ? 0.70% 84.20 ? 0.50%</cell></row><row><cell cols="2">BASELINE++ [37, 4] WRN</cell><cell>67.50 ? 0.64% 80.08 ? 0.32%</cell></row><row><cell>S2M2 [37]</cell><cell>WRN</cell><cell>74.81 ? 0.19% 87.47 ? 0.13%</cell></row><row><cell>METAQDA</cell><cell>WRN</cell><cell>75.83 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 : Few-shot classification results on CIFAR-FS.</head><label>3</label><figDesc></figDesc><table /><note>+ Our implementation. Gray: Use fixed pre-trained backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 : Cross domain few-shot classification results from miniImageNet to CUB and Cars datasets.</head><label>4</label><figDesc></figDesc><table /><note>+ Our implementation.Gray: Use fixed pre-trained backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 shows</head><label>7</label><figDesc>MetaQDA has superior uncertainty quantification compared to existing competitors. Vanilla QDA and SimpleShot are poorly calibrated, demonstrating the importance of our learned prior. The deeper WRN is also worse calibrated despite being more accurate, but MetaQDA ultimately compensates for this. Finally, we see that our fully-Bayesian (MetaQDA-FB, Sec 4.2) variant outperforms our MAP (MetaQDA-MAP, Sec 4.1) variant. (+4.89) 51.06 (+5.89) 47.69 (+6.53) 44.71 (+7.23) 42.08 (+6.56) 39.74 (+7.55) 37.66 (+8.20) 35.78 (+11.36)</figDesc><table><row><cell>Model</cell><cell cols="9">session 0 (60) session 1 (65) session 2 (70) session 3 (75) session 4 (80) session 5 (85) session 6 (90) session 7 (95) session 8 (100)</cell></row><row><cell>AL_MML [56]</cell><cell>61.31</cell><cell>50.09</cell><cell>45.17</cell><cell>41.16</cell><cell>37.48</cell><cell>35.52</cell><cell>32.19</cell><cell>29.46</cell><cell>24.42</cell></row><row><cell>NCC</cell><cell>46.62</cell><cell>43.26</cell><cell>40.87</cell><cell>39.04</cell><cell>37.50</cell><cell>35.96</cell><cell>34.13</cell><cell>33.19</cell><cell>32.26</cell></row><row><cell>METAQDA</cell><cell>59.57</cell><cell>54.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 : Class-incremental few-shot learning with ResNet18 on miniImageNet.</head><label>6</label><figDesc>Start with 60-way base classifier and add 5-way/5-shot per session. At each session, the models are evaluated on the test sets of the full set of classes encountered so far. (#): classifier-way at each session.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="4">ECE+TS 1-shot 5-shot 1-shot 5-shot ECE</cell></row><row><cell>LIN.CLASSIF.</cell><cell>Conv-4</cell><cell>3.56</cell><cell>2.88</cell><cell>8.54</cell><cell>7.48</cell></row><row><cell>SIMPLESHOT</cell><cell>Conv-4</cell><cell>3.82</cell><cell>3.35</cell><cell cols="2">33.45 45.81</cell></row><row><cell>QDA</cell><cell>Conv-4</cell><cell>8.25</cell><cell>4.37</cell><cell cols="2">43.54 26.78</cell></row><row><cell>MQDA-MAP</cell><cell>Conv-4</cell><cell>2.75</cell><cell>0.89</cell><cell>8.03</cell><cell>5.27</cell></row><row><cell>MQDA-FB</cell><cell>Conv-4</cell><cell>2.33</cell><cell>0.45</cell><cell>4.32</cell><cell>2.92</cell></row><row><cell cols="2">S2M2+LIN.CLASSIF WRN</cell><cell>4.93</cell><cell>2.31</cell><cell cols="2">33.23 36.84</cell></row><row><cell>SIMPLESHOT</cell><cell>WRN</cell><cell>4.05</cell><cell>1.80</cell><cell cols="2">39.56 55.68</cell></row><row><cell>QDA</cell><cell>WRN</cell><cell>4.52</cell><cell>1.78</cell><cell cols="2">35.95 18.53</cell></row><row><cell>MQDA-MAP</cell><cell>WRN</cell><cell>3.94</cell><cell>0.94</cell><cell cols="2">31.17 17.37</cell></row><row><cell>MQDA-FB</cell><cell>WRN</cell><cell>2.71</cell><cell>0.74</cell><cell cols="2">30.68 15.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Lower is better. TS indicates temperature scaling.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>LDA</cell><cell>Conv-4</cell><cell>-</cell><cell>64.24 ? 1.42%</cell></row><row><cell>QDA</cell><cell>Conv-4</cell><cell>-</cell><cell>34.45 ? 0.67%</cell></row><row><cell cols="2">LDA (PRIOR) Conv-4</cell><cell cols="2">54.84 ? 0.80% 71.48 ? 0.64%</cell></row><row><cell cols="2">QDA (PRIOR) Conv-4</cell><cell cols="2">54.84 ? 0.80% 71.40 ? 0.64%</cell></row><row><cell>METALDA</cell><cell>Conv-4</cell><cell cols="2">56.24 ? 0.80% 72.39 ? 0.64%</cell></row><row><cell>METAQDA</cell><cell>Conv-4</cell><cell cols="2">56.41 ? 0.80% 72.64 ? 0.62%</cell></row><row><cell>LDA</cell><cell>WRN</cell><cell>-</cell><cell>51.83 ? 1.29%</cell></row><row><cell>QDA</cell><cell>WRN</cell><cell>-</cell><cell>27.14 ? 0.59%</cell></row><row><cell cols="2">LDA (PRIOR) WRN</cell><cell cols="2">63.79 ? 0.83% 81.05 ? 0.56%</cell></row><row><cell cols="2">QDA (PRIOR) WRN</cell><cell cols="2">63.79 ? 0.83% 81.18 ? 0.56%</cell></row><row><cell>METALDA</cell><cell>WRN</cell><cell cols="2">64.92 ? 0.85% 83.18 ? 0.83%</cell></row><row><cell>METAQDA</cell><cell>WRN</cell><cell cols="2">67.83 ? 0.64% 84.28 ? 0.69%</cell></row></table><note>Calibration error (ECE) comparison on miniImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 : Comparison of different classifiers and hand-crafted vs. meta-learned prior measured on miniImageNet.</head><label>8</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>63.66 ? 0.80% 58.11 77.82 ? 0.62% 44.62 Bayesian ResNet-18 65.12 ? 0.66% 33.56 80.98 ? 0.75% 13.86</figDesc><table><row><cell cols="2">Meta Alg. Backbone</cell><cell>1-shot Acc.</cell><cell>ECE</cell><cell>5-shot Acc.</cell><cell>ECE</cell></row><row><cell>MAML</cell><cell>Conv-4</cell><cell cols="4">54.33 ? 0.78% 52.75 69.17 ? 0.77% 38.84</cell></row><row><cell>Bayesian</cell><cell>Conv-4</cell><cell cols="4">56.41 ? 0.80% 8.03 72.64 ? 0.62% 5.27</cell></row><row><cell>MAML</cell><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 : Comparison of Bayesian vs. non-Bayesian (MAML- based) realisaton of MetaQDA on miniImageNet.</head><label>9</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>ModelImageNet Omniglot Aircraft Birds DTD Quickdraw Fungi Flower Signs Mscoco MNIST CIFAR10 CIFAR100</figDesc><table><row><cell>MAML [8]</cell><cell>32.4</cell><cell>71.9</cell><cell>52.8</cell><cell>47.2</cell><cell>56.7</cell><cell>50.5</cell><cell>21.0</cell><cell>70.9</cell><cell>34.2</cell><cell>24.1</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>RELATIONNET [55]</cell><cell>30.9</cell><cell>86.6</cell><cell>69.7</cell><cell>54.1</cell><cell>56.6</cell><cell>61.8</cell><cell>32.6</cell><cell>76.1</cell><cell>37.5</cell><cell>27.4</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>MATCHINGNET [59]</cell><cell>36.1</cell><cell>78.3</cell><cell>69.2</cell><cell>56.4</cell><cell>61.8</cell><cell>60.8</cell><cell>33.7</cell><cell>81.9</cell><cell>55.6</cell><cell>28.8</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>FINETUNE [67]</cell><cell>43.1</cell><cell>71.1</cell><cell>72.0</cell><cell>59.8</cell><cell>69.1</cell><cell>47.1</cell><cell>38.2</cell><cell>85.3</cell><cell>66.7</cell><cell>35.2</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>PROTONET [52]</cell><cell>44.5</cell><cell>79.6</cell><cell>71.1</cell><cell>67.0</cell><cell>65.2</cell><cell>64.9</cell><cell>40.3</cell><cell>86.9</cell><cell>46.5</cell><cell>39.9</cell><cell>74.3</cell><cell>66.4</cell><cell>54.7</cell></row><row><cell>CNAP [48]</cell><cell>51.3</cell><cell>88.0</cell><cell>76.8</cell><cell>71.4</cell><cell>62.5</cell><cell>71.9</cell><cell>46.0</cell><cell>89.2</cell><cell>60.1</cell><cell>42.3</cell><cell>88.6</cell><cell>60.0</cell><cell>48.1</cell></row><row><cell>SCNAP [1]</cell><cell>58.6</cell><cell>91.7</cell><cell>82.4</cell><cell>74.9</cell><cell>67.8</cell><cell>77.7</cell><cell>46.9</cell><cell>90.7</cell><cell>73.5</cell><cell>46.2</cell><cell>93.9</cell><cell>74.3</cell><cell>60.5</cell></row><row><cell>SUR [7]</cell><cell>56.3</cell><cell>93.1</cell><cell>85.4</cell><cell>71.4</cell><cell>71.5</cell><cell>81.3</cell><cell>63.1</cell><cell>82.8</cell><cell>70.4</cell><cell>52.4</cell><cell>94.3</cell><cell>66.8</cell><cell>56.6</cell></row><row><cell>URT [34]</cell><cell>55.7</cell><cell>94.9</cell><cell>85.8</cell><cell>76.3</cell><cell>71.8</cell><cell>82.5</cell><cell>63.5</cell><cell>88.2</cell><cell>69.4</cell><cell>52.2</cell><cell>94.8</cell><cell>67.3</cell><cell>56.9</cell></row><row><cell>METAQDA</cell><cell>56.5</cell><cell>96.3</cell><cell>86.5</cell><cell>75.1</cell><cell>73.4</cell><cell>82.6</cell><cell>63.7</cell><cell>87.4</cell><cell>73.8</cell><cell>49.8</cell><cell>94.3</cell><cell>68.2</cell><cell>57.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 : Full details of testing performance on the extended meta-dataset benchmark.</head><label>10</label><figDesc>Left is the in-domain (seen) dataset performance, where MetaQDA ranks first 5 times in 8 domains. Right is the out-of-domain (unseen) dataset performance, where MetaQDA ranks first 3 times in 5 domains. Overall, MetaQDA has state-of-the-art performance.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements This work was supported by the Engineering and Physical Sciences Research Council of the UK (EPSRC) Grant number EP/S000631/1 and the UK MOD University Defence Research Collaboration (UDRC)</head><p>in Signal Processing, and EPSRC Grant EP/R026173/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fairness in deep learning: A computational perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selecting relevant features from a multi-domain representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis. Texts in statistical science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Chapman &amp; Hall / CRC</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unraveling meta-learning: Understanding feature representations for few-shot tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renkun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeriia</forename><surname>Cherepanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A broader study of cross-domain few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empirical bayes for learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Few-shot learning with metric-agnostic conditional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art?m</forename><surname>Yankov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">O</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hodas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungsoo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06663</idno>
		<title level="m">Ai benchmark: All about deep learning on smartphones in 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward Scalable Verification for Safety-Critical Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsey</forename><surname>Kuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gottschlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykel</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SysML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A universal representation transformer layer for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The evidence framework applied to classification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="720" to="736" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring calibration in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchuan</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian meta-learning for the few-shot setting via deep kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Incremental few-shot learning with attention attractor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Explanationguided training for cross-domain few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08790</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot class-incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cross-domain few-shot classification via learned feature-wise transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A comparison of machine learning methods for cross-domain few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AJCAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro O O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Meta-learning without memorization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">RelationNet2: deep comparison columns for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><surname>Flood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
