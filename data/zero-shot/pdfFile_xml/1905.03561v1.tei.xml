<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>ENS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inria</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>ENS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">CIIRC, CTU</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>ENS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inria</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CIIRC, CTU</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we address the problem of finding reliable pixel-level correspondences under difficult imaging conditions. We propose an approach where a single convolutional neural network plays a dual role: It is simultaneously a dense feature descriptor and a feature detector. By postponing the detection to a later stage, the obtained keypoints are more stable than their traditional counterparts based on early detection of low-level structures. We show that this model can be trained using pixel correspondences extracted from readily available large-scale SfM reconstructions, without any further annotations. The proposed method obtains state-of-the-art performance on both the difficult Aachen Day-Night localization dataset and the InLoc indoor localization benchmark, as well as competitive performance on other benchmarks for image matching and 3D reconstruction. Figure 1: Examples of matches obtained by the D2-Net method. The proposed method can find image correspondences even under significant appearance differences caused by strong changes in illumination such as day-to-night, changes in depiction style or under image degradation caused by motion blur.</p><p>ciently via (approximate) nearest neighbor search [37] and the Euclidean distance. Sparse features offer a memory efficient representation and thus enable approaches such as Structure-from-Motion (SfM) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">53]</ref> or visual localization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b58">58]</ref> to scale. The keypoint detector typically considers low-level image information such as corners <ref type="bibr" target="#b18">[19]</ref> or blob-like structures <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32]</ref>. As such, local features can</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Establishing pixel-level correspondences between images is one of the fundamental computer vision problems, with applications in 3D computer vision, video compression, tracking, image retrieval, and visual localization.</p><p>Sparse local features <ref type="bibr">[6-8, 13, 14, 19, 30, 32-34, 50, 55, 56,60,65]</ref> are a popular approach to correspondence estimation. These methods follow a detect-then-describe approach that first applies a feature detector <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b65">65]</ref> to identify a set of keypoints or interest points. The detector then provides image patches extracted around the keypoints to the following feature description stage <ref type="bibr">[6-8, 14, 30, 33, 55, 56, 60, 65]</ref>. The output of this stage is a compact representation for each patch. Sparse local features offer a set of advantages: Correspondences can be matched effi-often be accurately localized in an image, which is an important property for 3D reconstruction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">53]</ref>.</p><p>Sparse local features have been applied successfully under a wide range of imaging conditions. However, they typically perform poorly under extreme appearance changes, e.g., between day and night <ref type="bibr" target="#b71">[71]</ref> or seasons <ref type="bibr" target="#b46">[46]</ref>, or in weakly textured scenes <ref type="bibr" target="#b59">[59]</ref>. Recent results indicate that a major reason for this observed drop in performance is the lack of repeatability in the keypoint detector: While local descriptors consider larger patches and potentially encode higher-level structures, the keypoint detector only considers small image regions. As a result, the detections are unstable under strong appearance changes. This is due to the fact that the low-level information used by the detectors is often significantly more affected by changes in lowlevel image statistics such as pixel intensities. Nevertheless, it has been observed that local descriptors can still be matched successfully even if keypoints cannot be detected reliably <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b71">71]</ref>. Thus, approaches that forego the detection stage and instead densely extract descriptors perform much better in challenging conditions. Yet, this gain in robustness comes at the price of higher matching times and memory consumption.</p><p>In this paper, we aim at obtaining the best of both worlds, i.e., a sparse set of features that are robust under challenging conditions and efficient to match and to store. To this end, we propose a describe-and-detect approach to sparse local feature detection and description: Rather than performing feature detection early on based on low-level information, we propose to postpone the detection stage. We first compute a set of feature maps via a Deep Convolutional Neural Network (CNN). These feature maps are then used to compute the descriptors (as slices through all maps at a specific pixel position) and to detect keypoints (as local maxima of the feature maps). As a result, the feature detector is tightly coupled with the feature descriptor. Detections thereby correspond to pixels with locally distinct descriptors that should be well-suited for matching. At the same time, using feature maps from deeper layers of a CNN enables us to base both feature detection and description on higher-level information <ref type="bibr" target="#b69">[69]</ref>. Experiments show that our approach requires significantly less memory than dense methods. At the same time, it performs comparably well or even better under challenging conditions (c.f . <ref type="figure">Fig. 1</ref>) such as day-night illumination changes <ref type="bibr" target="#b46">[46]</ref> and weakly textured scenes <ref type="bibr" target="#b59">[59]</ref>. Our approach already achieves state-of-the-art performance without any training. It can be improved further by finetuning on a large dataset of landmark scenes <ref type="bibr" target="#b26">[27]</ref>.</p><p>Naturally, our approach has some drawbacks too: Compared to classical sparse features, our approach is less efficient due to the need to densely extract descriptors. Still, this stage can be done at a reasonable efficiency via a single forward pass through a CNN. Detection based on higher-/ / (a) detect-then-describe (b) detect-and-describe <ref type="figure">Figure 2</ref>: Comparison between different approaches for feature detection and description. Pipeline (a) corresponds to different variants of the two-stage detect-then-describe approach. In contrast, our proposed pipeline (b) uses a single CNN which extracts dense features that serve as both descriptors and detectors.</p><p>level information inherently leads to more robust but less accurate keypoints -yet, we show that our approach is still accurate enough for visual localization and SfM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Local features. The most common approach to sparse feature extraction -the detect-then-describe approach -first performs feature detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref> and then extracts a feature descriptor <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b45">45]</ref> from a patch centered around each keypoint. The keypoint detector is typically responsible for providing robustness or invariance against effects such as scale, rotation, or viewpoint changes by normalizing the patch accordingly. However, some of these responsibilities might also be delegated to the descriptor <ref type="bibr" target="#b68">[68]</ref>. <ref type="figure">Fig. 2a</ref> illustrates the common variations of this pipeline, from using hand-crafted detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref> and descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b45">45]</ref>, replacing either the descriptor <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref> or detector <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b70">70]</ref> with a learned alternative, or learning both the detector and descriptor <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b65">65]</ref>. For efficiency, the feature detector often considers only small image regions <ref type="bibr" target="#b65">[65]</ref> and typically focuses on low-level structures such as corners <ref type="bibr" target="#b18">[19]</ref> or blobs <ref type="bibr" target="#b30">[30]</ref>. The descriptor then captures higher level information in a larger patch around the keypoint. In contrast, this paper proposes a single branch describe-and-detect approach to sparse feature extraction, as shown in <ref type="figure">Fig. 2b</ref>. As a result, our approach is able to detect keypoints belonging to higher-level structures and locally unique descriptors. The work closest to our approach is SuperPoint <ref type="bibr" target="#b12">[13]</ref> as it also shares a deep representation between detection and description. However, they rely on different decoder branches which are trained independently with specific losses. On the contrary, our method shares all parameters between detection and description and uses a joint formulation that simultaneously optimizes for both tasks. Our experiments demonstrate that our describe-and-detect strategy performs significantly bet-ter under challenging conditions, e.g., when matching daytime and night-time images, than the previous approaches.</p><p>Dense descriptor extraction and matching. An alternative to the detect-then-describe approach is to forego the detection stage and perform the description stage densely across the whole image <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b53">53]</ref>. In practice, this approach has shown to lead to better matching results than sparse feature matching <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b71">71]</ref>, particularly under strong variations in illumination <ref type="bibr" target="#b71">[71]</ref>. This identifies the detection stage as a significant weakness in detect-thendescribe methods, which has motivated our approach. Image retrieval. The task of image retrieval <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62]</ref> also deals with finding correspondences between images in challenging situations with strong illumination or viewpoint changes. Several of these methods start by dense descriptor extraction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62]</ref> and later aggregate these descriptors into a compact image-level descriptor for retrieval. Works most related to our approach are <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b61">61]</ref>: <ref type="bibr" target="#b38">[38]</ref> develops an approach similar to ours, where an attention module is added on top of the dense description stage to perform keypoint selection. However, their method is designed to produce only a few reliable keypoints as to reduce the false positive matching rate during retrieval. Our experiments demonstrate that our approach performs significantly better for matching and camera localization; <ref type="bibr" target="#b61">[61]</ref> implicitly detects a set of keypoints as the global maxima of all feature maps, before pooling this information into a global image descriptor. <ref type="bibr" target="#b61">[61]</ref> has inspired us to detect features as local maxima of feature maps.</p><p>Object detection. The proposed describe-and-detect approach is also conceptually similar to modern approaches used in object detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref>. These methods also start by a dense feature extraction step, which is followed by the scoring of a set of region proposals. A non-maximalsuppression stage is then performed to select only the most locally-salient proposals with respect to a classification score. Although these methods share conceptual similarities, they target a very different task and cannot be applied directly to obtain pixel-wise image correspondences. This work builds on these previous ideas and proposes a method to perform joint detection and descriptions of keypoints, presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Detection and Description Pipeline</head><p>Contrary to the classical detect-then-describe approaches, which use a two-stage pipeline, we propose to perform dense feature extraction to obtain a representation that is simultaneously a detector and a descriptor. Because both detector and descriptor share the underlying representation, we refer to our approach as D2. Our approach is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>The first step of the method is to apply a CNN F on the input image I to obtain a 3D tensor F = F(I), F ? R h?w?n , where h?w is the spatial resolution of the feature maps and n the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Description</head><p>As in other previous work <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b59">59]</ref>, the most straightforward interpretation of the 3D tensor F is as a dense set of descriptor vectors d:</p><formula xml:id="formula_0">d ij = F ij: , d ? R n ,<label>(1)</label></formula><p>with i = 1, . . . , h and j = 1, . . . , w. These descriptor vectors can be readily compared between images to establish correspondences using the Euclidean distance. During the training stage, these descriptors will be adjusted such that the same points in the scene produce similar descriptors, even when the images contain strong appearance changes. In practice, we apply an L2 normalization on the descriptors prior to comparing them:</p><formula xml:id="formula_1">d ij = d ij / d ij 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Detection</head><p>A different interpretation of the 3D tensor F is as a collection of 2D responses D [61]:</p><formula xml:id="formula_2">D k = F ::k , D k ? R h?w ,<label>(2)</label></formula><p>where k = 1, . . . , n. In this interpretation, the feature extraction function F can be thought of as n different feature detector functions D k , each producing a 2D response map D k . These detection response maps are analogous to the Difference-of-Gaussians (DoG) response maps obtained in Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b30">[30]</ref> or to the cornerness score maps obtained in Harris' corner detector <ref type="bibr" target="#b18">[19]</ref>. In our work, these raw scores are post-processed to select only a subset of locations as the output keypoints. This process is described next.</p><p>Hard feature detection. In traditional feature detectors such as DoG, the detection map would be sparsified by performing a spatial non-local-maximum suppression. However, in our approach, contrary to traditional feature detectors, there exist multiple detection maps D k (k = 1, . . . , n), and a detection can take place on any of them. Therefore, for a point (i, j) to be detected, we require:</p><formula xml:id="formula_3">(i, j) is a detection ?? D k ij is a local max. in D k , with k = arg max t D t ij .<label>(3)</label></formula><p>Intuitively, for each pixel (i, j), this corresponds to selecting the most preeminent detector D k (channel selection), and then verifying whether there is a local-maximum at position (i, j) on that particular detector's response map D k .  (i) local descriptors dij are simply obtained by traversing all the n feature maps D k at a spatial position (i, j); (ii) detections are obtained by performing a non-local-maximum suppression on a feature map followed by a non-maximum suppression across each descriptor -during training, keypoint detection scores sij are computed from a soft local-maximum score ? and a ratio-to-maximum score per descriptor ?.</p><p>Soft feature detection. During training, the hard detection procedure described above is softened to be amenable for back-propagation. First, we define a soft local-max. score</p><formula xml:id="formula_4">? k ij = exp D k ij (i ,j )?N (i,j) exp D k i j ,<label>(4)</label></formula><p>where N (i, j) is the set of 9 neighbours of the pixel (i, j) (including itself). Then, we define the soft channel selection, which computes a ratio-to-max. per descriptor that emulates channel-wise non-maximum suppression:</p><formula xml:id="formula_5">? k ij = D k ij max t D t ij .<label>(5)</label></formula><p>Next, in order to take both criteria into account, we maximize the product of both scores across all feature maps k to obtain a single score map:</p><formula xml:id="formula_6">? ij = max k ? k ij ? k ij .<label>(6)</label></formula><p>Finally, the soft detection score s ij at a pixel (i, j) is obtained by performing an image-level normalization:</p><formula xml:id="formula_7">s ij = ? ij (i ,j ) ? i j .<label>(7)</label></formula><p>Multiscale Detection. Although CNN descriptors have a certain degree of scale invariance due to pre-training with data augmentations, they are not inherently invariant to scale changes and the matching tends to fail in cases with a significant difference in viewpoint. In order to obtain features that are more robust to scale changes, we propose to use an image pyramid <ref type="bibr" target="#b1">[2]</ref>, as typically done in hand-crafted local feature detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref> or even for some object detectors <ref type="bibr" target="#b15">[16]</ref>. This is only performed during test time.</p><p>Given the input image I, an image pyramid I ? containing three different resolutions ? = 0.5, 1, 2 (corresponding to half resolution, input resolution, and double resolution) is constructed and used to extract feature maps F ? at each resolution. Then, the larger image structures are propagated from the lower resolution feature maps to the higher resolution ones, in the following way:</p><formula xml:id="formula_8">F ? = F ? + ?&lt;? F ? .<label>(8)</label></formula><p>Note that the feature maps F ? have different resolutions. To enable the summation in <ref type="formula" target="#formula_8">(8)</ref>, feature maps F ? are resized to the resolution of F ? using bilinear interpolation. Detections are obtained by applying the post-processing described above to the fused feature mapsF ? . In order to prevent re-detecting features, we use the following response gating mechanism: Starting at the coarsest scale, we mark the detected positions; these masks are upsampled (nearest neighbor) to the resolutions of the next scales; detections falling into marked regions are then ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Jointly optimizing detection and description</head><p>This section describes the loss, the dataset used for training, and provides implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training loss</head><p>In order to train the proposed model, which uses a single CNN F for both detection and description, we require an appropriate loss L that jointly optimizes the detection and description objectives. In the case of detection, we want keypoints to be repeatable under changes in viewpoint or illumination. In the case of description, we want descriptors to be distinctive, so that they are not mismatched. To this end, we propose an extension to the triplet margin ranking loss, which has been successfully used for descriptor learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">35]</ref>, to also account for the detection stage. We will first review the triplet margin ranking loss, and then present our extended version for joint detection and description.</p><p>Given a pair of images (I 1 , I 2 ) and a correspondence c : A ? B between them (where A ? I 1 , B ? I 2 ), our version of the triplet margin ranking loss seeks to minimize the distance of the corresponding descriptorsd <ref type="bibr" target="#b0">(1)</ref> A and d <ref type="bibr" target="#b1">(2)</ref> B , while maximizing the distance to other confounding descriptorsd <ref type="bibr" target="#b0">(1)</ref> N1 ord <ref type="bibr" target="#b1">(2)</ref> N2 in either image, which might exist due to similarly looking image structures. To this end, we define the positive descriptor distance p(c) between the corresponding descriptors as:</p><formula xml:id="formula_9">p(c) = d (1) A ?d (2) B 2 ,<label>(9)</label></formula><p>The negative distance n(c), which accounts for the most confounding descriptor for eitherd</p><formula xml:id="formula_10">(1) A ord (2)</formula><p>B , is defined as:</p><formula xml:id="formula_11">n(c) = min d (1) A ?d (2) N2 2 , d (1) N1 ?d (2) B 2 ,<label>(10)</label></formula><p>where the negative samples d</p><formula xml:id="formula_12">(1) N1 and d<label>(2)</label></formula><p>N2 are the hardest negatives that lie outside of a square local neighbourhood of the correct correspondence: <ref type="bibr" target="#b10">(11)</ref> and similarly for N 2 . The triplet margin ranking loss for a margin M can be then defined as:</p><formula xml:id="formula_13">N 1 = arg min P ?I1 d (1) P ?d (2) B 2 s.t. P ? A ? &gt; K ,</formula><formula xml:id="formula_14">m(c) = max 0, M + p(c) 2 ? n(c) 2 .<label>(12)</label></formula><p>Intuitively, this triplet margin ranking loss seeks to enforce the distinctiveness of descriptors by penalizing any confounding descriptor that would lead to a wrong match assignment. In order to additionally seek for the repeatability of detections, an detection term is added to the triplet margin ranking loss in the following way:</p><formula xml:id="formula_15">L(I 1 , I 2 ) = c?C s (1) c s (2) c q?C s (1) q s (2) q m(p(c), n(c)) ,<label>(13)</label></formula><p>where s</p><p>c and s <ref type="bibr" target="#b1">(2)</ref> c are the soft detection scores <ref type="formula" target="#formula_7">(7)</ref> at points A and B in I 1 and I 2 , respectively, and C is the set of all correspondences between I 1 and I 2 .</p><p>The proposed loss produces a weighted average of the margin terms m over all matches based on their detection scores. Thus, in order for the loss to be minimized, the most distinctive correspondences (with a lower margin term) will get higher relative scores and vice-versa -correspondences with higher relative scores are encouraged to have a similar descriptors distinctive from the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Data</head><p>To generate training data on the level of pixel-wise correspondences, we used the MegaDepth dataset <ref type="bibr" target="#b26">[27]</ref> consisting of 196 different scenes reconstructed from 1,070,468 internet photos using COLMAP <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b54">54]</ref>. The authors provide camera intrinsics / extrinsics and depth maps from Multi-View Stereo for 102,681 images.</p><p>In order to extract the correspondences, we first considered all pairs of images with at least 50% overlap in the sparse SfM point cloud. For each pair, all points of the second image with depth information were projected into the first image. A depth-check with respect to the depth map of the first image was run to remove occluded pixels. In the end, we obtained 327,036 image pairs. This dataset was split in a validation dataset with 18,149 image pairs (from 78 scenes, each with less than 500 image pairs) and a training dataset from the remaining 118 scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>The VGG16 architecture <ref type="bibr" target="#b57">[57]</ref>, pretrained on Ima-geNet <ref type="bibr" target="#b11">[12]</ref> and truncated after the conv4 3 layer, was used to initialize the feature extraction network F.</p><p>Training. The last layer of the dense feature extractor (conv4 3) was fine-tuned for 50 epochs using Adam <ref type="bibr" target="#b23">[24]</ref> with an initial learning rate of 10 ?3 , which was further divided by 2 every 10 epochs. A fixed number (100) of random image pairs from each scene are used for training at every epoch in order to compensate the scene imbalance present in the dataset. For each pair, we selected a random 256 ? 256 crop centered around one correspondence. We use a batch size of 1 and make sure that the training pairs present at least 128 correspondences in order to obtain meaningful gradients.</p><p>Testing. At test time, in order to increase the resolution of the feature maps, the last pooling layer (pool3) from F with a stride of 2 is replaced by an average pooling layer with a stride of 1. Then, the subsequent convolutional layers (conv4 1 to conv4 3) are replaced with dilated convolutions <ref type="bibr" target="#b21">[22]</ref> with a rate of 2, so that their receptive field remains unchanged. With these modifications, the obtained feature maps have a resolution of one fourth of the input resolution, which allows for more tentative keypoints and a better localization. The position of the detected keypoints is improved using a local refinement at feature map level following the approach used in SIFT <ref type="bibr" target="#b30">[30]</ref>. The descriptors are then bilinearly interpolated at the refined positions.</p><p>Our implementation will be available at https:// github.com/mihaidusmanu/d2-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>The main motivation behind our work was to develop a local features approach that is able to better handle challenging conditions. Firstly, we evaluate our method on a standard image matching task based on sequences with illumination or viewpoint changes. Then, we present the results of our method in two more complex computer vision pipelines: 3D reconstruction and visual localization. In particular, the visual localization task is evaluated under extremely challenging conditions such as registering nighttime images against 3D models generated from day-time   <ref type="figure">Figure 4</ref>: Evaluation on HPatches <ref type="bibr" target="#b4">[5]</ref> image pairs. For each method, the mean matching accuracy (MMA) as a function of the matching threshold (in pixels) is shown. We also report the mean number of detected features and the mean number of mutual nearest neighbor matches. Our approach achieves the best overall performance after a threshold of 6.5px, both using a single (SS) and multiple (MS) scales.</p><p>imagery <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref> and localizing images in challenging indoor scenes <ref type="bibr" target="#b59">[59]</ref> dominated by weakly textured surfaces and repetitive structures. Qualitative examples of the results of our method are presented in <ref type="figure">Fig. 1</ref>. Please see the supplementary material for additional qualitative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Matching</head><p>In a first experiment, we consider a standard image matching scenario where given two images we would like to extract and match features between them. For this experiment, we use the sequences of full images provided by the HPatches dataset <ref type="bibr" target="#b4">[5]</ref>. Out of the 116 available sequences collected from various datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67]</ref>, we selected 108. <ref type="bibr" target="#b0">1</ref> Each sequence consists of 6 images of progressively larger illumination (52 sequences without viewpoint changes) or viewpoint changes (56 sequences without illumination changes). For each sequence, we match the first against all other images, resulting in 540 pairs.</p><p>Evaluation protocol. For each image pair, we match the features extracted by each method using nearest neighbor search, accepting only mutual nearest neighbors. A match is considered correct if its reprojection error, estimated using the homographies provided by the dataset, is below a given matching threshold. We vary the threshold and record the mean matching accuracy (MMA) <ref type="bibr" target="#b33">[33]</ref> over all pairs, i.e., the average percentage of correct matches per image pair.</p><p>As baselines for the classical detect-then-describe strategy, we use RootSIFT <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">30]</ref> with the Hessian Affine keypoint detector <ref type="bibr" target="#b32">[32]</ref>, a variant using a learned shape estimator (HesAffNet [36] -HAN) and descriptor (HardNet++ [35] -HN++ 2 ), and an end-to-end trainable variant (LF-Net <ref type="bibr" target="#b39">[39]</ref>). We also compare against SuperPoint <ref type="bibr" target="#b12">[13]</ref> and DELF <ref type="bibr" target="#b38">[38]</ref>, which are conceptually more similar to our approach.</p><p>Results. <ref type="figure">Fig. 4</ref> shows results for illumination and viewpoint changes, as well as mean accuracy over both conditions. For each method, we also report the mean number of detected features and the mean number of mutual nearest <ref type="bibr" target="#b0">1</ref> We left out sequences with an image resolution beyond 1200 ? 1600 pixels as not all methods were able to handle this resolution. <ref type="bibr" target="#b1">2</ref> HardNet++ was trained on the HPatches dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>neighbor matches per image. As can be seen, our method achieves the best overall performance for matching thresholds of 6.5 pixels or more. DELF does not refine its keypoint positions -thus, detecting the same pixel positions at feature map level yields perfect accuracy for strict thresholds. Even though powerful for the illumination sequences, the downsides of their method when used as a local feature extractor can be seen in the viewpoint sequences. For LF-Net, increasing the number of keypoints to more than the default value (500) worsened the results. However, <ref type="bibr" target="#b39">[39]</ref> does not enforce that matches are mutual nearest neighbors and we suspect that their method is not suited for this type of matching.</p><p>As can be expected, our method performs worse than detect-then-describe approaches for stricter matching thresholds: The latter use detectors firing at low-level bloblike structures, which are inherently better localized than the higher-level features used by our approach. At the same time, our features are also detected at the lower resolution of the CNN features.</p><p>We suspect that the inferior performance for the sequences with viewpoint changes is due to a major bias in our training dataset -roughly 90% of image pairs have a change in viewpoint lower than 20 ? (measured as the angle between the principal axes of the two cameras).</p><p>The proposed pipeline for multiscale detection improves the viewpoint robustness of our descriptors, but it also adds more confounding descriptors that negatively affect the robustness to illumination changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Reconstruction</head><p>In a second experiment, we evaluate the performance of our proposed describe-and-detect approach in the context of 3D reconstruction. This task requires well-localized features and might thus be challenging for our method.</p><p>For evaluation, we use three medium-scale internetcollected datasets with a significant number of different cameras and conditions (Madrid Metropolis, Gendarmenmarkt and Tower of London <ref type="bibr" target="#b64">[64]</ref>) from a recent local feature evaluation benchmark <ref type="bibr" target="#b52">[52]</ref>. All three datasets are small enough to allow exhaustive image matching, thus avoiding the need for using image retrieval.</p><p>Evaluation protocol. We follow the protocol defined by <ref type="bibr" target="#b52">[52]</ref> and first run SfM <ref type="bibr" target="#b51">[51]</ref>, followed by Multi-View Stereo (MVS) <ref type="bibr" target="#b54">[54]</ref>. For the SfM models, we report the number of images and 3D points, the mean track lengths of the 3D points, and the mean reprojection error. For the MVS models, we report the number of dense points. Except for the reprojection error, larger numbers are better. We use RootSIFT <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">30]</ref> (the best perfoming method according to the benchmark's website) and GeoDesc <ref type="bibr" target="#b31">[31]</ref>, a state-of-theart trained descriptor 3 as baselines. Both follow the detectthen-describe approach to local features.</p><p>Results. Tab. 1 shows the results of our experiment. Overall, the results show that our approach performs on par with state-of-the-art local features on this task. This shows that, even though our features are less accurately localized compared to detect-then-describe approaches, they are sufficiently accurate for the task of SfM as our approach is still able to register a comparable number of images.</p><p>Our method reconstructs fewer 3D points due to the strong ratio test filtering <ref type="bibr" target="#b30">[30]</ref> of the matches that is performed in the 3D reconstruction pipeline. While this filtering is extremely important to remove incorrect matches and prevent incorrect registrations, we noticed that for our method it also removes an important number of correct matches (20%-25%) <ref type="bibr" target="#b3">4</ref> , as the loss used for training our method does not take this type of filtering into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Localization under Challenging Conditions</head><p>The previous experiments showed that our approach performs comparable with the state-of-the-art in standard applications. In a final experiment, we show that our approach sets the state-of-the-art for sparse features under two very challenging conditions: Localizing images under severe illumination changes and in complex indoor scenes.</p><p>Day-Night Visual Localization. We evaluate our approach on the Aachen Day-Night dataset <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref> in a local reconstruction task <ref type="bibr" target="#b46">[46]</ref>: For each of the 98 night-time images contained in the dataset, up to 20 relevant day-time images with known camera poses are given. After exhaustive feature matching between the day-time images in each set, their known poses are used to triangulate the 3D structure of the scenes. Finally, these resulting 3D models are used to localize the night-time query images. This task was proposed in <ref type="bibr" target="#b46">[46]</ref> to evaluate the perfomance of local features in the context of long-term localization without the need for a specific localization pipeline.</p><p>We use the code and evaluation protocol from <ref type="bibr" target="#b46">[46]</ref> and report the percentage of night-time queries localized within  <ref type="bibr" target="#b52">[52]</ref>. Each method is used for the 3D reconstruction of each scene and different statistics are reported. Overall, our method obtains a comparable performance with respect to SIFT and its trainable counterparts, despite using less well-localized keypoints. a given error bound on the estimated camera position and orientation. We compare against upright RootSIFT descriptors extracted from DoG keypoints <ref type="bibr" target="#b30">[30]</ref>, HardNet++ descriptors with HesAffNet features <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>, DELF <ref type="bibr" target="#b38">[38]</ref>, Su-perPoint <ref type="bibr" target="#b12">[13]</ref> and DenseSfM <ref type="bibr" target="#b46">[46]</ref>. DenseSfM densely extracts CNN features using VGG16, followed by dense hierarchical matching (conv4 then conv3). For all methods with a threshold controlling the number of detected features (i.e. HAN + HN++, DELF, and SuperPoint), we employed the following tuning methodology: Starting from the default value, we increased and decreased the threshold gradually stopping as soon as the results started declining. Stricter localization thresholds were considered more important than looser ones. We reported the best results each method was able to achieve.</p><p>As can be seen from <ref type="figure">Fig. 5</ref>, our approach performs better than all baselines, especially for strict accuracy thresholds for the estimated pose. Our sparse feature approach even outperforms DenseSfM, even though the later is using significantly more features (and thus time and memory). The results clearly validate our describe-and-detect approach as it significantly outperforms detect-then-describe methods in this highly challenging scenario. The results also show that the lower keypoint accuracy of our approach does not prevent it from being used for applications aiming at estimating accurate camera poses.</p><p>Indoor Visual Localization. We also evaluate our approach on the InLoc dataset <ref type="bibr" target="#b59">[59]</ref>, a recently proposed benchmark dataset for large-scale indoor localization. The dataset is challenging due to its sheer size (?10k database images covering two buildings), strong differences in viewpoint and / or illumination between the database and query images, and changes in the scene over time.</p><p>For this experiment, we integrated our features into two variants of the pipeline proposed in <ref type="bibr" target="#b59">[59]</ref>, using the code released by the authors. The first variant, Direct Pose Es-  <ref type="figure">Figure 5</ref>: Evaluation on the Aachen Day-Night dataset <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref>. We report the percentage of images registered within given error thresholds. Our approach improves upon state-of-the art methods by a significant margin under strict pose thresholds.  timation (PE), matches features between the query image and the top-ranked database image found by image retrieval <ref type="bibr" target="#b2">[3]</ref> and uses these matches for pose estimation. In the second variant, Sparse PE, the query is matched against the top-100 retrieved images, and a spatial verification <ref type="bibr" target="#b40">[40]</ref> step is used to reject outliers matches. The query camera pose is then estimated using the database image with the largest number of verified matches. Tab. 2 compares our approach with baselines from <ref type="bibr" target="#b59">[59]</ref>: The original Direct / Sparse PE pipelines are based on affine covariant features with RootSIFT descriptors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref>. Dense PE matches densely extracted CNN descriptors between the images (using guided matching from the conv5 to the conv3 layer in a VGG16 network). As in <ref type="bibr" target="#b59">[59]</ref>, we report the percentage of query images localized within varying thresholds on their position error, considering only images with an orientation error of 10 ? or less. We also report the average memory usage of features per image. As can be seen, our approach outperforms both baselines.</p><p>In addition to Dense PE, the InLoc method proposed in <ref type="bibr" target="#b59">[59]</ref> also verifies its estimated poses using dense information (Dense Pose Verification (PV)): A synthetic image is rendered from the estimated pose and then compared to the query image using densely extracted SIFT descriptors. A similarity score is computed based on this comparison and used to re-rank the top-10 images after Dense PE. Only this baseline outperforms our sparse feature approach, albeit at a higher computational cost. Combining our approach with Dense PV also improves performance, but not to the level of InLoc. This is not surprising, given that InLoc is able to leverage dense data. Still, our results show that sparse methods can perform close to this strong baseline.</p><p>Finally, by combining our method and InLoc, we were able to achieve a new state of the art -we employed a pose selection algorithm using the Dense PV scores for the top 10 images of each method. In the end, 182 Dense PE poses and 174 Sparse PE (using D2 MS) poses were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a novel approach to local feature extraction using a describe-and-detect methodology. The detection is not done on low-level image structures but postponed until more reliable information is available, and done jointly with the image description. We have shown that our method surpasses the state-of-the-art in camera localization under challenging conditions such as day-night changes and indoor scenes. Moreover, even though our features are less well-localized compared to classical feature detectors, they are also suitable for 3D reconstruction.</p><p>An obvious direction for future work is to increase the accuracy at which our keypoints are detected. This could for example be done by increasing the spatial resolution of the CNN feature maps or by regressing more accurate pixel positions. Integrating a ratio test-like objective into our loss could help to improve the performance of our approach in applications such as SfM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>This supplementary material provides the following additional information: Section A details how we chose the threshold for Lowe's ratio test <ref type="bibr" target="#b30">[30]</ref> used for the 3D reconstructions in Section 5.2 in the paper. As mentioned in Section 4.3 in the paper, Section B provides implementation details on the architecture. In addition, the section also evaluates another backbone architecture (ResNet <ref type="bibr" target="#b19">[20]</ref>). Section C provides additional details on the loss function used to train our method. Section D shows qualitative examples for the matches found with our approach on the InLoc <ref type="bibr" target="#b59">[59]</ref> and Aachen Day-Night <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Impact of the ratio test on D2 features</head><p>Throughout our experiments on the local feature evaluation benchmark <ref type="bibr" target="#b52">[52]</ref>, we noticed that Lowe's ratio test <ref type="bibr" target="#b30">[30]</ref> plays an important role because it significantly reduces the number of wrong registrations due to repetitive structures and semantically similar scenes.</p><p>In order to find an adequate ratio threshold for our features, we employ Lowe's methodology <ref type="bibr" target="#b30">[30]</ref>: we compute the probability density functions (PDFs) of correct and incorrect matches with respect to the ratio test threshold. However, contrary to Lowe's evaluation, we considered only mutual nearest neighbors during the matching process.</p><p>Our evaluation was done on the entire HPatches <ref type="bibr" target="#b4">[5]</ref> image pairs dataset consisting of 580 pairs from 116 sequences (57 with illumination changes and 59 with viewpoint changes). A match is considered correct if its projection error, estimated using the homographies provided by the dataset, is below 4 pixels -the default threshold in COLMAP <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b54">54]</ref> during geometric verification and bundle adjustment. To take into account the possible errors in annotations and to have a clear separation between correct and incorrect matches, the threshold for incorrect matches is set to 20 pixels. Matches with projection errors between 4 and 20 pixels are therefore discarded during this evaluation. <ref type="figure" target="#fig_4">Figure 6</ref> shows the two PDFs. As can be seen, the D2 features do not work too well with ratio filtering because the mean ratio of correct matches is close to the one of incorrect matches. Still, we used thresholds of 0.90 for the off-the-shelf descriptors and 0.95 for the fine-tuned ones, which filter out 79.9% and 74.4% of incorrect matches, respectively. Unfortunately, these thresholds also discard a significant amount of correct matches (23.3% and 21.9%, respectively) which can have a negative impact on the number of registered images and sparse points.</p><p>In practice, we suggest not using the ratio test for camera localization under difficult conditions (e.g. day-night, indoors). For 3D reconstruction, using the threshold suggested above and / or increasing the minimum number of inlier matches required for an image pair to be considered  during Structure-from-Motion (SfM) should be sufficient to avoid most wrong registrations. Please note that, in the second case, the geometric verification can be significantly slower as RANSAC needs to handle a larger outlier ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the backbone architecture</head><p>For the feature extraction network F, we used a VGG16 network pretrained on the ImageNet dataset <ref type="bibr" target="#b11">[12]</ref>, truncated after the conv4 3 layer, as detailed in Section 4.3 of the paper. In addition, as also detailed in Section 4.3, we use a different image and feature resolution during training compared to testing. In particular, during testing, we take advantage of dilated convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b66">66]</ref> to increase the resolution of the feature maps -this is not done in training due to memory limitations. More detailed descriptions of the network architectures during the training and testing phases are provided in <ref type="table" target="#tab_5">Tables 3 and 4, respectively.</ref> We additionally assess the choice of the network used for feature extraction, by performing a comparison between the chosen VGG16 <ref type="bibr" target="#b57">[57]</ref> architecture and ResNet50 <ref type="bibr" target="#b19">[20]</ref> (which Layer Stride Dilation ReLU Resolution is the state of the art backbone architecture used in various other works). We evaluate them on the HPatches image pairs dataset using the same evaluation protocol that is described in Section 5.1 of the main paper.</p><formula xml:id="formula_17">input (256 ? 256) -3 ch. ?1 conv1 1 -3 ? 3, 64 ch. 1 1 ?1 conv1 2 -3 ? 3, 64 ch. 1 1 ?1 pool1 -2 ? 2, max. 2 1 ? 1 /2 conv2 1 -3 ? 3, 128 ch. 1 1 ? 1 /2 conv2 2 -3 ? 3, 128 ch. 1 1 ? 1 /2 pool2 -2 ? 2, max. 2 1 ? 1 /4 conv3 1 -3 ? 3, 256 ch. 1 1 ? 1 /4 conv3 2 -3 ? 3, 256 ch. 1 1 ? 1 /4 conv3 3 -3 ? 3, 256 ch. 1 1 ? 1 /4 pool3 -2 ? 2, max. 2 1 ? 1 /8 conv4 1 -3 ? 3, 512 ch. 1 1 ? 1 /8 conv4 2 -3 ? 3, 512 ch. 1 1 ? 1 /8 conv4 3 -3 ? 3, 512 ch. 1 1 ? 1 /8</formula><p>For both architectures, we used weights trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>. In the case of ResNet50, the network was truncated after conv4 6 (following the approach in DELF <ref type="bibr" target="#b38">[38]</ref>). At this point in the architecture, the resolution is 1 /16 th of the input resolution and the descriptors are 1024dimensional. However, in the case of the original VGG16, the output after conv4 3 has 1 /8 th of the input resolution and 512 channels. In order to account for this difference in resolution, we use dilated convolutions (also sometimes referred to as "atrous convolutions") to increase the resolution for the ResNet50 network. In addition, dilated convolutions are applied to both networks to further increase the feature resolution to 1 /4 th of the input resolution. For simplicity, only single-scale features are considered in this comparison.</p><p>The results can be seen in <ref type="figure" target="#fig_6">Figure 7</ref>. Dilated convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b66">66]</ref> increase the number of detections and the performance of D2 features especially in the case of viewpoint changes. The ResNet50 features also benefit from dilated convolutions and the increase in the resolution. However, although ResNet50 features seem slightly more robust to illumination changes and are able to outperform VGG16 features for thresholds larger than 6.5 pixels, they are less robust to viewpoint changes. Overall, ResNet50 features obtain worse results in this evaluation which motivated our decision to use VGG16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Stride Dilation ReLU Resolution the training architecture: the last pooling layer pool3 is replaced by an average pooling with a stride of 1 and the following convolutional layers are dilated by a factor of 2. This maintains the same receptive field but offers higher resolution feature maps.</p><formula xml:id="formula_18">input (? 1200 ? 1600) -3 ch. ?1 conv1 1 -3 ? 3, 64 ch. 1 1 ?1 conv1 2 -3 ? 3, 64 ch. 1 1 ?1 pool1 -2 ? 2, max. 2 1 ? 1 /2 conv2 1 -3 ? 3, 128 ch. 1 1 ? 1 /2 conv2 2 -3 ? 3, 128 ch. 1 1 ? 1 /2 pool2 -2 ? 2, max. 2 1 ? 1 /4 conv3 1 -3 ? 3, 256 ch. 1 1 ? 1 /4 conv3 2 -3 ? 3, 256 ch. 1 1 ? 1 /4 conv3 3 -3 ? 3, 256 ch. 1 1 ? 1 /4 pool3 -2 ? 2, avg. 1 1 ? 1 /4 conv4 1 -3 ? 3, 512 ch. 1 2 ? 1 /4 conv4 2 -3 ? 3, 512 ch. 1 2 ? 1 /4 conv4 3 -3 ? 3, 512 ch. 1 2 1 ? 1 /4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of the training loss</head><p>This section gives more insight into the loss L that we used for fine-tuning the conv4 3 layer of the VGG16 network. In particular, in <ref type="figure" target="#fig_7">Figure 8</ref> we explain in more detail the in-image-pair negative mining expressed in Equations (10) and (11) of the paper.</p><p>The parameter K controls the size of the neighbourhood from where negative samples are not selected. For a value of K = 0, all feature map pixels apart from the considered correspondence c : A ? B are considered as possible negatives. In this case, a value of the margin loss m(c) lower than M (p(c) &lt; n(c)) signifies that A and B would be matched using mutual nearest neighbors. This is due to the symmetric negative selection. However, in practice, this is too restrictive since adjacent pixels have a significant overlap in their receptive field so the descriptors can be very close. Since the receptive field at the conv4 3 level is around 65 ? 65 pixels at the input resolution, we choose a value of K = 4 at the feature map level, which enforces that potential negatives have less than 50% spatial overlap.</p><p>Another parameter of the training loss is the margin M . Since the descriptors are L2 normalized, the squared distance between two descriptors is guaranteed to be lower than 4. We have settled for M = 1 as in previous work <ref type="bibr" target="#b35">[35]</ref>. It is worth noting that, due to the the negative mining scheme, this margin is rarely reached, i.e., the detection scores continue to be optimized. <ref type="figure">Figure 9</ref> shows the soft detection scores before and after fine-tuning. As expected, some salient points have increased scores, while repetitive structures are weighted down. Even though most of our training data is from outdoors scenes, these observations seem to translate well to indoors images too.    Since adjacent pixels at feature map level have overlapping receptive fields in the input image, the negative descriptor is chosen to be at least K pixels away from the ground-truth correspondence. <ref type="figure">Figures 10 and 11</ref> show examples from the InLoc [59] dataset: firstly, we show a few good matches in challenging conditions (significant viewpoint changes and textureless areas) and then we illustrate the main failure modes of D2 features on indoors scenes (repeated objects / patterns). <ref type="figure">Figure 12</ref> shows some example matches on the difficult scenes from the Aachen Day-Night <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref> camera localization challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Off-the-shelf Trained <ref type="figure">Figure 9</ref>: Soft detection scores for different scenes before and after fine-tuning. White represents low soft-detection scores while red signifies higher ones. The training lowers the soft-detection scores on repetitive structures (e.g. ground, floor, walls) while it enhances the score on more distinctive points. This shown by the increased contrast of the trained soft-detection maps with respect to their off-the-shelf counterparts. <ref type="figure">Figure 10</ref>: Examples of correctly matched image pairs from the InLoc <ref type="bibr" target="#b59">[59]</ref> dataset. Our features are robust to significant changes in viewpoint as it can be seen in the first example. In textureless areas, our features act as an object matcher -correspondences are found between the furniture of different scenes. Sometimes, matches are even found across windows on nearby buildings. <ref type="figure">Figure 11</ref>: Failure cases from the InLoc <ref type="bibr" target="#b59">[59]</ref> dataset. Even though they are visually correct, the matches sometimes put in correspondence identical objects from different scenes. Another typical error case is due to repeated patterns (e.g. on carpets) which yield a significant number of inliers. <ref type="figure">Figure 12</ref>: Examples of correctly matched image pairs from the Aachen Day-Night <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b48">48]</ref> dataset. Our features consistently provide a significant number of good matches between images with strong illumination changes. The first two image pairs come from scenes where no other method was able to register the night-time image. For the last two, DELF <ref type="bibr" target="#b38">[38]</ref> was the only other method that succeeded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Proposed detect-and-describe (D2) network. A feature extraction CNN F is used to extract feature maps that play a dual role:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Ratio PDFs for D2 multi-scale features. PDF in terms of ratio on the full HPatches<ref type="bibr" target="#b4">[5]</ref> image pairs dataset for the D2 off-the-shelf and fine-tuned features. There is no clear separation between the mean ratios of correct and incorrect matches as in the case of SIFT<ref type="bibr" target="#b30">[30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Evaluation of different backbone architectures on the HPatches image pairs. The original networks are in bold -the others were obtained by removing the stride of the deepest layers and adding dilations to the subsequent ones. Dilated convolutions offer more keypoints and better performance in viewpoint sequences. VGG16 outperforms ResNet50 by a significant margin even at a similar feature map resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>In-image-pair negative mining procedure. For each correspondence c : A ? B, the negative sample is chosen between the hardest negative of A in I2 (N2) or of B in I1 (N1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on the Local Feature Evaluation Benchmark</figDesc><table><row><cell></cell><cell></cell><cell cols="3">#Reg. # Sparse. Track Reproj. # Dense</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Images Points Length Error Points</cell></row><row><cell>Madrid</cell><cell>RootSIFT [4, 30]</cell><cell>500</cell><cell>116K</cell><cell>6.32 0.60px 1.82M</cell></row><row><cell>Metropolis</cell><cell>GeoDesc [31]</cell><cell>495</cell><cell>144K</cell><cell>5.97 0.65px 1.56M</cell></row><row><cell cols="2">1344 images D2 MS (ours)</cell><cell>501</cell><cell>84K</cell><cell>6.33 1.28px 1.46M</cell></row><row><cell></cell><cell cols="2">D2 MS trained (ours) 495</cell><cell>144K</cell><cell>6.39 1.35px 1.46M</cell></row><row><cell cols="2">Gendarmen-RootSIFT [4, 30]</cell><cell>1035</cell><cell>338K</cell><cell>5.52 0.69px 4.23M</cell></row><row><cell>markt</cell><cell>GeoDesc [31]</cell><cell cols="2">1004 441K</cell><cell>5.14 0.73px 3.88M</cell></row><row><cell cols="2">1463 images D2 MS (ours)</cell><cell cols="2">1053 250K</cell><cell>5.08 1.19px 3.49M</cell></row><row><cell></cell><cell cols="2">D2 MS trained (ours) 965</cell><cell>310K</cell><cell>5.55 1.28px 3.15M</cell></row><row><cell>Tower of</cell><cell>RootSIFT [4, 30]</cell><cell>804</cell><cell>239K</cell><cell>7.76 0.61px 3.05M</cell></row><row><cell>London</cell><cell>GeoDesc [31]</cell><cell>776</cell><cell>341K</cell><cell>6.71 0.63px 2.73M</cell></row><row><cell cols="2">1576 images D2 MS (ours)</cell><cell>785</cell><cell>180K</cell><cell>5.32 1.24px 2.73M</cell></row><row><cell></cell><cell cols="2">D2 MS trained (ours) 708</cell><cell>287K</cell><cell>5.20 1.34px 2.86M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 1.0m, 5 ? 5.0m, 10 ? 10m, 25 ?</figDesc><table><row><cell></cell><cell>Upright RootSIFT</cell><cell cols="2">DenseSfM</cell><cell>HesAffNet + HardNet++</cell><cell cols="4">Correctly localized queries (%)</cell></row><row><cell>correct ly localized queries</cell><cell cols="2">0 SuperPoint 2.5 Dist ance t hreshold [ m ] 5 7.5 50% 60% 70% 80% 90% 100% DELF</cell><cell>10</cell><cell>D2 SS 0 Orient at ion t hreshold [ deg] D2 SS Trained 10 20</cell><cell>Method # Features 0.5m, 2 Upright RootSIFT [30] 11.3K 36.7 DenseSfM [46] 7.5K / 30K 39.8 HAN + HN++ [35, 36] 11.5K 39.8 SuperPoint [13] 6.6K 42.8 DELF [38] 11K 38.8 D2 SS (ours) 7K 41.8 D2 MS (ours) 11.4K 43.9 D2 SS Trained (ours) 14.5K 44.9 D2 MS Trained (ours) 19.3K 44.9</cell><cell>54.1 60.2 61.2 57.1 62.2 66.3 67.3 66.3 64.3</cell><cell>72.5 84.7 77.6 75.5 85.7 85.7 87.8 88.8 88.8</cell><cell>81.6 99.0 88.8 86.7 98.0 98.0 99.0 100 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the InLoc dataset<ref type="bibr" target="#b59">[59]</ref>. Our method outperforms SIFT by a large margin in both Direct PE and Sparse PE setups. It also outperforms the dense matching Dense PE method when used alone, while requiring less memory during pose estimation. By a combined approach of D2 and InLoc we obtained a new state-of-the art on this dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Training architecture. During training, we use the de-</figDesc><table><row><cell>fault VGG16 [57] architecture up to conv4 3, and fine-tune the</cell></row><row><cell>last layer (conv4 3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Testing architecture. At test time, we slightly modify</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In contrast to<ref type="bibr" target="#b31">[31]</ref>, we use the ratio test for matching with the threshold suggested by the authors -0.89.<ref type="bibr" target="#b3">4</ref> Please see the supplementary material for additional details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We noticed that ReLU has a significant negative impact on the off-theshelf descriptors, but not on the fine-tuned ones. Thus, we report results without ReLU for the off-the-shelf model and with ReLU for the fine-tuned one.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">Lindbjerg</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim Steenstrup</forename><surname>Pedersen</surname></persName>
		</author>
		<title level="m">Interesting interest points. IJCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">M</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HPatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Local Image Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BRIEF: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal Correspondence Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Increasing the accuracy of feature evaluation benchmarks using differential evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Differential Evolution (SDE)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SuperPoint: Self-Supervised Interest Point Detection and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-size pooling in local descriptors: DSP-SIFT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">E</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Zeeshan</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Deep Visual Representations for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Alvey Vision Conference</title>
		<meeting>the Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets, Time Frequency Methods and Phase Space</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consistent temporal variations in many outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BRISK: Binary robust invariant scalable keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Worldwide Pose Estimation using 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MegaDepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analyzing structures at different scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed; Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GeoDesc: Learning local descriptors by integrating geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timor</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="43" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiya</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Repeatability Is Not Enough: Learning Discriminative Affine Regions via Discriminability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji?i</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable Nearest Neighbor Algorithms for High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2227" to="2240" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Largescale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LF-Net: Learning local features from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Finetuning CNN Image Retrieval with No Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Benchmarking 6DoF outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matching neural paths: transfer from recognition to correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quad-networks: unsupervised learning to rank for interest point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihito</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Torsten Sattler, and Marc Pollefeys</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Comparative evaluation of hand-crafted and learned local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic Visual Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Deep Convolutional Feature Point Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning Local Feature Descriptors Using Convex Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1573" to="1585" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fredrik Kahl, and Magnus Oskarsson. City-Scale Localization for Cameras with Known Vertical Direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Sv?rm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olof</forename><surname>Enqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1455" to="1461" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">InLoc: Indoor visual localization with dense matching and view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DAISY: An Efficient Dense Descriptor Applied to Wide-Baseline Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">24/7 Place Recognition by View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving the robustness in feature detection by local contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Chrysostomou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Imaging Systems and Techniques (IST)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Rigas Kouskouridas, and Antonios Gasteratos</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust global translations with 1dsfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">ASIFT: An algorithm for fully affine invariant comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="11" to="38" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generic 3D Representation via Pose Estimation and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Wekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to Detect Features in Texture Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Evaluating Local Features for Day-Night Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
