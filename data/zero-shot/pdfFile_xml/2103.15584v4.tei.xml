<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Busy-Quiet Video Disentangling for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<email>adrian.bors@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Busy-Quiet Video Disentangling for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In video data, busy motion details from moving regions are conveyed within a specific frequency bandwidth in the frequency domain. Meanwhile, the rest of the frequencies of video data are encoded with quiet information with substantial redundancy, which causes low processing efficiency in existing video models that take as input raw RGB frames. In this paper, we consider allocating intenser computation for the processing of the important busy information and less computation for that of the quiet information. We design a trainable Motion Band-Pass Module (MBPM) for separating busy information from quiet information in raw video data. By embedding the MBPM into a two-pathway CNN architecture, we define a Busy-Quiet Net <ref type="figure">(BQN)</ref>. The efficiency of BQN is determined by avoiding redundancy in the feature space processed by the two pathways: one operating on Quiet features of low-resolution, while the other processes Busy features. The proposed BQN outperforms many recent video processing models on Something-Something V1, Kinetics400, UCF101 and HMDB51 datasets. The code is available at: https://github.com/guoxih/busy-quiet-net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video classification is a fundamental problem in many video-based tasks. Applications such as autonomous driving technology, controlling drones and robots are driving the demand for new video processing methods. An effective way to extend the usage of Convolutional Neural Networks (CNNs) from the image to the video domain is by expanding the convolution kernels from 2D to 3D <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Since the progress made by I3D <ref type="bibr" target="#b3">[4]</ref>, the main research effort in the video area has been directed towards designing new 3D architectures. However, 3D CNNs are more computationally intensive than 2D CNNs. Some recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> increase the efficiency of 3D CNNs by reducing the redundancy in the model parameters. However, these works have ignored another important factor that MBPM <ref type="figure">Figure 1</ref>: Motion Band-Pass Module (MBPM) distills busy information from the frame sequences. For every three consecutive RGB frames, the MBPM generates a one-frame output, which substantially reduces the redundancy. causes the heavy computation in video processing: natural video data contains substantial redundancy in the spatiotemporal dimensions. In video data, busy information describes fast-changing motion happening in the boundaries of moving regions which is crucial for defining movement in video. Meanwhile, the quiet information, such as smooth background textures whose information is shared by neighboring locations, contains substantial redundancy. For efficient processing, we disentangle a video into busy and quiet components. Subsequently, we would separately process the busy and quiet components, by allocating highcomplexity processing for the busy information and lowcomplexity processing for the quiet information.</p><p>In this study we propose a lightweight, end-to-end trainable motion feature extraction mechanism called Motion Band-Pass Module (MBPM), which can distill the motion information conveyed within a specific frequency bandwidth in the frequency domain. As illustrated in <ref type="figure">Figure 1</ref>, by applying the MBPM to a video of 3 segments the number of representative frames is reduced from 9 to 3 whilst re-taining the essential motion information. Our experiments demonstrate that by simply replacing the RGB frame input with the motion representation extracted by our MBPM, the performance of existing video models can be boosted. Secondly, we design a two-pathway multi-scale architecture, called the Busy-Quiet Net (BQN), the processing pipeline of which is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. One pathway, called Busy, is responsible for processing the busy information distilled by the MBPM. The other pathway, called Quiet, is devised to process the quiet information encoded with global smooth spatio-temporal structures. In order to fuse the information from different pathways, we design the Band-Pass Lateral Connection (BPLC) module, which is set up between the Busy and Quiet pathways. During the experiments, we demonstrate that the BPLC is the key factor to the overall model optimization success.</p><p>Compared with the frame summarization approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46]</ref>, MBPM retains the strict temporal order of the frame sequences, which is considered essential for long-term temporal relation modeling. Compared with optical flow-based motion representation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>, the motion representation captured by MBPM has a smaller temporal size (e.g. for every 3 RGB frames, the MBPM encodes only one frame), and can be employed on the fly. Meanwhile, efficient video models such as Octave Convolution <ref type="bibr" target="#b5">[6]</ref>, bL-Net <ref type="bibr" target="#b4">[5]</ref> and SlowFast networks <ref type="bibr" target="#b12">[13]</ref> only reduce the input redundancy along either the spatial or temporal dimensions. Instead, the proposed BQN reduces the redundancy in the joint spatio-temporal space.</p><p>Our contributions can be summarized as follows:</p><p>? A novel Motion Band-Pass Module (MBPM) is proposed for busy motion information distillation. The new motion cue extracted by the MBPM is shown to reduce temporal redundancy significantly.</p><p>? We design a two-pathway Busy-Quiet Net (BQN) that separately processes the busy and quiet information in videos. By separating the busy information using MBPM, we can safely downsample the quiet information to further reduce redundancy.</p><p>? Extensive experiments demonstrate the superiority of the proposed BQN over a wide range of models on four standard video benchmarks: Kinetics400 <ref type="bibr" target="#b3">[4]</ref>, Something-Something V1 <ref type="bibr" target="#b15">[16]</ref>, UCF101 <ref type="bibr" target="#b37">[38]</ref>, HMDB51 <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Spatio-temporal Networks. Early works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref> attempt to extend the success of 2D CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> in the image domain to the video domain. Representatively, the two-stream model <ref type="bibr" target="#b35">[36]</ref> and its variants <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref> utilize optical flow as an auxiliary input modality for effective temporal modeling. Other works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, given the progress in GPU performance, tend to exploit the computationally intensive 3D convolution. Meanwhile, some studies focus on improving the efficiency of 3D CNN, such as P3D <ref type="bibr" target="#b33">[34]</ref>, R(2+1)D <ref type="bibr" target="#b44">[45]</ref>, S3D <ref type="bibr" target="#b49">[50]</ref>, TSM <ref type="bibr" target="#b29">[30]</ref>, CSN <ref type="bibr" target="#b43">[44]</ref>, X3D <ref type="bibr" target="#b11">[12]</ref>. Non-local Net <ref type="bibr" target="#b47">[48]</ref> and its variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> introduce self-attention mechanisms to CNNs in order to learn long-range dependencies in the spatio-temporal dimension.</p><p>Our study is complementary to these methods: our Busy-Quiet Net (BQN) can benefit from the efficiency of these CNNs by simply adopting them as backbones. Motion Representation. Optical flow as a short-term motion representation has been widely used in many video applications. However, the optical flow estimation in largescale video datasets is inefficient. Some recent works use deep learning to improve the optical flow estimation quality, such as FlowNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. Some other methods aim to explore new end-to-end trainable motion cues, such as OFF <ref type="bibr" target="#b38">[39]</ref>, TVNet <ref type="bibr" target="#b9">[10]</ref>, EMV <ref type="bibr" target="#b52">[53]</ref>, Flow-of-Flow <ref type="bibr" target="#b32">[33]</ref>, Dynamic Image <ref type="bibr" target="#b1">[2]</ref>, Squeezed Image <ref type="bibr" target="#b19">[20]</ref> and PA <ref type="bibr" target="#b53">[54]</ref>. Compared with these approaches, our MBPM is rather as a basic video architecture component which results in higher accuracy while requiring less computation. Enforcing Low Information Redundancy. In the image field, bL-Net <ref type="bibr" target="#b4">[5]</ref> adopts a downsampling strategy that operates at the block level aiming to reduce the spatial redundancy of its feature maps. Octave Convolution <ref type="bibr" target="#b5">[6]</ref> replaces the convolutions in existing CNNs to decompose the low and high-frequency components in images, representing the former with lower resolution. In the video field, bLV-Net <ref type="bibr" target="#b10">[11]</ref> extends the idea of bL-Net <ref type="bibr" target="#b4">[5]</ref> to the temporal dimension. SlowFast networks <ref type="bibr" target="#b12">[13]</ref> introduce two pathways for slow and fast motion decomposition along the temporal dimension. However, the generalization of SlowFast to existing CNN architectures is poor, as it requires two specially customized CNNs to be its backbones. Unlike the previous methods, BQN reduces the feature redundancy in the joint spatio-temporal space by using a predefined trainable filter module, MBPM, to disentangle a video into busy and quiet components. Unlike SlowFast, BQN architecture shows excellent generalization to existing CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion Band-Pass Module (MBPM)</head><p>Firstly, we introduce a 3D band-pass filter, which can distill the video motion information conveyed within a specific spatio-temporal frequency bandwidth. A video clip of T frames can be defined as a function with three arguments, I (t) (x, y), where x, y indicate the spatial dimensions, while t is the temporal dimension. The value of I (t) (x, y) corresponds to the pixel value at position (x, y) in t-th frame of an arbitrary channel in the video. When considering the multi-channel case, we repeat the same procedure for each color channel, which is omitted here for the sake of simplification. The output ? of the 3D band-pass filter is given The backbone networks from the two pathways respectively take as inputs two complementary data components (i.e. busy and quiet), which are disentangled by the MBPM. The outputs of the two pathways are fused at various processing stages and the final prediction is obtained by averaging the prediction scores across multiple segments.</p><p>by:</p><formula xml:id="formula_0">?(x, y, t) = ? 2 ?t 2 I (t) (x, y) * LoG ? (x, y) , ? t?1?i?t+1 h(i) ? [I (i) (x, y) * LoG ? (x, y)], h(i) = 2 3 if i = t, ? 1 3 otherwise,</formula><p>(1) for t = 1, . . . , T and ' * ' represents the convolution operation. In equation <ref type="bibr" target="#b0">(1)</ref>, the second derivative with respect to t is numerically approximated by finite differences, literally implemented by function h(i). Meanwhile, LoG ? (x, y) is a two-dimensional Laplacian of Gaussian with the scale parameter ?:</p><formula xml:id="formula_1">LoG ? (x, y) = ? 2 G ? (x, y) = ? e ? x 2 +y 2 2? 2 ?? 4 1 ? x 2 + y 2 2? 2 .</formula><p>(2) From equations (1) and <ref type="bibr" target="#b1">(2)</ref> we can observe that the 3D filtering function is fully-differentiable. In order to make the 3D band-pass filtering compatible with CNNs, we approximate it with two sequential channel-wise 1 convolutional layers <ref type="bibr" target="#b34">[35]</ref>, as shown in <ref type="figure">Figure 1</ref>. We name the discrete approximation Motion Band-Pass Module (MBPM) which can be expressed in an engineering form as follows:</p><formula xml:id="formula_2">? ? MBPM(I) = H 3?1?1 s?1?1 (LoG 1?k?k ? (I)),<label>(3)</label></formula><p>where LoG 1?k?k ? is referred to as a spatial channel-wise convolutional layer <ref type="bibr" target="#b34">[35]</ref> with a k ? k kernel, each channel 1 Also referred to as "depth-wise". We use the term "channel-wise" to avoid confusions with the network depth. of which is initialized with a Laplacian of Gaussian distribution with scale ?. The sum of kernel values is normalized to 1. Meanwhile, H 3?1?1 s?1?1 is referred to as a temporal channel-wise convolutional layer with a temporal stride s.</p><formula xml:id="formula_3">In each channel, the kernel value of H 3?1?1 s?1?1 is initialized with [? 1 3 , 2 3 , ? 1 3 ]</formula><p>, which is a high-pass filter. In order to let the MBPM kernel parameters fine-tune on video streams, we embed the MBPM within a CNN for end-to-end training, optimized with the video classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Busy-Quiet Net (BQN)</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the BQN architecture contains two pathways, Busy and Quiet, operating in parallel on two distinct video data components, which are separated by the MBPM. The Busy and Quiet pathways are bridged by multiple Band-Pass Lateral Connections (see Section 4.2). These lateral connections enable information fusion between the two processing pathways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Busy and Quiet pathways</head><p>Busy pathway. The Busy pathway is designed to learn fine-grained movement features. It takes as input the information filtered by the MBPM, which contains critical motion information located at the boundaries of objects or regions that have significant temporal change. The stride of H 3?1?1 s?1?1 from equation <ref type="formula" target="#formula_2">(3)</ref> is set to s = 3, which means that for every three consecutive RGB frames, the MBPM generates one-frame output. The MBPM output preserves the temporal order of the video frames while significantly reducing the redundant temporal information. We intend to utilize larger spatial input sizes for the Busy pathway to extract more distinct textures. Quiet pathway. The Quiet pathway focuses on processing quiet information, representing the characteristics of large regions of movement, such as the movement happening in the plain-textured background regions. The input to the Quiet pathway is considered to be the complementary of the MBPM output:</p><formula xml:id="formula_4">2D-DownSamp(Avg 3?1?1 3?1?1 (I) ? ?),<label>(4)</label></formula><p>where Avg 3?1?1 3?1?1 is a temporal average pooling with a stride of 3. In the spatial dimensions, we perform bilinear downsampling (i.e. 2D-DownSamp) to reduce the redundant spatial information shared by neighboring locations. In Section 5.3, we explore the effect on performance when varying the input size of the Quiet pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Band-Pass Lateral Connection (BPLC)</head><p>In the proposed BQN, we include a novel Band-Pass Lateral Connection (BPLC) module which has an MBPM embedded. The BPLCs established between the two pathways, Busy and Quiet, provide a mechanism for information exchange, enabling an optimal fusion of video information characterized by different frequency bands. Different from the lateral connections in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>, the BPLC, enabled by MBPM, performs feature fusion and feature selection simultaneously, which shows higher performance than other lateral connection designs, according to the experimental results. We denote the two inputs of BPLC from the i-th residual blocks in the Busy and Quiet pathways, as x i f and x i c , respectively. For simplifying the notation, we assume that x i f and x i c are of the same size. When their sizes are different, we adopt bilinear interpolation to match them in size. The output y i f and y i c for the Busy and Quiet, respectively, is given by</p><formula xml:id="formula_5">y i f = BN(MBPM(x i c )) + x i f if mod(i, 2) = 0, x i f otherwise, y i c = x i c if mod(i, 2) = 0, BN(?(x i f )) + x i c otherwise, i = 1, 2, . . . , B<label>(5)</label></formula><p>where B denotes the number of residual blocks in the backbone network (considered as the network with residual block designs in the experiments). ?(?) is the linear transformation that can be implemented as a 1 ? 1 ? 1 convolution, or alternatively, when the channel number is very large, as a bottleneck MLP for reducing computation. BN indicates Batch Normalization <ref type="bibr" target="#b23">[24]</ref>, the weights of which are initialized to zero. For the MBPM in BPLC, the convolutional stride of H 3?1?1 s?1?1 from equation <ref type="formula" target="#formula_2">(3)</ref> is set to s = 1, maintaining the same temporal size.</p><p>The fusion direction of BPLC reverses back and forth, as indicated in <ref type="figure" target="#fig_0">Figure 2</ref>, providing better communication Predict ion Accuracy (%) t rained 3 x 3 unt rained 3 x 3 t rained 7 x 7 unt rained 7 x 7 t rained 9 x 9 unt rained 9 x 9 for the two pathways than the unidirectional lateral connections in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref> whose information fusion direction is fixed always fusing the information from a certain pathway to the other. By default, we place a BPLC between the two pathways right after each pair of residual blocks. The MBPM embedded in BPLC acts as a soft feature selection gate, which allows only for the busy information from the Quiet pathway to flow to the Busy pathway during the information fusion process. The exploration of various lateral connection designs is provided in Section 5.3 and this setting is shown to give the best performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we provide the results of the experiments for the proposed MBPM and BQN. Then, we compare these results with the state-of-the-art. Unless otherwise stated, we use ResNet50 (R50) with TSM <ref type="bibr" target="#b29">[30]</ref>, as the backbone of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Implementation Details</head><p>Datasets. We evaluate our approach on Something-Something V1 <ref type="bibr" target="#b15">[16]</ref>, Kinetics400 <ref type="bibr" target="#b3">[4]</ref>, UCF101 <ref type="bibr" target="#b37">[38]</ref> and HMDB51 <ref type="bibr" target="#b26">[27]</ref>. Most of the videos in Kinetics400 (K400), UCF101 and HMDB51 can be accurately classified by only considering their background scene information, while the temporal relation between frames is not very important. In Something-Something (SS) V1, many action categories are symmetrical (e.g. "Pulling something from left to right" and "Pulling something from right to left"). Discriminating these symmetric actions requires models with strong temporal modeling ability.</p><p>Training &amp; Testing. Aside from X3D-M <ref type="bibr" target="#b11">[12]</ref>, the backbone networks are pretrained on ImageNet <ref type="bibr" target="#b6">[7]</ref>. For training, we utilize the dense sampling strategy <ref type="bibr" target="#b47">[48]</ref> for Kinetics400.   As for the other datasets, we utilize the uniform sampling strategy as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, where a video is equally di-vided into N segments, and 3 consecutive frames in each segment are randomly sampled to constitute a video clip of length T = 3N . Unless specified otherwise, a default video clip is composed of N = 8 segments with a spatial size of 224 2 . During the tests, we sample a single clip per video with center cropping for efficient inference <ref type="bibr" target="#b29">[30]</ref>, which is used in our ablation studies. When pursuing high accuracy, we consider sampling multiple clips&amp;crops from the video and averaging the prediction scores of multiple space-time "views" (spatial crops?temporal clips) used in <ref type="bibr" target="#b12">[13]</ref>. More training details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies for MBPM</head><p>Instantiations and Settings. In the MBPM, the scale ? from equation <ref type="formula">(2)</ref> and the kernel size of the spatial channelwise convolution LoG 1?k?k ? have a significant impact on the performance. We vary the scale ? and the kernel size to search for the optimal settings. Meanwhile, in order to highlight the importance of the training for the MBPM, we compare the performance when using trained MBPM with that of untrained MBPM whose kernel weights are not optimized with the classification loss. The results on SS V1 are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We summarize two facts: 1) the optimal value of ? for the MBPM changes when the kernel size changes, and the MBPM with ? = 1.1 and a spatial kernel size 9 ? 9 gives the best performance within the searching range. 2) optimizing the parameters of MBPM with the video classification loss generally produces higher prediction accuracy. In our preliminary work, we have verified that different datasets share the same optimal settings of MBPM. More results on other datasets are provided in Appendix B. In the following experiments, we set MBPM in the Busy pathway as trainable with the scale ? = 1.1 and the kernel size of 9 ? 9, unless specified otherwise.  Efficiency and Effectiveness of the MBPM. We draw an apple-to-apple comparison between the proposed MBPM and other motion representation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. The comparison results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The motion representations produced by these methods are used as inputs to the backbone network. The prediction scores are obtained by the average consensus of eight temporal segments <ref type="bibr" target="#b46">[47]</ref>. More details about the implementations can be found in Appendix C. The proposed MBPM outperforms the other motion representation methods by big margins, while its computation is nearly negligible, which strongly demonstrates the high efficiency and effectiveness of the MBPM. Moreover, the two-stream fusion of "RGB+MBPM" has higher accuracy than the fusion of "RGB+Flow", according to the results in <ref type="table" target="#tab_1">Table 2</ref>. Generalization to different CNNs. The proposed MBPM is a generic plug-and-play unit. The performance of existing video models could be boosted by simply placing an MBPM after their input layers. <ref type="table" target="#tab_1">Table 2</ref> show that MobileNetV2 <ref type="bibr" target="#b34">[35]</ref> and X3D-M <ref type="bibr" target="#b11">[12]</ref> have steady performance improvement after being equipped with our MBPM. Visualization Analysis. We provide the visualization results of four videos and their corresponding MBPM outputs in the top and bottom rows from Figures 4 (1)-(4). From these results it can be observed that the extracted representations are stable when jittering and other camera movements are present. Also the results from Figures 4 (1)-(4) show that MBPM not only suppresses the stationary information and the background movement, but also highlights the boundaries of moving objects, which are of vital importance for action discrimination. For example, in the "spinning poi" video, from <ref type="figure" target="#fig_3">Figure 4-(1)</ref>, MBPM highlights the poi's movement rather than the movement of the background or that of the performer. More visualization results and visual comparison with other motion representation methods are provided in Appendix D.  <ref type="bibr" target="#b35">[36]</ref> by averaging the predictions of two pathways trained separately. <ref type="table" target="#tab_2">Table 3a</ref> shows that the simple fusion of two individual pathways (Quiet+Busy) generates higher top-1 accuracy (50.3%) than the individual pathways, which indicates that the features learned by the Quiet pathway and by the Busy pathway are complementary. Surprisingly, BQN has 51.6% top-1 accuracy, which is 1.3% better than the Quiet+Busy fusion. The high-performance gain strongly demonstrates the advantages of the proposed BQN architecture. Fusion strategies applied at the end of the Busy and Quiet pathways also influence the performance of BQN. <ref type="table" target="#tab_2">Table 3c</ref> shows the results of different fusions. We observe that the average fusion gives the best result among the listed methods, while the concatenation fusion is second only to the averaging. Besides, placing the average fusion layer after the fully-connected layer is better than placing it before. Effectiveness of the BPLC. We can set a maximum of up to 16 BPLCs in the BQN architecture when using TSM R50 <ref type="bibr" target="#b29">[30]</ref> as the backbone 2 . For the BPLCs in stage res2, res3 and res4, we set the spatial kernel size of MBPM as 7 ? 7, and the scale ? = 0.9. As for the stage res5, whose feature size is relatively small, the kernel size is therefore set to 3 ? 3.  <ref type="figure" target="#fig_4">Figure 5</ref>, where LC-I and LC-II are unidirectional, and LC-III is bidirectional. The results from <ref type="table" target="#tab_2">Table 3e</ref> show that the bidirectional design LC-III has higher accuracy than the unidirectional designs LC-I and LC-II. Among the listed designs, the proposed BPLC, which reverses the information fusion direction back and forth, provides the highest accuracy. We also compare the BPLC with LC-V that does not contain an MBPM. As a result, LC-V shows lower accuracy than the BPLC, which demonstrates the importance of MBPM for the BPLC.</p><p>Spatial-temporal input size. In BQN, the Busy pathway takes as input the MBPM output, which has the same spatial size as the raw video clip, while the temporal size is onethird of the raw video clip length. Meanwhile, the Quiet pathway takes as input the complementary of the MBPM output, given by equation <ref type="formula" target="#formula_4">(4)</ref>. <ref type="table" target="#tab_2">Table 3d</ref> shows that with the same temporal size of 8 for the inputs, the spatial size combination of 160 2 and 256 2 for the Quiet and Busy, respectively, has slightly better top-1 accuracy (+0.1%) than the combination of 224 2 and 224 2 but saves 5.2 GFLOPs in computational cost. We also attempt to reduce the temporal input size of the Quiet pathway. However, this would result in a performance drop. One possible explanation is that due to the temporal average pooling in the Quiet pathway, the input's temporal size is already reduced to one-third of the raw video clip. An even smaller temporal size could fail to preserve the correct temporal order of the video, and therefore harms the temporal relation modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with the State-of-the-Art</head><p>We compare BQN with current state-of-the-art methods on the four datasets. In BQN, the Quiet and Busy pathways' spatial input size is set to 160 2 and 256 2 , respectively. Results on Something-Something V1. <ref type="table" target="#tab_3">Table 4</ref> summarizes the comprehensive comparison, including the inference protocols, corresponding computational costs (FLOPs) and the prediction accuracy. Our method surpasses all other methods by good margins. For example, the multiclip accuracy of BQN 24f with TSM R50 is 7.2% higher than NL I3D GCN 32f <ref type="bibr" target="#b48">[49]</ref> while requiring 5? fewer FLOPs. Among the models based on ResNet50, BQN 48f has the highest top-1 accuracy (54.3%), which surpasses the second-best, TEA 16f <ref type="bibr" target="#b28">[29]</ref>, by a margin of +2%. Furthermore, our signal-clip BQN 24f has higher accuracy (51.7%) than most other multi-clip models, requiring only 60 GFLOPs. By adopting a deeper backbone (TSM R101), BQN 48f has 54.9% top-1 accuracy, higher than any single model. When using X3D-M as the backbone, BQN achieves the ultimate efficiency, possessing very low redundancy in both feature channel and spatio-temporal dimensions. BQN with X3D-M processes 4? more video frames than vanilla X3D-M, with only 50% additional FLOPs. Compared with TSM R50 16f , BQN with X3D-M trained from scratch produces 3.4% higher top-1 accuracy with the computation complexity of 14% of TSM R50 16f . The ensemble version BQN En achieves the state-of-the-art top-1/5 accuracy (57.1%/84.2%). <ref type="table">Table 6</ref>: Results on HMDB51 and UCF101. We report the mean class accuracy over the three official splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone HMDB51 UCF101 Results on Kinetics400, UCF101 and HMDB51. <ref type="table" target="#tab_5">Table 5</ref> shows the comparison results on Kinetics400. For fair comparison, we only list the models with the spatial input size of 256 2 . BQN 72f with TSM R50 achieves 77.3%/93.2% top-1/5 accuracy, which is better than the 3D CNN-based architecture, I3D <ref type="bibr" target="#b3">[4]</ref>, by a big margin of +6.2%/3.9%. When BQN uses TSM R50 or X3D-M as the backbone, it consistently shows higher accuracy than SlowFast 4?16 . Particularly, BQN with X3D-M has 1.5% higher top-1 accuracy than SlowFast 4?16 , while requiring 3.7? fewer FLOPs. Meanwhile, BQN 72f with TSM R50 is 2.7% better than Oct-I3D <ref type="bibr" target="#b5">[6]</ref> for top-1 accuracy. The results on two smaller datasets, UCF101 and HMDB51, are shown in <ref type="table">Table 6</ref>, where we report the mean class accuracy over the three official splits. We pretrain our model on Kinetics400 to avoid overfitting. The accuracy of our method is obtained by the inference protocol (3 crops?2 clips). BQN with TSM R50 outperforms most other methods except for I3D RGB+Flow , which uses additional optical flow input modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper develops a novel video representation learning mechanism called Motion Band-Pass Module (MBPM). The MBPM can distill important motion cues corresponding to a set of band-pass spatio-temporal frequencies. We design a spatio-temporal architecture called Bus-Quiet Net (BQN), enabled by MBPM, to separately process busy and quiet video data information. The busy-quiet disentangling enables efficient video processing by allocating additional resources to the Busy stream and less to the Quiet. Our methods can also be used for video processing and analysis in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More training details</head><p>We train our models in 16 or 64 GPUs (NVIDIA Tesla V100), using Stochastic Gradient Descent (SGD) with momentum 0.9 and cosine learning rate schedule. In order to prevent overfitting, we add a dropout layer before the classification layer of each pathway in the BQN model. Following the experimental settings in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b46">47]</ref>, the learning rate and weight decay parameters for the classification layers are 5 times of the convolutional layers. Meanwhile, we only apply L2 regularization to the weights in the convolutional and classification layers to avoid overfitting. Hyperparameters for models based on ResNet. For Ki-netics400 <ref type="bibr" target="#b3">[4]</ref>, the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.08, 512 (8 samples per GPU), 100, 2e-4 and 0.5, respectively. For Something-Something V1 <ref type="bibr" target="#b15">[16]</ref>, these hyperparameters are set to 0.12, 256, 50, 8e-4 and 0.8, respectively. We use linear warm-up <ref type="bibr" target="#b31">[32]</ref> for the first 7 epochs to overcome early optimization difficulty. When fine-tuning the Kinetics models on UCF101 <ref type="bibr" target="#b37">[38]</ref> and HMDB51 <ref type="bibr" target="#b26">[27]</ref>, we freeze all of the batch normalization <ref type="bibr" target="#b23">[24]</ref> layers except for the first one to avoid overfitting, following the recipe in <ref type="bibr" target="#b46">[47]</ref>. The initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.001, 64 (4 samples per GPU), 10, 1e-4 and 0.8, respectively. Hyperparameters for models based on X3D-M. For Ki-netics400, the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.4, 256 (16 samples per GPU), 256, 5e-5 and 0.5, respectively. For Something-Something V1, the models trained from scratch use the followings hyperparameters: learning rate 0.2, batch size 256, total epochs 100, weight decay 5e-5 and dropout ratio 0.5. When fine-tuning the Kinetics models, the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.12, 256 (16 samples per GPU), 60, 4e-4 and 0.8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Studies for the MBPM settings</head><p>We search for the optimal settings of the scale ? and kernel size k ? k of MBPM on UCF101. The results are presented in <ref type="figure">Figure 6</ref>. We observe that the experimental results vary greatly under different settings. Nevertheless, the optimal scale is ? = 1.1 when setting the kernel size as 9 ? 9, which is the same as that on Something-Something dataset. Furthermore, we try a larger kernel (11 ? 11), but it shows a performance drop. We speculate that this is caused by insufficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details of Efficiency and Effectiveness of the MBPM</head><p>These additional explanations are useful for Section 5.2 from the main paper. We provide the implementation de- Predict ion Accuracy (%) t rained 3 x 3 unt rained 3 x 3 t rained 7 x 7 unt rained 7 x 7 t rained 9 x 9 unt rained 9 x 9 t rained 11 x 11 unt rained 11 x 11 <ref type="figure">Figure 6</ref>: Results on UCF101 when varying the scale ? and kernel size k ? k of the spatial channel-wise convolution in MBPM.</p><p>tails for the comparative experiments of MBPM with other mainstream motion representation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. We follow the experimental settings on PA <ref type="bibr" target="#b53">[54]</ref> for fair comparison. The backbone network for all the methods is ResNet50 <ref type="bibr" target="#b18">[19]</ref>. We use the computer code provided by the original authors for these methods to generate the network inputs. For any kind of motion representation, we divide the representation of a video into 8 segments and randomly select one frame of the representation for each segment. Following the practices in TSN <ref type="bibr" target="#b46">[47]</ref> and PA <ref type="bibr" target="#b53">[54]</ref>, the activation outputs of 8 segments are averaged for the final prediction score. In our reimplementation, Dynamic Image <ref type="bibr" target="#b1">[2]</ref> generates one dynamic image for every 6 consecutive RGB frames, which consumes the same number of RGB frames as PA <ref type="bibr" target="#b53">[54]</ref>. Our MBPM generates one representative frame for every 3 consecutive RGB frames. As for TVNet <ref type="bibr" target="#b9">[10]</ref> and TV-L1 Flow <ref type="bibr" target="#b51">[52]</ref>, a one-frame input to the backbone network is formed by stacking 5 frames of the estimated flow along the channel dimension, which totally consumes 6 RGB frames. All the models are pretrained on ImageNet. For Something-Something V1 and Kinetics400, we use the hyperparameters in Appendix A to train all the models. For UCF101, we set the initial learning rate, batch size, total epochs, weight decay and dropout ratio to 0.01, 64 (4 samples per GPU), 80, 1e-4 and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization examples</head><p>These additional explanations and results are useful for Section 5.2 from the main paper. In order to visually observe the difference between our MBPM and other motion representation method, in <ref type="figure">Figure 9</ref>, we show some example video frames and their corresponding motion representations generated by different methods. For a better view, we use the optical flow visualization approach used in <ref type="bibr" target="#b0">[1]</ref> to vi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Kernel visualization</head><p>In <ref type="figure" target="#fig_7">Figure 8</ref>, we visualize the kernel of the spatial convolution LoG 1?k?k ? of MBPM in the Busy pathway. Interestingly, before and after training, kernels always present a similar shape to Mexican hats in 3-dimensional space. In  <ref type="figure">Figure 9</ref>: Comparison between visualizations of different motion representations on the UCF101. TV-L1 Flow <ref type="bibr" target="#b51">[52]</ref> evaluates the movement in every spatial position, while TVNet <ref type="bibr" target="#b9">[10]</ref>, PA <ref type="bibr" target="#b53">[54]</ref> and our MBPM capture the outline of the moving objects. Best viewed in color and zoomed in. <ref type="figure" target="#fig_6">Figure 7</ref>, we visualize the first channel of the 64 filters in the first layers of the BQN and the baseline (TSM ResNet50). We can observe that the Busy and Quiet pathways' filters have quite distinct shapes in their kernels, suggesting that the Busy and Quiet pathways learned different types of features after training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>BQN is made up of two parallel pathways: Busy and Quiet. 'lc' indicates Band-Pass Lateral Connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results on SS V1 when varying the scale ? and kernel size k ? k of the spatial channel-wise convolution in MBPM. The results represent averages of multiple runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Videos and their MBPM outputs. The video clips (1)-(4) are from Kinetics, SS V1, UCF and HMDB, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Diagrams of various lateral connection (LC) designs. Bilinear interpolation is used for resizing the feature maps when x i c and x i f do not have the same spatial size. i refers to the index of the residual block. W ? and W 1 denote the weights of the linear transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the first channels of the 64 conv1 filters of BQN after training on Kinetics400. All the 64 filters have a size of 7?7. From left to right, in (a), (b) and (c), we respectively present the trained conv1 filters in the Busy pathway, Quiet pathway and TSM ResNet50. We observe that the kernels of the 64 filters in the Busy pathway have a similar line-like shape, while those for the filters in the Quiet pathway are more like larger blobs. The conv1 in TSM ResNet50 (baseline) contains both types of filters from the Busy and Quiet pathways. Best viewed in color and zoomed in.sualize the output of MBPM. The optical flow estimates the instantaneous velocity and direction of movement in every position (The color represents the direction of movement while the brightness represents the absolute value of instantaneous velocity in a position). In contrast, TVNet<ref type="bibr" target="#b9">[10]</ref>, PA<ref type="bibr" target="#b53">[54]</ref> and MBPM are more absorbed in the visual information presented in boundary regions where motion happens.Figures 10-11display the motion representation extracted by the MBPM for eight different sequences from various video datasets used for the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the spatial channel-wise convolution LoG 1?k?k ? of MBPM in the Busy pathway before and after training on Kinetics400. The 9 ? 9 channel-wise convolution is initialized with a Laplacian of Gaussian with the scale parameter ? = 1.1. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>"Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Playing violin" from UCF101 "Swing baseball" from HMDB51 "Apply eye makeup" from UCF101 "Draw sword" from HMDB51 Examples of video and the corresponding motion representations extracted by MBPM. "Playing poker" from Kinetics400 "Moving something away from something" from Something-Something V1 "Playing basketball" from Kinetics400 "Holding something next to something" from Something-Something V1 Examples of video and the corresponding motion representations extracted by MBPM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MBPM vs. other motion representation methods. ? denotes our reimplementation. The additional parameters and the required computation (FLOPs) are reported.</figDesc><table><row><cell>Rep. Method</cell><cell cols="5">Efficiency Metrics UCF101 SS V1 K400</cell></row><row><cell></cell><cell cols="2">FLOPs #Param.</cell><cell></cell><cell></cell></row><row><cell>RGB (baseline)</cell><cell>-</cell><cell>-</cell><cell>87.1</cell><cell cols="2">46.5 71.2</cell></row><row><cell>RGB Diff [47]</cell><cell>-</cell><cell>-</cell><cell>87.0</cell><cell cols="2">46.6 71.4</cell></row><row><cell cols="2">TV-L1 Flow [52] -</cell><cell>-</cell><cell>88.5</cell><cell cols="2">37.4 55.7</cell></row><row><cell>DI ? [2]</cell><cell>-</cell><cell>-</cell><cell>86.2</cell><cell cols="2">43.4 68.3</cell></row><row><cell cols="3">FlowNetC ? [23] 444G 39.2M</cell><cell>87.3</cell><cell>26.3</cell><cell>-</cell></row><row><cell cols="3">FlowNetS ? [23] 356G 38.7M</cell><cell>86.8</cell><cell>23.4</cell><cell>-</cell></row><row><cell>TVNet ? [10]</cell><cell>3.3G</cell><cell>0.2K</cell><cell>88.6</cell><cell cols="2">45.2 58.5</cell></row><row><cell>PA [54]</cell><cell>2.8G</cell><cell>1.1K</cell><cell>89.5</cell><cell cols="2">45.1 57.3</cell></row><row><cell>MBPM</cell><cell>0.3G</cell><cell>0.2K</cell><cell>90.3</cell><cell cols="2">48.0 72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Using different CNNs as backbones on SS V1. ResNet50 and MobileNetV2 have TSM<ref type="bibr" target="#b29">[30]</ref> embedded.</figDesc><table><row><cell>CNN backbone</cell><cell>Modality</cell><cell cols="3">pretrain Seg. (N ) Acc.</cell></row><row><cell></cell><cell>RGB</cell><cell></cell><cell></cell><cell>46.5</cell></row><row><cell>ResNet50 [19]</cell><cell>RGB+Flow MBPM</cell><cell>ImageNet</cell><cell>8</cell><cell>49.8 48.0</cell></row><row><cell></cell><cell>RGB+MBPM</cell><cell></cell><cell></cell><cell>50.3</cell></row><row><cell>MobileNetV2 [35]</cell><cell>RGB MBPM</cell><cell>ImageNet</cell><cell>8</cell><cell>38.7 39.8</cell></row><row><cell>X3D-M [12]</cell><cell>RGB MBPM</cell><cell>None</cell><cell>16</cell><cell>45.5 46.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation Studies for BQN on Something-Something V1. We show top-1 and top-5 prediction accuracy (%), as well as computational complexity measured in GFLOPs for a single crop &amp; single clip. Effect of BPLCs. The BQN model with more BPLCs has higher accuracy. Fusion Strategies. The fully-connected layers of the two pathways share the parameters.</figDesc><table><row><cell cols="8">Model Quiet Busy Quiet+Busy 50.3 79.0 Top-1 Top-5 GFLOPs 46.5 75.3 32.8 48.0 76.8 32.8 65.7 BQN 51.6 80.5 65.9 (a) Complementarity of Quiet and Busy. "Quiet" and "Busy" refer to that the Quiet and Busy pathways are trained separately. (c) Input size # BPLC Top-1 Top-5 GFLOPs 0 49.6 78.9 65.7 4 50.2 79.2 65.8 8 50.7 79.7 65.8 16 51.6 80.5 65.9 Average Average Max Concatenation before fc 51.3 80.2 before fc 50.9 79.8 after fc 51.6 80.5 after fc 50.1 78.7 (b) Fusion Method Position Top-1 Top-5 for Quiet Input size Design Top-1 Top-5 Stages # BPLC Top-1 Top-5 Top-1 Top-5 GFLOPs for Busy LC-I 50.9 79.8 res2 1 49.8 79.1</cell></row><row><cell>224 2 ? 8 224 2 ? 8 51.6 80.5</cell><cell>65.9</cell><cell>LC-II</cell><cell>50.9</cell><cell>79.7</cell><cell>res2,res3</cell><cell>2</cell><cell>50.1 78.7</cell></row><row><cell>160 2 ? 8 224 2 ? 8 51.3 80.1</cell><cell>50.5</cell><cell>LC-III</cell><cell>51.5</cell><cell>80.2</cell><cell>res2,res3,res4</cell><cell>3</cell><cell>50.2 79.0</cell></row><row><cell>160 2 ? 8 256 2 ? 8 51.7 80.5</cell><cell>60.7</cell><cell>BPLC</cell><cell>51.6</cell><cell>80.5</cell><cell>res2,res3,res4,res5</cell><cell>4</cell><cell>50.2 79.2</cell></row><row><cell>160 2 ? 6 256 2 ? 8 49.6 78.3</cell><cell>55.5</cell><cell>LC-V</cell><cell>51.3</cell><cell>79.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">(e) Various LC designs. 16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">LCs are set in the BQN.</cell><cell></cell><cell></cell><cell></cell></row></table><note>(d) Spatio-temporal input size. The input size is formatted with (width 2 ? time).(f) Stage for adding BPLCs. In each stage, we set one BPLC after its first residual block.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on Something V1. "N/A" indicates the numbers are not available. ? denotes our reimplementation.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>Backbone</cell><cell>Frames?Crops?Clips</cell><cell>FLOPs</cell><cell cols="3">#Param. Top-1 (%) Top-5 (%)</cell></row><row><cell>NL I3D GCN [49]</cell><cell></cell><cell>3D R50</cell><cell>32?3?2</cell><cell>303G?3?2</cell><cell>62.2M</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>TRNRGB+Flow [55]</cell><cell cols="2">BNInception</cell><cell>(8+48)?1?1</cell><cell>N/A</cell><cell>36.6M</cell><cell>42.0</cell><cell>-</cell></row><row><cell>TSMEn [30] TEA [29]</cell><cell>ImageNet</cell><cell>R50 R50</cell><cell>(16+8)?1?1 16?3?10</cell><cell>98G 70G?3?10</cell><cell>48.6M 24.4M</cell><cell>49.7 52.3</cell><cell>78.5 81.9</cell></row><row><cell>bLVNet-TAM [11]</cell><cell></cell><cell>bLR50</cell><cell>8?1?2</cell><cell>12G?1?2</cell><cell>25M</cell><cell>46.4</cell><cell>76.6</cell></row><row><cell>PAN Full [54]</cell><cell></cell><cell>TSM R50</cell><cell>40?1?2</cell><cell>67.7G?1?2</cell><cell>-</cell><cell>50.5</cell><cell>79.2</cell></row><row><cell>ir-CSN [44]</cell><cell>None</cell><cell>3D R152</cell><cell>32?1?10</cell><cell>96.7G?1?10</cell><cell>-</cell><cell>49.3</cell><cell>-</cell></row><row><cell>TSM R50 [30]</cell><cell></cell><cell>R50</cell><cell>16?1?1</cell><cell>65G?1?1</cell><cell>24.3M</cell><cell>47.2</cell><cell>77.1</cell></row><row><cell>BQN</cell><cell></cell><cell>TSM R50</cell><cell>24?1?1</cell><cell>60G?1?1</cell><cell>47.4M</cell><cell>51.7</cell><cell>80.5</cell></row><row><cell>BQN</cell><cell>ImageNet</cell><cell>TSM R50</cell><cell>24?3?2</cell><cell>60G?3?2</cell><cell>47.4M</cell><cell>53.3</cell><cell>82.0</cell></row><row><cell>BQN</cell><cell></cell><cell>TSM R50</cell><cell>48?3?2</cell><cell>121G?3?2</cell><cell>47.4M</cell><cell>54.3</cell><cell>82.0</cell></row><row><cell>BQN</cell><cell></cell><cell>TSM R101</cell><cell>48?3?2</cell><cell>231G?3?2</cell><cell>85.4M</cell><cell>54.9</cell><cell>81.7</cell></row><row><cell>X3D-M ? [12]</cell><cell>None</cell><cell>-</cell><cell>16?3?2</cell><cell>6.4G?3?2</cell><cell>3.3M</cell><cell>46.7</cell><cell>75.5</cell></row><row><cell>BQN</cell><cell>None</cell><cell>X3D-M</cell><cell>48?3?2</cell><cell>9.7G?3?2</cell><cell>6.6M</cell><cell>50.6</cell><cell>79.2</cell></row><row><cell>BQN</cell><cell>K400</cell><cell>X3D-M</cell><cell>48?3?2</cell><cell>9.7G?3?2</cell><cell>6.6M</cell><cell>53.7</cell><cell>81.8</cell></row><row><cell>BQNEn</cell><cell cols="2">ImageNet TSM R101 + K400 +X3D-M</cell><cell>(48+48)?3?2</cell><cell>241G?3?2</cell><cell>92M</cell><cell>57.1</cell><cell>84.2</cell></row><row><cell cols="2">5.3. Ablation Studies for BQN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>BQN vs. Quiet+Busy. In order to evaluate the effective- ness of the proposed BQN architecture, we compare BQN with the simple fusion (Quiet+Busy), which mimics the two-stream model</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3f</head><label>3f</label><figDesc></figDesc><table /><note>, illustrates that adding BPLCs to all processing stages is helpful for improving performance. From Table 3b, we can observe that the model performance improves gradually as the number of BPLCs increases. The substantial performance gains demonstrate the importance of using BPLCs for BQN. Lateral Connection (LC) Designs. In order to illustrate the rationality of the proposed BPLC design, we compare it with other LC designs. The diagrams of different LC designs are illustrated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison results on Kinetics400. We report the inference cost of multiple "views" (spatial crops ? temporal clips). ? denotes our reimplementation.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Frames ? views</cell><cell>FLOPs</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell cols="2">bLVNet-TAM [11] bLR50</cell><cell>16?9</cell><cell cols="3">561G 72.0 90.6</cell></row><row><cell>TSM [30]</cell><cell>R50</cell><cell cols="3">16?30 2580G 74.7</cell><cell>-</cell></row><row><cell>STM [25]</cell><cell>R50</cell><cell cols="4">16?30 2010G 73.7 91.6</cell></row><row><cell>X3D-M ? [12]</cell><cell>-</cell><cell>16?30</cell><cell cols="3">186G 75.1 92.2</cell></row><row><cell cols="2">SlowFast4?16 [13] 3D R50</cell><cell cols="4">32?30 1083G 75.6 92.1</cell></row><row><cell>ip-CSN [44]</cell><cell cols="5">3D R101 32?30 2490G 76.7 92.3</cell></row><row><cell>SmallBigNet [28]</cell><cell>R101</cell><cell cols="4">32?12 6552G 77.4 93.3</cell></row><row><cell>PAN Full</cell><cell cols="2">TSM R50 40?2</cell><cell cols="3">176G 74.4 91.6</cell></row><row><cell>I3DRGB [4]</cell><cell cols="5">Inc. V1 64?N/A N/A 71.1 89.3</cell></row><row><cell>Oct-I3D [6]</cell><cell>-</cell><cell cols="3">N/A?N/A N/A 74.6</cell><cell>-</cell></row><row><cell>NL I3D [48]</cell><cell cols="5">3D R101 128?30 10770G 77.7 93.3</cell></row><row><cell>BQN</cell><cell cols="5">TSM R50 48?10 1210G 76.8 92.4</cell></row><row><cell>BQN</cell><cell cols="5">TSM R50 72?10 1820G 77.3 93.2</cell></row><row><cell>BQN</cell><cell>X3D-M</cell><cell>48?30</cell><cell cols="3">291G 77.1 92.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ResNet50 contains four stages, named res2, res3, res4, res5, respectively. These stages are composed of 3, 4, 6, 3 residual blocks, respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work made use of the facilities of the N8 Centre of Excellence in Computationally Intensive Research (N8 CIR) provided and funded by the N8 research partnership and EPSRC (Grant No. EP/T022167/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Nonlocal networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision Workshops (ICCV-w)</title>
		<meeting>IEEE Int. Conference Computer Vision Workshops (ICCV-w)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03848</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3435" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Longterm recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artif</title>
		<meeting>AAAI Conference on Artif</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8401" to="8408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representations with temporal squeeze pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2103" to="2107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Region-based non-local operation for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Pattern Recognition (ICPR)</title>
		<meeting>Int. Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10010" to="10017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR 37</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference Mach. Learn. (ICML)</title>
		<meeting>Int. Conference Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9945" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conference Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference Mach. Learn. (ICML)</title>
		<meeting>Int. Conference Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy tradeoffs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Pattern Recog. Symp</title>
		<meeting>Joint Pattern Recog. Symp</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4713</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time action recognition with deeply transferred motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2326" to="2339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pan: Persistent appearance network with an efficient motion cue for fast action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conference Multimedia</title>
		<meeting>ACM Int. Conference Multimedia</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV), vol LNCS 11205</title>
		<meeting>European Conference Computer Vision (ECCV), vol LNCS 11205</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
