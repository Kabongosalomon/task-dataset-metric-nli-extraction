<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-08">8 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
							<email>sungheonpark@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Convergence Science and Technology</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Convergence Science and Technology</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Convergence Science and Technology</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-08">8 Sep 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human pose estimation</term>
					<term>convolutional neural network</term>
					<term>2D- 3D joint optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While there has been a success in 2D human pose estimation with convolutional neural networks (CNNs), 3D human pose estimation has not been thoroughly studied. In this paper, we tackle the 3D human pose estimation task with end-to-end learning using CNNs. Relative 3D positions between one joint and the other joints are learned via CNNs. The proposed method improves the performance of CNN with two novel ideas. First, we added 2D pose information to estimate a 3D pose from an image by concatenating 2D pose estimation result with the features from an image. Second, we have found that more accurate 3D poses are obtained by combining information on relative positions with respect to multiple joints, instead of just one root joint. Experimental results show that the proposed method achieves comparable performance to the state-of-the-art methods on Human 3.6m dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Both 2D and 3D human pose recovery from images are important tasks since the retrieved pose information can be used to other applications such as action recognition, crowd behavior analysis, markerless motion capture and so on. However, human pose estimation is a challenging task due to the dynamic variations of a human body. Various skin colors and clothes also make the estimation difficult. Especially, pose estimation from a single image requires a model that is robust to occlusion and viewpoint variations.</p><p>Recently, 2D human pose estimation achieved a great success with convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Strong representation power and the ability to disentangle underlying factors of variation are characteristics of CNNs that enable learning discriminative features automatically <ref type="bibr" target="#b3">[4]</ref> and show superior performance to the methods based on hand-crafted features. On the other hands, 3D human pose estimation using CNNs has not been studied thoroughly compared to the 2D cases. Estimating a 3D human pose from a single image is more challenging than 2D cases due to the lack of depth information. However, CNN can be a powerful framework for learning discriminative image features and estimating 3D poses from them. In the case where the target object is fixed such as human body, it is able to learn useful features directly from images without keypoint matching step in the typical 3D reconstruction tasks.</p><p>Though recent algorithms that are based on CNNs for 3D human pose estimation have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, they do not make use of 2D pose information which can provide additional information for 3D pose estimation. From 2D pose information, undesirable 3D joint positions which generate unnatural human pose may be discarded. Therefore, if the information that contains the 2D position of each joint in the input image is used, the results of 3D pose estimation can be improved.</p><p>In this paper, we propose a simple yet powerful 3D human pose estimation framework based on the regression of joint positions using CNNs. We introduce two strategies to improve the regression results from the baseline CNNs. Firstly, not only the image features but also 2D joint classification results are used as input features for 3D pose estimation. This scheme successfully incorporates the correlation between 2D and 3D poses. Secondly, rather than estimating relative positions with respect to only one root joint, we estimated the relative 3D positions with respect to multiple joints. This scheme effectively reduces the error of the joints that are far from the root joint. Experimental results validate the proposed framework significantly improves the baseline method and achieves comparable performance to the state-of-the-art methods on Human 3.6m dataset <ref type="bibr" target="#b7">[8]</ref> without utilizing the temporal information.</p><p>The rest of the paper is organized as follows. Related works are reviewed in Section 2. The structure of CNNs used in this paper and two key ideas of our method, 1) the integration of 2D joint classification results into 3D pose estimation and 2) multiple 3D pose regression from various root nodes, are explained in Section 3. Details of implementation and training procedures are explained in Section 4. Experimental results are illustrated in Section 5, and finally conclusions are made in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human pose estimation has been a fundamental task since early computer vision literature, and numerous researches have been conducted on both 2D and 3D human pose estimation. In this section, we will cover both 2D and 3D human pose estimation methods focusing on the CNN-based methods.</p><p>Early works for 2D human pose estimation which are based on deformable parts model <ref type="bibr" target="#b8">[9]</ref>, pictorial structure <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, or poselets <ref type="bibr" target="#b12">[13]</ref> train the relationship between body appearance and body joints using hand-crafted features. Recently proposed CNN based methods drastically improve the performance over the previous hand-crafted feature based methods. DeepPose <ref type="bibr" target="#b0">[1]</ref> used CNN-based structure to regress joint locations with multiple iterations. Firstly, it predicts an initial pose using holistic view and refine the currently predicted pose using relevant parts of the image. Xiaochuan et al. <ref type="bibr" target="#b13">[14]</ref> integrated both the local part appearance and the holistic view of an image using dual-source CNN. Convolu-tional pose machine <ref type="bibr" target="#b2">[3]</ref> is a systematic approach to improve prediction of each stage. Each stage operates a CNN which accepts both the original image and confidence maps from preceding stages as an input. The performance is improved by combining the joint prediction results from the previous step with features from CNN. Joao et al. <ref type="bibr" target="#b1">[2]</ref> proposed a self-correcting method by a top-down feedback. It iteratively learns a human pose using a self-correcting CNN model which gradually improves the initial result by feeding back error predictions. Xiao et al. <ref type="bibr" target="#b14">[15]</ref> proposed an end-to-end learning system which captures the relationships among feature maps of joints. Geometrical transform kernels are introduced to learn features and their relationship jointly.</p><p>Similar to the 2D case, early stage of 3D human pose estimation is also based on the low-level features such as local shape context <ref type="bibr" target="#b15">[16]</ref> or segmentation results <ref type="bibr" target="#b16">[17]</ref>. With the extracted features, 3D pose estimation is formulated as a regression problem using relevance vector machines <ref type="bibr" target="#b15">[16]</ref>, structured SVMs <ref type="bibr" target="#b16">[17]</ref>, or random forest classifiers <ref type="bibr" target="#b17">[18]</ref>. Recently, CNNs have drew a lot of attentions also for the 3D human pose estimation tasks. Since search space in 3D is much larger than 2D image space, 3D human pose estimation is often formulated as a regression problem rather than a classification task. Li and Chan <ref type="bibr" target="#b4">[5]</ref> firstly used CNNs to learn 3D human pose directly from input images. Relative 3D position to the parent joint is learned by CNNs via regression. They also used 2D part detectors of each joints in a sliding window fashion. They found that loss function which combines 2D joint classification and 3D joint regression helps to improve the 3D pose estimation results. Li et al. <ref type="bibr" target="#b5">[6]</ref> improved the performance of 3D pose estimation by integrating a structured learning framework into CNNs. Recently, Tekin et al. <ref type="bibr" target="#b6">[7]</ref> proposed a structured prediction framework which learns 3D pose representations using an auto-encoder. Temporal information from video sequences also helps to predict more accurate pose estimation result. Zhou et al. <ref type="bibr" target="#b18">[19]</ref> used the result of 2D pose estimation to reconstruct a 3D pose. They represented a 3D pose as a weighted sum of shape bases similar to typical non-rigid structure from motion, and they designed an EM-algorithm which formulates the 3D pose as a latent variable when 2D pose estimation results are available. The method achieved the state-of-the-art performance for 3D human pose estimation when combined with 2D pose predictions learned from CNN. Tekin et al. <ref type="bibr" target="#b19">[20]</ref> used multiple consecutive frames to build a spatio-temporal features, and the features are fed to a deep neural network regressor to estimate the 3D pose.</p><p>The method proposed in this paper aims to provide an end-to-end learning framework to estimate 3D structure of a human body from a single image. Similar to <ref type="bibr" target="#b4">[5]</ref>, 3D and 2D pose information are jointly learned in a single CNN. Unlike the previous works, we directly propagate the 2D classification results to the 3D pose regressors inside the CNNs. Using additional information such as 2D classification results and the relative distance from multiple joints, we improve the performance of 3D human pose estimation over the baseline method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Cross Entropy Loss</head><p>Ng ? Ng ? Nj <ref type="figure">Fig. 1</ref>. The baseline structure of CNN used in this paper. Convolutional and pooling layers are shared for both 2D and 3D losses, and the losses are attached to different fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D-2D Joint Estimation of Human Body Using CNN</head><p>The task of 3D human pose estimation is defined as predicting the 3D joint positions of a human body. Specifically, we estimate the relative 3D position of each joint with respect to the root joint. The number of joints N j is set to 17 in this paper according to the dataset used in the experiment. The key idea of our method is to train CNN which performs 3D pose estimation using both image features from the input image and 2D pose information retrieved from the same CNN. In other words, the proposed CNN is trained for both 2D joint classification and 3D joint regression tasks simultaneously. Details of each part is explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structure of the Baseline CNN</head><p>The CNN used in this experiment consists of five convolutional layers, three pooling layers, two parallel sets of two fully connected layers, and loss layers for 2D and 3D pose estimation tasks. The CNN accepts a 225 ? 225 sized image as an input. The sizes and the numbers of filters as well as the strides are specified in <ref type="figure">Figure 1</ref>. The filter sizes of convolutional and pooling layers are the same as those of ZFnet <ref type="bibr" target="#b20">[21]</ref>, but we reduced the number of feature maps to make the network smaller.</p><p>Joint optimization using both 3D and 2D information helps CNN to learn more meaningful features than the optimization using 3D regression alone. Li et al. <ref type="bibr" target="#b4">[5]</ref> trained a CNN both for 2D joint detection task and for 3D pose regression task. Since both tasks share the same convolutional layers, features that are useful for estimating both 2D and 3D positions of joints in an image are learned in convolutional layers. Following the idea, we also used both 2D and 3D loss functions in the CNN. Convolutional layers are shared, and the feature maps after the last pooling layer are connected to two different fully connected layers, each of which is connected to 2D loss function and 3D loss function respectively (See <ref type="figure">Figure 1)</ref>.</p><p>We formulated 2D pose estimation as a classification problem. For the 2D classification task, we divided an input image into N g ? N g grids and treat each grid as a separate class, which results in N 2 g classes per joint. The ground truth label is assigned in accordance with the ground truth position of each joint. When the ground truth joint position is near the boundary of a grid, zero-one labeling that is typically used for multi-class classification may give unprecise information. Therefore, we used a soft label which assigns non-zero probability to the four nearest neighbor grids from the ground truth joint position. The target probability for the ith grid g i of the jth joint is inversely proportional to the distance from the ground truth position, i.e.,</p><formula xml:id="formula_0">p j (g i ) = d ?1 (? j , c i )I(g i ) N 2 g k=1 d ?1 (? j , c k )I(g k ) ,<label>(1)</label></formula><p>where d ?1 (x, y) is the inverse of the Euclidean distance between the point x and y in the 2D pixel space,? j is the ground truth position of the jth joint in the image, and c i is the center of the grid g i . I(g i ) is an indicator function that is equal to 1 if the grid g i is one of the four nearest neighbors, i.e.,</p><formula xml:id="formula_1">I(g i ) = 1 if d(? j , c i ) &lt; w g 0 otherwise,<label>(2)</label></formula><p>where w g is the width of a grid. Hence, higher probability is assigned to the grid closer to the ground truth joint position, andp j (g i ) is normalized so that the sum of the class probabilities is equal to 1. Finally, the objective of the 2D classification task for the jth joint is to minimize the following cross entropy loss function.</p><formula xml:id="formula_2">L 2D (j) = ? N 2 g i=1p j (g i ) log p j (g i ),<label>(3)</label></formula><p>where p j (g i ) is the probability that comes from the softmax output of the CNN. On the other hand, estimating 3D position of joints is formulated as a regression task. Since the search space is much larger than the 2D case, it is undesirable to solve 3D pose estimation as a classification task. The 3D loss function is designed as a square of the Euclidean distance between the prediction and the ground truth. We estimate 3D position of each joint relative to the root node. Hence, the loss function for the jth joint when the root node is the rth joint becomes</p><formula xml:id="formula_3">L 3D (j, r) = R j ? (? j ?? r ) 2 ,<label>(4)</label></formula><p>where R j is the predicted relative 3D position of the jth joint from the root node, J j is the ground truth 3D position of the jth joint, and? r is that of the root node. The overall cost function of the CNN combines <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref> with weights, i.e., </p><formula xml:id="formula_4">L all = ? 2D Nj j=1 L 2D (j) + ? 3D Nj j =r L 3D (j, r).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Joint Regression with 2D Classification Features</head><p>In the baseline architecture in <ref type="figure">Figure 1</ref>, 2D and 3D losses are separated with different fully connected layers. Though convolutional layers learn features relevant to both 2D and 3D pose estimation thanks to the shared convolutional layers, the probability distribution that comes from 2D classification may give more stable and meaningful information in estimating 3D pose. The joint locations in an image are usually a strong cue for guessing 3D pose. To exploit 2D classification result as a feature for the 3D pose estimation, we concatenate the outputs of softmax in the 2D classification task with the outputs of the fully connected layers in the 3D loss part. The proposed structure after the last pooling layer is shown in <ref type="figure">Figure 2</ref>. First, the 2D classification result is concatenated (probs 2D layer in <ref type="figure">Figure 2</ref>) and passes the fully connected layer (fc probs 2D ). Then, the feature vectors from 2D and 3D part are concatenated (fc 2D-3D ), which is used for 3D pose estimation task. Note that the error from the fc probs 2D layer is not back-propagated to the probs 2D layer to ensure that layers used for the 2D classification are trained only by the 2D loss part. The idea of using 2D classification result as an input for another task is similar to <ref type="bibr" target="#b2">[3]</ref>, which repeatedly uses the 2D classification result as an input by concatenating it with feature maps from CNN. Unlike <ref type="bibr" target="#b2">[3]</ref>, we simply vectorized the softmax result to produce N g ? N g ? N j feature vector rather than convolving the probability map with features in the convolutional layers.</p><p>The proposed framework can be trained end-to-end via back-propagation algorithm. Because 2D classification will give an inaccurate prediction in the early stage of training, it is possible that 3D regression may be disturbed by the classification result. However, we empirically found that 3D loss converges successfully, and the performance of 3D pose estimation improves as well, as explained in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple 3D Pose Regression from Different Root Nodes</head><p>In the baseline architecture, we predicted the relative 3D position of each joint with respect to only one root node which is around the position of the hip. When joints such as wrists or ankles are far from the root node, the accuracy of regression may be degraded. Li et al. <ref type="bibr" target="#b4">[5]</ref> designed a 3D regression loss to estimate the relative position between each joint and its parent joint. However, errors may be accumulated when intermediate joint produces inaccurate result in this scheme. As an alternative solution, we estimate the relative position over multiple joints. We denote the number of selected root nodes as N r . For the experiments in this paper, we set N r = 6 and selected six joints so that most joints can either be the root node or their neighbor nodes. The selected joints are visualized in <ref type="figure" target="#fig_2">Figure 3</ref>(b). Therefore, there are six 3D regression losses in the network, which is illustrated in <ref type="figure">Figure 2</ref>. Then, the overall loss becomes</p><formula xml:id="formula_5">L all = ? 2D Nj j=1 L 2D (j) + ? 3D r?R Nj j =r L 3D (j, r),<label>(6)</label></formula><p>where R is the set containing the joint indices that are used as root nodes. When the 3D losses share the same fully connected layers, the trained model outputs the same pose estimation results across all joints. To break this symmetry, we put the fully connected layers for each 3D losses (fc2 3D layers in <ref type="figure">Figure 2</ref>). At the test time, all the pose estimation results are translated so that the mean of each pose becomes zero. Final prediction is generated by averaging the translated results. In other words, the 3D position of the jth joint X j is calculated as</p><formula xml:id="formula_6">X j = r?R X (r) j N r ,<label>(7)</label></formula><p>where X (r) j is the predicted 3D position of the jth joint when the rth joint is set to a root node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>The proposed method is implemented using Caffe framework <ref type="bibr" target="#b21">[22]</ref>. Batch normalization <ref type="bibr" target="#b22">[23]</ref> is applied to all convolutional and fully connected layers. Also, drpoout <ref type="bibr" target="#b23">[24]</ref> is applied to every fully connected layers with drop probability of 0.3. Stochastic gradient descent of batch size 128 is used for optimization. Initial learning rate is set to 0.01, and it is decreased by a factor of 0.5 for every 4 epochs. The optimization is finished after 28 epochs. The momentum and the weight decay parameters are set to 0.9 and 0.001 respectively. The weighting parameter ? 2D and ? 3D are initially set to 0.1 and 0.5 respectively. ? 2D is decreased to 0.01 after 16 epochs because we believe that 2D pose information plays an important role in learning informative features especially in the early stage of training.</p><p>Input images are cropped using the segmentation information provided with the dataset so that a person is located around the center of an image. The cropped image is resized to 250 ? 250. We randomly cropped the resized image into an image of 225 ? 225 size, then it is fed into the CNN as an input image. During the test time, only the center crop is evaluated for the pose prediction. Data augmentation based on the principal component analysis of training images <ref type="bibr" target="#b24">[25]</ref> is also applied. N g is set to 16, so the input image is divided into 256 square grids for 2D loss calculation. N r is set to 6, and the position of the root nodes are illustrated in <ref type="figure" target="#fig_2">Figure 3(b)</ref>.</p><p>For the ground truth 3D pose that is used in the training step, we firstly translated the joints to make the shape to be zero mean. Then, we scaled the 3D shape so that the Frobenius norm of the 3D shape becomes 1. Since different person has different height and size, we believe that the normalization helps to reduce ambiguity of scale and to predict scale-invariant poses. During the testing phase, scale should be recovered to evaluate the performance of the algorithm. Similar to <ref type="bibr" target="#b18">[19]</ref>, we infer the scale using the training data. The lengths of all connected joints from the training set are averaged. The scale of the result from the test data is determined so that the length of connected joints in the estimated shape is equal to the pre-calculated average length. Since the lengths for arms and legs from the estimated shape often have a large variation, we only used the length of joints in the torso which is stable in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We used Human 3.6m dataset <ref type="bibr" target="#b7">[8]</ref> to evaluate our method and compared the proposed method with the other 3D human pose estimation algorithms. The dataset provides 3D human pose information acquired by a motion capture system with synchronized RGB images. It consists of 15 different sequences which <ref type="table">Table 1</ref>. Quantitative results on Human 3.6m dataset. The best and the second best methods for each sequence are marked as <ref type="bibr" target="#b0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>   <ref type="bibr" target="#b0">(1)</ref> 109.31 <ref type="bibr" target="#b0">(1)</ref> 87.05 <ref type="bibr" target="#b0">(1)</ref> 103.16 <ref type="bibr" target="#b0">(1)</ref> 116.18 <ref type="bibr" target="#b1">(2)</ref> 143.32 <ref type="bibr" target="#b0">(1)</ref> Our method 100.34 <ref type="bibr" target="#b1">(2)</ref> 116.19 <ref type="bibr" target="#b1">(2)</ref> 89.96 116.49 <ref type="bibr" target="#b1">(2)</ref> 115.34 <ref type="bibr" target="#b0">(1)</ref> 149.55 <ref type="bibr" target="#b1">(2)</ref> Posing  <ref type="bibr" target="#b0">(1)</ref> 99.78 <ref type="bibr" target="#b0">(1)</ref> 124.52 <ref type="bibr" target="#b0">(1)</ref> 199.23 <ref type="bibr" target="#b1">(2)</ref> 107.42 <ref type="bibr" target="#b1">(2)</ref> 118.09 <ref type="bibr" target="#b0">(1)</ref> Our method 117.57 106.94 <ref type="bibr" target="#b1">(2)</ref> 137.21 <ref type="bibr" target="#b1">(2)</ref> 190.82 <ref type="bibr" target="#b0">(1)</ref> 105.78 <ref type="bibr" target="#b0">(1)</ref>   <ref type="bibr" target="#b5">[6]</ref> 132.17 69.97 --Tekin et al. <ref type="bibr" target="#b6">[7]</ref> 130.53 65.75 --Tekin et al. <ref type="bibr" target="#b19">[20]</ref> 126.29 <ref type="bibr" target="#b1">(2)</ref> 55.07 <ref type="bibr" target="#b0">(1)</ref> 65.76 <ref type="bibr" target="#b0">(1)</ref> 124.97 Zhou et al. <ref type="bibr" target="#b18">[19]</ref> 114.23 <ref type="bibr" target="#b0">(1)</ref> 79.39 97.70 113.01 (1) Our method 131.90 62.64 <ref type="bibr" target="#b1">(2)</ref> 96.18 <ref type="bibr" target="#b1">(2)</ref> 117.34 <ref type="bibr" target="#b1">(2)</ref> contain specific actions such as discussion, eating, walking, and so on. There are 7 different persons who perform all 15 actions. We trained and tested each action individually. Following the previous works on the dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, we used 5 subjects (S1, S5, S6, S7, S8) as a training set, and 2 subjects (S9, S11) as a test set. The training and the testing procedures are conducted on a single PC with a Titan X GPU. Training procedure takes 7-10 hours for one action sequence depending on the number of training images. For the evaluation metric, we used the mean per joint position error (MPJPE).</p><p>First, we compared the performance of our method with the conventional methods on Human 3.6m dataset. <ref type="table">Table 1</ref> shows the MPJPE of our method and the previous works. The smallest and the second smallest errors for each sequence are marked. Our method achieves the best performance in 3 sequences and shows the second best performance in 9 sequences. Note that the methods of <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b18">[19]</ref> make use of temporal information from multiple frames. Meanwhile, our method produce a 3D pose from a single image. Our method is also beneficial against <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b18">[19]</ref> in terms of running time and the simplicity of the algorithm since the estimation is done by a forward pass of the CNN and simple averaging. Moreover, from <ref type="table">Table 1</ref>, it is justified that our method outperforms the CNN based methods that predict 3D pose from a single image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Next, we measured the effect of our contribution, 1) the integration of 2D classification results and 2) regression from multiple root nodes, by comparing their performance with the baseline CNN. Note that the 2D classification loss is also used in the baseline CNN. The difference of the baseline CNN is that 2D classification results are not propagated to the 3D loss part, i.e., probs 2D, fc probs 2D and fc 2D-3D layers in <ref type="figure">Figure 2</ref> are deleted in the baseline CNN. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Multiple regression from different root nodes and the integration of 2D classification results are denoted as Multi-reg and 2D-cls respectively. Both modifications improve the result over the baseline CNN in all tested sequences. 2D classification integration showed larger error reduction rate than the multiple regression strategy, which proves that the 2D classification information is indeed a useful feature for 3D pose estimation. Multiple regression can be considered as an ensemble of different estimation results, which improves the overall performance. It can be found that the error reduction rate for the case that both 2D classification result integration and multiple regression are applied is slightly bigger than the sum of the reduction rates when they are individually applied in most sequences. Since each 3D pose regressor takes advantage of 2D classification feature, there is a synergy effect between the two schemes.</p><p>We also analyzed the effect of integrating 2D classification result in terms of 3D losses. Training losses are measured every 50 iterations and testing losses are measured every 4 epochs. The results on the Walking sequence are illustrated in <ref type="figure">Figure 4</ref>. For the training data, loss is slightly smaller when 2D classification information is not used <ref type="figure">(Figure 4(a)</ref>). However, test loss is much lower when 2D classification information is used <ref type="figure">(Figure 4(b)</ref>). This indicates that 2D classification information impose generalization power and reduce overfitting for CNN regressor. Since the 2D joint probabilities provide more abstracted and subjectindependent information compared to the features obtained from an image, the CNN model is able to learn representations that are robust to variability of subjects in the image.</p><p>Finally, we illustrated qualitative results of our method in <ref type="figure" target="#fig_4">Figure 5</ref>. Input images, ground truth poses, and the estimation results with and without 2D classification information are visualized. Different colors are used to distinguish the left and right sides of human bodies. It can be found that 2D pose estimation results help reducing the error of 3D pose estimation. While the CNN which does not use 2D classification information gives poor results, the estimated results are much more improved when 2D classification information is used for 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose novel strategies which improve the performance of the CNN that estimates 3D human pose. By reusing 2D joint classification result, the relationship between 2D pose and 3D pose is implicitly learned during the training phase. Moreover, multiple regression results with different root nodes gives an effect of ensemble learning. When both strategies are combined, 3D pose estimation results are significantly improved and showed comparable performance to the state-of-the-art methods without exploiting any temporal information of video sequences.</p><p>We expect that the performance can be further improved by incorporating temporal information to the CNN by applying the concepts of recurrent neural network or 3D convolution <ref type="bibr" target="#b25">[26]</ref>. Also, efficient aligning method for multiple regression results may boost the accuracy of pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Ground Truth Without 2D info With 2D info </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 (Fig. 2 .</head><label>32</label><figDesc>Nj ? 1)) loss 3D 2 (3(Nj ? 1)) loss 3D Nr (3(Nj ? 1)) Structure of fully connected layers and loss functions in the proposed CNN. The numbers in parentheses indicate the dimensions of the corresponding output feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of joints to be estimated (Red and green dots). (a) Baseline method predicts relative position of the joints with respect to one root node (Green dot). (b) For multiple pose regression, the positions of joints are estimated with respect to multiple root nodes (Green dots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of our method on Human 3.6m test dataset. The estimation results are compared with the results from the baseline method. First column : input images. Second column : Ground truth 3D position. Third column : Pose estimation result without 2D classification information integration. Fourth column : Pose estimation result with 2D classification information integration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our method with the baseline.</figDesc><table><row><cell></cell><cell></cell><cell cols="10">Discussion Eating Greeting Phoning Photo Walking</cell></row><row><cell>Baseline CNN</cell><cell></cell><cell>125.45</cell><cell cols="2">95.21</cell><cell>120.69</cell><cell></cell><cell cols="2">119.66</cell><cell cols="2">153.76</cell><cell>72.55</cell></row><row><cell>Multi-reg</cell><cell></cell><cell>122.71</cell><cell cols="2">94.67</cell><cell>119.70</cell><cell></cell><cell cols="2">119.25</cell><cell cols="2">153.54</cell><cell>71.19</cell></row><row><cell>2D-cls</cell><cell></cell><cell>118.19</cell><cell cols="2">91.39</cell><cell>118.19</cell><cell></cell><cell cols="2">115.84</cell><cell cols="2">149.97</cell><cell>64.27</cell></row><row><cell cols="2">Multi-reg+2D-cls</cell><cell>116.19</cell><cell cols="2">89.96</cell><cell>116.49</cell><cell></cell><cell cols="4">115.34 149.55</cell><cell>62.64</cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>With 2D class info</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">With 2D class info</cell><cell></cell></row><row><cell>0.07</cell><cell></cell><cell></cell><cell>Without 2D class info</cell><cell></cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Without 2D class info</cell><cell></cell></row><row><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>7</cell><cell>14</cell><cell>21</cell><cell>28</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>24</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">Fig. 4. The 3D losses of Walking sequence with and without 2D classification result</cell></row><row><cell cols="9">integration. (a) Losses for training data. (b) Losses for test data.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<title level="m">Human pose estimation with iterative error feedback</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00134</idno>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-ofparts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1347" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
