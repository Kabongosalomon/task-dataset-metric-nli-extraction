<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyun</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Sun</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Guo</surname></persName>
							<email>ywguo@nju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hubei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hubei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Encoder-decoder models have been widely used in RGBD semantic segmentation, and most of them are designed via a two-stream network. In general, jointly rea-soning the color and geometric information from RGBD is beneficial for semantic segmentation. However, most existing approaches fail to comprehensively utilize multi-modal information in both the encoder and decoder. In this paper, we propose a novel attentionbased dual supervised decoder for RGBD semantic segmentation. In the encoder, we design a simple yet effective attention-based multi-modal fusion module to extract and fuse deeply multi-level paired complementary information. To learn more robust deep representations and rich multi-modal information, we introduce a dual-branch decoder to effectively leverage the correlations and complementary cues of different tasks. Extensive experiments on NYUDv2 and SUN-RGBD datasets demonstrate that our method achieves superior performance against the state-of-the-art methods. * Work done while interning at Hubei University of Technology ? Corresponding author ? Corresponding author (a) Atrous Conv. (b) Encoder-decoder for segmentation (c) Encoder-decoder for multi-task including segmentation (d) Dual supervised decoder for segmentation (ours)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, scene understanding has received considerable attention due to the wide applications in AR/VR <ref type="bibr" target="#b37">[38]</ref>, autonomous driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53]</ref>, UAVs <ref type="bibr" target="#b45">[46]</ref>, simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b31">[32]</ref>, Robotics <ref type="bibr" target="#b32">[33]</ref>, and other artificial intelligence fields. As a result, semantic segmentation for scene understanding becomes extremely important. However, there still exists many challenges in RGBD semantic segmentation caused by the complexity of the environment, the influence of inaccurate depth, and the joint reasoning of multi-modal information.</p><p>Deep learning technique has been applied to the semantic segmentation problem with great success. Though dif- ferent architectures are developed, the convolutional neural networks (CNNs) are still prevalent due to their ability to model non-linear, high-dimensional functions. Generally, atrous/dilated convolution-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> allow us to effectively enlarge the field-of-view of filters to incorporate multi-scale context (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), especially for atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b3">[4]</ref>. But there exists a 'gridding' problem <ref type="bibr" target="#b47">[48]</ref>, and they fail to capture small objects with accurate boundaries. Furthermore, it is computationally intensive if denser output features are extracted for this type of models.</p><p>The encoder-decoder models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref> allow for faster computation in the encoder path and recovering sharp object boundaries in the decoder. These models, however, only use RGB data for semantic segmentation which cannot achieve a satisfactory performance. Compared with color, the depth data provide geometric cues to reduce the uncertainty of the segmentation of objects in which the color is similar to the background <ref type="bibr" target="#b16">[17]</ref>. It is thus meaningful and crucial to develop effective models to combine these complementary modalities for segmentation. To achieve this goal, numerous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7]</ref> focus on designing a two-stream network which processes the RGB and geometry information in terms of depth or HHA, separately. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the features from two modalities are further fused by various mechanisms such as the element-wise summation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, gate <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, and attention <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref> in the encoder. Such approaches only process the paired complementary cues in the encoder, but ignoring the cross-modal information during decoding. Moreover, training such a model is usually difficult to converge due to this imbalance of the encoder and decoder. Since other related tasks such as depth estimation could facilitate semantic segmentation, recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref> have attempted to solve the segmentation problem via a multi-task learning framework. Fully convolutional encoder-decoder networks have become the mainstream. During the joint learning, different task-specific decoders explore the correlations between these tasks as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>. Note that these methods perform the multi-task distillation at a fixed scale (i.e. backbone features) with specific receptive field in the decoder. However, in fact, the influence between two tasks is different for various sizes of receptive field <ref type="bibr" target="#b46">[47]</ref>. Furthermore, the capacity of fully convolutional encoder-decoder, whose encoder and decoder are simply integrated together (e.g. skip connection <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58]</ref>, multi-scale feature aggregation <ref type="bibr" target="#b55">[56]</ref>), is limited for such a complex task of semantic segmentation.</p><p>In this paper, we design a simple symmetric yet effective network (in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>) to efficiently use the multilevel cross-modal information for RGBD semantic segmentation. Motivated by the above observations, we first propose an attention-based multi-modal fusion module to process the multi-level paired complementary information in a two-stream encoder. To learn cross-modal information during decoding, we introduce a novel dual-branch decoder in which the primary is designed for semantic segmentation supervised by another task-guided branch. Such design enables us to incorporate multi-scale context by the ASPP module at the end of primary-branch, which contains the pyramid supervision for enhancing the deep representation. This specific dual-branch decoder is capable of improving the performance of semantic segmentation through multitask distillation, while facilitating the convergence of training to solve the imbalance problem of the encoder and decoder. We conduct experiments on the NYUDv2 and SUN-RGBD datasets to validate the superior performance of our method in comparison with the state-of-the-arts.</p><p>Our contributions are summarized as follows.</p><p>? We propose a novel attention-based dual supervised decoder to utilize the complementary information across modalities for RGBD semantic segmentation.</p><p>? We design a simple yet effective attention multi-modal fusion module to extract and fuse deeply multi-level paired complementary information.</p><p>? We propose a dual-branch decoder to learn more robust deep representations and rich multi-modal information for the improvement of semantic segmentation performance and the efficiency of training.</p><p>? The proposed method achieves superior performance against the state-of-the-art methods on public benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In recent years, CNN-based methods have been successfully applied to the RGBD semantic segmentation 1 . In terms of structure, these methods can be roughly divided into the following three groups.</p><p>Atrous/dilated Convolution. Several works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30]</ref> utilized the atrous/dilated convolution to incorporate multi-scale context for RGBD semantic segmentation. For example, Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposed a dilated convolution which can enhance the receptive field while keep the resolution of the feature map. Qi et al. <ref type="bibr" target="#b38">[39]</ref> introduced a 3D graph neural network (3DGNN) to model accurate context with geometry cues provided by depth based on the dilated convolution. Lin et al. <ref type="bibr" target="#b29">[30]</ref> presented RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. However, dilated convolution can result in losing the continuity of feature maps. In addition, it is only effective for some large objects and invalid for small objects, which is not helpful to extract accurate edges.</p><p>Encoder-decoder. Many efforts <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44]</ref> concerning encoder-decoder <ref type="figure">Figure 2</ref>: Overview of the proposed ADSD architecture. We employ a two-stream encoder and a dual-branch decoder. The input of the network is a pair of RGB-Depth images. The feature maps of backbone encoders are fused through AMF module, which are further used to output the results through upsampling modules in the dual-branch decoder. At the end of primary branch, the ASPP is introduced to improve the final segmentation performance. Meanwhile, each upsampling block predicts a side output for pyramid supervision. In addition to the semantic supervision, the secondary branch requires supervision from normal estimation, depth estimation, or semantic segmentation task.</p><p>architectures have been devoted to RGBD semantic segmentation. For instance, DeconvNet <ref type="bibr" target="#b35">[36]</ref> used stacked deconvolutional layers to produce high-resolution prediction and more semantic details. SegNet <ref type="bibr" target="#b0">[1]</ref> shared a similar idea using indices in pooling layers to promote the recovery process. To learn the optimal fusion of multi-modal features, RDFNet <ref type="bibr" target="#b27">[28]</ref> extended the core idea of residual learning to RGBD semantic segmentation. Hu et al. <ref type="bibr" target="#b20">[21]</ref> proposed a architecture ACNet with three parallel branches and a channel attention-based module that extracts weighted features from RGB and depth branches. Chen et al. <ref type="bibr" target="#b5">[6]</ref> proposed a spatial information guided convolution network (SGNet) which allows to integrate 2D and 3D spatial information. ESANet <ref type="bibr" target="#b43">[44]</ref> used two ResNet-based encoders with an attention-based fusion for incorporating depth information, and a decoder utilizing a learned upsampling. However, these methods only perform the multi-modal information in the encoder, but ignore the cross-modal cues in the decoder. Moreover, when a large number of encoder parameters are passed to the decoder, it is difficult to train such a model to converge quickly.</p><p>Multi-task Learning. Numerous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b46">47]</ref> have also explored the idea of combining networks for complementary tasks to improve learning efficiency and generalization across different tasks. For exam-ple, Eigen et al. <ref type="bibr" target="#b12">[13]</ref> proposed a single multi-scale network (MSCNN) to address three different computer vision tasks. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> proposed a joint task-recursive learning (TRL) framework to refine the results of both semantic segmentation and monocular depth estimation through serialized task-level interactions. Zhang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a pattern affinitive propagation (PAP) method to utilize the matched affinity information across tasks. Zhou et al. <ref type="bibr" target="#b57">[58]</ref> proposed intra-task and inter-task pattern-structure diffusion (PSD) to learn long-distance propagation and transfer cross-task structures. Different from the previous works, we incorporate multi-modal information in the both encoder and decoder through attention-based dual supervised decoder to provide a unified pixel-wise scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the proposed attention-based dual supervised decoder (ADSD) in detail. First, we briefly describe the overall architecture. Then, we discuss multilevel fusion strategy and attention block used in attentionbased fusion module for multi-modal features in the encoder. Moreover, we give a detailed depiction of our dualbranch decoder which significantly improves the performance of semantic segmentation. Finally, we introduce the objective function for optimizing the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Network Architecture</head><p>The entire network architecture of our ADSD is presented in <ref type="figure">Fig. 2</ref>. For clear illustration, we use blocks with different colors to indicate different layers. Note that each convolution layer in our network is followed by a batch normalization layer <ref type="bibr" target="#b21">[22]</ref> before the activated function of rectified linear unit (ReLU), and it is omitted in the figure for simplification. The whole network can be divided into a two-stream encoder and a dual-branch decoder. In the decoder, the primary branch with pyramid supervision is designed for semantic segmentation, and the secondary branch requires supervision from the other task such as normal estimation, depth estimation, or semantic segmentation.</p><p>In the encoder part, we design two independent branches to extract features from RGB and depth images separately. In these two branches, we simply choose ResNet-50 <ref type="bibr" target="#b17">[18]</ref> as the backbone to extract multi-scale hierarchical feature maps from inputs. The output features from RGB and depth branch are combined to produce fusion features (Fuse0?Fuse4) through the attention-based multi-modal fusion (AMF) module, where the details are given in Section 3.2. It is worth noting that there is no connection between fusion features at different scales.</p><p>In the decoder part, we feed the above fusion features into each task-branch to decode pixel-level information. To produce high resolution predictions, we decode these convolutional features and then combine with the same scale fused features by upsampling blocks to produce taskspecific features as shown in Section 3.3. Specially, at the end of the primary branch, the ASPP is introduced to improve the final segmentation performance. Meanwhile, each upsampling block predicts a side output for pyramid supervision, which are introduced in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder</head><p>The conventional fusion branch <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref> integrates multiscale features by coarse-to-fine CNNs and general attention mechanisms. Such approaches are computationally expensive leading to information redundancy easily. Considering the complementarity between paired RGB and depth cues in multiple layers, we design a simple yet effective AMF module to fully extract and fuse multi-level paired complementary information. As illustrated in the middle part of <ref type="figure">Fig. 2</ref>, we show the main process of AMF while leveraging the high performance of the attention block. In our implementation, our AMF includes all five scales (i.e.1/2, 1/4, 1/8, 1/16, 1/32) of the backbone network.</p><p>To improve the performance of semantic segmentation, the channel attention <ref type="bibr" target="#b53">[54]</ref> allows the network to concentrate on more useful channels and flattens the distribution of information among channels with the effective utilization of complementary features. The architecture of the channel attention is illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Assuming an input fea-  </p><formula xml:id="formula_0">ture map U = [u 1 , u 2 , ..., u C ] ? R C?H?W that passes through channel attention block A C (?) to generate output feature map V ac ? R C?H?W .</formula><p>Here, H and W are the height and width respectively, with C being the number of channels. Channel attention is firstly performed by a global average pooling to produce a vector Z ? R C?1?1 with its t-th element</p><formula xml:id="formula_1">Z t = 1 H ? W H i W j u t (i, j) .<label>(1)</label></formula><p>Then Z is transformed to? = W 1?1 (?(W 1?1 (Z))), with W 1?1 being the weight of a 1?1 convolutional layer and the ReLU operator ?(?). A sigmoid ?(?) is applied to activate the convolution result, constraining the value of weight vector to the interval [0,1]. Finally, we perform an elementwise multiplication, and the result V ac can be expressed as:</p><formula xml:id="formula_2">V ac = A C (U ) = [?(? 1 )u 1 , ?(? 2 )u 2 , ..., ?(? C )u C ] (2)</formula><p>In contrast to channel attention, spatial attention <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref> has fewer parameters with a simpler structure. The architecture of the spatial attention is illustrated in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. We consider an alternative slicing of an input tensor U = [u 1,1 , ..., u i,j , ..., u H,W ] that passes through the spatial attention block A S (?) to generate output V sc , where u i,j ? R C?1?1 corresponding to the spatial location (i, j). The spatial attention is firstly performed by a 1?1 convolution to generate a projection tensor Q ? R H?W . Each Q i,j of the projection describes the linearly combined representation of a spatial location (i, j). This projection is then performed on a sigmoid ?(?) to rescale activations to [0,1].</p><p>And the result V sc can be expressed as</p><formula xml:id="formula_3">V sc = A S (U ) = [?(Q 1,1 )u 1,1 , ?(Q 1,2 )u 1,2 , ..., ?(Q i,j )u i,j , ..., ?(Q H,W )u H,W ] .<label>(3)</label></formula><p>Specifically, this operation provides more importance to relevant spatial locations and ignores irrelevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>Benefit from the exploration of correlation between different tasks in multi-task learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref>, we propose a novel dual-branch decoder to learn more robust deep representations and multi-modal information. It is well-known that low-level layers of the CNNs usually have more positional information, while high-level layers contain more semantic cues. Both the positional and semantic cues play a key role in semantic segmentation. Inspired by upsampling strategy in <ref type="bibr" target="#b20">[21]</ref> and skip connection like <ref type="bibr" target="#b17">[18]</ref>, we use transposed convolutional layers to upsample the features at different pyramid scales, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>In particular, the fused feature map V K of AMF is firstly calculated by a 1?1 convolution W 1?1 to project the feature map W 1?1 (V K ) with lower channel, allowing the decoder to have a lower memory consumption. And it passes through upsampling block B U (?) to generate the feature map S K of K-th slide output.</p><formula xml:id="formula_4">S K = B U [W 1?1 (V K )] .<label>(4)</label></formula><p>Then S K is used to produce the next slide output as follows:</p><formula xml:id="formula_5">S K?1 = S K + B U [W 1?1 (V K?1 )] ,<label>(5)</label></formula><p>where + is element-wise summation. Repeatedly, we continue to upscale feature maps and perform the above decoding process to produce a higher scale of feature maps. The scale factor of each upsampling block is set to 2. All slide outputs are employed for pyramid supervision which will be introduced in Section 3.4. In particular, the ASPP is introduced to incorporate multi-scale context at the end of this branch. In our experiments, the dilated convolution rate is set as 12, 24, and 36 in the ASPP. In secondary branch of decoder, we repeat the operations on the primary branch to upsample the fused feature map V a . The final upsampling feature map S 0 is directly used to generate the predict which can be surface normal, estimated depth, or segmentation result. In practice, we propose a more efficient training method that takes advantage of multi-modal feature sharing during training. Inspired by the training strategy in <ref type="bibr" target="#b14">[15]</ref>, we train the model for a depth-guided branch decoder at the pre-training stage and the semantic-guided branch decoder at the fine-tuning stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>Pyramid Supervision. The pyramid supervised training scheme alleviates the gradient disappearance problem by introducing supervised learning at different levels <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the primary-branch of decoder computes K slide outputs by upsampling blocks with different spatial resolutions. In our implementation, the K is set to 4, and the slide outputs are defined as Scale 1 to 4. The resolution scales are 1/2, 1/4, 1/8, and 1/16, and the final result is a full resolution. We calculate the score map of each output through a 1?1 convolution, and then feed it into a softmax layer and cross-entropy function to build the loss function L P k (k ? [1, K]). Loss Function. For semantic segmentation, most methods utilize cross-entropy to measure the difference between the prediction and ground-truth. However, for existing datasets, the distribution of semantic labels is extremely imbalanced. This will bias the learning towards the dominant samples and lead to low accuracy in minority categories. To alleviate the data imbalance issues, we re-weight the training loss of each class in the cross-entropy function using the median frequency setting proposed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. That is, we weight each pixel by a factor of ? c = p m /p c , where c denotes the ground-truth category. p c is the pixel probability of that category, p m is the median of all the probabilities of these categories.</p><p>For different task supervision, we use task-guided loss functions defined as L T which can be normal L N , depth L D or semantic L S . Following the depth estimation algorithms, we use berHu loss <ref type="bibr" target="#b26">[27]</ref> for the depth supervision:</p><formula xml:id="formula_6">L D = i |d i ? D i |, |d i ? D i | ? (di?Di) 2 +? 2 2? , |d i ? D i | &gt; ? ,<label>(6)</label></formula><p>where d i is the predicted depth for pixel i, and D i is the ground-truth. ? = 1 5 max(|d i ? D i |). Such a loss function can provide more obvious gradients at the locations where the depth difference is low, and thus can help to better train the network. As for surface normal, we also use the berHu loss <ref type="bibr" target="#b26">[27]</ref>. Together with the above pyramid supervision loss L P k for semantic prediction at intermediate layers, the total loss L can be defined as:</p><formula xml:id="formula_7">L = L S + L T + K k=1 L P k .<label>(7)</label></formula><p>Finally, a fully end-to-end optimization is computed by using gradient back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate our proposed method, we conduct extensive experiments on NYUDv2 dataset <ref type="bibr" target="#b33">[34]</ref> and SUN-RGBD dataset <ref type="bibr" target="#b42">[43]</ref>. We start with the introduction of experimental setup such as implementation details, datasets, and evaluation metrics. We then conduct ablation experiments to determine whether our network improve performance. Finally, we compare our method with the existing methods for semantic segmentation on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement our method using the publicly available Pytorch. For the optimizer, we use Adam <ref type="bibr" target="#b24">[25]</ref> with (? 1 , ? 2 ) = (0.9, 0.999). For NYUDv2 dataset, we train the model for 600 epochs and fine-tune 50 epochs with a learning rate of 0.0002 and 0.00002, respectively. For SUN-RGBD dataset, we train the model for 300 epochs and finetune it for 30 epochs with the same learning rate. We adopt the step learning rate policy whose learning rate is updated after each 300 epochs. Specifically, all experiments are trained with batch size 8 on a single NVIDIA Tesla V100 GPU. To avoid overfitting, similarly with <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30]</ref>, we employ general data augmentation strategies, including random scaling in the range of [0.8, 1.4], random horizontal flipping, and random cropping. In particular, we resized the inputs to a resolution of 480?640 for the above datasets. During the inference, we only obtain the prediction results from the primary decoder for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Metrics</head><p>We use the NYUDv2 dataset <ref type="bibr" target="#b33">[34]</ref> for the main evaluation of our method and further use the SUN-RGBD dataset <ref type="bibr" target="#b42">[43]</ref>   for extensive comparison with the state-of-the-arts. The NYUDv2 dataset consists of 1449 RGBD images showing interior scenes. We use the segmentation labels provided in <ref type="bibr" target="#b15">[16]</ref>, in which all labels are mapped to 40 classes. We use the standard training/test split with 795 and 654 images, respectively. The SUN-RGBD contains 10335 RGBD images labeled with 37 classes. We use the official training set with 5285 images to train our network, and the official testing set with 5050 images for evaluation. Compared with NYUDv2, SUN-RGBD has more complex scene and depth conditions, which are probably more suitable to measure the generality of our method. For the evaluation of semantic segmentation results, we follow the recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref> and use three common metrics for evaluation, including pixel accuracy (PixAcc.), mean accuracy (mAcc.), and mean intersection over union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>To discover the functionality of each component in our method, we conduct an ablation study on the NYUDv2 dataset. Taking the network consisting of a two-stream encoder and a simple decoder (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) as a baseline. In the encoder, the combination operation of multi-modal features is the element-wise summation (like FuseNet <ref type="bibr" target="#b16">[17]</ref>). We evaluate the effectiveness of our dual-branch decoder by choosing different task-guided branches and changing the location of AASP module. The results can be found in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. For task-guided branches of the <ref type="figure">Figure 5</ref>: The visual results of ablation analysis on NYUDv2 dataset. From left to right, we show the inputs, ground-truths, the results of baseline, with AMF, with AMF and dual-branch decoder, and our method, respectively. <ref type="table">Table 3</ref>: Ablation study of the proposed method on NYUDv2 dataset. The Dual-decoder means dual-branch decoder. The FT means fine-tuning stage in our training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PixAcc. mAcc. mIoU Baseline ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref> secondary decoder, the normal-guided performs better than the depth-guided, semantic-guided and the combination of depth-guided and semantic-guided. For the location of ASPP (at the end of different Fuse modules) in the primary decoder, the ASPP with Fuse0 (in <ref type="figure" target="#fig_3">Fig. 4</ref>) performs better than the ASPP with Fuse1 and Fuse2 modules. The results of ablation analysis are shown in <ref type="table">Table 3</ref>. For the multi-model feature fusion solutions <ref type="figure" target="#fig_2">(Fig. 3)</ref>, our AMF with channel attention (CA) performs better than the element-wise summation (i.e. baseline), separation-andaggregation gate <ref type="bibr" target="#b6">[7]</ref>, attentional feature fusion (AFF) <ref type="bibr" target="#b9">[10]</ref>, bottleneck attention module (BAM) <ref type="bibr" target="#b36">[37]</ref>, and spatial attention (SA). We owe this to the representation of channel attention that ignores less important channels of fused features and emphasizes the important ones. The results demonstrate that our AMF can significantly improve the performance of semantic segmentation. This observation also clarifies that incorporating depth information can greatly improve the performance, which reveals the effectiveness of reasoning color and geometry information together. By introducing the dual-branch decoder, the performance is further improved. The final fine-tuning (FT) stage of training strategy (see details in Section 3.3) for our model gives another rise in the performance.</p><p>We show qualitative results of our method on NYUDv2 dataset for semantic segmentation in <ref type="figure">Fig. 5</ref>. For comparison, we also include the visual results of baseline, with the proposed AMF, with the AMF and dual-branch decoder, with the AMF, dual-branch decoder and fine-tuning (FT) stage (our method). The results show that the geometry information is well distilled by our AMF, which can distinguish the objects with similar color. Moreover, when incorporating the dual-branch decoder, our network can recover more context information and more accurate object masks. As mentioned before, we argue that the training is going faster and more robust along with our dual-branch decoder. This decoder can also deal with the imbalance problem caused by the phenomenon that the encoder uses multimodal information, while the decoder dose not. To verify this statement, we report the loss values of our method and the methods without a dual-branch decoder during training. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we observe that the multi-loss L of our method is rapidly reduced only after 150 epochs from the beginning. However, the loss of the methods without a dual-branch decoder waves violently due to the imbalance of encoder and decoder. We also find that the dual-branch decoder can facilitate the convergence of training, which is capable of reducing the adverse effects on this imbalance. To evaluate the performance of our model on the imbalanced distributed data, we also show the results on each category, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Clearly, our method performs better than other methods in most categories. Specially, our method still achieves a relatively higher IoU on some "hard" categories such as bed, curtain, dresser, shower, and board, etc. Following previous methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8]</ref>, we also report the mACC. of our method on SUN-RGBD dataset. As shown in <ref type="table" target="#tab_4">Table 5</ref>, we achieve 62.1% mean accuracy with 4.1% improvement over the recent method <ref type="bibr" target="#b7">[8]</ref>. Specifically, we yield performance gains over 26 classes, which demonstrates the effectiveness of the proposed approach. We owe the robustness among almost all the categories to the effectively learned multi-modal cues in the encoder, and the cross-modal information in the decoder. Note that our method achieves unsatisfactory performance on some categories (e.g. blinds, person, bag) of NYUDv2 dataset, which may due to our joint reasoning on color and geometric cues, as the depth may vary greatly compared with the corresponding color appearance in different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Compared with State-of-the-arts</head><p>NYUDv2. The comparison results on the NYUDv2 dataset with 40-category are shown in <ref type="table">Table 6</ref>. We use ResNet-50 and single-scale inference strategy for a fair comparison. Following our training method mentioned in Section 3.3, we use a normal-guided branch decoder at the pre-training stage and the semantic-guided branch decoder at the finetuning stage. Our method can still achieve 77.5% PixAcc., 65.3% mAcc., and 52.5% mIoU, which is better than the  state-of-the-art methods. Specifically, we can find that utilizing depth and normal as extra supervision could make network more robust than general RGBD methods that take both RGB and depth as inputs. Besides, it can be observed that the methods try to use atrous/dilated convolution or gate fusion to extract complementary feature, which are more implicit than our model in selecting valid feature from complementary information.</p><p>SUN-RGBD. We also compare our method with the stateof-the-arts on the large-scale SUN-RGBD dataset. Due to the lacking of surface normal ground-truths, we use a depth-guided branch decoder at the pre-training stage and the semantic-guided branch decoder at the fine-tuning stage. As summarized in <ref type="table">Table 6</ref>, our ADSD achieves 81.8% Pix-Acc., 62.1% mAcc., and 49.6% mIoU, which is the best results on mAcc. in comparison with the pervious methods. Moreover, our method obtains the superior performance than the approaches based on atrous/dilated convolution and encoder-decoder network, suggesting its superiority and high performance for RGBD semantic segmentation. We can observe that our proposed ADSD is slightly weaker than multi-task learning based methods such as PAP <ref type="bibr" target="#b55">[56]</ref> and PSD <ref type="bibr" target="#b57">[58]</ref> on both PixAcc. and mIoU metrics.</p><p>The main reason is that we perform the dual-supervised decoder with a depth-guided branch at the pre-training stage on SUN-RGBD dataset. Note that there are many lowquality depth maps in SUN-RGBD dataset caused by the capture device <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28]</ref>, which may affect the auxiliary utility from the depth. More details of qualitative results are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed a novel encoder-decoder framework for RGBD semantic segmentation, which can take full advantage of the complementary information across modalities. The color and depth data were jointly reasoned by forming a two-stream encoder. The multi-level paired complementary cues can be processed by our pro- <ref type="table">Table 6</ref>: Comparison with state-of-the-arts on NYUDv2 test set in 40-class and SUN-RGBD test set in 37-class. Percentage (%) of PixAcc., mAcc., and mIoU are shown for evaluation. In category, the 'AC', 'ED', and 'MT' denote atrous/dilated convolution, encoder-decoder, and encoder-decoder for multi-task, respectively. In scale, the 'S', and 'M' denote single-scale inference strategy and multi-scale inference strategy, respectively. posed AMF in the encoder. We then introduced a dualbranch decoder to effectively leverage the correlation and complementation of different tasks. In the decoder, the primary branch was used to incorporate multi-scale context by the ASPP with pyramid supervision. In addition, it was further supervised by another task-branch like normal estimation to improve the performance of segmentation and training convergence speed. Experiments on NYUDv2 and SUN-RGBD datasets demonstrated the superiority of our method compared with the previous approaches on RGBD semantic segmentation. In the future, we will generalize our method on more vision tasks and improve its efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of typical structures for RGBD semantic segmentation. The blue color and gray color indicate the RGB and depth streams, separately. The F denotes the combination operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Detailed structure of channel attention and spatial attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Detailed diagram of the proposed dual-branch decoder. The primary branch computes multi-scale fused features through 1?1 convolutional layers and upsampling blocks. The final features is refined by the ASPP to predict segmentation result, which is supervised via the output generated by normal-/depth-/semantic-guided branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Statistics of loss values during a training procedure on NYUDv2 dataset. Our dual-branch decoder can reduce the adverse effects on imbalance between encoder and decoder to facilitate the convergence of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance analysis of different task-guided branches in the secondary decoder on NYUDv2 dataset. During the inference, we only obtain the prediction results from the primary decoder for semantic segmentation.</figDesc><table><row><cell>Decoder</cell><cell>PixAcc.</cell><cell>mAcc.</cell><cell>mIoU</cell></row><row><cell>Semantic-guided</cell><cell>75.9</cell><cell>61.6</cell><cell>49.0</cell></row><row><cell>Depth-guided</cell><cell>76.8</cell><cell>64.6</cell><cell>51.2</cell></row><row><cell>Normal-guided</cell><cell>77.3</cell><cell>64.7</cell><cell>51.5</cell></row><row><cell>Depth-guided+ Normal-guided</cell><cell>76.8</cell><cell>64.0</cell><cell>51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance analysis for the location of ASPP module (at the end of different Fuse modules) in the primary decoder on NYUDv2 dataset.</figDesc><table><row><cell>ASPP Location</cell><cell>PixAcc.</cell><cell>mAcc.</cell><cell>mIoU</cell></row><row><cell>with Fuse2</cell><cell>76.2</cell><cell>63.5</cell><cell>50.1</cell></row><row><cell>with Fuse1</cell><cell>76.4</cell><cell>64.2</cell><cell>50.4</cell></row><row><cell>with Fuse0 (Fig. 4)</cell><cell>77.3</cell><cell>64.7</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-arts on each category of the NYUDv2 dataset. Percentage (%) of IoUs are shown for evaluation, with best performance marked in bold. 62.0 34.2 45.3 53.4 27.7 42.6 23.9 11.2 58.8 53.2 54.1 80.4 59.2 45.5 52.6 15.9 12.7 16.4 29.3 STD2P[19] 22.2 55.9 29.8 41.7 52.5 21.1 34.4 15.5 7.8 29.2 60.7 42.2 62.7 47.4 38.6 28.5 7.3 18.8 5.1 31.4 RDFNet [28] 26.9 69.1 35.0 58.9 63.8 34.1 41.6 38.5 11.6 54.0 80.0 45.3 65.7 62.1 47.1 57.3 19.1 30.7 20.6 39.0 76.0 32.9 57.8 70.8 28.6 40.3 48.2 12.1 78.3 67.3 57.1 77.9 63.2 46.5 62.2 9.6 33.4 22.2 39.6</figDesc><table><row><cell>Method</cell><cell>wall</cell><cell>floor</cell><cell>cabinet</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>door</cell><cell>window</cell><cell>bookshelf</cell><cell>picture</cell><cell>counter</cell><cell>blinds</cell><cell>desk</cell><cell>shelves</cell><cell>curtain</cell><cell>dresser</cell><cell>pillow</cell><cell>mirror</cell><cell>floormat</cell></row><row><cell>DeepLab [3]</cell><cell cols="20">67.9 83.0 53.1 66.8 57.8 57.8 43.4 19.4 45.5 41.5 49.3 58.3 47.8 15.5 7.3 32.9 34.3 40.2 23.7 15.0</cell></row><row><cell>FCN [42]</cell><cell cols="20">69.9 79.4 50.3 66.0 47.5 53.2 32.8 22.1 39.0 36.1 50.5 54.2 45.8 11.9 8.6 32.5 31.0 37.5 22.4 13.6</cell></row><row><cell cols="21">Mutex Constraints [11] 65.6 79.2 51.9 66.7 41.0 55.7 36.5 20.3 33.2 32.6 44.6 53.6 49.1 10.8 9.1 47.6 27.6 42.5 30.2 32.7</cell></row><row><cell>BI (3000) [14]</cell><cell cols="14">61.7 68.1 45.2 50.6 38.9 40.3 26.2 20.9 36.0 34.4 40.8 31.6 48.3 9.3</cell><cell cols="6">7.9 30.8 22.9 19.5 13.9 16.1</cell></row><row><cell>LSD-GF [8]</cell><cell cols="20">78.5 87.1 56.6 70.1 65.2 63.9 46.9 35.9 47.1 48.9 54.3 66.3 51.7 20.6 13.7 49.8 43.2 50.4 48.5 32.2</cell></row><row><cell>STD2P[19]</cell><cell cols="20">72.7 85.7 55.4 73.6 58.5 60.1 42.7 30.2 42.1 41.9 52.9 59.7 46.7 13.5 9.4 40.7 44.1 42.0 34.5 35.6</cell></row><row><cell>RDFNet [28]</cell><cell cols="20">79.7 87.0 60.9 73.4 64.6 65.4 50.7 39.9 49.6 44.9 61.2 67.1 63.9 28.6 14.2 59.7 49.0 49.9 54.3 39.4</cell></row><row><cell>DeepLab-LFOV [4]</cell><cell cols="20">70.2 85.2 55.3 68.9 60.5 59.8 44.5 25.4 47.8 42.6 47.9 57.7 52.4 20.7 9.1 36.0 36.9 41.4 32.5 16.0</cell></row><row><cell>DeepLabV3 [5]</cell><cell cols="20">78.8 83.4 56.7 61.9 57.0 59.4 41.3 39.9 44.5 45.1 60.3 56.9 54.9 22.9 14.2 52.4 40.6 40.1 31.3 30.8</cell></row><row><cell>DCN [9]</cell><cell cols="20">77.0 83.0 56.4 64.7 57.0 60.8 39.9 35.5 44.6 44.7 59.3 55.8 59.9 20.3 12.3 55.9 51.2 39.8 36.2 34.2</cell></row><row><cell>VCD [51]</cell><cell cols="20">78.2 83.7 57.4 66.1 57.2 60.9 40.1 39.5 45.1 46.8 59.4 58.1 56.6 21.9 16.0 55.2 47.0 42.7 36.2 34.3</cell></row><row><cell>ADSD (Ours)</cell><cell cols="20">82.3 87.7 66.5 78.2 66.1 68.3 48.0 44.4 48.8 47.1 63.9 71.6 58.4 28.5 19.7 66.9 60.0 51.7 58.4 33.7</cell></row><row><cell>Method</cell><cell>clothes</cell><cell>ceiling</cell><cell>books</cell><cell>fridge</cell><cell>tv</cell><cell>paper</cell><cell>towel</cell><cell>shower</cell><cell>box</cell><cell>board</cell><cell>person</cell><cell>nightstand</cell><cell>toilet</cell><cell>sink</cell><cell>lamp</cell><cell>bathtub</cell><cell>bag</cell><cell>ot. struct.</cell><cell>ot. furn.</cell><cell>ot. props.</cell></row><row><cell>DeepLab [3]</cell><cell cols="8">20.2 55.1 22.1 30.6 49.4 21.8 32.1 6.4</cell><cell cols="12">5.8 14.8 55.3 37.7 57.9 47.7 40.0 44.7 6.6 18.0 12.9 33.8</cell></row><row><cell>FCN [42]</cell><cell cols="20">18.3 59.1 27.3 27.0 41.9 15.9 26.1 14.1 6.5 12.9 57.6 30.1 61.3 44.8 32.1 39.2 4.8 15.2 7.7 30.0</cell></row><row><cell cols="10">Mutex Constraints [11] 12.6 56.7 8.9 21.6 19.2 28.0 28.6 22.9 1.6</cell><cell>1.0</cell><cell cols="6">9.6 30.6 48.4 41.8 28.1 27.6</cell><cell>0</cell><cell>9.8</cell><cell cols="2">7.6 24.5</cell></row><row><cell>BI (3000) [14]</cell><cell cols="9">13.7 42.5 21.3 16.6 30.9 14.9 23.3 17.8 3.3</cell><cell cols="11">9.9 44.7 15.8 53.8 32.1 22.8 19.0 0.1 12.3 5.3 23.2</cell></row><row><cell cols="9">LSD-GF [8] 24.7 DeepLab-LFOV [4] 17.8 58.4 20.5 45.1 48.0 21.0 41.5 9.4</cell><cell cols="12">8.0 14.3 67.0 41.8 69.7 46.8 40.1 45.1 2.1 20.7 12.4 33.5</cell></row><row><cell>DeepLabV3 [5]</cell><cell cols="20">20.7 69.8 30.3 42.8 52.5 27.7 33.2 24.5 13.6 68.9 73.3 37.7 65.1 51.3 39.2 36.4 12.5 27.7 15.2 36.6</cell></row><row><cell>DCN [9]</cell><cell cols="20">22.3 63.3 26.9 52.8 58.7 29.9 39.8 40.4 14.9 65.3 76.2 39.9 67.1 50.3 38.7 40.1 7.3 26.7 16.5 36.9</cell></row><row><cell>VCD [51]</cell><cell cols="20">22.2 67.0 30.0 50.9 57.0 30.7 36.7 40.6 15.6 72.6 77.5 41.2 69.1 51.8 43.0 39.4 9.5 27.7 18.3 37.0</cell></row><row><cell>ADSD (Ours)</cell><cell>24.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-arts on each category of the SUN-RGBD dataset. Percentage (%) of IoUs are shown for evaluation, with best performance marked in bold. 88.8 61.5 51.4 71.7 37.3 51.4 2.9 46.0 54.2 49.1 44.6 82.2 74.2 64.7 77.0 47.6 58.0 87.6 59.6 66.9 68.6 43.4 49.8 29.5 45.9 64.6 62.6 46.8 88.1 76.4 62.8 84.2 42.6 62.1</figDesc><table><row><cell>Method</cell><cell>wall</cell><cell>floor</cell><cell>cabinet</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>door</cell><cell>window</cell><cell>bookshelf</cell><cell>picture</cell><cell>counter</cell><cell>blinds</cell><cell>desk</cell><cell>shelves</cell><cell>curtain</cell><cell>dresser</cell><cell>pillow</cell><cell>mirror</cell></row><row><cell cols="9">Song et al. [43] 36.4 45.8 15.4 23.3 19.9 11.6 19.3 6.0</cell><cell cols="3">7.9 12.8 3.6</cell><cell>5.2</cell><cell>2.2</cell><cell>7.0</cell><cell>1.7</cell><cell>4.4</cell><cell>5.4</cell><cell>3.1</cell><cell>5.6</cell></row><row><cell>Liu et al. [31]</cell><cell cols="8">37.8 48.3 17.2 23.6 20.8 12.1 20.9 6.8</cell><cell cols="3">9.0 13.1 4.4</cell><cell>6.2</cell><cell>2.4</cell><cell>6.8</cell><cell>1.0</cell><cell>7.8</cell><cell>4.8</cell><cell>3.2</cell><cell>6.4</cell></row><row><cell>Ren et al. [40]</cell><cell cols="19">43.2 78.6 26.2 42.5 33.2 40.6 34.3 33.2 43.6 23.1 57.2 31.8 42.3 12.1 18.4 59.1 31.4 49.5 24.8</cell></row><row><cell cols="20">DeconvNet [8] 90.4 92.7 57.7 75.9 83.0 61.2 64.2 43.0 64.7 42.3 59.8 42.5 48.3 29.5 17.5 64.9 54.0 61.7 51.3</cell></row><row><cell>LSD-GF [8]</cell><cell cols="19">91.9 94.7 61.6 82.2 87.5 62.8 68.3 47.9 68.0 48.4 69.1 49.4 51.3 35.0 24.0 68.7 60.5 66.5 57.6</cell></row><row><cell>ADSD (Ours)</cell><cell cols="19">92.1 96.0 70.9 84.0 86.7 74.5 72.5 58.5 70.4 51.7 71.8 57.0 54.3 29.6 21.6 78.1 67.2 64.9 64.0</cell></row><row><cell>Method</cell><cell>floormat</cell><cell>clothes</cell><cell>ceiling</cell><cell>books</cell><cell>fridge</cell><cell>tv</cell><cell>paper</cell><cell>towel</cell><cell>shower</cell><cell>box</cell><cell>board</cell><cell>person</cell><cell>nightstand</cell><cell>toilet</cell><cell>sink</cell><cell>lamp</cell><cell>bathtub</cell><cell>bag</cell><cell>mACC.</cell></row><row><cell>Song et al. [43]</cell><cell>0</cell><cell cols="3">1.4 35.8 6.1</cell><cell>9.5</cell><cell>0.7</cell><cell>1.4</cell><cell>0.2</cell><cell>0.0</cell><cell>0.6</cell><cell>7.6</cell><cell>0.7</cell><cell cols="4">1.7 12.0 15.2 0.9</cell><cell>1.1</cell><cell>0.6</cell><cell>9.0</cell></row><row><cell>Liu et al. [31]</cell><cell>0</cell><cell cols="5">1.6 49.2 8.7 10.1 0.6</cell><cell>1.4</cell><cell>0.2</cell><cell>0.0</cell><cell>0.8</cell><cell>8.6</cell><cell>0.8</cell><cell cols="4">1.8 14.9 16.8 1.2</cell><cell>1.1</cell><cell cols="2">1.3 10.1</cell></row><row><cell>Ren et al. [40]</cell><cell cols="19">5.6 27.0 84.5 35.7 24.2 36.5 26.8 19.2 9.0 11.7 51.4 35.7 25.0 64.1 53.0 44.2 47.0 18.6 36.3</cell></row><row><cell>DeconvNet [8]</cell><cell cols="19">0.4 39.8 78.3 55.0 43.9 59.6 29.4 45.2 1.5 35.9 47.7 45.3 36.0 77.6 66.6 51.2 66.1 35.8 51.9</cell></row><row><cell cols="3">LSD-GF [8] 44.7 ADSD (Ours) 0 0 55.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Yangzhangcst/RGBD-semantic-segmentation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SegNet: a deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mono-sf: Multiview geometry meets single-view depth for monocular scene flow estimation of dynamic traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brickwedde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2780" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spatial information guided convolution for real-time rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04534</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentional feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oehmcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgbd images with mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic segmentation with context encoding and multi-path decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3520" to="3533" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FuseNet: incorporating depth into semantic segmentation via fusionbased cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">STD2P: rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ACNet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RedNet: residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian SegNet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RDFNet: rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RefineNet: multi-path refinement networks for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SIFT Flow: dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CPA-SLAM: consistent plane-model alignment for direct rgb-d slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1285" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning densities in feature space for reliable segmentation of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moraldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gawel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1032" to="1038" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pushmeet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time joint semantic segmentation and depth estimation using asymmetric annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BAM: bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comparison in depth perception between virtual reality and augmented reality systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1124" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RGB-(D) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel &apos;squeeze &amp; excitation&apos; in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient rgb-d semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06961</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Attentionguided chained context aggregation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12041</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aerial single-view depth completion with image-guided uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1055" to="1062" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MTI-Net: multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Malleable 2.5D convolution: Learning receptive fields along the depth-axis for rgb-d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Variational context-deformable convnets for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3991" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PAD-Net: multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rgb-d co-attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pattern-structure diffusion for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
