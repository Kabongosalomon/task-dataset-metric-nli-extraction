<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
							<email>bincao@aidigger.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Eigen Technologies</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
							<email>xiaofeihe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Machine comprehension(MC) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence <ref type="bibr" target="#b17">(Shen et al., 2016;</ref><ref type="bibr" target="#b16">Seo et al., 2016)</ref>. Its task is to teach machine to understand the content of a given passage and then answer the question related to it. <ref type="figure" target="#fig_0">Figure 1</ref> shows a simple example from the popular dataset SQuAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref>.</p><p>Many significant works are based on this task, and most of them focus on the improvement of a sequence model that is augmented with an attention mechanism. However, the encoding of the words is also crucial and a better encoding layer can lead to substantial difference to the final performance. Many powerful methods <ref type="bibr" target="#b22">Wang et al., 2017;</ref><ref type="bibr" target="#b16">Seo et al., 2016)</ref> only represent their words in two ways, word-level embeddings and character-level embeddings. They use pre-train vectors, like GloVe(Pennington The majority of the work was done while the authors were interning at the Eigen Technologies .  <ref type="bibr">et al., 2014)</ref>, to do the word-level embeddings, which ignore syntactic information and name entity of the words. <ref type="bibr" target="#b8">Liu et al. (2017)</ref> construct a sequence of syntactic nodes for the words and encodes the sequence into a vector representation. However, they neglected the optimization of the initial embedding and didn't take the semantic information of the words into account, which are very important parts in the vector representations of the words. For example, the word "Apple" is a fixed vector in GloVe and noun in syntactics whatever it represents the fruit or the company, but name entity tags can help recognize.</p><p>Moreover, the attention mechanism can be divided into two categories: one dimensional attention <ref type="bibr" target="#b2">Dhingra et al., 2016;</ref><ref type="bibr" target="#b7">Kadlec et al., 2016)</ref> and two dimensional attention <ref type="bibr" target="#b1">(Cui et al., 2016;</ref><ref type="bibr" target="#b16">Seo et al., 2016)</ref>. In one dimensional attention, the whole query is represented by one embedding vector, which is usually the last hidden state in the neural network. However, using only one vector to represent the whole query will attenuate the attention of key words. On the contrary, every word in the query has its own embedding vector in the situation of two dimensional attention, but many words in the question sentence are useless even if disturbing, such as the stopwords.</p><p>In this paper, we introduce the Multi-layer Embedding with Memory Networks(MEMEN), an end-to-end neural network for machine comprehension task. Our model consists of three parts: 1) the encoding of context and query, in which we add useful syntactic and semantic information in the embedding of every word, 2) the high-efficiency multilayer memory network of full-orientation matching to match the question and context, 3) the pointer-network based answer boundary prediction layer to get the location of the answer in the passage. The contributions of this paper can be summarized as follows.</p><p>? First, we propose a novel multi-layer embedding of the words in the passage and query. We use skip-gram model to train the part-of-speech(POS) tags and name-entity recognition(NER) tags embedding that represent the syntactic and semantic information of the words respectively. The analogy inference provided by skip-gram model can make the similar attributes close in their embedding space such that more adept at helping find the answer. ? Second, we introduce a memory networks of fullorientation matching.To combines the advantages of one dimensional attention and two dimensional attention, our novel hierarchical attention vectors contain both of them.</p><p>Because key words in query often appear at ends of the sentence, one-dimensional attention, in which the bidirectional last hidden states are regarded as representation, is able to capture more useful information compared to only applying two dimensional attention. In order to deepen the memory and better understand the passage according to the query, we employ the structure of multihops to repeatedly read the passage. Moreover, we add a gate to the end of each memory to improve the speed of convergence. ? Finally, the proposed method yields competitive results on the large machine comprehension bench marks SQuAD and the state-of-the-art results on TriviaQA dataset. On SQuAD, our model achieves 75.37% exact match and 82.66% F1 score. Moreover, our model avoids the high computation complexity self-matching mechanism which is popular in many previous works, thus we spend much less time and memory when training the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Structure</head><p>As <ref type="figure" target="#fig_1">Figure 2</ref> shows, our machine reading model consists of three parts. First, we concatenate several layers of embedding of questions and contexts and pass them into a bidirectional RNN <ref type="bibr" target="#b9">(Mikolov et al., 2010)</ref>. Then we obtain the relationship between query and context through a novel fullorientation matching and apply memory networks in order to deeply understand. In the end, the output layer helps locate the answer in the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding of Context and Query</head><p>In the encoding layer, we represent all tokens in the context and question as a sequence of embeddings and pass them as the input to a recurrent neural network. Word-level embeddings and character-level embeddings are first applied.We use pre-trained word vectors GloVe <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> to obtain the fixed word embedding of each word.The character-level embeddings are generated by using Convolutional Neural Networks(CNN) which is applied to the characters of each word <ref type="bibr">(Kim, et al., 2014)</ref>. This layer maps each token to a high dimensional vector space and is proved to be helpful in handling out-ofvocab(OOV) words.</p><p>We also use skip-gram model to train the embeddings of part-of-speech(POS) tags and named-entity recognition(NER) tags. We first transform all of the given train- <ref type="figure">Figure 3</ref>: The passage and its according transformed "passages". The first row(green) is the original sentence from the passage, the second row(red) is the name-entity recognition(NER) tags, and the last row(blue) is the part-of-speech(POS) tags.</p><p>ing set into their part-of-speech(POS) tags and namedentity recognition(NER) tags, which can be showed in <ref type="figure">Figure</ref> 3. Then we employ skip-sram model, which is one of the core algorithms in the popular off-the-shelf embedding word2vec <ref type="bibr" target="#b10">(Mikolov et al., 2013)</ref>, to the transformed "passage" just like it works in word2vec for the normal passage. Given a sequence of training words in the transformed passage: w 1 , w 2 , ..., w N , the objective of the skip-gram model is to maximize the average log probability:</p><formula xml:id="formula_0">1 N N n=1 ?c?i?c,p =0 log p(w n+i |w n )</formula><p>where c is the size of the context which can be set manually, a large c means more accurate results and more training time. The p(w n+i |w n ) is defined by:</p><formula xml:id="formula_1">p(w O |w I ) = exp(v T w O v w I ) V w=1 exp(v T w v w I )</formula><p>where v w and v w are the input and output vector of w, and V is the vocabulary size.</p><p>We finally get the fixed length embedding of each tag. Although the number of tags limits the effect of word analogy inference, it still be very helpful compared to simple one hot embedding since similar tags have similar surroundings.</p><p>In the end, we use a BiLSTM to encode both the context and query embeddings and obtain their representations {r P t } n t=1 and {r Q t } m t=1 and the last hidden state of both directions of query representation u Q .</p><formula xml:id="formula_2">r P t = BiLSTM([w P t ; c P t ; s P t ]), t ? [1, ..., n] r Q t = BiLSTM([w Q t ; c Q t ; s Q t ]), t ? [1, ..., m]</formula><p>where w, c, s represent word-level embedding, characterlevel embedding and tags embedding respectively. u Q is the concatenation of both directions' last hidden state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Network of Full-Orientation Matching</head><p>Attention mechanism is a common way to link and blend the content between the context and query. Unlike previous methods that are either two dimensional matching or one dimensional matching, we propose a full-orientation matching layer that synthesizes both of them and thus combine the advantages of both side and hedge the weakness. After concatenating all the attention vectors, we will pass them into a bi-directional LSTM. We start by describing our model in the single layer case, which implements a single memory hop operation. We then show it can be stacked to give multiple hops in memory.</p><p>Integral Query Matching The input of this step is the representations {r P t } n t=1 , {r Q t } m t=1 and u Q . At first we obtain the importance of each word in passage according to the integral query by means of computing the match between u Q and each representation r P t by taking the inner product followed by a softmax:</p><formula xml:id="formula_3">c t = softmax( u Q , r P t ) Subsequently the first matching module is the sum of the inputs {r P t } n t=1 weighted by attention c t : m 1 = t c t r P t</formula><p>Query-Based Similarity Matching We then obtain an alignment matrix A ? R n?m between the query and context by <ref type="bibr">Seo et al. (2017)</ref>, we use this alignment matrix to compute whether the query words are relevant to each context word. For each context word, there is an attention weight that represents how much it is relevant to every query word: B = softmax row (A) ? R n?m softmax row (A) means the softmax function is performed across the row vector, and each attention vector is</p><formula xml:id="formula_4">A ij = w T 1 [r P i ; r Q j ; r P i ? r Q j ], w 1 is the weight param- eter, ? is elementwise multiplication. Like</formula><formula xml:id="formula_5">M 2 t = B ? r Q t ,</formula><p>which is based on the query embedding. Hence the second matching module is M 2 , where each M 2 t is the column of M 2 .</p><p>Context-Based Similarity Matching When we consider the relevance between context and query, the most representative word in the query sentence can be chosen by e = max row (A) ? R n , and the attention is d = softmax(e).</p><p>Then we obtain the last matching module</p><formula xml:id="formula_6">m 3 = t r P t ? d t</formula><p>which is based on the context embedding. We put all of the memories in a linear function to get the integrated hierarchical matching module:</p><formula xml:id="formula_7">M = f (M 1 , M 2 , M 3 )</formula><p>where f is an simple linear function, M 1 and M 3 are matrixes that are tiled n times by m 1 and m 3 .</p><p>Moreover, <ref type="bibr" target="#b22">Wang et al. (2017)</ref> add an additional gate to the input of RNN:</p><formula xml:id="formula_8">g t = sigmoid(W g M ) M * = g t M</formula><p>The gate is based on the integration of hierarchical attention vectors, and it effectively filtrates the part of tokens that are helpful in understanding the relation between passage and query. Additionally, we add a bias to improve the estimation:</p><formula xml:id="formula_9">g t = sigmoid(W g M + b)</formula><p>Experiments prove that this gate can also accelerate the speed of convergence. Finally, the integrated memory M is passed into a bi-directional LSTM, and the output will captures the interaction among the context words and the query words:</p><formula xml:id="formula_10">O t = BiLSTM(O t?1 , M )</formula><p>In multiple layers, the integrated hierarchical matching module M can be regarded as the input {r P t } n t=1 of next layer after a dimensionality reduction processing. We call this memory networks of full-orientation matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output layer</head><p>In this layer, we follow <ref type="bibr" target="#b20">Wang and Jiang (2016)</ref> to use the boundary model of pointer networks <ref type="bibr" target="#b19">(Vinyals et al., 2015)</ref> to locate the answer in the passage. Moreover, we follow <ref type="bibr" target="#b22">Wang et al. (2017)</ref> to initialize the hidden state of the pointer network by a query-aware representation:</p><formula xml:id="formula_11">z j = s T tanh(W Q r Q j + b Q ) a i = exp(z i ) m j=1 exp(z j ) l 0 = m i=1 a i r Q i</formula><p>where s T ,W Q and b Q are parameters, l 0 is the initial hidden state of the pointer network. Then we use the passage representation along with the initialized hidden state to predict the indices that represent the answer's location in the passage:</p><formula xml:id="formula_12">z k j = c T tanh(W P O j + W h l 0 ) a k i = exp(z k i ) n j=1 exp(z k j )</formula><p>p k = argmax(a k 1 , ..., a k n ) where W h is parameter, k = 1, 2 that respectively represent the start point and the end point of the answer, O j is the vector that represents j-th word in the passage of the final output of the memory networks. To get the next layer of hidden state, we need to pass O weighted by current predicted probability a k to the Gated Recurrent Unit(GRU) <ref type="bibr">(Chung et al., 2014)</ref>:</p><formula xml:id="formula_13">v k = n i=1 a k i O i l 1 t = GRU(l 1 t?1 , v k )</formula><p>For the loss function, we minimize the sum of the negative probabilities of the true start and end indices by the predicted distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Implementation Settings</head><p>The tokenizers we use in the step of preprocessing data are from Stanford CoreNLP <ref type="bibr" target="#b13">(Manning et al., 2014)</ref>. We also use part-of-speech tagger and named-entity recognition tagger in Stanford CoreNLP utilities to transform the passage and question. For the skip-gram model, our model refers to the word2vec module in open source software library, Tensorflow, the skip window is set as 2. The dataset we use to train the embedding of POS tags and NER tags are the training set given by SQuAD, in which all the sentences are tokenized and regrouped as a list. To improve the reliability and stabllity, we screen out the sentences whose length are shorter than 9. We use 100 one dimensional filters for CNN in the character level embedding, with width of 5 for each one. We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout <ref type="bibr" target="#b17">(Srivastava et al., 2014)</ref> between layers with a dropout ratio as 0.2. We use the AdaDelta (Zeiler, 2012) optimizer with a initial learning rate as 0.001. For the memory networks, we set the number of layer as 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TriviaQA Results</head><p>We first evaluate our model on a large scale reading comprehension dataset TriviaQA version1.0 <ref type="bibr" target="#b6">(Joshi et al., 2017)</ref>. TriviaQA contains over 650K question-answer-evidence triples, that are derived from Web search results and Wikipedia pages with highly differing levels of information redundancy. TriviaQA is the first dataset where questions are authored by trivia enthusiasts, independently of the evidence documents.  There are two different metrics to evaluate model accuracy: Exact Match(EM) and F1 Score, which measures the weighted average of the precision and recall rate at character level. Because the evidence is gathered by an automated process, the documents are not guaranteed to contain all facts needed to answer the question. In addition to distant supervision evaluation, we also evaluate models on a verified subsets. Because the test set is not released, we train our model on training set and evaluate our model on dev set. As we can see in <ref type="figure" target="#fig_2">Figure 4</ref>, our model outperforms all other baselines and achieves the state-of-the-art result on all subsets on TriviaQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD Results</head><p>We also use the Stanford Question Answering Dataset (SQuAD) v1.1 to conduct our experiments. Passages in the dataset are retrieved from English Wikipedia by means of Project Nayuki's Wikipedia's internal PageRanks. They sampled 536 articles uniformly at random with a wide range of topics, from musical celebrities to abstract concepts.The dataset is partitioned randomly into a training set(80%), a development set(10%), and a hidden test set(10%). The host of SQuAD didn't release the test set to the public, so everybody has to submit their model and the host will run it on the test set for them. <ref type="figure" target="#fig_3">Figure 5</ref> shows the performance of our model and competing approaches on the SQuAD. The results of this dataset are all exhibited on a leaderboard, and top methods are almost all ensemble models, our model achieves an exact match score of 75.37% and an F1 score of 82.66%, which is competitive to state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Details</head><p>The main current ensemble methods in the machine comprehension is simply choosing the answer with the highest sum of confidence scores among several single models which are exactly identical except the random initial seed. However, the performance of ensemble model can obviously be better if there is some diversity among single models. In our SQuAD experiment, we get the value of learning rate and dropout ratio of each model by a gaussian distribution, in which the mean value are 0.001 and 0.2 respectively. learning rate ? N (0.001, 0.0001) dropout ? N (0.2, 0.05)</p><p>To keep the diversity in a reasonable scope, we set the variance of gaussian distribution as 0.0001 and 0.05 respectively. Finally, we build an ensemble model which consists of 14 single models with different parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speed and Efficiency</head><p>Compared to <ref type="bibr" target="#b22">(Wang et al., 2017)</ref>, which achieves state-ofthe-art result on the SQuAD test set, our model doesn't contain the self-matching attention layer which is stuck with high computational complexity. Our MEMEN was trained with NVIDIA Titan X GPU, and the training process of the 3-hops model took roughly 5 hours on a single GPU. However, an one-hop model took 22 hours when we added self-matching layer in attention memory. Although the accuracy is improved a little compared to one-hop MEMEN model, it declined sharply as the number of hops increased, not to speak of the disadvantage of running time. The reason might be that multi-hops model with self-matching layer is too complex to efficiently learn the features for this dataset. As a result, our model is competitive both in accuracy and efficiency.</p><p>Hops and Ablations <ref type="figure" target="#fig_4">Figure 6</ref> shows the performance of our single model on SQuAD dev set with different number of hops in the memory network. As we can see, both the EM and F1 score increase as the number of hops enlarges until it arrives 3. After the model achieves the best performance with 3 hops, the performance gets worse as the number of hops gets large, which might result in overfitting.</p><p>We also run the ablations of our single model on SQuAD dev set to evaluate the individual contribution. As <ref type="figure" target="#fig_5">Figure  7</ref> shows, both syntactic embeddings and semantic embeddings contribute towards the model's performance and the POS tags seem to be more important. The reason may be that the number of POS tags is larger than that of NER tags so the embedding is easier to train. For the full-orientation matching, we remove each kind of attention vector respectively and the linear function can handle any two of the rest hierarchical attention vectors. For ablating integral query matching, the result drops about 2% on both metrics and it shows that the integral information of query for each word in passage is crucial. The query-based similarity matching accounts for about 10% performance degradation, which proves the effectiveness of alignment context words against query. For context-based similarity matching, we simply took out the M 3 from the linear function and it is proved to be contributory to the performance of full-orientation matching.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Machine Reading Comprehension Dataset.</p><p>Several benchmark datasets play an important role in progress of machine comprehension task and question answering research in recent years. MCTest <ref type="bibr" target="#b15">(Richardson et al., 2013)</ref> is one of the famous and high quality datasets. There are 660 fictional stories and 4 multiple choice questions per story contained in it, and the labels are all made by humans. Researchers also released cloze-style datasets <ref type="bibr" target="#b4">(Hill et al., 2015;</ref><ref type="bibr" target="#b3">Hermann et al., 2015;</ref><ref type="bibr" target="#b11">Onishi et al., 2016;</ref><ref type="bibr" target="#b12">Paperno et al., 2016)</ref>. However, these datasets are either not large enough to support deep neural network models or too easy to challenge natural language.</p><p>Recently, <ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref> released the Stanford Question Answering dataset (SQuAD), which is almost two orders of magnitude larger than all previous hand-annotated datasets. Moreover, this dataset consists 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding passage, rather than a limited set of multiple choices or entities. TriviaQA <ref type="bibr" target="#b6">(Joshi et al., 2017)</ref> is also a large and high quality dataset, and the crucial difference between TriviaQA and SQuAD is that TriviaQA questions have not been crowdsourced from pre-selected passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Based Models for Machine Reading</head><p>Many works are based on the task of machine reading comprehension, and attention mechanism have been particularly successful <ref type="bibr" target="#b23">(Xiong et al., 2016;</ref><ref type="bibr" target="#b1">Cui et al., 2016;</ref><ref type="bibr" target="#b16">Seo et al., 2016;</ref><ref type="bibr" target="#b17">Shen et al., 2016;</ref><ref type="bibr" target="#b22">Wang et al., 2017)</ref>. <ref type="bibr" target="#b23">Xiong et al. (2016)</ref> present a coattention encoder and dynamic decoder to locate the answer. <ref type="bibr" target="#b1">Cui et al. (2016)</ref> propose a two side attention mechanism to compute the matching between the passage and query.  match the passage and query from several perspectives and predict the answer by globally normalizing probability distributions. <ref type="bibr" target="#b16">Seo et al. (2016)</ref> propose a bi-directional attention flow to achieve a query-aware context representation.  propose self-aware representation and multi-hop query-sensitive pointer to predict the answer span. <ref type="bibr" target="#b17">Shen et al. (2016)</ref> propose iterarively inferring the answer with a dynamic number of steps trained with reinforcement learning. <ref type="bibr" target="#b22">Wang et al. (2017)</ref> employ gated self-matching attention to obtain the relation between the question and passage. Our MEMEN construct a hierarchical orientation attention mechanism to get a wider match while applying memory network <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015)</ref> for deeper understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduce MEMEN for Machine comprehension style question answering. We propose the multilayer embedding to encode the document and the memory network of full-orientation matching to obtain the interaction of the context and query. The experimental evaluation shows that our model achieves the state-of-the-art result on TriviaQA dataset and competitive result in SQuAD. Moreover, the ablations and hops analysis demonstrate the im-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example from the SQuAD dataset. The answer is a segment text of context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>MEMEN structure overview. In the figure above, there are two hops in the memory network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The performance of our MEMEN and baselines on TriviaQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The performance of our MEMEN and competing approaches on SQuAD dataset as we submitted our model(May,  22, 2017). * indicates ensemble models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Performance comparison among different number of hops on the SQuAD dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Ablation results on the SQuAD dev set.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>portance of every part of the hierarchical attention vectors and the benefit of multi-hops in memory network. For future work, we will focus on question generative method and sentence ranking in machine reading tasks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02858</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Structural embedding of syntactic trees for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cernock?, and Sanjeev Khudanpur. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Who did what: A largescale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05457</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germ?n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fern?ndez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">The lambada dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu Chen ; Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05284</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reasonet: Learning to stop reading in machine comprehension</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
