<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Head: Unifying Object Detection Heads with Attentions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
							<email>xidai@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><forename type="middle">Chen</forename><surname>Bin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Dongdong</forename><surname>Chen</surname></persName>
							<email>dochen@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>USA</roleName><forename type="first">Microsoft</forename><surname>Redmond</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Head: Unifying Object Detection Heads with Attentions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The complex nature of combining localization and classification in object detection has resulted in the flourished development of methods. Previous works tried to improve the performance in various object detection heads but failed to present a unified view. In this paper, we present a novel dynamic head framework to unify object detection heads with attentions. By coherently combining multiple self-attention mechanisms between feature levels for scaleawareness, among spatial locations for spatial-awareness, and within output channels for task-awareness, the proposed approach significantly improves the representation ability of object detection heads without any computational overhead. Further experiments demonstrate that the effectiveness and efficiency of the proposed dynamic head on the COCO benchmark. With a standard ResNeXt-101-DCN backbone, we largely improve the performance over popular object detectors and achieve a new state-of-the-art at 54.0 AP. Furthermore, with latest transformer backbone and extra data, we can push current best COCO result to a new record at 60.6 AP. The code will be released at https: //github.com/microsoft/DynamicHead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is to answer the question "what objects are located at where" in computer vision applications. In the deep learning era, nearly all modern object detectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref> share the same paradigm -a backbone for feature extraction and a head for localization and classification tasks. How to improve the performance of an object detection head has become a critical problem in existing object detection works.</p><p>The challenges in developing a good object detection head can be summarized into three categories. Firstly, the head should be scale-aware, since multiple objects with vastly distinct scales often co-exist in an image. Secondly, the head should be spatial-aware, since objects usually ap-pear in vastly different shapes, rotations, and locations under different viewpoints. Thirdly, the head needs to be taskaware, since objects can have various representations (e.g., bounding box <ref type="bibr" target="#b11">[12]</ref>, center <ref type="bibr" target="#b27">[28]</ref>, and corner points <ref type="bibr" target="#b33">[33]</ref>) that own totally different objectives and constraints. We find recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref> only focus on solving one of the aforementioned problems in various ways. It remains an open problem how to develop a unified head that can address all these problems simultaneously.</p><p>In this paper, we propose a novel detection head, called dynamic head, to unify scale-awareness, spatial-awareness, and task-awareness all together. If we consider the output of a backbone (i.e., the input to a detection head) as a 3-dimensional tensor with dimensions level ? space ? channel, we discover that such a unified head can be regarded as an attention learning problem. An intuitive solution is to build a full self-attention mechanism over this tensor. However, the optimization problem would be too difficult to solve and the computational cost is not affordable.</p><p>Instead, we can deploy attention mechanisms separately on each particular dimension of features, i.e., level-wise, spatial-wise, and channel-wise. The scale-aware attention module is only deployed on the dimension of level. It learns the relative importance of various semantic levels to enhance the feature at a proper level for an individual object based on its scale. The spatial-aware attention module is deployed on the dimension of space (i.e., height ? width). It learns coherently discriminative representations in spatial locations. The task-aware attention module is deployed on channels. It directs different feature channels to favor different tasks separately (e.g., classification, box regression, and center/key-point learning.) based on different convolutional kernel responses from objects.</p><p>In this way, we explicitly implement a unified attention mechanism for the detection head. Although these attention mechanisms are separately applied on different dimensions of a feature tensor, their performance can complement each other. Extensive experiments on the MS-COCO benchmark demonstrate the effectiveness of our approach. It offers a great potential for learning a better representation that can be utilized to improve all kinds of object detection models with 1.2% ? 3.2% AP gains. With the standard ResNeXt-101-DCN backbone, the proposed method achieves a new state of the art 54.0% AP on COCO. Besides, compared with EffcientDet <ref type="bibr" target="#b26">[27]</ref> and SpineNet <ref type="bibr" target="#b7">[8]</ref>, dynamic head uses 1/20 training time, yet with a better performance. Furthermore, with latest transformer backbone and extra data from self-training, we can push current best COCO result to a new record at 60.6 AP (see appendix for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent studies focus on improving object detectors from various perspectives: scale-awareness, spatial-awareness and task-awareness.</p><p>Scale-awareness. Many researches have empathized the importance of scale-awareness in object detection as objects with vastly different scales often co-exist in natural images. Early works have demonstrated the significance of leveraging image pyramid methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> for multi-scale training. Instead of image pyramid, feature pyramid <ref type="bibr" target="#b14">[15]</ref> was proposed to improve efficiency by concatenating a pyramid of down-sampled convolution features and had become a standard component in modern object detectors. However, features from different levels are usually extracted from different depth of a network, which causes a noticeable semantics gap. To solve this discrepancy, <ref type="bibr" target="#b17">[18]</ref> proposed to enhance the features in lower layers by bottom-up path augmentation from feature pyramid. Later, <ref type="bibr" target="#b19">[20]</ref> improved it by introducing balanced sampling and balanced feature pyramid. Recently, <ref type="bibr" target="#b31">[31]</ref> proposed a pyramid convolution to extract scale and spatial features simultaneously based on a modified 3-D convolution.</p><p>In this work, we present a scale-aware attention in the detection head, which makes the importance of various feature level adaptive to the input.</p><p>Spatial-awareness. Previous works have tried to improve the spatial-awareness in object detection for better semantic learning. Convolution neural networks were known to be limited in learning spatial transformations existed in images <ref type="bibr" target="#b43">[41]</ref>. Some works mitigate this problem by either increasing the model capability (size) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">32]</ref> or involving expensive data augmentations <ref type="bibr" target="#b13">[14]</ref>, resulting in extremely high computational cost in inference and training. Later, new convolution operators were proposed to improve the learning of spatial transformations. <ref type="bibr" target="#b35">[34]</ref> proposed to use dilated convolutions to aggregate contextual information from the exponentially expanded receptive field. <ref type="bibr" target="#b6">[7]</ref> proposed a deformable convolution to sample spatial locations with ad-ditional self-learned offsets. <ref type="bibr" target="#b38">[37]</ref> reformulated the offset by introducing a learned feature amplitude and further improved its ability.</p><p>In this work, we present a spatial-aware attention in the detection head, which not only applies attention to each spatial location, but also adaptively aggregates multiple feature levels together for learning a more discriminative representation.</p><p>Task-awareness. Object detection was originated from a two-stage paradigm <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b5">6]</ref>, which first generates object proposals and then classifies the proposals into different classes and background. <ref type="bibr" target="#b22">[23]</ref> formalized the modern twostage framework by introducing Region Proposal Networks (RPN) to formulate both stages into a single convolution network. Later, one-stage object detector <ref type="bibr" target="#b21">[22]</ref> became popular due to its high efficiency. <ref type="bibr" target="#b15">[16]</ref> further improved the architecture by introducing task-specific branches to surpass the accuracy of two-stage detectors while maintaining the speed of previous one-stage detectors.</p><p>Recently, more works have discovered that various representations of objects could potentially improve the performance. <ref type="bibr" target="#b11">[12]</ref> first demonstrated that combining bounding box and segmentation mask of objects can further improve the performance. <ref type="bibr" target="#b27">[28]</ref> proposed to use center representations to solve object detection in a per-pixel prediction fashion. <ref type="bibr" target="#b36">[35]</ref> further improved the performance of centerbased method by automatically selecting positive and negative samples according to statistical characteristics of object. Later, <ref type="bibr" target="#b33">[33]</ref> formulated object detection as representative key-points to ease the learning. <ref type="bibr" target="#b8">[9]</ref> further improved the performance by detecting each object as a triplet, rather than a pair of key-points to reduce incorrect predictions. Most recently, <ref type="bibr" target="#b20">[21]</ref> proposed to extract border features from the extreme points of each border to enhance the point feature and archived the state-of-the-art performance.</p><p>In this work, we present a task-aware attention in the detection head, which allows attention to be deployed on channels, which can adaptively favor various tasks, for either single-/two-stage detectors, or box-/center-/keypoint-based detectors.</p><p>More importantly, all the above properties are integrated into a unified attention mechanism in our head design. To our best knowledge, it is the first general detection head framework which takes a step towards understanding what role attention plays in the success of object detection head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In order to enable scale-awareness, spatial-awareness and task-awareness simultaneously in a unified object de- <ref type="figure">Figure 1</ref>. An illustration of our Dynamic Head approach. It contains three different attention mechanisms, each focusing on a different perspective: scale-aware attention, spatial-aware attention, and task-aware attention. We also visualize how the feature maps are improved after each attention module.</p><p>tection head, we need to generally understand previous improvements on object detection heads.</p><p>Given a concatenation of features F in = {F i } L i=1 from L different levels in a feature pyramid, we can resize consecutive level features towards the scale of the median level feature using up-sampling or down-sampling. The re-scaled feature pyramid can be denoted as a 4-dimensional tensor F ? R L?H?W ?C , where L represents the number of levels in the pyramid, H, W , and C represent height, width, and the number of channels of the median level feature respectively. We further define S = H ? W to reshape the tensor into a 3-dimensional tensor F ? R L?S?C . Based on this representation, we will explore the role of each tensor dimension.</p><p>? The discrepancy of object scales is related to features at various levels. Improving the representation learning across different levels of F can benefit scaleawareness of object detection. ? Various geometric transformations from dissimilar object shapes are related to features at various spatial locations. Improving the representation learning across different spatial locations of F can benefit spatialawareness of object detection. ? Divergent object representations and tasks can be related to the features at various channels. Improving the representation learning across different channels of F can benefit task-awareness of object detection.</p><p>In this paper, we discover that all above directions can be unified in an efficient attention learning problem. Our work is the first attempt to combine multiple attentions on all three dimensions to formulate a unified head for maximizing their improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Head: Unifying with Attentions</head><p>Given the feature tensor F ? R L?S?C , the general formulation of applying self-attention is:</p><formula xml:id="formula_0">W (F) = ?(F) ? F<label>(1)</label></formula><p>where ?(?) is an attention function. A na?ve solution to this attention function is implemented by fully connected layers. But directly learning the attention function over all dimensions is computationally costly and practically not affordable due to the high dimensions of the tensor. Instead, we convert the attention function into three sequential attentions, each focusing on only one perspective:</p><formula xml:id="formula_1">W (F) = ? C ? S ? L (F) ? F ? F ? F,<label>(2)</label></formula><p>where ? L (?), ? S (?), and ? C (?) are three different attention functions applying on dimension L, S, and C, respectively.</p><p>Scale-aware Attention ? L . We first introduce a scaleaware attention to dynamically fuse features of different scales based on their semantic importance.</p><formula xml:id="formula_2">? L (F) ? F = ? f 1 SC S,C F ? F<label>(3)</label></formula><p>where f (?) is a linear function approximated by a 1 ? 1 convolutional layer, and ?(x) = max(0, min(1, x+1 2 )) is a hard-sigmoid function.</p><p>Spatial-aware Attention ? S . We apply another spatialaware attention module based on the fused feature to focus on discriminative regions consistently co-existing among both spatial locations and feature levels. Considering the high dimensionality in S, we decompose this module into two steps: first making the attention learning sparse by using deformable convolution <ref type="bibr" target="#b6">[7]</ref> and then aggregating features across levels at the same spatial locations:</p><formula xml:id="formula_3">? S (F)?F = 1 L L l=1 K k=1 w l,k ?F(l; p k +?p k ; c)??m k ,<label>(4)</label></formula><p>where K is the number of sparse sampling locations, p k + ?p k is a shifted location by the self-learned spatial offset ?p k to focus on a discriminative region and ?m k is a selflearned importance scalar at location p k . Both are learned from the input feature from the median level of F.</p><p>Task-aware Attention ? C . To enable joint learning and generalize different representations of objects, we deploy a task-aware attention at the end. It dynamically switches ON and OFF channels of features to favor different tasks:</p><formula xml:id="formula_4">? C (F)?F = max ? 1 (F)?F c +? 1 (F), ? 2 (F)?F c +? 2 (F) ,<label>(5)</label></formula><p>where F c is the feature slice at the c-th channel and [? 1 , ? 2 , ? 1 , ? 2 ] T = ?(?) is a hyper function that learns to control the activation thresholds. ?(?) is implemented similar to <ref type="bibr" target="#b2">[3]</ref>, which first conducts a global average pooling on L ? S dimensions to reduce the dimensionality, then uses two fully connected layers and a normalization layer, and finally applies a shifted sigmoid function to normalize the output to [?1, 1].</p><p>Finally, since the above three attention mechanisms are applied sequentially, we can nest Equation 2 multiple times to effectively stack multiple ? L , ? S , and ? C blocks together. The detailed configuration of our dynamic head (i.e., Dy-Head for simplification) block is shown in <ref type="figure" target="#fig_0">Figure 2</ref> (a).</p><p>As a summary, the whole paradigm of object detection with our proposed dynamic head is illustrated in <ref type="figure">Figure 1</ref>. Any kinds of backbone network can be used to extract feature pyramid, which is further resized to the same scale, forming a 3-dimensional tensor F ? R L?S?C , and then used as the input to the dynamic head. Next, several Dy-Head blocks including scale-aware, spatial-aware, and taskaware attentions are stacked sequentially. The output of the dynamic head can be used for different tasks and representations of object detection, such as classification, center/box regression, etc..</p><p>At the bottom of <ref type="figure">Figure 1</ref>, we show the output of each type of attention. As we can see, the initial feature maps from backbones are noisy due to the domain difference from ImageNet pre-training. After passing through our scaleaware attention module, the feature maps become more sensitive to the scale differences of foreground objects; After further passing through our spatial-aware attention module, the feature maps become more sparse and focused on discriminative spatial locations of foreground objects. Finally, after passing through our task-aware attention module, the feature maps re-form into different activations based on the requirements of different down-stream tasks. These visualizations well demonstrate the effectiveness of each attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalizing to Existing Detectors</head><p>In this section, we demonstrate how the proposed dynamic head can be integrated into existing detectors to effectively improve their performances.</p><p>One-stage Detector. One-stage detector predicts object locations by densely sampling locations from feature map, which simplifies the detector design. Typical one-stage detector (e.g., RetinaNet <ref type="bibr" target="#b15">[16]</ref>) is composed of a backbone network to extract dense features and multiple task-specific sub-network branches to handle different tasks separately. As shown in previous work <ref type="bibr" target="#b2">[3]</ref>, object classification subnetwork behaves very differently from bounding box re-gression sub-network. Controversial to this conventional approach, we only attach one unified branch instead of multiple branches to the backbone. It can handle multiple tasks simultaneously, thanks to the advantage of our multiple attention mechanisms. In this way, the architecture can be further simplified and the efficiency is improved as well. Recently, anchor-free variants of one-stage detectors became popular, for example, FCOS <ref type="bibr" target="#b27">[28]</ref>, ATSS <ref type="bibr" target="#b36">[35]</ref> and RepPoint <ref type="bibr" target="#b33">[33]</ref> re-formulated objects as centers and key-points to improve performance. Compared to RetinaNet, these methods require to attach a centerness prediction, or a keypoint prediction to either the classification branch or the regression branch, which makes the constructions of task-specific branches non-trivial. By contrast, deploying our dynamic head is more flexible since it only appends various types of predictions to the end of head, shown in <ref type="figure" target="#fig_0">Figure 2 (b)</ref>.</p><p>Two-stage Detector. Two-stage detectors utilize region proposal and ROI-pooling <ref type="bibr" target="#b22">[23]</ref> layers to extract intermediate representations from feature pyramid of a backbone network. To cooperate this characteristic, we first apply our scale-aware attention and spatial-aware attention on feature pyramid before a ROI-pooling layer and then use our taskaware attention to replace the original fully connected layers, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relation to Other Attention Mechanisms</head><p>Deformable. Deformable convolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">37]</ref> has significantly improved the transformation learning of traditional convolutional layers by introducing sparse sampling. It has been widely used in object detection backbones to enhance the feature representations. Although it is rarely utilized in object detection head, we can regard it as solely modeling the S sub-dimension in our representation. We find the deformable module used in the backbone can be complementary to the proposed dynamic head. In fact, with the deformable variant of ResNext-101-64x4d backbone, our dynamic head achieves a new state-of-the-art object detection result. <ref type="bibr" target="#b30">[30]</ref> is a pioneer work of utilizing attention modules to enhance the performance of object detection. However, it uses a simple formulation of dot-product to enhance a pixel feature by fusing other pixels' features from different spatial locations. This behavior can be regarded as modeling only the L?S sub-dimensions in our representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-local. Non-Local Networks</head><p>Transformer. Recently, there is a trend to introduce the Transformer module <ref type="bibr" target="#b29">[29]</ref> from natural language processing into computer vision tasks. Preliminary works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b4">5]</ref> have demonstrated promising results in improving object detection. Transformer provides a simple solution to learn cross-attention correspondence and fuse features from different modalities by applying multi-head fully connected layers. This behavior can be viewed as modeling only the S ? C sub-dimensions in our representation.</p><p>The aforementioned three types of attention works only partially model sub-dimensions in the feature tensor. As a unified design, our dynamic head combines attentions on different dimensions into one coherent and efficient implementation. The following experiments show such a dedicated design can help existing object detectors achieve remarkable gains. Besides, our attention mechanisms explicitly address the challenges of object detection, in contrast to implicit working principles in existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate our approach on the MS-COCO dataset <ref type="bibr" target="#b16">[17]</ref> following the commonly used settings. MS-COCO contains 80 categories of around 160K images collected from the web. The dataset is split into the train2017, val2017, and test2017 subsets with 118K, 5K, 41K images respectively. The standard mean average precision (AP ) metric is used to report results under different IoU thresholds and object scales. In all our experiments, we only train on the train2017 images without using any extra data. For experiments of ablation studies, we evaluate the performances on the val2017 subset. When comparing to state-of-the-art methods, we report the official result returned from the test server on test-dev subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement our dynamic head block as a plugin, based on the popular implementation of Mask R-CNN benchmark <ref type="bibr" target="#b11">[12]</ref>. If it is not specifically mentioned, our dynamic head is trained with the ATSS framework <ref type="bibr" target="#b36">[35]</ref> . All models are trained using one compute node of 8 V100 GPUs each with 32GB memory.</p><p>Training. We use ResNet-50 as the model backbone in all ablation studies and train it with the standard 1x configuration. Other models are trained with the standard 2x training configurations as introduced in <ref type="bibr" target="#b11">[12]</ref>. We use an initial learning rate of 0.02 with weight decay of 1e ? 4 and momentum of 0.9 . The learning rate is stepped down by a factor of 0.1 at the 67% and 89% of training epochs. Standard augmentation with random horizontal flipping is used. To compare with previous methods trained with multi-scale inputs, we also conduct multi-scale training for selective models.</p><p>Inference. To compare with state-of-the-art methods reported using test time augmentation, we also evaluate our best model with multi-scale testing. Other tricks, such as model EMA, mosaic, mix-up, label smoothing, soft-NMS or adaptive multi-scale testing <ref type="bibr" target="#b24">[25]</ref>, are not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We conduct a series of ablation studies to demonstrate the effectiveness and efficiency of our dynamic head. Effectiveness of Attention Modules. We first conduct a controlled study on the effectiveness of different components in our dynamic head block by gradually adding them to the baseline. As shown in <ref type="table" target="#tab_0">Table 1</ref>, "L.", "S.", "C." represent our scale-aware attention module, spatial-aware attention module, and task-aware module, respectively. We can observe that individually adding each component to the baseline implementation improves its performance by 0.9 AP , 2.4 AP and 1.3 AP . It is expected to see the spatialaware attention module archives the biggest gain because of its dominant dimensionality among three modules. When we add both "L." and "S" to the baseline, it continuously improves the performance by 2.9 AP . Finally, our full dynamic head block significantly improves the baseline by 3.6 AP . This experiment demonstrates that different components work as a coherent module.</p><p>Effectiveness on Attention Learning. We then demonstrate the effectiveness of attention learning in our dynamic head module. <ref type="figure" target="#fig_1">Figure 3</ref> shows the trend of the learned scale ratios (calculated by dividing the learned weight of higher resolution by the learned weight of lower resolution) on different level of features in our scale-aware attention module. The histogram is calculated using all images from the COCO val2017 subset. It is clear to see that our scale-aware attention module tends to regulate higher resolution feature maps ("level 5" purple histogram in the figure) toward lower resolution and lower resolution feature maps ("level 1" blue histogram in the figure) toward higher resolution to smooth  the scale discrepancy form different feature levels. This proves the effectiveness of scale-aware attention learning. <ref type="figure" target="#fig_2">Figure 4</ref> visualizes the feature map output before and after applying different number (i.e. 2,4,6) of blocks of attention modules. Before applying our attention modules, the feature maps extracted from the backbone are very noisy and fail to focus on the foreground objects. As the feature maps pass through more attention modules (from block 2 to block 6 as shown in the figure), it is obvious to see the feature maps cover more foreground objects and focus more accurately on their discriminative spatial locations. This visualization well demonstrates the effectiveness of the spatial-aware attention learning Efficiency on the Depth of Head. We evaluate the efficiency of our dynamic head by controlling the depth (number of blocks). As shown in <ref type="table">Table 2</ref>, we vary the number of used DyHead blocks (e.g., 1, 2, 4, 8, 10 blocks) and compare their performances and computational costs (GFLOPs) with the baseline. Our dynamic head can benefit from the increase of depth by stacking more blocks until 8. It is worth noting that our method with 2 blocks has already outperformed the baseline at even lower computation cost. Meanwhile, even with 6 blocks, the increment of computational cost is negligible compared to the computation cost of the backbone, while largely improving the accuracy. It demonstrates the efficiency of our method.</p><p>Generalization on Existing Object Detectors. We evaluate the generalization ability of the dynamic head by plugging it to popular object detectors, such as Faster-RCNN <ref type="bibr" target="#b22">[23]</ref>, RetinaNet <ref type="bibr" target="#b15">[16]</ref>, ATSS <ref type="bibr" target="#b36">[35]</ref>, FCOS <ref type="bibr" target="#b27">[28]</ref>, and RepPoints <ref type="bibr" target="#b33">[33]</ref>. These methods represent a wide variety of object detection frameworks (e.g., two-stage vs. one-stage, anchor-based vs. anchor-free, box-based vs. point-based). As shown in <ref type="table">Table 3</ref>, our dynamic head significantly boosts all popular object detectors by 1.2 ? 3.2 AP . It demonstrates the generality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State of the Art</head><p>We compare the performance of the dynamic head with several standard backbones and state-of-the-art object detectors.</p><p>Cooperate with Different Backbones. We first demonstrate the compatibility of dynamic head with different backbones. As shown in <ref type="table">Table 4</ref>, we evaluate the performances of object detector by integrating dynamic head with the ResNet-50, ResNet-101 and ResNeXt-101 backbones, and compare with recent methods with similar configurations, including Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>, Cascade-RCNN <ref type="bibr" target="#b0">[1]</ref>, FCOS <ref type="bibr" target="#b27">[28]</ref>, ATSS <ref type="bibr" target="#b36">[35]</ref> and BorderDet <ref type="bibr" target="#b20">[21]</ref>. Our method consistently outperforms previous methods with a big margin. When compared to the best detector BorderDet <ref type="bibr" target="#b20">[21]</ref> Method AP AP50 AP75</p><p>anchor-based two-stage:</p><p>Faster R-CNN <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>. Ablation study on the generalization of our dynamic head when applying to popular object detection methods.</p><p>with same settings, our method outperforms it by 1.1 AP with the ResNet-101 backbone and by 1.2 AP with the ResNeXt-64x4d-101 backbone, where the improvement is significant due to the challenges in the COCO benchmark.</p><p>Compared to State-of-the-Art Detectors. We compare our methods with state-of-the-art detectors <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref>, including some concurrent works <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b4">5]</ref>. As shown in <ref type="table" target="#tab_3">Table 5</ref>, we summarize these existing work into two categories: one using multi-scale training, and the other using both multi-scale training and multi-scale testing. Compared with methods with only multi-scale training, our method achieves a new state of the art at 52.3 AP with only 2x training schedule. Our method is competitive and more efficient to learn compared with EffcientDet <ref type="bibr" target="#b26">[27]</ref> and SpineNet <ref type="bibr" target="#b7">[8]</ref>, with a significantly less 1/20 training time. Compared with the latest work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b4">5]</ref>, which utilize Transformer modules as attention, our dynamic head is superior to these methods with more than 2.0 AP gain, while using less training time than theirs. It demonstrates that our dynamic head can coherently combine multiple modalities of attentions from different perspectives into a unified head, resulting in better efficiency and effectiveness.</p><p>We further compare our method with state-of-the-art results <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b4">5]</ref> with test time augmentation (TTA) using both multi-scale training and multi-scale testing. Our dynamic head helps achieve a new state-of-the-art result at 54.0 AP , which significantly outperforms concurrent best methods by 1.3 AP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Iteration AP AP50 AP75 APS APM APL </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a novel object detection head, which unify the scale-aware, spatial-aware, and taskaware attentions in a single framework. It suggests a new view of object detection head with attentions. As a plugin block, the dynamic head can be flexibly integrated into any existing object detector framework to boost its perfor-mance. Moreover, it is efficient to learn. Our study shows that designing and learning attentions in the object detection head is an interesting direction which deserves more focused studies. This work only takes a step, and could be further improved in these aspects: how to make the full attention model easy to learn and efficient to compute, and how to systematically consider more modalities of attentions into the head designing for better performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A detailed design of Dynamic Head. (a) shows the detailed implementation of each attention module. (b) shows how to apply our dynamic head blocks to one-stage object detector. (c) shows how to apply our dynamic head blocks to two-stage object detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Ablation study on the effectiveness of our scale-aware attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>A visualization on the effectiveness of our spatial-aware attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">L. S. C.</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell cols="3">? ? ? 39.0</cell><cell>57.2</cell><cell>42.4</cell><cell>22.1</cell><cell>43.1</cell><cell>50.2</cell></row><row><cell cols="3">? ? 39.9</cell><cell>57.8</cell><cell>43.5</cell><cell>25.4</cell><cell>44.0</cell><cell>52.4</cell></row><row><cell>?</cell><cell cols="2">? 41.4</cell><cell>58.5</cell><cell>45.2</cell><cell>26.8</cell><cell>45.2</cell><cell>54.3</cell></row><row><cell>? ?</cell><cell></cell><cell>40.3</cell><cell>58.3</cell><cell>43.9</cell><cell>24.2</cell><cell>44.6</cell><cell>53.7</cell></row><row><cell>?</cell><cell></cell><cell>42.0</cell><cell>59.5</cell><cell>45.5</cell><cell>25.5</cell><cell>46.1</cell><cell>55.2</cell></row><row><cell>?</cell><cell></cell><cell>40.6</cell><cell>58.6</cell><cell>44.4</cell><cell>24.6</cell><cell>44.8</cell><cell>53.3</cell></row><row><cell></cell><cell cols="2">? 41.9</cell><cell>59.2</cell><cell>45.6</cell><cell>24.8</cell><cell>46.1</cell><cell>54.5</cell></row><row><cell></cell><cell></cell><cell>42.6</cell><cell>60.1</cell><cell>46.4</cell><cell>26.1</cell><cell>46.8</cell><cell>56.0</cell></row></table><note>Ablation study on the effectiveness of each attention mod- ule in our dynamic head block.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the state-of-the-art results on the MS COCO test-dev set</figDesc><table><row><cell>two-stage detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN[12]</cell><cell>ResNet-101</cell><cell>2x</cell><cell>38.2</cell><cell>60.3</cell><cell></cell><cell>41.7</cell><cell>20.1</cell><cell>41.1</cell><cell>50.2</cell></row><row><cell>Cascade-RCNN[1]</cell><cell>ResNet-50</cell><cell>3x</cell><cell>40.6</cell><cell>59.9</cell><cell></cell><cell>44.0</cell><cell>22.6</cell><cell>42.7</cell><cell>52.1</cell></row><row><cell>Cascade-RCNN[1]</cell><cell>ResNet-101</cell><cell>3x</cell><cell>42.8</cell><cell>62.1</cell><cell></cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell></row><row><cell>one-stage detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCOS[28]</cell><cell>ResNet-101</cell><cell>2x</cell><cell>41.5</cell><cell>60.7</cell><cell></cell><cell>45.0</cell><cell>24.4</cell><cell>44.8</cell><cell>51.6</cell></row><row><cell>FCOS[28]</cell><cell>ResNeXt-64x4d-101</cell><cell>2x</cell><cell>43.2</cell><cell>62.8</cell><cell></cell><cell>46.6</cell><cell>26.5</cell><cell>46.2</cell><cell>53.3</cell></row><row><cell>ATSS[35]</cell><cell>ResNet-101</cell><cell>2x</cell><cell>43.6</cell><cell>62.1</cell><cell></cell><cell>47.4</cell><cell>26.1</cell><cell>47.0</cell><cell>53.6</cell></row><row><cell>ATSS[35]</cell><cell>ResNeXt-64x4d-101</cell><cell>2x</cell><cell>45.6</cell><cell>64.6</cell><cell></cell><cell>49.7</cell><cell>28.5</cell><cell>48.9</cell><cell>55.6</cell></row><row><cell>BorderDet[21]</cell><cell>ResNet-101</cell><cell>1x</cell><cell>43.2</cell><cell>62.1</cell><cell></cell><cell>46.7</cell><cell>24.4</cell><cell>46.3</cell><cell>54.9</cell></row><row><cell>BorderDet[21]</cell><cell>ResNet-101</cell><cell>2x</cell><cell>45.4</cell><cell>64.1</cell><cell></cell><cell>48.8</cell><cell>26.7</cell><cell>48.3</cell><cell>56.5</cell></row><row><cell>BorderDet[21]</cell><cell>ResNeXt-64x4d-101</cell><cell>2x</cell><cell>46.5</cell><cell>65.7</cell><cell></cell><cell>50.5</cell><cell>29.1</cell><cell>49.4</cell><cell>57.5</cell></row><row><cell>DyHead</cell><cell>ResNet-50</cell><cell>1x</cell><cell>43.0</cell><cell>60.7</cell><cell></cell><cell>46.8</cell><cell>24.7</cell><cell>46.4</cell><cell>53.9</cell></row><row><cell>DyHead</cell><cell>ResNet-101</cell><cell>2x</cell><cell>46.5</cell><cell>64.5</cell><cell></cell><cell>50.7</cell><cell>28.3</cell><cell>50.3</cell><cell>57.5</cell></row><row><cell>DyHead</cell><cell>ResNeXt-64x4d-101</cell><cell>2x</cell><cell>47.7</cell><cell>65.7</cell><cell></cell><cell>51.9</cell><cell>31.5</cell><cell>51.7</cell><cell>60.7</cell></row><row><cell cols="8">Table 4. Comparison with results using different backbones on the MS COCO test-dev set</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Iteration</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell></row><row><cell>multi-scale training:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ATSS[35]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>47.7</cell><cell>66.5</cell><cell>51.9</cell><cell>29.7</cell><cell>50.8</cell><cell>59.4</cell></row><row><cell>SEPC[31]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>50.1</cell><cell>69.8</cell><cell>54.3</cell><cell>31.3</cell><cell>53.3</cell><cell>63.7</cell></row><row><cell>BorderDet[21]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>48.0</cell><cell>67.1</cell><cell>52.1</cell><cell>29.4</cell><cell>50.7</cell><cell>60.5</cell></row><row><cell>RepPoints v2[4]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>49.4</cell><cell>68.9</cell><cell>53.4</cell><cell>30.3</cell><cell>52.1</cell><cell>62.3</cell></row><row><cell>RelationNet++[5]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>50.3</cell><cell>69.0</cell><cell>55.0</cell><cell>32.8</cell><cell>55.0</cell><cell>65.8</cell></row><row><cell>DETR[2]</cell><cell>ResNet-101</cell><cell>?25x</cell><cell></cell><cell>44.9</cell><cell>64.7</cell><cell>47.7</cell><cell>23.7</cell><cell>49.5</cell><cell>62.3</cell></row><row><cell>Deformable DETR[38]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>?4x</cell><cell></cell><cell>50.1</cell><cell>69.7</cell><cell>54.6</cell><cell>30.6</cell><cell>52.8</cell><cell>64.7</cell></row><row><cell>EfficientDet[27]</cell><cell>Efficient-B7</cell><cell>?50x</cell><cell></cell><cell>52.2</cell><cell>71.4</cell><cell>56.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpineNet[8]</cell><cell>SpineNet-190</cell><cell>?40x</cell><cell></cell><cell>52.1</cell><cell>71.8</cell><cell>56.5</cell><cell>35.4</cell><cell>55.0</cell><cell>63.6</cell></row><row><cell>DyHead</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>52.3</cell><cell>70.7</cell><cell>57.2</cell><cell>35.1</cell><cell>56.2</cell><cell>63.4</cell></row><row><cell cols="2">multi-scale training and multi-scale testing:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ATSS[35]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>50.7</cell><cell>68.9</cell><cell>56.3</cell><cell>33.2</cell><cell>52.9</cell><cell>62.4</cell></row><row><cell>BorderDet[21]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>50.3</cell><cell>68.9</cell><cell>55.2</cell><cell>32.8</cell><cell>52.8</cell><cell>62.3</cell></row><row><cell>RepPoints v2[4]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>52.1</cell><cell>70.1</cell><cell>57.5</cell><cell>34.5</cell><cell>54.6</cell><cell>63.6</cell></row><row><cell>Deformable DETR[38]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>?4x</cell><cell></cell><cell>52.3</cell><cell>71.9</cell><cell>58.1</cell><cell>34.4</cell><cell>54.4</cell><cell>65.6</cell></row><row><cell>RelationNet++[5]</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>52.7</cell><cell>70.4</cell><cell>58.3</cell><cell>35.8</cell><cell>55.3</cell><cell>64.7</cell></row><row><cell>DyHead</cell><cell>ResNeXt-64x4d-101-DCN</cell><cell>2x</cell><cell></cell><cell>54.0</cell><cell>72.1</cell><cell>59.3</cell><cell>37.1</cell><cell>57.2</cell><cell>66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Comparison with results using transformer backbone on the MS COCO validation set. Comparison with latest methods on the MS COCO test-dev set. ? demonstrates method with extra data.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Backbone</cell><cell>Iteration</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell cols="2">APS</cell><cell>APM</cell><cell>APL</cell></row><row><cell cols="2">Mask R-CNN[12]</cell><cell>Swin-T</cell><cell>3x</cell><cell>46.0</cell><cell>68.1</cell><cell>50.3</cell><cell>31.2</cell><cell></cell><cell>49.2</cell><cell>60.1</cell></row><row><cell cols="2">Cascade Mask R-CNN[1]</cell><cell>Swin-T</cell><cell>3x</cell><cell>50.4</cell><cell>69.2</cell><cell>54.7</cell><cell>33.8</cell><cell></cell><cell>54.1</cell><cell>65.2</cell></row><row><cell cols="2">RepPoints v2[4]</cell><cell>Swin-T</cell><cell>3x</cell><cell>50.0</cell><cell>68.5</cell><cell>54.2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SparseRCNN[26]</cell><cell>Swin-T</cell><cell>3x</cell><cell>47.9</cell><cell>67.3</cell><cell>52.3</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>ATSS[35]</cell><cell></cell><cell>Swin-T</cell><cell>3x</cell><cell>47.2</cell><cell>66.5</cell><cell>51.3</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>DyHead</cell><cell></cell><cell>Swin-T</cell><cell>2x</cell><cell>49.7</cell><cell>68.0</cell><cell>54.3</cell><cell>33.3</cell><cell></cell><cell>54.2</cell><cell>64.2</cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell>Iteration</cell><cell>AP val</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell></row><row><cell>CenterNet2 ? [36]</cell><cell cols="2">Res2Net-101-DCN</cell><cell>8x</cell><cell>56.1</cell><cell>56.4</cell><cell>74.0</cell><cell>61.6</cell><cell>38.7</cell><cell>59.7</cell><cell>68.6</cell></row><row><cell>CopyPaste ? [10]</cell><cell cols="2">Efficient-B7</cell><cell>8x</cell><cell>57.0</cell><cell>57.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HTC++[19]</cell><cell></cell><cell>Swin-L</cell><cell>6x</cell><cell>58.0</cell><cell>58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DyHead</cell><cell></cell><cell>Swin-L</cell><cell>2x</cell><cell>58.4</cell><cell>58.7</cell><cell>77.1</cell><cell>64.5</cell><cell>41.7</cell><cell>62.0</cell><cell>72.8</cell></row><row><cell>DyHead ?</cell><cell></cell><cell>Swin-L</cell><cell>2x</cell><cell>60.3</cell><cell>60.6</cell><cell>78.5</cell><cell>66.6</cell><cell>43.9</cell><cell>64.0</cell><cell>74.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We keep improving our performance after submission. Recently, there is a hot trend on adapting transformers as vision backbones and demonstrating promising performance. When training our dynamic head with latest transformer backbone <ref type="bibr" target="#b18">[19]</ref>, extra data and increased input size, we can further improve the current SOTA on COCO benchmark.</p><p>Cooperate with Transformer Backbones. We cooperate our dynamic head with the latest transformer-based backbones, such as <ref type="bibr" target="#b18">[19]</ref>. Shown in <ref type="table">Table 6</ref>, our dynamic head is competitive to <ref type="bibr" target="#b0">[1]</ref> which requires extra mask ground-truth to help boost performance. Meanwhile, compared to the baseline method <ref type="bibr" target="#b36">[35]</ref> used in our framework, we further improve its performance by 2.5 AP . This well proves that our dynamic head is complementary to transformer-based backbone to further improve its performance on downstream object detection task.</p><p>Cooperate with Larger Inputs and Extra Data. We find that our dynamic head can further benefit from larger input size and extra data generated using self-training method <ref type="bibr" target="#b42">[40]</ref>. We increase the maximum image side from 1333 to 2000 and use a multi-scale training with minimum image side varying from 480 to 1200. Similar to the training scheme described in section 4.1, we avoid using more tricks to ensure reproducibility. As shown in <ref type="table">Table 7</ref>, our dynamic head leads significant gain compared to latest works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">36]</ref> and matches the performance of <ref type="bibr" target="#b18">[19]</ref> without using extra mask ground-truth. Meanwhile, our dynamic head requires less than 1/3 of training time of these works. This demonstrates our superior efficiency and effectiveness. Fur-thermore, we follow <ref type="bibr" target="#b42">[40]</ref> to generate pseudo labels on Im-ageNet dataest and use it as an extra data. Our dynamic head can largely benefit from large scale data and further improve the COCO state-of-the-art result to a new record high at 60.6 AP .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dynamic relu. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reppoints v2: Verification meets regression for object detection. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Relationnet++: Bridging visual representations for object detection via transformer decoder. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1605.06409</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Kaiwen Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CACM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">1) updated annotation pipeline description and figures; 2) added new section describing datasets splits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>list. 5</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haifang Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<title level="m">SparseR-CNN: End-to-end object detection with learnable proposals</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mingxing Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scale-equalizing pyramid convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="13356" to="13365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9656" to="9665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchorfree detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Probabilistic two-stage detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deformable convnets v2: More deformable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>better results</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-toend object detection. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking pretraining and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Object detection in 20 years: A survey. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
