<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Haoran</forename><forename type="middle">Xie</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghua</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Mingqiang</forename><forename type="middle">Wei</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
						</author>
						<title level="a" type="main">CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-CSDN</term>
					<term>point cloud completion</term>
					<term>cross modality</term>
					<term>multi-feature fusion !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How will you repair a physical object with some missings? You may imagine its original shape from previously captured images, recover its overall (global) but coarse shape first, and then refine its local details. We are motivated to imitate the physical repair procedure to address point cloud completion. To this end, we propose a cross-modal shape-transfer dual-refinement network (termed CSDN), a coarse-to-fine paradigm with images of full-cycle participation, for quality point cloud completion. CSDN mainly consists of "shape fusion" and "dual-refinement" modules to tackle the cross-modal challenge. The first module transfers the intrinsic shape characteristics from single images to guide the geometry generation of the missing regions of point clouds, in which we propose IPAdaIN to embed the global features of both the image and the partial point cloud into completion. The second module refines the coarse output by adjusting the positions of the generated points, where the local refinement unit exploits the geometric relation between the novel and the input points by graph convolution, and the global constraint unit utilizes the input image to fine-tune the generated offset. Different from most existing approaches, CSDN not only explores the complementary information from images but also effectively exploits cross-modal data in the whole coarse-to-fine completion procedure. Experimental results indicate that CSDN performs favorably against ten competitors on the cross-modal benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Refine</head> <ref type="figure">Fig. 1</ref><p>. Image is an important modality to provide complementary information (both structure and detail information) for 3D (sparse) data. Inspired by how a man/woman repairs physical objects, we consider that the image should participate in the whole procedure of coarse-tofine repair. Differently from existing efforts in this topic, CSDN is a new point cloud completion paradigm that exploits cross modalities in the whole coarse-to-fine completion procedure (Yellow arrows: the shape fusion stage; Orange arrows: the dual-refinement stage).</p><p>resolutions and richer textures than incomplete point clouds <ref type="bibr" target="#b4">[5]</ref>, using images from different views could provide complementary information to constrain the completion of point clouds.</p><p>To address the cross-modal challenge in point cloud completion, one can absorb the wisdom of physical object repair. Imagine arXiv:2208.00751v1 [cs.CV] 1 Aug 2022 how a professional restorer repairs a physical object with some missings. She/he will perceive the object's original shape from previously captured images and first recover its global yet coarse shape and then refine its local details. We attempt to imitate the aforementioned physical repair procedure to design a novel crossmodal point cloud completion paradigm. We propose a crossmodal shape-transfer dual-refinement network (termed CSDN), which better exploits the complementary information of images and input partial geometry for point cloud completion.</p><p>CSDN is designed to fuse the features of both the image and point cloud based on their role in conveying a complete shape. For effective learning of both global and local features at different completion stages, we exploit different strategies to tackle the cross-modal challenge. To fully integrate image information for shape completion in a coarse-to-fine manner, CSDN consists of a "shape fusion" module to generate a coarse yet complete shape and a "dual-refinement" module to obtain the completion result.</p><p>First, rather than generating the complete model with only single modality data, we develop a novel solution called IPAdaIN (a variant of Adaptive Instance Normalization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>) to embed the global features of both the images and the partial point cloud. To fuse the information from the two modalities, IPAdaIN adaptively transfers the "shape style" (i.e., the global shape information) obtained from the image to the point cloud domain by feature normalization, which facilitates obtaining an initial complete 3D shape with the aid of images.</p><p>Then, the dual-refinement module takes the generated point cloud as input and further refines the shape details. Specifically, we design this module with a pair of refinement units, called local refinement and global constraint, respectively. To recover geometric details, we apply graph convolution in the local refinement unit. The global constraint unit leverages image features to constrain the offset generated by local refinement. This dual-path refinement strategy enables CSDN to precisely capture both global shape structures and local details.</p><p>CSDN explores the complementary structure information from images, and it synergizes both point and image features in the whole coarse-to-fine completion procedure, as shown in <ref type="figure">Figure 1</ref>. This makes CSDN distinct from the recently proposed ViPC <ref type="bibr" target="#b2">[3]</ref>. ViPC also exploits an additional single-view image to provide the global structural priors in its coarse-completion stage. However, it cannot recover high-frequency details and local topology for complex shapes in the refinement stage since the features learned from different modalities are simply concatenated. As a result, the completed shapes from ViPC are often noisy and lack finer geometric details. In contrast, our CSDN enables the recovery of finer details and topology by disentangling features for local refinement and features for global constraint.</p><p>Extensive experiments clearly show the superiority of our CSDN over ten competitors. For example, CSDN reduces the Mean Chamfer Distance by 0.281 compared to PoinTr <ref type="bibr" target="#b7">[8]</ref>, and it outperforms ViPC <ref type="bibr" target="#b2">[3]</ref> by a large margin of 22.3% on the benchmark dataset ShapeNet-ViPC <ref type="bibr" target="#b2">[3]</ref>. The main contributions can be summarized as follows.</p><p>? By imitating the physical repair procedure, we propose a novel cross-modal shape-transfer dual-refinement network for point cloud completion. Compared with previous cross-modal methods, CSDN applies the disentangled feature fusion strategy, which significantly improves the completion performance.</p><p>? We propose a shape fusion module for generating a coarse yet complete shape, in which a novel solution called IPAdaIN is designed to adaptively transfer the global shape information obtained from the image to the point cloud domain by feature normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a dual-refinement module, enabling CSDN to capture both global shape structures and local details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section reviews 3D shape completion from traditional to learning-based methods, followed by cross-modal feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Shape Completion</head><p>Traditional methods are often geometry-or template-based. The geometry-based ones either fill small holes in 3D models by local interpolations <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, or infer the complete shapes by exploiting structure regularities, such as symmetry and repetitive patterns <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. The template-based ones <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> complete the geometry by matching the partial shape with template models from a database. However, for this conventional wisdom of shape completion, users have to tweak parameters multiple times to obtain satisfied completion results in practical scenarios. This inconvenience heavily discounts the efficiency and user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning-based Shape Completion</head><p>Early approaches <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref> employ voxel-based representations for 3D convolutional neural completion networks, but suffering from expensive computation cost. Later, the PointNet-based encoder-decoder architecture <ref type="bibr" target="#b22">[23]</ref> inspires many new works. For example, the pioneering one, PCN <ref type="bibr" target="#b23">[24]</ref>, generates points in a coarse-to-fine manner and folds the 2D grids to reconstruct a complex complete shape. This coarse-to-fine completion framework has also been employed by <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>. TopNet <ref type="bibr" target="#b25">[26]</ref> introduces a hierarchical rooted tree structure decoder to generate points of different levels of details. The following work MSN <ref type="bibr" target="#b26">[27]</ref> tackles the completion problem by generating different parts of shape with a residual sub-network to refine the coarse points. PF-Net <ref type="bibr" target="#b27">[28]</ref> extracts multi-scale features to encode both local and global information and then fuses them to reconstruct complete point clouds. Besides, the adversarial training strategy is used to further enhance the point quality <ref type="bibr" target="#b27">[28]</ref>. Also, Wang et al. <ref type="bibr" target="#b28">[29]</ref> propose to synthesize the dense and complete object shapes in a cascaded refinement manner, and jointly optimize the reconstruction loss and an adversarial loss. RL-GAN-Net <ref type="bibr" target="#b29">[30]</ref> first tries to use an RL agent to drive a generative adversarial network to predict a complete point cloud. PMP-Net <ref type="bibr" target="#b30">[31]</ref> formulates shape prediction as a point cloud deformation problem, and generates complete point clouds by moving input points to appropriate positions iteratively with a minimum moving distance. Researchers have also introduced Transformer <ref type="bibr" target="#b31">[32]</ref> for point cloud completion, like <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[33]</ref>. PoinTr <ref type="bibr" target="#b7">[8]</ref> reformulates point cloud completion as a set-to-set translation problem, so that a transformer-based encoder-decoder architecture is naturally adopted. SnowflakeNet <ref type="bibr" target="#b32">[33]</ref> utilizes transformer and point-wise feature deconvolutional modules to refine the initial point cloud iteratively.</p><p>Apart from the above supervised approaches, there are also a few studies concerning the unpaired point cloud completion task. AML <ref type="bibr" target="#b33">[34]</ref> introduces a weakly-supervised approach by learning the maximum likelihood. Pcl2Pcl <ref type="bibr" target="#b34">[35]</ref> proposes a GAN </p><formula xml:id="formula_0">F p F I 1 ? C 1 ? C P 0 Local Refinement Global Constraint N ? 3 H ? W ? 3 N r ? C Offset Regression C N r ? 3 N r ? 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>Shape Fusion Dual-Refinement framework to bridge the semantic gap between incomplete and complete shapes. The aforementioned methods only consider the single-side correspondence from incomplete shapes to complete ones. In contrast, Cycle4Completion <ref type="bibr" target="#b35">[36]</ref> enhances the completion performance by establishing the geometric correspondence between complete shapes and incomplete shapes from both directions. Although achieving remarkable performance on point cloud completion, recent efforts generally use the mono-modality input. Hence, it is difficult to infer an accurate mapping from an incomplete point cloud with a large-scale incompleteness to a complete point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal Feature Fusion</head><p>When it comes to data of "cross-modality" (i.e., 3D point cloud and 2D image), the main difficulty is to fuse information provided by different modalities, which is a well-recognized challenge in various tasks such as 3D object detection, point cloud registration, and shape completion. ImVoteNet <ref type="bibr" target="#b36">[37]</ref> fuses 2D detection into a 3D pipeline with camera parameters and pixel depths. DeepI2P <ref type="bibr" target="#b37">[38]</ref> utilizes the attention mechanism to weight image features by point clouds. Image2Point <ref type="bibr" target="#b38">[39]</ref> investigates the potential for the transferability between single images and points. In the same problem setting, ViPC <ref type="bibr" target="#b2">[3]</ref> also infers the missing points guided by an additional image. It directly maps a color image to the point cloud domain, explicitly aligns the generated rough geometry, and finally fine-tunes the coarse geometry. Although exploiting multimodal information, ViPC heavily relies on the 3D reconstruction from a single image as well as requires an accurate alignment. Different from ViPC, our CSDN formulates multi-modal fusion as a shape style transfer problem, and it disentangles different types of features with a dual-refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We assume in point cloud completion with a cross-modal manner that 1) images supply both the global-structure and local-detail information to facilitate the inference of a complete and geometryrich shape, and 2) images should participate in the whole coarseto-fine completion procedure. To this end, we design a Cross-modal Shape-transfer Dual-refinement Network (CSDN). Following the encoder-decoder architecture to implement the cross-modal coarse-to-fine effort, CSDN is designed to have three modules: feature extraction, shape fusion, and dual-refinement, as illustrated in <ref type="figure">Figure 2</ref>. Denote P in = {p 1 , ..., p N } ? R 3 with the size of N to be an input partial point cloud, and I with the size of H ? W to be the corresponding single-view color image. Our goal is to predict a point cloud P out from its dual-domain partial observations P in and I, and P out represents the complete underlying shape of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>CSDN embeds both the pair of a partial point cloud and a singleview image into their individual feature space. Since we adopt a coarse-to-fine strategy (complete its overall shape first, and then complete its detail), we intend to extract global features from both P in and I in this stage: 1) for P in , the simple yet effective PointNet <ref type="bibr" target="#b22">[23]</ref> is leveraged to yield its global feature F p with the size of 1 ? C; 2) for I, a sub-network with seven convolutional layers is utilized to extract 7 ? 7 ? C feature maps and then obtain its global feature F I through average pooling. </p><formula xml:id="formula_1">F p F I [0,1] 2 Sampling 1 ? C C F g MLP IPAdaIn MLP IPAdaIn MLP 1 ? C IPAdaIn N r '? 3 N r '? (C + 2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shape Fusion</head><p>We design the shape fusion module to fuse the features of different modalities, which outputs a coarse but complete set of shape points. By proposing an image and point cloud co-supported AdaIN <ref type="bibr" target="#b5">[6]</ref> (IPAdaIN for short), we transfer the intrinsic geometric style from the image to the point cloud.</p><p>This module takes F p and F I as input and outputs a coarse point cloud P 0 (see <ref type="figure" target="#fig_2">Figure 3</ref>). Specifically, similar to Fold-ingNet <ref type="bibr" target="#b39">[40]</ref>, we first sample N r 2D points from a unit square [0, 1] 2 . Each of them is appended with the global point cloud feature F p . Then, the concatenated feature is combined with the global image feature F I to fold the 2D grids to a point surface, by the IPAdaIN layer. Note that we repeat (but without sharing the network parameters) the above operations M times to reconstruct M surfaces, similar to <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Finally, all of them are concatenated to synthesize a complete complex point cloud P 0 with N r points, i.e., N r = M ? N r . M is a hyperparameter fixed to 4 in our experiments. Also, we can change K and N r to adapt the proposed method to the point clouds of different resolutions.</p><p>IPAdaIN. Inspired by StyleGAN <ref type="bibr" target="#b6">[7]</ref> that employs AdaIN for the style transfer task and SpareNet <ref type="bibr" target="#b41">[42]</ref> that generates points with a similar batch normalization operation, we reformulate the "cross-modality" fusion as a shape style transfer problem. Accordingly, we propose IPAdaIN, a new solution that is built based on the instance normalization layer to better capture the image style. Specifically, we feed F p to multiple MLPs, where each MLP is followed by an IPAdaIN layer and generates a new point-wise feature f. The normalization process is formulated as</p><formula xml:id="formula_2">IP AdaIN (f, F I ) = ?( f ? ?(f ) ?(f ) ) + ?, ? = L a (F I ), ? = L b (F I ),<label>(1)</label></formula><p>where ?(f ) and ?(f ) are the means and standard deviations of the channel-wise activation of f . L a and L b are two different non-linear mappings implemented by two MLPs. The point-wise feature f is scaled and biased. Unlike AdaIN, IPAdaIN guides shape folding with the learned shape characteristics, which can be interpreted as a way to reconstruct a 3D shape from its style contained in the image. Thus, the new feature space can be used to correctly recover the complete shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dual-Refinement</head><p>The dual-refinement module aims to generate a set of coordinate offsets ?C with the dimension of N r ? 3 to fine-tune the coarse point cloud P 0 . The whole module contains two units named Local Refinement and Global Constraint, respectively. The local refinement finds the relation between points in P 0 and P in and generates one set of offset features, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. To refine P 0 without being corrupted by missing parts, the global constraint exploits image features to generate another set of offset features. Finally, the combined feature is fed into the Offset Regression unit that is built with an MLP to obtain the set of offsets ?C. Thus, the two units of local refinement and global constraint complement each other to capture both the global structures and local details of the completed object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Local Refinement</head><p>It is a very common situation that some missing high-frequency details already appear in the partial point cloud. Thus an intuition is to exploit local structure information with the aid of the partial point cloud. The local refinement unit is to integrate the structure information of the input partial shape with the coarse point cloud itself. Since a point cloud can be naturally regarded as a graphlike structure, we build this unit with graph convolution, given its effectiveness in point cloud analysis <ref type="bibr" target="#b42">[43]</ref>. Given the partial point cloud P in and the coarse point cloud P 0 , a directed graph G(V, E) is established from the two point clouds, where V = {1, ..., N + N r } represents the set of nodes, and E ? V ? V is the set of edges. Observing that certain areas in P in have much higher quality than those in P 0 , like the chair back and the three complete legs in <ref type="figure" target="#fig_3">Figure 4</ref>, we can build E encoding the structure information from both P 0 and P in . Hence, for each point in P 0 , its k-nearest neighbours are respectively searched in P in and P 0 to build a local graph which has 2k vertices, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Then, denoting the central point in the graph convolution by p i and the neighborhood set by N (i) = {j|(i, j) ? E}, we can compute edge features as e ij = ?([p i , p j ?p i ]), where ? is a non-linear function implemented by Shared-MLP and [?, ?] is the concatenation operation. Finally, each point's feature is aggregated by</p><formula xml:id="formula_3">N r ? 2K ? 6 Shared MLPs N r ? C N r ? C N r ? C Res-MLP Maxpooling Local Refinement Global Constraint</formula><formula xml:id="formula_4">f i = max j?N (i) e ij ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_5">F of f P = {f i } Nr i=1 ,<label>(3)</label></formula><p>where max denotes a channel-wise max-pooling function and f i is the feature of the i-th point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Global Constraint</head><p>We query k-nearest neighbors from the partial point cloud P in for each point in P 0 . Inevitably, the points in the generated missing areas cannot contain a reliable local graph similar to the corresponding local shape in the ground-truth point cloud. To this end, we propose the global constraint unit to fix F of f P with learned image features which contain the global shape information. The unit first projects 3D points into the last four feature maps (from 2D encoders) by the camera parameters and pooling them from four nearby pixels using bilinear interpolation (see the bottom of <ref type="figure" target="#fig_3">Figure 4</ref>). The obtained point-wise feature is then fed into a residual MLP block to obtain the offset feature F of f I . Finally, we add F of f I and F of f P to obtain F of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>The loss function measures the difference between P out and the ground truth P gt . We utilize the Chamfer distance (CD) as the loss function. To achieve the coarse-to-fine generation process, we regularize the training by calculating the loss function as</p><formula xml:id="formula_6">L = L CD (P 0 , P gt ) + ?L CD (P out , P gt ),<label>(4)</label></formula><p>where L CD is defined as</p><formula xml:id="formula_7">L CD (X, Y ) = 1 |X| x?X min y?Y ||x ? y|| + 1 |Y | y?Y min x?X ||y ? x||.</formula><p>(5) In the implementation, we increase ? from 0.01 to 2.0 during the first 30k iterations since the initial point set P 0 is less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training Dataset</head><p>We use the benchmark dataset ShapeNet-ViPC <ref type="bibr" target="#b2">[3]</ref>, which is derived from ShapeNet <ref type="bibr" target="#b44">[45]</ref>. It contains 13 categories and 38,328 objects, including airplane, bench, cabinet, car, chair, monitor, lamp, speaker, firearm, sofa, table, cellphone, and watercraft. The complete ground-truth point cloud is generated by uniformly sampling 2048 points on the mesh surface from ShapeNet <ref type="bibr" target="#b44">[45]</ref>. Each object has a complete point cloud, 24 rendered images under 24 view points, and 24 corresponding incomplete point clouds. The incomplete point cloud is generated from the corresponding viewpoint (with occlusion) and also contains 2048 points. The image data is rendered from 24 view points as ShapeNetRendering in 3D-R2N2 <ref type="bibr" target="#b45">[46]</ref>. During training, the image view point is randomly chosen for each training data pair and the point cloud will be aligned to the chosen image. For a fair comparison, we use the same training setting as in ViPC <ref type="bibr" target="#b2">[3]</ref>, i.e., 80% of the eight categories for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Test Dataset</head><p>The test dataset from ShapeNet-ViPC <ref type="bibr" target="#b2">[3]</ref> consists of two parts: one contains the remaining 20% of the 8 categories of objects for training; another one contains 4 categories that are not used during training. For each of the new categories, we randomly choose 200 pairs of point clouds and images. We also test our method on partial point clouds from real-world LiDAR scans, i.e., the KITTI dataset <ref type="bibr" target="#b46">[47]</ref>. Specifically, we extract point clouds labeled as cars within the corresponding bounding boxes and then select point clouds having more than 2048 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our implementation, the number of point surfaces in Shape Fusion is k = 4 and each surface contains 512 points. The feature dimension is set to be 1024, i.e., C = 1024. Thus, the generated coarse point cloud has 2048 points. For IPAdaIN, we set three layers for every surface. In the Local Refinement unit, the number of nearest neighbors is 16. In the Global Constraint unit, the size of feature maps used for projection are 56 ? 56, 28 ? 28, 14 ? 14, and 7 ? 7.</p><p>For the 3D encoder, we set a five-layer MLP. The feature dimensions are 64, 64, 64, 128, and 1024, respectively. For the 2D encoder, its architecture is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Note that we reuse the last four feature maps in the global constraint unit and their dimensions are 56 ? 56, 28 ? 28, 14 ? 14, and 7 ? 7, respectively. In the local refinement unit, the feature F of f P is produced by a three-layer MLP whose output dimensions are 32, 128, and 512, respectively. Res-MLP in the global constraint unit is implemented by a Con1d ResNet block <ref type="bibr" target="#b47">[48]</ref>. The final offset regression is implemented by four Conv1d layers whose output dimensions are 256, 128, 32, and 3, respectively. The coordinate offset is calculated through the tanh activation function.</p><p>For quantitative evaluation, we use both the Chamfer Distance (CD) and F-Score as evaluation metrics. The whole network is trained end-to-end with a learning rate of 5 ? 10 ?5 for 50 epochs and the Adam optimizer. The learning rate is also decayed by 0.1 for every 10 epochs. All the networks are implemented using PyTorch and trained on an NVIDIA RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Result on ShapeNet-ViPC</head><p>We compare our CSDN with ten state-of-the-art point cloud completion methods. They are Point-based, i.e., AtlasNet <ref type="bibr" target="#b40">[41]</ref>, PCN <ref type="bibr" target="#b23">[24]</ref>, MSN <ref type="bibr" target="#b26">[27]</ref>, Transformer-based, i.e., PoinTr <ref type="bibr" target="#b7">[8]</ref>, Seedformer <ref type="bibr" target="#b43">[44]</ref>, Folding-based, i.e., FoldingNet <ref type="bibr" target="#b39">[40]</ref>, TopNet <ref type="bibr" target="#b25">[26]</ref>, GAN-based, i.e., PF-Net <ref type="bibr" target="#b27">[28]</ref>, Convolution-based, i.e., GRNet <ref type="bibr" target="#b21">[22]</ref>, and Cross-modality-based, i.e., ViPC <ref type="bibr" target="#b2">[3]</ref>. For AtlasNet, FoldingNet, PCN, and TopNet, we directly took the quantitative statistics from <ref type="bibr" target="#b2">[3]</ref>. For the other methods, we retrain their models using the released codes on ShapeNet-ViPC. For a fair comparison, we uniformly downsample the results of PoinTr to 2048 points, the same as the output of the other methods. Among these methods, ViPC also exploits an additional image, while the others take only a partial point cloud as input. <ref type="figure" target="#fig_5">Figure 6</ref> shows the point clouds completed by the proposed CSDN and its competitors. For some cases of symmetric structures and small missing regions (e.g., plane and car), most methods work well and generate complete shapes similar to the ground truths. Our CSDN recovers more clear overall shapes and more detailed structures than Seedformer, PoinTr, and ViPC, while GR-Net generates noisy points. For those challenging cases of complex shapes and large missing regions, like the table case, our CSDN generates the complete desktop and leg while the other methods only predict a coarse result. Compared to ViPC, our CSDN is better to recover latent geometric structures (e.g., holes in the desktop and the back of the chair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Qualitative Results on Known Categories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Quantitative Results on Known Categories</head><p>We calculate both CD and F-Score on the 2,048 points of each object, as reported in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. It can be observed that our CSDN achieves almost the best results among all the competitors. It is worth noting that CSDN reduces the average CD value by 0.281 compared to the result of the second-ranked PoinTr <ref type="bibr" target="#b7">[8]</ref>. Also, CSDN outperforms ViPC <ref type="bibr" target="#b2">[3]</ref> by a large margin of 22.3%. Meanwhile, CSDN achieves the best results in all categories in terms of F-Score.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Quantitative Results on Novel Categories</head><p>We have also tested different methods on four novel categories which are not used for training, to evaluate the category agnostic ability of the proposed CSDN. The results are reported in <ref type="table" target="#tab_3">Table  3</ref> and <ref type="table" target="#tab_4">Table 4</ref>. A conclusion can be drawn that our CSDN generalizes well to novel shapes that have not been seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on Real-world Scans</head><p>We train our CSDN and two other methods on the car category of ShapeNet-ViPC to evaluate their performance on real-world scans.</p><p>Ideally, a point cloud completion network trained on a cross-modal dataset should produce satisfactory results, since the cars and their corresponding real-world images cropped with 2D bounding boxes are provided. As reported in <ref type="bibr" target="#b2">[3]</ref>, the real-to-synthetic domain gap led by rendered images in the training dataset makes ViPC not applicable to real-world point cloud completion. Thus we replace the input image with the rendered car image in ShapeNet-ViPC. <ref type="figure">Figure 7</ref> shows the visual comparisons. It is found that ViPC still cannot handle the domain gap problem and PoinTr generates coarse results due to the low resolution of the training dataset. In contrast, CSDN is capable of generating more plausible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Input GR-Net PoinTr ViPC Seedformer Ours GT    <ref type="figure">Fig. 7</ref>. Visual comparisons of recent point cloud completion methods on the real-scanned point clouds (from KITTI <ref type="bibr" target="#b46">[47]</ref>). CSDN produces the most complete and detailed structures compared to the two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We first remove and change the main components of CSDN to ablate our network architecture. The ablation variants can be classified as ablation on Shape Fusion and Dual Refinement. The quantitative results are shown in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Ablation on Shape Fusion</head><p>We replace IPAdaIN in Variant A with a regular instance normalization. Without the global information provided by images, Variant A cannot guarantee the completeness of the final shape and typically generates point clouds with higher CD and F-Score. Variant B, switching the positions of F P and F I , also produces points with higher CD and F-Score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Ablation on Dual-Refinement and Modality</head><p>Variant C, without the Local Refinement unit, leads to the generated results that are coarser. Variant D, without the Global Constraint unit, typically has lower performance. Variant E changes the parallel structure of dual-refinement to the serial structure (i.e., P 0 is first sent to Local Refinement for the first refinement and the refined result is sent to Global Constraint to obtain the final result). The performance drop of variant E demonstrates the superiority of the proposed parallel architecture. Then, we ablate on the single-view images in version F, where both IPAdaIN and Global Constraint are removed. The large-margin performance drop demonstrates the significance of our feature fusion strategy. Version F shows the metric between the coarse output of SD-Net and the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Visual Ablation on Images</head><p>To evaluate the effects of Image and the Dual Refinement module, we also qualitatively compare the final and coarse results (i.e., version F in our ablation study) of CSDN and CSDN without the input image (i.e., the version E in our Ablation Study). It is observed from <ref type="figure">Figure 8</ref> that our method struggles to infer the missing regions without the input image. Meanwhile, we find that the coarse point cloud generated by Shape Fusion has already a relatively complete shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Ablation on Numbers of KNN</head><p>We also conduct an ablation experiment on the number of nearest neighbors in the local refinement unit. k is set to be 4, 8, 16, and 24, respectively. It is observed from <ref type="table" target="#tab_1">Table 1</ref> that CSDN has the best performance when k = 16. However, we believe that this parameter needs to be fine-tuned for different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effect of Different-view Images</head><p>This section studies the stability of CSDN when images can be collected from different views. Ideally, any good method of cross-modal point cloud completion should be view-independent. However, it is observed that images captured from different views may slightly affect the completion results. A "good" view is considered to possess complementary information for the input partial point clouds. The images with a good view tend to produce the completion results with a lower CD value.</p><p>To validate the aforementioned statement, we randomly select some partial objects and test them with the individual images from all 24 views as input. As observed in <ref type="figure">Figure 9</ref>, the proposed method performs better on the images that contain more information about the missing parts. For example, the image containing only the front or the end of a truck generates the results with the highest CD losses.</p><p>To further demonstrate the stability of CSDN over the other cross-modal method, i.e., ViPC <ref type="bibr" target="#b2">[3]</ref>, we select 100 objects from each known category to evaluate how CSDN and ViPC perform with the images captured from different viewpoints. Specifically, we obtain 24 results for each incomplete object with the help of each image of the 24 different-view images. We calculate the standard deviations for the 24 CD values to compare the stability of model performance, as shown in <ref type="table" target="#tab_7">Table 7</ref>. The quantitative results clearly show that our CSDN is more view-independent than ViPC <ref type="bibr" target="#b2">[3]</ref> with a 42.4% drop in the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effect of Different Similar-Images</head><p>There may exist objects that have similar structures. Hence, we randomly select some models and replace the corresponding input images with similar images in the same category. <ref type="figure">Figure 10</ref> shows the results of one example. When inputting the pair of the original image and the partial point cloud, CSDN generates the completion result with a CD loss of 0.707 ? 10 ?3 . Then, we replace the original image with seven images of the other models in ShapeNet-ViPC <ref type="bibr" target="#b2">[3]</ref>. It is found that our method still works with similar images as input but produces results with a higher CD loss. <ref type="figure">Figure  10</ref> shows that the model performs better when the input image has similar structures and colors. However, the last two images have nearly the same shape as the original image, but our method generates results with higher CD losses. We believe that it is caused by the remarkable differences in colors. Please note that we do not use such unpaired data during the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Comparisons about Model Sizes</head><p>We compare and report the model size of ViPC, PoinTr, and our CSDN in <ref type="table" target="#tab_8">Table 8</ref>. The comparison indicates that our CSDN has a slightly larger model size than ViPC but much better performance, while PoinTr has the largest model size owing to its transformer architecture. <ref type="table" target="#tab_10">Table 9</ref> summarizes the limitations among these compared methods to provoke further research insights. Besides, we visualize some failure cases whose CD losses are rather larger than the average error of that category, as shown in <ref type="figure">Figure 11</ref>. Although the shape fusion step succeeds to recover the complete shape to a certain extent, and the dual-refinement module also removes noisy points, the final results are still less pleasing. This is especially the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS AND FAILURE CASES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present a novel point cloud completion paradigm that mimics the physical object repairing process. The paradigm is implemented as a cross-modal shape-transfer double-refinement network (CSDN). Experiments on standard point cloud completion benchmarks demonstrate the significant improvement of our CSDN over the pioneer multi-modal method ViPC and the other SOTAs. Our work has revealed that the fusion of features from cross-modalities facilitates the generation of reliable geometries to complete partial point clouds. This benefits from the fact that features learned from images not only constrain the generation of the overall shape of an object but also help to refine its local details, indicating the potential of using images to recover finer geometry of objects from partial point clouds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Partial ViPC PoinTr Seedformer Coarse Ours <ref type="figure">Fig. 11</ref>. Failure cases. Similar to its competitors, our CSDN may generate poor completion results (the small structures cannot be reconstructed) when the partial point cloud lacks the main body and the assembled image has very low resolution. Please note that in such challenging cases, our CSDN can still recover the overall shape, which its competitors are not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Limitation</head><p>AltasNet <ref type="bibr" target="#b40">[41]</ref> Rely on repeated reconstruction to represent a 3D shape as the union of several surface elements. FoldingNet <ref type="bibr" target="#b39">[40]</ref> Overlook the local geometric characteristics. PCN <ref type="bibr" target="#b23">[24]</ref> Only rely on a single global feature to generate points, hence incapable of synthesizing local details. TopNet <ref type="bibr" target="#b25">[26]</ref> Results tend to have noisy points around the generated surface. PF-Net <ref type="bibr" target="#b27">[28]</ref> Fail to recover local details to certain extent. MSN <ref type="bibr" target="#b26">[27]</ref> Lack of sufficient refinement and fail to generate fine-grained details.</p><p>GRNet <ref type="bibr" target="#b21">[22]</ref> It is subject to the resolution of the voxel representation, which often leads to distortion in local complex structures. PoinTr <ref type="bibr" target="#b7">[8]</ref> The model has relatively more parameters owing to the transformer architecture.</p><p>ViPC <ref type="bibr" target="#b2">[3]</ref> Rely on the 3D reconstruction from a single image as well as an accurate 3D alignment. The surface of the results is thus noisy. Seedformer <ref type="bibr" target="#b43">[44]</ref> The results are not evenly distributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Rely on the dual-domain training dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>N r ? 3 Fig. 2 .</head><label>32</label><figDesc>The architecture of Cross-modal Shape-transfer Dual-refinement Network (CSDN). How will you repair a physical broken object? You may adopt a coarse-to-fine repair manner with the object's existing image of full-cycle participation. Thus, CSDN is designed as an image of full-cycle participated coarse-to-fine network, which consists of three modules: feature extraction, shape fusion, and dual-refinement. Different from existing wisdom, the cross-modal data is involved in the whole coarse-to-fine completion procedure of CSDN, ensuring the completion result with both rich structures and details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the IPAdaIN layer. IPAdaIN normalizes features of the folding operation by global image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the dual-refinement module. The upper path denotes Local Refinement and the lower one denotes Global Constraint. Each sub-network generates an offset feature, and they are added to yield the feature F of f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of our 2D Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparisons of recent point cloud completion methods on ShapeNet-ViPC. CSDN produces the most complete yet detailed structures compared to its competitors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Images captured from different views often lead to the completion results with slight differences. A "good" view that contains more complementary information for the input partial point clouds tends to produce the completion results with a lower CD value (?10 ?3 ). Results of using similar but intrinsically different images to bind the partial point cloud for completion. The object's structures and colors will affect the final completion results. The partial point cloud assembled by its original image can infer a completion result with the smallest error of CD (?10 ?3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Quantitative results on known categories of ShapNet-ViPC using CD with 2,048 points. The best is highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell>Pub</cell><cell>Avg</cell><cell cols="3">Mean CD per point ?10 ?3 (lower is better) Airplane Cabinet Car Chair Lamp Sofa</cell><cell cols="2">Table Watercraft</cell></row><row><cell>AtlasNet [41]</cell><cell cols="2">CVPR'18 6.062</cell><cell>5.032</cell><cell>6.414</cell><cell cols="2">4.868 8.161 7.182 6.023 6.561</cell><cell>4.261</cell></row><row><cell cols="3">FoldingNet [40] CVPR'18 6.271</cell><cell>5.242</cell><cell>6.958</cell><cell cols="2">5.307 8.823 6.504 6.368 7.080</cell><cell>3.882</cell></row><row><cell>PCN [24]</cell><cell>3DV'18</cell><cell>5.619</cell><cell>4.246</cell><cell>6.409</cell><cell cols="2">4.840 7.441 6.331 5.668 6.508</cell><cell>3.510</cell></row><row><cell>TopNet [26]</cell><cell cols="2">CVPR'19 4.976</cell><cell>3.710</cell><cell>5.629</cell><cell cols="2">4.530 6.391 5.547 5.281 5.381</cell><cell>3.35</cell></row><row><cell>PF-Net [28]</cell><cell cols="2">CVPR'20 3.873</cell><cell>2.515</cell><cell>4.453</cell><cell cols="2">3.602 4.478 5.185 4.113 3.838</cell><cell>2.871</cell></row><row><cell>MSN [27]</cell><cell cols="2">AAAI'20 3.793</cell><cell>2.038</cell><cell>5.06</cell><cell cols="2">4.322 4.135 4.247 4.183 3.976</cell><cell>2.379</cell></row><row><cell>GRNet [27]</cell><cell cols="2">ECCV'20 3.171</cell><cell>1.916</cell><cell>4.468</cell><cell cols="2">3.915 3.402 3.034 3.872 3.071</cell><cell>2.160</cell></row><row><cell>PoinTr [8]</cell><cell cols="2">ICCV'21 2.851</cell><cell>1.686</cell><cell>4.001</cell><cell cols="2">3.203 3.111 2.928 3.507 2.845</cell><cell>1.737</cell></row><row><cell>ViPC [3]</cell><cell cols="2">CVPR'21 3.308</cell><cell>1.760</cell><cell>4.558</cell><cell>3.138 2.476 2.867 4.481</cell><cell>4.99</cell><cell>2.197</cell></row><row><cell cols="3">Seedformer [44] ECCV'22 2.902</cell><cell>1.716</cell><cell>4.049</cell><cell cols="2">3.392 3.151 3.226 3.603 2.803</cell><cell>1.679</cell></row><row><cell>Ours</cell><cell>-</cell><cell>2.570</cell><cell>1.251</cell><cell>3.670</cell><cell cols="2">2.977 2.835 2.554 3.240 2.575</cell><cell>1.742</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Quantitative results on known categories of ShapNet-ViPC using F-Score with 2,048 points. The best is highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell>Pub</cell><cell>Avg</cell><cell cols="4">F-Score@0.001 (higher is better) Airplane Cabinet Car Chair Lamp Sofa</cell><cell cols="2">Table Watercraft</cell></row><row><cell>AtlasNet [41]</cell><cell cols="2">CVPR'18 0.410</cell><cell>0.509</cell><cell>0.304</cell><cell>0.379</cell><cell cols="2">0.326 0.426 0.318 0.469</cell><cell>0.551</cell></row><row><cell cols="3">FoldingNet [40] CVPR'18 0.331</cell><cell>0.432</cell><cell>0.237</cell><cell>0.300</cell><cell cols="2">0.204 0.360 0.249 0.351</cell><cell>0.518</cell></row><row><cell>PCN [24]</cell><cell>3DV'18</cell><cell>0.407</cell><cell>0.578</cell><cell>0.27</cell><cell>0.331</cell><cell cols="2">0.323 0.456 0.293 0.431</cell><cell>0.577</cell></row><row><cell>TopNet [26]</cell><cell cols="2">CVPR'19 0.467</cell><cell>0.593</cell><cell>0.358</cell><cell>0.405</cell><cell cols="2">0.388 0.491 0.361 0.528</cell><cell>0.615</cell></row><row><cell>PF-Net [28]</cell><cell cols="2">CVPR'20 0.551</cell><cell>0.718</cell><cell>0.399</cell><cell>0.453</cell><cell cols="2">0.489 0.559 0.409 0.614</cell><cell>0.656</cell></row><row><cell>MSN [27]</cell><cell cols="2">AAAI'20 0.578</cell><cell>0.798</cell><cell>0.378</cell><cell>0.380</cell><cell cols="2">0.562 0.652 0.410 0.615</cell><cell>0.708</cell></row><row><cell>GRNet [27]</cell><cell cols="2">ECCV'20 0.601</cell><cell>0.767</cell><cell>0.426</cell><cell>0.446</cell><cell cols="2">0.575 0.694 0.450 0.639</cell><cell>0.704</cell></row><row><cell>PoinTr [8]</cell><cell cols="2">ICCV'21 0.683</cell><cell>0.842</cell><cell>0.516</cell><cell>0.545</cell><cell cols="2">0.662 0.742 0.547 0.723</cell><cell>0.780</cell></row><row><cell>ViPC [3]</cell><cell cols="2">CVPR'21 0.591</cell><cell>0.803</cell><cell>0.451</cell><cell cols="3">0.5118 0.529 0.706 0.434 0.594</cell><cell>0.73</cell></row><row><cell cols="3">Seedformer [44] ECCV'22 0.688</cell><cell>0.835</cell><cell>0.551</cell><cell>0.544</cell><cell cols="2">0.668 0.777 0.555 0.716</cell><cell>0.786</cell></row><row><cell>Ours</cell><cell>-</cell><cell>0.695</cell><cell>0.862</cell><cell>0.548</cell><cell>0.560</cell><cell cols="2">0.669 0.761 0.557 0.729</cell><cell>0.782</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Quantitative results on the Novel categories of ShapNet-ViPC using CD with 2,048 points. The best is highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell cols="4">Mean Chamfer Distance per point ?10 ?3 Avg Bench Monitor Speaker Phone</cell></row><row><cell cols="2">PF-Net [28] 5.011 3.684</cell><cell>5.304</cell><cell>7.663</cell><cell>3.392</cell></row><row><cell>MSN [27]</cell><cell>4.684 2.613</cell><cell>4.818</cell><cell>8.259</cell><cell>3.047</cell></row><row><cell cols="2">GRNet [27] 4.096 2.367</cell><cell>4.102</cell><cell>6.493</cell><cell>3.422</cell></row><row><cell>PoinTr [8]</cell><cell>3.755 1.976</cell><cell>4.084</cell><cell>5.913</cell><cell>3.049</cell></row><row><cell>ViPC [3]</cell><cell>4.601 3.091</cell><cell>4.419</cell><cell>7.674</cell><cell>3.219</cell></row><row><cell>Ours</cell><cell>3.656 1.834</cell><cell>4.115</cell><cell>5.690</cell><cell>2.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Quantitative results on the Novel categories of ShapNet-ViPC using F-Score with 2,048 points. The best is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Partial</cell><cell>ViPC</cell><cell>PoinTr</cell><cell>Ours</cell></row><row><cell>Methods</cell><cell>Avg</cell><cell cols="4">F-Score@0.001 Bench Monitor Speaker Phone</cell></row><row><cell cols="3">PF-Net [28] 0.468 0.584</cell><cell>0.433</cell><cell>0.319</cell><cell>0.534</cell></row><row><cell>MSN [27]</cell><cell cols="2">0.533 0.706</cell><cell>0.527</cell><cell>0.291</cell><cell>0.607</cell></row><row><cell cols="3">GRNet [27] 0.548 0.711</cell><cell>0.537</cell><cell>0.376</cell><cell>0.569</cell></row><row><cell>PoinTr [8]</cell><cell cols="2">0.619 0.797</cell><cell>0.599</cell><cell>0.454</cell><cell>0.627</cell></row><row><cell>ViPC [3]</cell><cell cols="2">0.498 0.654</cell><cell>0.491</cell><cell>0.313</cell><cell>0.535</cell></row><row><cell>Ours</cell><cell cols="2">0.631 0.798</cell><cell>0.598</cell><cell>0.485</cell><cell>0.644</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Comparisons between CSDN and its variants.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell>CD</cell><cell cols="2">Known F-Score</cell><cell>CD</cell><cell>Novel F-Score</cell></row><row><cell cols="2">w/o IPAdaIN (A)</cell><cell></cell><cell cols="2">3.658</cell><cell>0.648</cell><cell>4.528</cell><cell>0.602</cell></row><row><cell>SW (B)</cell><cell></cell><cell></cell><cell cols="2">2.647</cell><cell>0.681</cell><cell>3.765</cell><cell>0.625</cell></row><row><cell cols="2">w/o LocalGraph (C)</cell><cell></cell><cell cols="2">3.428</cell><cell>0.648</cell><cell>6.284</cell><cell>0.561</cell></row><row><cell cols="5">w/o GlobalConstraint (D) 2.700</cell><cell>0.687</cell><cell>4.015</cell><cell>0.624</cell></row><row><cell>serial (E)</cell><cell></cell><cell></cell><cell cols="2">3.036</cell><cell>0.659</cell><cell>4.227</cell><cell>0.611</cell></row><row><cell cols="2">w/o Image (F)</cell><cell></cell><cell cols="2">4.179</cell><cell>0.629</cell><cell>4.974</cell><cell>0.588</cell></row><row><cell>Coarse (G)</cell><cell></cell><cell></cell><cell cols="2">3.752</cell><cell>0.632</cell><cell>6.757</cell><cell>0.548</cell></row><row><cell cols="2">Ours (CSDN)</cell><cell></cell><cell cols="2">2.570</cell><cell>0.695</cell><cell>3.656</cell><cell>0.631</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 6</cell></row><row><cell cols="6">Quantitative comparisons with different KNN numbers.</cell></row><row><cell>Methods</cell><cell>CD</cell><cell cols="3">Known F-Score</cell><cell>CD</cell><cell>Novel F-Score</cell></row><row><cell>K = 4</cell><cell cols="2">2.643</cell><cell cols="2">0.691</cell><cell>4.061</cell><cell>0.625</cell></row><row><cell>K = 8</cell><cell cols="2">2.636</cell><cell cols="2">0.688</cell><cell>4.053</cell><cell>0.622</cell></row><row><cell>K = 16</cell><cell cols="2">2.570</cell><cell cols="2">0.695</cell><cell>3.656</cell><cell>0.631</cell></row><row><cell>K = 24</cell><cell cols="2">2.656</cell><cell cols="2">0.685</cell><cell>4.116</cell><cell>0.624</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Standard deviations of CD on different-view images. The lower the value is, the more view-independent the method is.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="5">Standard deviations of CD calculated on different images</cell></row><row><cell></cell><cell>Avg</cell><cell cols="2">Airplane Cabinet</cell><cell>Car</cell><cell>Chair Lamp</cell><cell>Sofa</cell><cell>Table Watercraft</cell></row><row><cell cols="2">ViPC [3] 2.104</cell><cell>0.908</cell><cell>1.989</cell><cell cols="3">1.158 2.916 4.513 1.828 2.933</cell><cell>1.192</cell></row><row><cell>Ours</cell><cell>1.212</cell><cell>0.58</cell><cell>1.339</cell><cell cols="3">0.816 1.697 1.312 1.479 1.560</cell><cell>0.914</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Comparisons on model sizes and performance. partial point clouds have large missing regions both in their main bodies and detailed structures. Meanwhile, the input images are of very low resolutions, which can only provide limited complementary information for the recovery of fine geometries (e.g., the ship cabin). This problem could be solved by training on a dataset that has real-world high-resolution images and dense point clouds. Besides, although the motivation of the proposed CSDN is to reconstruct better shapes with the help of single-view images, multi-view images and neural rendering techniques would also help point cloud completion, which is our future work.</figDesc><table><row><cell>Methods</cell><cell cols="2">Model size (Mib) Performance (CD)</cell></row><row><cell>ViPC [3]</cell><cell>53.79</cell><cell>3.308</cell></row><row><cell>PoinTr [8]</cell><cell>170.18</cell><cell>2.851</cell></row><row><cell>Ours</cell><cell>70.89</cell><cell>2.570</cell></row><row><cell>case when the</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Summary of the limitations of our CSDN and its competitors.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transformers in 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2205.07417</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comprehensive review of deep learning-based 3d point cloud completion processing and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2203.03311</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">View-guided point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="890" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2d-3d lifting for shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-feature fusion votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multim. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">State of the art in surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2014-State of the Art Reports</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="161" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filling holes in complex surfaces using volumetric diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. First International Symposium on 3D Data Processing Visualization and Transmission</title>
		<meeting>First International Symposium on 3D Data Processing Visualization and Transmission</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Laplacian mesh optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and</title>
		<meeting>the 4th international conference on Computer graphics and interactive techniques in Australasia and<address><addrLine>Southeast Asia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partial and approximate symmetry detection for 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="560" to="568" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Symmetry in 3d geometry: Extraction and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust piecewise-planar 3d reconstruction and completion from large-scale unstructured point data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Chauve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Pons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1261" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bottom-up/top-down image parsing with attribute grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A probabilistic model for component-based shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detail preserved point cloud completion via separated feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="790" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5898" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pmp-net: Point cloud completion by learning multi-step point moving paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7443" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5499" to="5509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning 3d shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unpaired point cloud completion on real scans using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00069</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cy-cle4completion: Unpaired point cloud completion using cycle transformation with missing region coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="80" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4404" to="4413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepi2p: Image-to-point cloud registration via deep classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="960" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04180</idno>
		<title level="m">Image2point: 3d point-cloud understanding with pretrained 2d convnets</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A papierm?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Style-based point generator with adversarial rendering for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4619" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive graph convolution for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4945" to="4954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Seedformer: Patch seeds based point cloud completion with upsample transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.10315" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
