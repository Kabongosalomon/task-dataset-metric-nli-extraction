<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Chen</surname></persName>
							<email>chensjvin@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing single-stage detectors for locating objects in point clouds often treat object localization and category classification as separate tasks, so the localization accuracy and classification confidence may not well align. To address this issue, we present a new single-stage detector named the Confident IoU-Aware Single-Stage object Detector (CIA-SSD). First, we design the lightweight Spatial-Semantic Feature Aggregation module to adaptively fuse high-level abstract semantic features and low-level spatial features for accurate predictions of bounding boxes and classification confidence. Also, the predicted confidence is further rectified with our designed IoU-aware confidence rectification module to make the confidence more consistent with the localization accuracy. Based on the rectified confidence, we further formulate the Distance-variant IoU-weighted NMS to obtain smoother regressions and avoid redundant predictions. We experiment CIA-SSD on 3D car detection in the KITTI test set and show that it attains top performance in terms of the official ranking metric (moderate AP 80.28%) and above 32 FPS inference speed, outperforming all prior single-stage detectors. The code is available at https://github.com/Vegeta2020/CIA-SSD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To detect objects in autonomous driving, point clouds are often adopted to offer robust information. In general, there are two classes of methods to detect objects in point clouds: single-stage and two-stage. Though two-stage detectors usually attain higher average precisions benefited from an extra refinement stage, single-stage detectors are typically more efficient due to their simpler network structures. Also, the detection precisions of recent single-stage detectors <ref type="bibr" target="#b28">(He et al. 2020;</ref><ref type="bibr" target="#b30">Yang et al. 2020;</ref><ref type="bibr" target="#b26">Shi and Rajkumar 2020)</ref> gradually approach that of the state-of-the-art two-stage detectors. The advantages of time efficiency and competitive precision motivate us to focus this work on single-stage detectors.</p><p>Existing 3D object detectors often treat object localization and category classification as separate tasks, so the localization accuracy and classification confidence may not align well <ref type="bibr" target="#b8">(Jiang et al. 2018)</ref>. Hence, two-stage detectors <ref type="bibr" target="#b23">Shi et al. 2020a</ref>) extract features from the region Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 5HDOWLPH <ref type="bibr">[CVPR 2020]</ref> [ <ref type="bibr">CVPR 2020]</ref> [ <ref type="bibr">CVPR 2020]</ref> [ <ref type="bibr">AAAI 2020]</ref> [CVPR 2019] [Sensor 2018] <ref type="figure">Figure 1</ref>: Our CIA-SSD attains top performance (official rank: moderate AP 80.28%) and real-time speed (30.76 ms) on 3D car detection in KITTI test set <ref type="bibr" target="#b4">(Geiger et al. 2013</ref>), compared with the state-of-the-art single-stage detectors.</p><p>proposals generated by the first-stage backbone and predict the IoUs between the regressed bounding boxes and groundtruth boxes in the second stage to refine the confidence predictions. Compared with hard-category labels, the soft IoU labels are usually more consistent with the localization qualities, thus leading to more accurate confidence predictions.</p><p>Compared with two-stage detectors, single-stage detectors cannot train features extracted from their predicted bounding boxes with a second-stage network. Also, their features are learned mostly based on the pre-defined anchors or classified positive points, so the resulting IoU predictions may not be as accurate as those in the two-stage networks. Hence, general single-stage detectors cannot effectively rectify confidence predictions like the two-stage ones.</p><p>To resolve this issue, SASSD <ref type="bibr" target="#b28">(He et al. 2020)</ref>, a very recent single-stage detector, exploits an interpolation approach to obtain the region proposal features for confidence rectification. Their approach is, however, very complex with the interpolation operation. In this work, we design a new confidence rectification module embedded in our Confident IoU-Aware Single-Stage object Detector (CIA-SSD) to address the issue more elegantly. Our key idea is based on the finding that anchor-feature-based IoU predictions are discriminative especially between the precise and imprecise regressions of the bounding boxes. Thus, by utilizing a convex function to augment the discrimination, we polarize the effect of IoU predictions between the precise and imprecise regressions and effectively rectify the confidence in the post processing.</p><p>Besides, we design a lightweight Spatial-Semantic Feature Aggregation (SSFA) module to adaptively fuse highlevel abstract semantic features and low-level spatial features for more accurate predictions of bounding boxes and classification confidence. Compared with the commonlyused 2D feature extraction module <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b28">He et al. 2020)</ref>, our SSFA module boosts the performance effectively with a moderate increase in the model complexity. Further, we notice that there are often more redundant false-positive predictions and strong oscillations for bounding boxes regressed at large distances from the viewpoint. Hence, we formulate a novel Distance-variant IoU-weighted NMS (DI-NMS) to obtain smoother regressions and reduce redundant predictions, by considering the depth factor not encountered in 2D NMS. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the quality of our results with an example, in which our method can find better-aligned bounding boxes and avoid redundant predictions, compared with the state-of-the-art single-stage detector SASSD. Below, we summarize the contributions of this work: (i) a lightweight spatial-semantic feature aggregation module that adaptively fuses high-level abstract semantic features and low-level spatial features for more accurate bounding box regression and classification; (ii) an IoU-aware confidence rectification module to alleviate the misalignment between the localization accuracy and classification confidence; and (iii) a distance-variant IoU-weighted NMS to smooth the regressions and avoid redundant predictions. CIA-SSD attains top performance (moderate AP 80.28%) and real-time speed (30.76 ms) on 3D car detection in KITTI test set <ref type="bibr" target="#b4">(Geiger et al. 2013)</ref>, as illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are two types of LiDAR-based 3D object detectors: (i) Two-stage detectors usually generate region proposals in the first stage, then feed these regions of interests into a secondstage network for refinement; and (ii) In contrast, singlestage detectors have simpler networks, since they directly regress class scores and bounding boxes in one stage.</p><p>Among the two-stage detectors, PointRCNN <ref type="bibr" target="#b24">(Shi, Wang, and Li 2019)</ref> uses PointNet++ <ref type="bibr" target="#b22">(Qi et al. 2017)</ref> as the backbone in both stages and devises an anchor-free strategy to generate 3D proposals. Based on PointRCNN, Part-A 2 <ref type="bibr" target="#b25">(Shi et al. 2020b)</ref> replaces the PointNet++ backbone with a sparse convolutional network, and proposes RoI-aware point cloud feature pooling in the refinement. STD ) exploits spherical anchors to generate proposals, together with a segmentation branch, to reduce the number of positive anchors. PV-RCNN <ref type="bibr" target="#b23">(Shi et al. 2020a</ref>) uses set abstraction modules to extract point features from multi-scale voxel features in the first stage to refine the region proposals. Among the single-stage detectors, VoxelNet (Zhou and Tuzel 2018) voxelizes the point cloud and proposes a voxel feature encoding layer to extract point features in each voxel and produce fixed-length features for batch training. Point-Pillar <ref type="bibr" target="#b11">(Lang et al. 2019</ref>) divides a point cloud into pillars instead of voxels for feature extraction, then utilizes a 2D convolutional detection architecture for object localization. SECOND <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref> exploits the sparse convolution <ref type="bibr" target="#b17">(Liu et al. 2015)</ref> and submanifold sparse convolution <ref type="bibr" target="#b5">(Graham, Engelcke, and Van Der Maaten 2018)</ref> to replace conventional 3D convolution. TANet  presents a triple attention module embedded in voxel feature extraction and combines it with a proposed cascaded refinement network. Very recently, Point-GNN (Shi and Rajkumar 2020) proposes a graph neural network to extract point features and achieves a decent performance. 3DSSD  proposes a fusion sampling strategy by combining both feature-and point-based farthest point sampling for better classification performance. SASSD <ref type="bibr" target="#b28">(He et al. 2020)</ref> presents an auxiliary network in parallel with a sparse convolutional network to regress the box centers and semantic classes for each point with interpolated voxel features.</p><p>As we shall show in the results, recent single-stage detectors have achieved comparable performance (average precision) with the state-of-the-art two-stage detectors. Given their high efficiency, single-stage detectors have great potential for real-time applications. This motivates us to focus this research on developing a new single-stage detector CIA-SSD, which has attained top performance and real-time speed, compared with all previous single-stage detectors.</p><p>3 Confident IoU-Aware Single Stage Detector <ref type="figure" target="#fig_2">Figure 3</ref> shows our model's pipeline, which has three parts: (i) the sparse convolutional network (SPConvNet) for encoding the input point cloud; (ii) the SSFA module for extracting robust spatial-semantic features; and (iii) the multi-task head with a confidence function for rectifying the classification score and DI-NMS for post-processing. . First, we encode the input point cloud (a) with a sparse convolutional network denoted by SPConvNet (b), followed by our spatial-semantic feature aggregation (SSFA) module (c) for robust feature extraction, in which an attentional fusion module (d) is adopted to adaptively fuse the spatial and semantic features. Then, the multi-task head (e) realizes the object classification and localization, with our introduced confidence function (CF) for confidence rectification. In the end, we further formulate the distance-variant IoUweighted NMS (DI-NMS) for post-processing. Note that "box cls," "iou reg," "box reg," and "dir cls" in (e) denote bounding box classification, IoU prediction regression, bounding box regression, and direction classification, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Point Cloud Encoder</head><p>To encode a point cloud, we voxelize it and calculate the mean coordinates and intensities of points in each voxel as the initial feature. We then utilize the sparse convolutional network SPConvNet (see <ref type="figure" target="#fig_2">Figure 3</ref>(b)), following the settings of SECOND <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref> to extract features from the sparse voxels. The SPConvNet consists of four blocks, each comprising several submanifold sparse convolution (SSC) (Graham, Engelcke, and Van Der Maaten 2018) layers and one sparse convolution (SC) <ref type="bibr" target="#b17">(Liu et al. 2015)</ref> layer. Specifically, our four blocks have {2, 2, 3, 3} SSC layers, respectively, and an SC layer appended to the end of each block for 2x downsampling on the 3D feature maps. Lastly, we transform the sparse voxel features to dense feature maps and concatenate the features in z to produce the BEV feature maps as inputs to the SSFA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial-Semantic Feature Aggregation</head><p>To detect cars in autonomous driving, we have to (i) regress precise car locations and also to (ii) classify each regressed bounding box as a positive/negative sample. In such processes, it is crucial to consider both low-level spatial features and high-level abstract semantic features. However, when we enrich the high-level abstract semantics in the feature maps, e.g., through stacked convolution layers, the quality of the low-level spatial information often declines as a result. Hence, the commonly-used BEV feature extraction module <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b28">He et al. 2020)</ref>, which applies stacked convolution layers, could not effectively obtain robust features with rich spatial information.</p><p>In this work, we design the spatial-semantic feature aggregation (SSFA) module. As shown in Figures 3 (c) &amp; (d), our SSFA module contains two groups of convolution layers and an attentional fusion module at the end. The two convolution groups are named the spatial and semantic groups (with corresponding layer-wise features), indicated by the red and green blocks in <ref type="figure" target="#fig_2">Figure 3</ref>(c), respectively, and their outputs are named the spatial and semantic features, respectively. Specifically, we keep the dimensions (number of channels and feature map size) of the spatial feature to be the same as the input to avoid loss of spatial information. For the semantic group, we aim to gain more high-level abstract semantic information by taking the spatial feature as input, doubling the number of channels, and reducing the spatial size by half with an initial convolution layer of stride two. Also, we adopt a 2D DeConv layer to recover the dimensions of the semantic feature map to be the same as the spatial feature map before the element-wise addition to producing the enriched spatial feature (the blue block in <ref type="figure" target="#fig_2">Figure 3(c)</ref>). On the other hand, we use another 2D DeConv layer to produce the upsampled semantic feature (the yellow block in <ref type="figure" target="#fig_2">Figure 3</ref>(c)) before the attentional fusion.</p><p>To adaptively fuse the enriched spatial feature and the upsampled semantic feature, we adopt the attention module shown in <ref type="figure" target="#fig_2">Figure 3(d)</ref>. First, we compress the channels of each feature to one and concatenate the results. We then use Softmax to normalize the two concatenated channels and split them into two BEV attention maps, in which Softmax builds the dependence between the two features for adaptive feature fusion. Lastly, we take the learned BEV attention maps as weights on the respective features and perform an element-wise addition (? in <ref type="figure" target="#fig_2">Figure 3(d)</ref>) to fuse the weighted features. The SSFA module helps extract more robust features with rich spatial and semantic information for more accurate predictions of bounding boxes and classification confidence (see the ablation study in Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IoU-Aware Confidence Rectification</head><p>To alleviate the misalignment between the localization accuracy and classification confidence without having an additional network stage, we design the IoU-aware confidence rectification module for post-processing the confidence. In 3D object detection, we usually define anchors by evenly distributing bounding boxes of fixed size on the BEV feature maps; then, we can train the network by regressing the offsets between the anchors and ground truths. Suffering from the misaligned features of the anchors, the IoUs predicted by anchor-based single-stage detectors are often not as accurate as the IoUs predicted with region proposals in two-stage detectors. However, we observe from an experiment that the IoUs predicted with anchors are still rather discriminative. <ref type="figure" target="#fig_3">Figure 4</ref> shows the experimental result of IoU prediction ("iou reg" in <ref type="figure" target="#fig_2">Figure 3</ref>(e)) conducted on the KITTI validation set. Specifically, "real IoU" refers to the IoU computed between the anchor-based predicted bounding box and its nearest ground-truth box, whereas "predicted IoU" refers to the network-predicted IoU on the corresponding anchorbased predicted bounding box. From the scatterplot shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>, we can see that although the predicted IoU cannot perfectly match the real one and the prediction deviation increases when the real IoU drops, high predicted IoUs often associate with high real IoUs, thus allowing us to differentiate between the precise and imprecise bounding box regressions. Typically, if an anchor feature leads to precise regression, the feature should also predict high IoU with high certainty, as the feature already contains sufficient location information. On the other hand, if an anchor feature produces imprecise regression, e.g., when the anchor is far away from the ground truths, the feature will likely lead to low-IoU prediction with high uncertainty.</p><p>To suppress the uncertainties of low-IoU predictions and further augment the discrimination between low-IoU and high-IoU predictions, we introduce the rectification item g:</p><formula xml:id="formula_0">g = i ? ,<label>(1)</label></formula><p>where i denotes the predicted IoU and ? is a hyperparameter that controls the degrees of suppressing the low-IoU predictions and augmenting the high-IoU predictions. As shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>, the predictions of high IoUs can become more discriminative, e.g., when setting ? = 4. Hence, we propose to rectify the classification score with g and formulate the following Confidence Function f in the multi-task head:</p><formula xml:id="formula_1">f = c ? g = c ? i ?<label>(2)</label></formula><p>where c is the classification score of the predicted bounding box. By this means, we can polarize the effect of low-IoU and high-IoU predictions for better rectification of c.</p><p>Algorithm 1 Distance-variant IoU-weighted NMS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>C is the N ?7 matrix of predicted bounding box parameters <ref type="bibr">(x, y, z, w, l, h, r)</ref>, where N is the number of bounding boxes, xyz denote box center, wlh denote box dimensions, and r denotes box orientation angle; S is the set of N rectified confidence values of the corresponding predicted bounding boxes; I is the set of N IoU prediction values of the corresponding predicted bounding boxes;</p><p>A is the N ?7 matrix of corresponding anchors for the predicted box parameters (x, y, z, w, l, h, r); C = {c 1 , c 2 , ..., c N }; I = {iou 1 , iou 2 , ..., iou N }; S = {s 1 , s 2 , ..., s N }; and A = {a 1 , a 2 , ..., a N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensure:</head><p>Selected bounding boxes: During the training, we train the IoU prediction branch simultaneously with the bounding box regression and classification branches and detach the predicted bounding boxes from the computation graph in the IoU predictions to avoid the gradients to back-propagate from the IoU prediction loss to the bounding box regressions. Only in the testing, we make use of the Confidence Function in Eq. (2) to rectify the classification score and produce the rectified classification confidence f for use in the following NMS process.</p><formula xml:id="formula_2">B = ?, L = {1, 2, ..., N } 1: dist = { c i,[0,1] ? a i,[0,1] 2 | c i ? C, a i ? A, i ? L} 2: S = {s i ? (1 ? sof tmax i (dist)) | s i ? S, i ? L} 3: iou thres = 0.3, cnt thres = ? 4: while L = ? do 5: cnt = 0, idx = arg max i?L S, c = c idx 6: L = {i | i ? L, IoU (c i , c ) &gt; iou thres, c i ? C} 7: cnt = ? i?L iou i ? IoU (c i , c ), c i ? C, iou i ? I 8: b = ? i?L ioui?ci?e ?(1?IoU (c i ,c )) 2 /? 2 ? i?L ioui?e ?(1?IoU (c i ,c )) 2 /? 2 , c i ? C, iou i ? I ? ? c i,[0,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Distance-Variant IoU-Weighted NMS</head><p>Distant objects are often predicted with low classification confidence and high regression uncertainties, caused by the point sparsity at distant regions. Hence, we often observe (i) strong oscillations on the regressed bounding boxes and (ii) redundant false-positive predictions that do not overlap with any ground-truth bounding box (in which a few anchors are incorrectly activated to perform the regressions).</p><p>To avoid these issues, we formulate the distance-variant IoU-weighted NMS (DI-NMS) for post-processing the predictions. Algorithm 1 outlines the procedure: (i) In Steps 1-2, we calculate the L 2 distance between the BEV centers of each pair of anchor and predicted box, and take the distance offset to refine the confidence; (ii) In Steps 5-7, we select box c with the highest classification confidence and find indices of its overlapped boxes L filtered with an IoU threshold, in which c and L are the candidate box and indices of auxiliary boxes, respectively. Next, we calculate a cumulative sum of products between the IoU predictions of the auxiliary boxes and their real IoUs with the candidate box, in which we design the cumulative sum to filter redundant (zero-IoU) false positives; and (iii) In Step 8, we use a Gaussian weighted average to obtain a smooth regression from the auxiliary boxes.</p><p>Note, auxiliary boxes that have large IoU predictions or high IoUs with the candidate box are assigned with large Gaussian weights. Also, considering that the localization uncertainty increases with distance, we utilize ? to adjust the smoothness degree of the Gaussian weights, in which ? is positively correlated with the BEV distance between the box center and viewpoint. Thus, we can use the smooth Gaussian weights to evenly consider the oscillated auxiliary boxes in distant predictions, while using differentiated weights to focus on the auxiliary boxes that are highly overlapped with the candidate box in short-range predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>For model optimization, we follow the conventional settings of box encoding and loss functions in <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref>. Specifically, we use the Focal loss <ref type="bibr" target="#b16">(Lin et al. 2017</ref>), Smooth-L 1 loss <ref type="bibr" target="#b18">(Liu et al. 2016)</ref>, and cross-entropy loss in the bounding box classification (L cls ), box regression (L box ), and direction classification (L dir ). Besides, we encode the IoUs between the predicted boxes and the groundtruth boxes to keep them in the range of [-1, 1]:</p><formula xml:id="formula_3">iou t = 2 ? (iou ? 0.5)<label>(3)</label></formula><p>where iou denotes the real IoU between the predicted bounding box and ground-truth box, and iou t denotes the encoded target for the IoU prediction. Then, we apply the Smooth-L 1 loss on the IoU regressions to calculate the IoU prediction loss L iou . Similar to the bounding box regression loss L box , we calculate the IoU prediction loss L iou only on the positive samples. The overall loss L is defined as L = L cls + ?L box + ?L dir + ?L iou (4) where we empirically set ? = 2.0, ? = 0.2, and ? = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our CIA-SSD on the KITTI 3D object benchmark <ref type="bibr" target="#b4">(Geiger et al. 2013)</ref> with 7,481 training samples and 7,518 test samples. The training samples are further divided into a training set (3,712 samples) and a validation set (3,769 samples). Following previous works, e.g., SASSD <ref type="bibr" target="#b28">(He et al. 2020)</ref> and SECOND <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref>, we conducted experiments on the most commonly-used car category and evaluated the results by average precision (AP) with IoU threshold 0.7. Also, the dataset has three difficulty levels (easy, moderate, and hard) based on the object size, occlusion, and truncation levels. <ref type="figure" target="#fig_5">Figure 5</ref> shows some of our predicted bounding boxes projected onto color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We voxelize the input point cloud into a grid of resolutions [0.05, 0.05, 0.1] meters in ranges <ref type="bibr">[0, 70.4]</ref>, <ref type="bibr">[-40, 40]</ref>, and [-3, 1] meters along the x, y, and z axes, respectively. The anchors are pre-defined evenly on the BEV feature map with the same dimensions (width=1.6m, length=3.9m, height=1.56m) and two possible orientations (0 ? or 90 ? ). Further, they are divided into three categories (positive, negative, and ignored) based on the matching strategy in <ref type="bibr" target="#b34">(Zhou and Tuzel 2018)</ref> with IoU thresholds 0.45 and 0.6.</p><p>We consider four types of data augmentations to enhance our model's generalization ability. The first type is a global  we can see that our CIA-SSD attains the top performance compared with all 1-stage detectors and is comparable with the top 2-stage detector on this challenging problem.</p><p>augmentation on the entire point cloud, including random rotation, scaling, and flipping. The second type is a local augmentation on a portion of the point cloud around a ground-truth object, including random rotation and translation. The third type is a ground-truth augmentation following SECOND <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref>. Last, we filter out objects with difficulty levels not attributed to easy, moderate, and hard to improve the quality of the positive samples, and take also objects of similar categories, such as van for car, as the targets to alleviate model confusion in the training. In the SSFA module, the spatial and semantic groups have three stacked convolution layers of kernel 3x3 with a number of channels 128 and 256, respectively. After the spatial and semantic groups, there is one 1x1 convolution layer with 128 and 256 channels separately. The 2D DeConv layers have 3x3 kernels and 128 output channels with stride two. Before the attentional fusion, we use a 3x3 convolution layer with 128 output channels to transform each group feature. In the attentional fusion, the convolutional layers have 3x3 kernels and one output channel. Other network settings follow those in SECOND <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref>.</p><p>We use the ADAM optimizer (Kingma and Ba 2014) with the cosine annealing learning rate <ref type="bibr" target="#b20">(Loshchilov and Hutter 2016)</ref> to train our model with a batch size of four on a single GPU card for 60 epochs. Further, we empirically set ? = 4 (in the confidence function), ? = 2.6 (in DI-NMS), and ? = {0.0009, 0.009, 0.1, 1} (in DI-NMS) for BEV distances in ranges <ref type="bibr">[0, 20m), [20m, 40m), [40m, 60m), and [60m, 70.4m</ref>], respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Arts</head><p>We compare our CIA-SSD with the state-of-the-art methods listed in <ref type="table" target="#tab_1">Table 1</ref>. As shown in the table, our model ranks the 1 st place in terms of moderate and easy AP, i.e., 80.28% and 89.59% respectively, among all the single-stage detectors.</p><p>The "moderate AP" is the official ranking metric for 3D detection on the KITTI official test server. Our CIA-SSD outperforms all the state-of-the-art single-stage detectors, including the very recent ones, including Point-GNN, 3DSSD, and SASSD by about 0.5 to 0.8 points under this metric. Besides, while two-stage detectors generally perform better than single-stage detectors due to the extra second-stage refinement, our proposed single-stage detector still outperforms most of the recent two-stage detectors, e.g., 3D-CVF, STD, and Part-A 2 by about 0.6 to 1.8 points on moderate AP. Furthermore, we shall show the high efficiency of our model compared with two-stage detectors in Section 4.4. Although our model sets the new state-of-the-art singlestage results on easy and moderate APs for the KITTI test set, the corresponding APs are slightly lower than some of the state-of-the-art results on the validation set, as shown in <ref type="table" target="#tab_3">Table 2</ref>. Also, our hard AP is lower than the state-of-the-art methods on the test set, while being the top on the validation set. We argue that such inconsistency may be caused by the mismatched distributions between the KITTI val and test splits, as mentioned in Part-A 2 <ref type="bibr" target="#b25">(Shi et al. 2020b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We adopt the 2D feature extraction module <ref type="bibr" target="#b29">(Yan, Mao, and Li 2018)</ref> grouped by seven stacked convolutional layers, the normal NMS, and multi-task head without the IoU prediction branch as the baseline modules in the ablation study.</p><p>Effect of data processing <ref type="table">Table 3</ref> shows results that reveal the effect of each data processing technique on the baseline modules. Both global and local augmentations effectively improve easy and moderate APs. Also, the ground truth augmentation, training with the similar type of objects, and filtering objects with suitable difficulty levels for ground-truth augmentation boost all levels of AP effectively. All these techniques help to build up a strong baseline for better validation of our proposed modules.  <ref type="table">Table 3</ref>: Ablation study on our implemented data processing techniques on the baseline modules, in which we report the 3D average precisions of 11 sampling recall points for car detection on the KITTI val split. Here, "glo.," "loc," "gt aug.," "sim.," and "diff." denote the global augmentation, local augmentation, ground truth augmentation, training with similar type of objects, and filtering objects with difficulty levels not attributed to easy, moderate, &amp; hard, respectively.  <ref type="table">Table 4</ref>: Ablation study on our modules: SSFA, CF, and DI-NMS. Here, we report the 3D average precisions of 11 sampling recall points for car detection on the KITTI val split. <ref type="table">Table 4</ref>, our SSFA module contributes an improvement of 0.37, 0.44, and 0.32 on the moderate, easy, and hard APs, respectively. Besides, compared with the baseline 2D feature extraction module under a batch size of four, our SSFA module and the corresponding module in SASSD <ref type="bibr" target="#b28">(He et al. 2020</ref>) increase our framework's GPU occupation by about 10% and 27%, respectively, validating that our SSFA module is lightweight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of SSFA module As shown in</head><p>Effect of confidence function In <ref type="table">Table 4</ref>, our confidence function in the multi-task head improves the easy, moderate, and hard APs by 0.20, 0.46, and 0.76, respectively. Also, we use the metric Pearson Correlation Coefficient (PCC) (D., W., and L. 2008) to measure the correlation between the IoU and confidence of our predicted boxes and calculate the corresponding moderate AP for different ? values in <ref type="table" target="#tab_7">Table 5</ref>. We can see that setting ? = 4 leads to the highest PCC and moderate AP. Besides, the AP still increases from 79.17 to 79.30 when setting ? = 0 (i.e., without rectifying the classification scores), because the IoU prediction optimizes the model to make the learned features aware of the relative locations between the predicted boxes and ground truths.</p><p>Effect of DI-NMS Our DI-NMS raises the easy AP (by 0.38), moderate AP (by 0.18), and hard AP (by 0.16); see <ref type="table">Table 4</ref>. However, different from 2D detection with region proposals that often fully cover the objects, candidate boxes in 3D detection may not have good object coverage, due to the missing points around the boundary, so it is hard to produce well-aligned boxes with the weighted average in DI-NMS. Hence, the increase in AP with DI-NMS is lower than that with the SSFA module and confidence function.     <ref type="table" target="#tab_8">Table 6</ref> compares the runtime of our CIA-SSD with five very recent state-of-the-art single-stage detectors, and we can see that our CIA-SSD is the fastest one. Notice that our CIA-SSD is faster than SASSD as the number of channels in our SSFA module is only half of that of SA-SSD in most layers. Next, <ref type="table" target="#tab_9">Table 7</ref> compares the runtime of CIA-SSD with four recent two-stage detectors. From the table, we can see that CIA-SSD runs significantly faster than these two-stage detectors, confirming the high runtime efficiency of singlestage detectors. The average inference time of CIA-SSD is 30.76ms, including (i) 2.84ms for data processing before the network forwarding; (ii) 24.33ms for data processing in the network; and (iii) 3.59ms for post-processing to produce the final predictions. All the reported timing results were averaged from five runs of our program on an Intel Xeon Silver CPU and a single TITAN Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Runtime Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a new object detector on point clouds, named Confident IoU-Aware Single-Stage object Detector (CIA-SSD). Our main contributions include the spatialsemantic feature aggregation module for extracting robust spatial-semantic features for object predictions, the formulation of a confidence function to rectify the classification score and alleviate the misalignment between the localization accuracy and classification confidence, and the distance-variant IoU-weighted NMS to obtain smoother results and avoid redundant (zero-IoU) false positives. The experimental results show that CIA-SSD achieves the stateof-the-art 3D detection performance on the official ranking metric (Moderate AP) for the KITTI benchmark, compared with all the existing single-stage detectors. Also, CIA-SSD attains real-time detection efficiency and runs the fastest, compared with the very recent state-of-the-art detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>As shown in this example, our CIA-SSD (c) can predict bounding boxes (green) that better align with the ground truths (red) and avoid redundant predictions, as compared with the very recent single-stage detector SASSD (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The pipeline of our proposed Confident IoU-Aware Single-Stage object Detector (CIA-SSD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Scatterplots: (a) real IoUs vs. predicted IoUs, showing that high predicted IoU often associates with high real IoU; and (b) real IoUs vs. (predicted IoUs) ? with ? = 4, such that the rectified IoUs become more discriminative for us to locate good IoU predictions with high certainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>cnt &gt; cnt thres, B ? B ? {b} 11: end while 12: return B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Snapshots of our 3D detection results on the KITTI validation set. The predicted and ground-truth bounding boxes are shown in green and red, respectively, and are projected back onto the color images (1 st &amp; 3 rd rows) for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state-of-the-art methods on the KITTI test set. The 3D average precisions of 40 sampling recall points for car detection are evaluated on the KITTI official server; from the official ranking metric "Moderate AP,"</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art single-stage detectors on the KITTI validation set, in which the 3D average precisions for car detection are based on 11 sampling recall points (vs. 40 points in the KITTI test set).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>glo. loc. gt aug. sim. diff. Easy Mod Hard 81.29 66.39 65.40 86.53 75.73 74.77 87.18 76.33 68.64 87.97 78.03 76.80 88.81 78.35 77.26 89.09 78.73 77.56</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>APMod 79.30 79.37 79.56 79.61 79.63 79.62 79.61 PCC 0.460 0.471 0.502 0.509 0.511 0.510 0.509</figDesc><table><row><cell>?</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter analysis on ? in the confidence function. Here, we report the 3D moderate average precision (AP Mod ) and Pearson Correlation Coefficient (PCC) between the IoU and confidence of the predicted boxes for different ? values in car detection on the KITTI val split.</figDesc><table><row><cell>1-stage</cell><cell cols="6">Point-GNN Associate-3Ddet SASSD 3DSSD TANet Ours (1-stage)</cell></row><row><cell>time (ms)</cell><cell>643</cell><cell>60</cell><cell>40.1</cell><cell>38</cell><cell>34.75</cell><cell>30.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparing the runtime (in millisecond) of our model with very recent state-of-the-art single-stage detectors, showing that our model runs the fastest among them.</figDesc><table><row><cell>2-stage</cell><cell cols="5">PointRCNN Part-A 2 STD Fast PointRCNN Ours (1-stage)</cell></row><row><cell>time (ms)</cell><cell>100</cell><cell>80</cell><cell>80</cell><cell>65</cell><cell>30.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparing the runtime (in millisecond) of our model with state-of-the-art two-stage detectors, showing that our single-stage detector has much higher efficiency.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank reviewers for the valuable comments. This work is funded by the Research Grants Council of the Hong Kong Special Administrative Region (Proj. no. CUHK 14201918).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast Point R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multivariate Probability Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D.; W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Statistics with Applications</title>
		<meeting><address><addrLine>Belmont, California</addrLine></address></meeting>
		<imprint>
			<publisher>Brooks/Cole</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13329" to="13338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision Meets Robotics: The KITTI Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure Aware Single-stage 3D Object Detection from Point Cloud</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acquisition of Localization Confidence for Accurate Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3D Proposal Generation and Object Detection from View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitterecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04093</idno>
		<title level="m">Patch Refinement-Localized 3D Object Detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04962</idno>
		<title level="m">3D IoU-Net: IoU Guided 3D Object Detector for Point Clouds</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task Multi-sensor Fusion for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Continuous Fusion for Multi-sensor 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<title level="m">Sparse Convolutional Neural Networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TANet: Robust 3D Object Detection from Point Clouds with Triple Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From Points to Parts: 3D Object Detection from Point Cloud with Part-Aware and Part-Aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1742" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PI-RCNN: An Efficient Multi-Sensor 3D Object Detector with Point-Based Attentive Cont-Conv Fusion Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12460" to="12467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3DSSD: Pointbased 3D Single Stage Object Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">STD: Sparse-to-Dense 3D Object Detector for Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">IoU Loss for 2D/3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
