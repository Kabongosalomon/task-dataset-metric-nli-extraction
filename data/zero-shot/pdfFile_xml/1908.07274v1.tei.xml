<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards High-Resolution Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards High-Resolution Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural network based methods have made a significant breakthrough in salient object detection. However, they are typically limited to input images with low resolutions (400 ? 400 pixels or less). Little effort has been made to train deep neural networks to directly handle salient object detection in very high-resolution images. This paper pushes forward high-resolution saliency detection, and contributes a new dataset, named High-Resolution Salient Object Detection (HRSOD). To our best knowledge, HRSOD is the first high-resolution saliency detection dataset to date. As another contribution, we also propose a novel approach, which incorporates both global semantic information and local high-resolution details, to address this challenging task. More specifically, our approach consists of a Global Semantic Network (GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network (GLFN). GSN extracts the global semantic information based on down-sampled entire image. Guided by the results of GSN, LRN focuses on some local regions and progressively produces high-resolution predictions. GLFN is further proposed to enforce spatial consistency and boost performance. Experiments illustrate that our method outperforms existing state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable or even better performance than them on widely-used saliency benchmarks. The HRSOD dataset is available at https://github.com/yi94code/HRSOD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection, aiming at accurately detecting and segmenting the most distinctive object regions in a scene, has drawn increasing attention in recent years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b47">48]</ref>. It is regarded as a very important task that can facilitate a wide range of applications, such as image understanding <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b43">44]</ref>, object segmentation <ref type="bibr" target="#b17">[18]</ref>, image * Corresponding author.  <ref type="bibr" target="#b48">[49]</ref>. Best viewed by zooming in. captioning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40]</ref> and light field 3D display <ref type="bibr" target="#b34">[35]</ref>.</p><p>Deep Neural Networks (DNNs), e.g., VGG <ref type="bibr" target="#b29">[30]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, have achieved remarkable success in computer vision tasks using the typical input size such as 224 ? 224, 384 ? 384, etc. For most applications, such as image classification, object detection and visual tracking, the typical input size is enough to obtain satisfied results. For dense prediction tasks, e.g., image segmentation and saliency detection, deep learning based approaches also show impressive performance. But the inherited defect is very apparent, i.e., blurry boundary. Many research efforts have been made to remedy this problem. For example, Zhang et al. <ref type="bibr" target="#b48">[49]</ref> employ deep recursive supervision and integrate multi-level features for accurate boundary prediction. However, the improvement is not significant, as illustrated in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>.</p><p>Furthermore, the resolution of the images taken by electronic products (e.g., smartphones) becomes very high, e.g., 720p, 1080p and 4K. When processing high-resolution images, the above defect becomes more severe. The stateof-the-art saliency detection methods generally down-scale the inputs to extract semantic information. In this pro-cess, many details are inevitably lost. Thus, they are not suitable for high-resolution saliency detection task. Meanwhile, there is little research effort to train neural networks to directly handle salient object segmentation in very highresolution images.</p><p>However, this line of work is very important since it can inspire or enable many practical tasks such as image editing <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b22">23]</ref>, medical image analysis <ref type="bibr" target="#b3">[4]</ref>, etc. Specifically, when served as a pre-processing step of background replacement and depth-of-field, high-resolution salient object detection should be as accurate as possible to provide users with realistic composite images <ref type="bibr" target="#b28">[29]</ref>. If the predicted boundaries are not accurate, there may be artifacts which certainly affect users' experience. Thus, this paper pushes forward the task of high-resolution salient object detection.</p><p>To our best knowledge, our approach is the first work for high-resolution salient object detection. Since there is no high-resolution training and test dataset for saliency detection, we contribute a new dataset, High-Resolution Salient Object Detection (HRSOD). More details about our HRSOD will be presented in Section 3.</p><p>As for developing high-resolution saliency detection methods, there are three intuitive methods. The first is simply increasing the input size to maintain a relative high resolution and object details after a series of pooling operations. However, the large input size results in significant increases in memory usage. Moreover, it remains a question that if we can effectively extract details from lower-level layers in such a deep network through back propagation. The second method is partitioning inputs into patches and making predictions patch-by-patch. However, this type of method is time-consuming and can easily be affected by background noise. The third one includes some post-processing methods such as CRF <ref type="bibr" target="#b18">[19]</ref> or graph cuts <ref type="bibr" target="#b27">[28]</ref>, which can address this issue to a certain degree. But very few works attempted to solve it directly within the neural network training process. As a result, the problem of applying DNNs for highresolution salient object detection is fairly unsolved.</p><p>To address above issues, we propose a novel deep learning approach for high-resolution salient object detection without any post-processing. It has a Global Semantic Network (GSN) for extracting global semantic information, and a Local Refinement Network (LRN) for optimizing local object details. A global semantic guidance is introduced from GSN to LRN in order to ensure global consistency. Besides, an Attended Patch Sampling (APS) scheme is proposed to enforce LRN to focus on uncertain regions, and this scheme provides a good trade-off between performance and efficiency. Finally, a Global-Local Fusion Network (GLFN) is proposed to enforce spatial consistency and further boost performance at high resolution.</p><p>To summarize, our contributions are as follows:</p><p>? We introduce the first high-resolution salient object de-tection dataset (HRSOD) with rich boundary details and accurate pixel-wise annotations.</p><p>? We provide a new paradigm for high-resolution salient object detection which first uses GSN for extracting semantic information, and a guided LRN for optimizing local details, and finally GLFN for prediction fusion.</p><p>? We perform extensive experiments to demonstrate that our method outperforms other state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable performance on some widely used saliency benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past few decades, lots of approaches have been proposed to solve the saliency detection problem. Early researches are mainly based on low-level features, such as image contrast <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>, texture <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> and background prior <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>. These models are efficient and effective in simple scenarios, but they are not always robust in handling challenging cases. A detailed survey of these methods can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>More recently, learning based saliency detection methods have achieved expressive performance, and they can coarsely be divided into two categories, i.e., patch-based saliency and FCN-based saliency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Patch-based Saliency</head><p>Existing patch-based methods make saliency prediction for each image patch. For example, Wang et al. <ref type="bibr" target="#b31">[32]</ref> present a saliency detection algorithm by integrating both local estimation and global search. Then, Li et al. <ref type="bibr" target="#b20">[21]</ref> propose to utilize multi-scale features in multiple generic CNNs to predict the saliency degree of each superpixel. With the same purpose of predicting the saliency degree of each superpixel, Zhao et al. <ref type="bibr" target="#b51">[52]</ref> use a multi-context deep CNN to predict saliency maps taking global and local context into account. The above methods include several fully connected layers to make predictions in superpixel-level, resulting in expensive computational cost and the loss of spatial information. What's more, all of them make very coarse predictions and lack low-level details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">FCN-based Saliency</head><p>Liu et al. <ref type="bibr" target="#b23">[24]</ref> design a deep hierarchical saliency network and progressively recover image details via integrating local context information. Zhang et al. <ref type="bibr" target="#b48">[49]</ref> propose a generic framework to integrate multi-level features into different resolutions for finer saliency maps. In order to better integrate features from different levels, Zhang et al. <ref type="bibr" target="#b44">[45]</ref> propose a bi-directional message passing module with a gate function to integrate multi-level features. Wang et al. <ref type="bibr" target="#b35">[36]</ref> use a boundary refinement network to learn propagation coefficients for each spatial position.</p><p>Lots of research efforts have been made to recover image details in final predictions. However, for high-resolution images, all existing FCN-based methods down-sample the inputs, thus lose high-resolution details and fail to predict fine-grained saliency maps.</p><p>Several researchers attempt to remedy this problem by using post-processing techniques for finer predictions. However, traditional CRF <ref type="bibr" target="#b18">[19]</ref> and guided filtering are very time-consuming and their improvement is very limited. Wu et al. <ref type="bibr" target="#b37">[38]</ref> propose a more efficient guided filtering layer. However, their performance is just comparable with the CRF. To reduce this gap, we propose a method to combine the advantages of patch-based methods (maintaining details and saving memory) and FCN-based methods (having rich contextual information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">High-Resolution Saliency Detection Dataset</head><p>There exist several datasets for saliency detection, but none of them is specifically designed for high-resolution salient object detection. Three main drawbacks are apparent. First, all images in current datasets have extremely limited resolutions. Concretely, the longest edge of each image is less than 500 pixels. These low-resolution images are not representative for today's image processing applications. Second, to relieve the burden of users, it is essential to output masks with extremely high accuracy in boundary regions. But images in existing saliency detection datasets are inadequate in providing rich object boundary details for training DNNs. In addition, widely used saliency datasets also have some problems in annotation quality, such as failing to cover all saliency regions <ref type="figure" target="#fig_2">(Figure 2</ref>  To address the above urgent issues, we contribute a High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr 1 with the license of all creative commons. Pixellevel ground truths are manually annotated by 40 subjects. The shortest edge of each image in our HRSOD is more than 1200 pixels. <ref type="figure" target="#fig_2">Figure 2</ref> presents the image size comparison between our HRSOD and existing saliency detection datasets. For existing datasets, we only show the results on HKU-IS dataset <ref type="bibr" target="#b20">[21]</ref>, and the results hold the same on other datasets. Besides, we provide an analysis of shape complexity in supplementary material. Compared with existing saliency datasets, our HRSOD avoids low-level mistakes via careful check by over 5 subjects (an example shown in <ref type="figure" target="#fig_2">Figure 2</ref> (f)). To our best knowledge, HRSOD is cur- Concretely, (c) is from HKU-IS <ref type="bibr" target="#b20">[21]</ref>. (d) is from DUTS-Test <ref type="bibr" target="#b32">[33]</ref>. (e) is from THUR <ref type="bibr" target="#b4">[5]</ref>. And (f) is an example of our HRSOD. Best viewed by zooming in.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula><p>rently the first high-resolution dataset for salient object detection. It is specifically designed for training and evaluating DNNs aiming at high-resolution salient object detection. The whole dataset is publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method</head><p>In this paper, we propose a novel method for detecting salient objects in high-resolution images with limited GPU memory. Our framework includes three branches, i.e., Global Semantic Network (GSN), Local Refinement Network (LRN) and Global-Local Fusion Network (GLFN). <ref type="figure" target="#fig_3">Figure 3</ref> shows an overall illustration of the proposed approach. GSN aims at extracting semantic knowledge in a global view. Guided by GSN, LRN is designed to refine uncertain sub-regions. Finally, GLFN takes high-resolution images as inputs and further enforces spatial consistency of the fused predictions from GSN and LRN.</p><p>To be specific, let through GSN to obtain a coarse saliency map F i , denoted as:</p><formula xml:id="formula_1">{X i = (I i , L i )} N i=1 be</formula><formula xml:id="formula_2">F i = U P (GSN (DS(I i ), ?))<label>(1)</label></formula><p>where DS(?) denotes down-sampling images to 384 ? 384 while U P (?) denotes up-sampling predictions to original size. ? denotes all parameters in GSN. Then I i is put into our proposed Attended Patch Sampling (APS) scheme (Algorithm 1) to generate sub-images {P Ii m } M m=1 , which are attended to uncertain regions (M is the total number of subimages for each input I i ). Subsequently, each P Ii m is fed forward through LRN to get a refined saliency map R Ii m . Semantic guidance is introduced from GSN to LRN (Section 4.2 ). Finally, the outputs of GSN and LRN are fused and fed forward through GLFN for final prediction S i . These two stages can be formulated as:</p><formula xml:id="formula_3">{R Ii m } M m=1 = LRN ({P Ii m } M m=1 , ?)<label>(2)</label></formula><formula xml:id="formula_4">S i = GLF N (I i , F use({R Ii m } M m=1 , F i ), ?)<label>(3)</label></formula><p>where ? and ? denote the parameters of LRN and GLFN, respectively. F use(?) denotes fusion operation (more details can be seen in Section 4.4).  We adopt the same backbone for GSN and LRN. Our model is simply built on the FCN architecture with the pretrained 16-layer VGG network <ref type="bibr" target="#b29">[30]</ref>. The original VGG-16 network <ref type="bibr" target="#b29">[30]</ref> is trained for image classification task while our model is trained for saliency detection, a pixel-wise prediction task. Therefore, we simply abandon all layers after conv5 3 to maintain a higher resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture for GSN and LRN</head><p>In order to enlarge receptive field, we employ dilated convolutional layers <ref type="bibr" target="#b42">[43]</ref> to capture contextual information. Dilated convolution, also known as atrous convolution, has a superior ability to enlarge the field of view without increasing the number of parameters. As shown in <ref type="figure" target="#fig_5">Figure 4</ref> (a), we add four dilated convolutional layers on the top of conv3-3, conv4-3 and conv5-3 in our revised VGG-16. All the dilated convolutional layers have the same kernel size and output channels, i.e., k = 3 and c = 32. The rates of the four dilated convolutional layers in the same block are set with dilation = 1, 3, 5, 7 respectively.</p><p>To improve the output resolution, we first generate three saliency score maps through the last three blocks. Secondly, we add three additional deconvolutional layers, the first two of which have 2? upsampling factors and the last of which has a 4? upsampling factor. Thirdly, inspired by <ref type="bibr" target="#b24">[25]</ref>, we build two skip connections from the saliency score maps generated by block 3 and block 4 to combine high-level features with meaningful semantic information and low-level features with large amount of details (See <ref type="figure" target="#fig_5">Figure 4 (a)</ref>). More details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Guidance from GSN to LRN</head><p>The saliency maps generated by GSN are based on the full image and embedded with rich contextual information. Nevertheless, due to its small input size of 384?384, lots of low-level details are lost, especially when the original images have very high resolutions (e.g., 1920?1080). That is to say, it barely learns to capture saliency properties at a coarse scale. As a result, GSN is competent in giving a rough saliency prediction but insufficient to precisely localize salient objects. In contrary, LRN takes sub-images as input, avoiding down-sampling which results in the loss of details. However, since sub-images are too local to indicate which area is more salient, LRN may be confused about which region should be highlighted. Also, LRN alone may have false alarms in some locally salient regions. Therefore, we propose to introduce the semantic guidance from GSN to LRN, in order to enhance global contextual knowledge while maintain high-resolution details.</p><p>Specifically, we incorporate global semantic guidance in the decoder part. As illustrated in <ref type="figure" target="#fig_5">Figure 4</ref> (b), given the coarse result F i of GSN, a patch P Fi m is first cropped according to the location of patch P Ii m in LRN. Then we concatenate P Fi m with the corresponding feature maps in LRN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Focus on Uncertain Regions</head><p>Compared with previous patch-based methods, our LRN has a notable difference. Traditional patch-based methods usually infer every patch in the image by sliding window or superpixels, which is extremely time-consuming. We note that GSN has already succeeded to assign most pixels with right labels. Therefore, LRN only needs to focus on harder regions. Such a hierarchical prediction manner (GSN for easy regions and LRN for harder regions) makes our method more efficient and accurate. An Attended Patch Sampling (APS) scheme is proposed for this task. Guided by the results of GSN, it can generate sub-images attended to uncertain regions. Algorithm 1 presents a rough procedure of APS (More details can be seen in supplementary material.). We use the attention map A i to indicate all uncertain pixels and it can be formulated as:</p><formula xml:id="formula_5">A i (x, y) = 1 T 1 &lt; F i (x, y) &lt; T 2 0 otherwise<label>(4)</label></formula><p>In Algorithm 1, w denotes the width of non-zero area in A i . X L and X R are the x coordinates of the leftmost and rightmost non-zero pixels in A i . n is a constant, which controls the overlapping between different patches. r is</p><formula xml:id="formula_6">Algorithm 1 Attended Patch Sampling. Require: RGB image I i , ground truth label L i , base crop- ping size D. Ensure: RGB patch set {P Ii m } M m=1 , ground truth patch set {P Li m } M m=1 . 1: Generate attention map A i from F i , as in Equ. 4. 2: N x = w/D + n 3: for t = 1, . . . , N x + 1 do 4: C = D + r 5: X t = min{X L + (t ? 1) ? w/N x , X R } 6: Y = {y | A i (X t , y) = 1} 7:</formula><p>Pick out J pixels (X t , y(j)) J j=1 from (X t , Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Taking C as cropping size, (X t , y(j)) J j=1 as center pixels, crop {P Ii j } J j=1 and {P Li j } J j=1 from I i and L i , respectively. 9: end for a random numbers for generating sub-images with varied sizes. We have performed grid search for setting these hyper-parameters and found that the results were not sensitive to their specific choices. Therefore, we set them empirically in this work. We set D = 384, n = 5, T 1 = 50, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Global-Local Fusion Network</head><p>As illustrated in above sections, GSN and LRN are inherently complementary with each other. Our method leverages GSN to classify easy regions and LRN to refine harder ones. Then the final predictions can be obtained by fusing their results. A simple way to do this is to replace the saliency values of uncertain regions in F i (the result of GSN) by {R Ii m } M m=1 (the result of LRN). Overlapped areas will be averaged. However, this kind of fusion lacks spatial consistency and does not leverage rich details in original high-resolution images.</p><p>We propose to directly train a network to incorporate  high-resolution information to help the fusion of GSN and LRN. To maintain all the high-resolution details from images, this network should not include any pooling layers or convolutional layers with large strides. With limited GPU memory, popular backbones (e.g., VGG and ResNet) can not be trained with such a high-resolution input size (more than 1000 ? 1000 pixels). Therefore, We propose a lightweighted network, name as Global-Local Fusion Network (GLFN). As shown in <ref type="figure">Figure 5</ref>, high-resolution RGB images and combined maps from GSN and LRN are concatenated together to be the inputs of GLFN. GLFN consists of some convolution layers with dense connectivity as in <ref type="bibr" target="#b14">[15]</ref>. We set the growth rate g to be 2 for saving memory. Similar to <ref type="bibr" target="#b14">[15]</ref>, we let the bottleneck layers (1 ? 1 convolution) produce 4g feature maps. On the top of these densely connected layers, we add four dilated convolutional layers to enlarge receptive field. All the dilated convolutional layers have the same kernel size and output channels, i.e., k = 3 and c = 2. The rates of the four dilated convolutional layers are set with dilation = 1, 6, 12, 18 respectively. At last, a 3 ? 3 convolution is appended for final prediction. What is worth mentioning is that our proposed GLFN has an extremely small model size (i.e., 11.9 kB).</p><formula xml:id="formula_7">Method HRSOD-Test DAVIS-S DUTS-Test HKU-IS THUR F ? S-m MAE F ? S-m MAE F ? S-m MAE F ? S-m MAE F ? S-m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>High-Resolution Saliency Detection Datasets. We mainly use our proposed HRSOD-Test to evaluate the performance of our method along with other state-of-the-art methods. To enrich the diversity, we also collect 92 images which are suitable for saliency detection from DAVIS <ref type="bibr" target="#b26">[27]</ref>, a densely annotated high-resolution video segmentation dataset. Images in this dataset are precisely annotated and have very high resolutions (i.e.,1920?1080). We ignore the categories of the objects and generate saliency ground truth masks for this dataset. For convenience, the collected dataset is named as DAVIS-S. Low-Resolution Saliency Detection Datasets. In addition, we evaluate our method on three widely used benchmark datasets: THUR <ref type="bibr" target="#b4">[5]</ref>, HKU-IS <ref type="bibr" target="#b20">[21]</ref> and DUTS <ref type="bibr" target="#b32">[33]</ref>. THUR and HKU-IS are large-scale datasets, with 6232 and 4447 images, respectively. DUTS is a large saliency detection benchmark, contains 5019 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Metrics</head><p>We use four metrics to evaluate all methods: Precision-Recall (PR) curves, F ? measure, Mean Absolute Error (MAE) and structure-measure <ref type="bibr" target="#b8">[9]</ref>. PR curves are generated by binarizing the saliency map with a varied threshold from 0 to 255, then comparing the binary maps with the ground truth. F ? measure is defined as F ? = (1+? 2 )?precision?recall ? 2 ?precision+recall . The precision and recall are computed under the threshold of twice the mean saliency value. ? 2 is set to 0.3 as suggested in <ref type="bibr" target="#b0">[1]</ref> to emphasize precision. MAE measures the average error of saliency maps. Structure-measure simultaneously evaluates region-aware and object-aware structural similarity between a saliency map and a ground truth mask. For detailed implementations, we refer readers to <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation Details</head><p>All experiments are conducted on a PC with an i7-8700 CPU and a 1080 Ti GPU, with the Caffe toolbox <ref type="bibr" target="#b16">[17]</ref>. In our method, every stage is trained to minimize a pixelwise softmax loss function, by using the stochastic gradient descent (SGD). Empirically, the momentum parameter is set to 0.9 and the weight decay is set to 0.0005. For GSN and LRN, the inputs are first warped into 384 ? 384 and the batch size is set to 32. The weights in block 1 to block 5 are initialized with the pre-trained VGG model <ref type="bibr" target="#b29">[30]</ref>, while  <ref type="figure">Figure 8</ref>. Visual comparison of our method with variations using Dense CRF <ref type="bibr" target="#b18">[19]</ref>.</p><p>weight parameters of newly-added convolutional layers are randomly initialized by using the "msra" method <ref type="bibr" target="#b11">[12]</ref>. The learning rates of the pre-trained and newly-added layers are set to 1e-3 and 1e-2, respectively. GLFN is trained from scratch, and its weight parameters of convolutional layers are also randomly initialized by using the "msra" method. Its inputs are warped into 1024 ? 1024 and the batch size is set to 2. Source code will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the State-of-the-arts</head><p>We compare our algorithm with 9 state-of-the-art methods, including RFCN <ref type="bibr" target="#b33">[34]</ref>, DHS <ref type="bibr" target="#b23">[24]</ref>, UCF <ref type="bibr" target="#b49">[50]</ref>, Amulet <ref type="bibr" target="#b48">[49]</ref>, NLDF <ref type="bibr" target="#b25">[26]</ref>, DSS <ref type="bibr" target="#b13">[14]</ref>, RAS <ref type="bibr" target="#b2">[3]</ref>, DGF <ref type="bibr" target="#b37">[38]</ref> and DGRL <ref type="bibr" target="#b35">[36]</ref>. For a fair comparison, we use either the implementations with recommended parameter settings or the saliency maps provided by the authors. To demonstrate the effectiveness of our approach, we provide two versions of our results. Ours-D represents for training on DUTS while Ours-DH represents for training on DUTS and HRSOD.</p><p>One thing deserves to be mentioned is that in our framework, GSN and LRN can be any saliency detection model. We just choose simple FCNs to validate the effectiveness of our framework. With our method, even simple FCNs can outperform other complicated models.</p><p>Quantitative Evaluation. F ? measure, S-measure and MAE scores are given in <ref type="table" target="#tab_0">Table 1</ref>. As can be seen, our method outperforms all the existing state-of-the-art methods on our new-built high-resolution datasets with a large margin. It also achieves comparable or even superior per-formance than them on some widely used saliency detection datasets. We provide the PR curves in the supplementary material due to limited space.</p><p>Qualitative Evaluation. <ref type="figure">Figure 8</ref> shows a visual comparison of our method with respect to others. It can be seen that our method is capable of accurately detecting salient objects as well as suppressing the background clutter. Further, our saliency maps have better boundary shape and are much closer to the ground truth maps in challenging cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Ablation Analysis</head><p>In this section, we provide the results about different variants of our method to further verify our main contributions.</p><p>LRN, GLFN vs CRF. In our method, LRN learns to refine uncertain regions under the guidance of GSN. To demonstrate its effectiveness, We also compare it with CRF <ref type="bibr" target="#b18">[19]</ref>, a widely used post-processing for saliency detection. The parameters are set as in <ref type="bibr" target="#b13">[14]</ref>. We employ the CRF to refine predictions of GSN and LRN, denoted as GSN+CRF and GSN+APS+LRN+CRF, respectively. The results in <ref type="table">Table 4</ref> show that our method outperforms CRF by a large margin. <ref type="figure">Figure 8</ref> shows the qualitative results. We find that our LRN and GLFN progressively improve details of saliency maps while the CRF fails to recover lost details.</p><p>APS vs RPS. To demonstrate the effectiveness of the proposed APS scheme, we train LRN on patches which are randomly sampled. For fair comparison, we set the num- ber and size of sampled patches to be the same with our proposed APS. We denote this setting as GSN+RPS+LRN. Various metrics in <ref type="table">Table 4</ref> demonstrate that APS significantly outperforms RPS, which indicates the important role of our proposed APS.</p><p>Performance vs number of patches. For traditional patch-based methods, refining more patches brings more performance gain, but results in more computational cost. It seems like a tricky trade-off problem. Our proposed APS can ingeniously relieve this problem thanks to its focusing on uncertain regions. <ref type="figure" target="#fig_10">Figure 9</ref> shows that our APS is less sensitive to number of patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">More Discussion</head><p>Running time and model size. <ref type="table">Table 3</ref> shows a comparison of running time and model size. Since other methods can not directly handle high-resolution images, the running time analysis of the compared methods is conducted with the same input size (384 ? 384) for fair. Also, we provide our running time for 1024 ? 1024 inputs, denoted as Ours*.</p><p>As it can be seen, our method is the fasted among all the compared methods and is quite efficient when directly handling high-resolution images. Boundary quality. To further evaluate the precision of boundaries, we compare different methods by the Boundary Displacement Error (BDE) metric <ref type="bibr" target="#b10">[11]</ref>. This metric measures the average displacement error of boundary pixels between two predictions, which can be formulated as:</p><formula xml:id="formula_8">BDE(X, Y ) = 1 2 1 N X x inf y?Y d(x, y) + 1 N Y y inf x?X d(x, y)</formula><p>where X and Y are two boundary pixel sets, and x, y are pixels in them, respectively. N X and N Y denote the number of pixels in X and Y . inf represents for the infimum and d(?) denotes Euclidean distance. We only compute the BDE on high-resolution datasets because other benchmarks are not qualified enough on boundaries in pixel-level due to relatively poor annotations. The BDE for the state-ofthe-art methods on HRSOD-Test and DAVIS-S are listed in <ref type="table">Table 2</ref>. The results indicate that our predictions have better boundary shape and are closer to the ground truth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we push forward high-resolution saliency detection task and provide a high-resolution saliency detection dataset (HRSOD) for facilitating studies in highresolution saliency prediction. A novel approach is proposed to address this challenging task. It leverages both global semantic information and local high-resolution details to accurately detect salient objects in high-resolution images. Extensive evaluations on high-resolution datasets and popular benchmark datasets verify the effectiveness of our method. We will explore to develop weakly supervised high-resolution salient object detection in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Pipeline comparison with state-of-the-art methods. (a) Input image. (b) Ground truth mask. (c) Our method. (d) Amulet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c)), including background disturbance into foreground annotation (Figure 2 (d)), or low contour accuracy(Figure 2 (e)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>(a) The histogram of diagonal length on HKU-IS [21] (The maximum is less than 600.). (b) The histogram of diagonal length on our HRSOD (The minimum is over 1000.). (c)-(f) Sample images from various dataset, with ground truth masks overlayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>the training set, containing both the training image I i and its pixel-wise saliency label L i . The input image I i is first fed forward Overview of the network architecture. GSN and LRN takes downscaled entire images and attended sub-images as input, respectively. The guidance from GSN provides some semantic knowledge and ensures that our APS and LRN are attended to uncertain regions. A GLFN is appended to directly leverage highresolution information to fuse the predictions from GSN and LRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>(a) Network architecture for both GSN and LRN. (b) Incorporate global guidance only for LRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Some sub-images produced by APS algorithm. (a) Original input image. (b)-(f) Typical sub-images produced by APS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>T 2 =</head><label>2</label><figDesc>200, and r ? [? D 6 , D 6 ] in all our experiments. Some image patches produced by APS are shown in Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>MAERFCN<ref type="bibr" target="#b33">[34]</ref> 0.530 0.608 0.121 0.728 0.842 0.062 0.712 0.792 0.091 0.835 0.746 0.079 0.627 0.793 0.100 DHS<ref type="bibr" target="#b23">[24]</ref> 0.746 0.848 0.059 0.774 0.865 0.034 0.724 0.817 0.067 0.855 0.746 0.053 0.673 0.803 0.082 UCF<ref type="bibr" target="#b49">[50]</ref> 0.700 0.819 0.095 0.648 0.827 0.080 0.629 0.778 0.117 0.808 0.747 0.074 0.645 0.785 0.112 Amulet [49] 0.717 0.830 0.075 0.755 0.848 0.042 0.676 0.803 0.085 0.839 0.772 0.052 0.670 0.797 0.094 NLDF [26] 0.763 0.853 0.055 0.718 0.858 0.042 0.743 0.815 0.066 0.874 0.770 0.048 0.697 0.801 0.080 DSS [14] 0.756 0.840 0.060 0.728 0.865 0.041 0.791 0.822 0.057 0.895 0.779 0.041 0.731 0.801 0.073 RAS[3] 0.773 0.842 0.058 0.763 0.867 0.038 0.755 0.839 0.060 0.871 0.887 0.045 0.696 0.787 0.082 DGRL [36] 0.789 0.848 0.053 0.772 0.859 0.038 0.768 0.841 0.051 0.882 0.802 0.037 0.716 0.816 0.077 DGF [38] 0.795 0.824 0.058 0.785 0.847 0.037 0.776 0.803 0.062 0.893 0.869 0.043 0.734 0.799 0.070 Ours-D 0.857 0.876 0.040 0.850 0.875 0.029 0.796 0.827 0.052 0.891 0.882 0.042 0.740 0.820 0.067 Ours-DH 0.888 0.897 0.030 0.888 0.876 0.026 0.791 0.822 0.051 0.886 0.877 0.042 0.749 0.826 0.064</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Visual comparison. All images are from HRSOD-Test dataset. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Refinement quality versus patch of numbers for different approaches. (a) S-measure vs. patch of numbers. (b) MAE versus patch of numbers. Results are measured on the outputs of LRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with other state-of-the-arts in term of F-measure (larger is better) and MAE (smaller is better) on five dataset. The best results are shown in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Dataset Ours NLDF UCF DHS DSS Amulet RAS DGRL DGF RFCN HRSOD-Test 17.57 22.34 22.84 22.85 25.53 25.75 26.26 30.10 32.91 68.98 DAVIS-S 8.18 23.56 15.69 18.34 19.35 21.11 18.49 14.48 19.77 21.00 Table 2. The Boundary Displacement Error (smaller is better) of the state-of-the-art methods on high-resolution datasets. The best results are shown in bold. Running time and model size of the state-of-the-art methods. Comparison of the different variants on HRSOD-Test.</figDesc><table><row><cell></cell><cell cols="11">Ours* Ours DGF DGRL RAS DSS NLDF Amulet UCF DHS RFCN</cell></row><row><cell>Time (s)</cell><cell>0.39</cell><cell>0.05</cell><cell>0.41</cell><cell>0.52</cell><cell>0.08</cell><cell>5.12</cell><cell>2.31</cell><cell>0.05</cell><cell>0.14</cell><cell>0.05</cell><cell>4.54</cell></row><row><cell cols="5">Model Size(MB) 129.6 129.6 248.9 648.0</cell><cell>81</cell><cell cols="2">447.3 425.9</cell><cell>132.6</cell><cell cols="3">117.9 376.2 1126.4</cell></row><row><cell cols="2">Network Structure</cell><cell>F ?</cell><cell cols="2">S-m MAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSN</cell><cell></cell><cell cols="3">0.842 0.866 0.047</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSN+CRF</cell><cell></cell><cell cols="3">0.858 0.852 0.038</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GSN+RPS+LRN</cell><cell cols="3">0.860 0.871 0.037</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GSN+APS+LRN</cell><cell cols="3">0.877 0.883 0.036</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">GSN+APS+LRN+CRF 0.880 0.875 0.033</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">GSN+APS+LRN+GLFN 0.888 0.897 0.030</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.flickr.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/yi94code/HRSOD</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning active contour models for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Vallabhaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Czanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11632" to="11640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salientshape: Group saliency in image collections. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure-measure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yet another survey on image segmentation: Region and boundary information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Mart?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Cuf?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="408" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4410" to="4419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency guided dictionary learning for weakly-supervised image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baisheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive local adjustment of tonal values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeev</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlocal deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sky is not the limit: semantic-aware sky replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Salience guided depth calibration for perceptually optimized compressive light field 3d display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Surman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2031" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Autoretoucher (art)-a framework for background replacement and image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03954</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency-guided unsupervised feature learning for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2175" to="2184" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You He, and Gang Wang. A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hyperfusion-net: Hyper-densely reflective feature fusion for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="521" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Salient object detection with lossless feature reflection and weighted structural loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3048" to="3060" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Non-rigid object tracking via deep multi-scale spatialtemporal discriminative saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Agile amulet: Real-time salient object detection with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised object class discovery via saliency-guided multiple class learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="862" to="875" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
