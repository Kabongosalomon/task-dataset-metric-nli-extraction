<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Englesson</surname></persName>
							<email>engless@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">KTH Stockholm</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
							<email>azizpour@kth.se</email>
							<affiliation key="aff1">
								<orgName type="institution">KTH Stockholm</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior works have found it beneficial to combine provably noise-robust loss functions e.g., mean absolute error (MAE) with standard categorical loss function e.g. cross entropy (CE) to improve their learnability. Here, we propose to use Jensen-Shannon divergence as a noise-robust loss function and show that it interestingly interpolate between CE and MAE with a controllable mixing parameter. Furthermore, we make a crucial observation that CE exhibits lower consistency around noisy data points. Based on this observation, we adopt a generalized version of the Jensen-Shannon divergence for multiple distributions to encourage consistency around data points. Using this loss function, we show state-of-the-art results on both synthetic (CIFAR), and real-world (e.g. WebVision) noise with varying noise rates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Labeled datasets, even the systematically annotated ones, contain noisy labels <ref type="bibr" target="#b0">[1]</ref>. Therefore, designing noise-robust learning algorithms are crucial for the real-world tasks. An important avenue to tackle noisy labels is to devise noise-robust loss functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Similarly, in this work, we propose two new noise-robust loss functions based on two central observations as follows.</p><p>Observation I: Provably-robust loss functions can underfit the training data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Observation II: Standard networks show low consistency around noisy data points 1 , see <ref type="figure" target="#fig_8">Figure 1</ref>.</p><p>We first propose to use Jensen-Shannon divergence (JS) as a loss function, which we crucially show interpolates between the noise-robust mean absolute error (MAE) and the cross entropy (CE) that better fits the data through faster convergence. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the CE-MAE interpolation. Regarding Observation II, we adopt the generalized version of Jensen-Shannon divergence (GJS) to encourage predictions on perturbed inputs to be consistent, see <ref type="figure" target="#fig_11">Figure 3</ref>. Notably, Jensen-Shannon divergence has previously shown promise for test-time robustness to domain shift <ref type="bibr" target="#b5">[6]</ref>, here we further argue for its training-time robustness to label noise. The key contributions of this work 2 are:</p><p>? We make a novel observation that a network predictions' consistency is reduced for noisylabeled data when overfitting to noise, which motivates the use of consistency regularization. ? We propose using Jensen-Shannon divergence (JS) and its multi-distribution generalization (GJS) as loss functions for learning with noisy labels. We relate JS to loss functions that are based on the noise-robustness theory of Ghosh et al. <ref type="bibr" target="#b1">[2]</ref>. In particular, we prove that JS generalizes CE and MAE. Furthermore, we prove that GJS generalizes JS by incorporating consistency regularization in a single principled loss function. ? We provide an extensive set of empirical evidences on several datasets, noise types and rates.</p><p>They show state-of-the-art results and give in-depth studies of the proposed losses. (c) Consistency Noisy <ref type="figure" target="#fig_8">Figure 1</ref>: Evolution of a trained network's consistency as it overfits to noise using CE loss. Here we plot the evolution of the validation accuracy (a) and network's consistency (as measured by GJS) on clean (b) and noisy (c) examples of the training set of CIFAR-100 for varying symmetric noise rates when learning with the cross-entropy loss. The consistency of the learnt function and the accuracy closely correlate. This suggests that enforcing consistency may help avoid fitting to noise. Furthermore, the consistency is degraded more significantly for the noisy data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generalized Jensen-Shannon Divergence</head><p>We propose two loss functions, the Jensen-Shannon divergence (JS) and its multi-distribution generalization (GJS). In this section, we first provide background and two observations that motivate our proposed loss functions. This is followed by definition of the losses, and then we show that JS generalizes CE and MAE similarly to other robust loss functions. Finally, we show how GJS generalizes JS to incorporate consistency regularization into a single principled loss function. We provide proofs of all theorems, propositions, and remarks in this section in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background &amp; Motivation</head><p>Supervised Classification. Assume a general function class <ref type="bibr" target="#b2">3</ref> F where each f ? F maps an input x ? X to the probability simplex ? K?1 , i.e. to a categorical distribution over K classes y ? Y = {1, 2, . . . , K}. We seek f * ? F that minimizes a risk R L (f ) = E D [L(e (y) , f (x))], for some loss function L and joint distribution D over X ? Y, where e (y) is a K-vector with one at index y and zero elsewhere. In practice, D is unknown and, instead, we use S = {(x i , y i )} N i=1 which are independently sampled from D to minimize an empirical risk 1 N N i=1 L(e (yi) , f (x i )). Learning with Noisy Labels. In this work, the goal is to learn from a noisy training distribution D ? where the labels are changed, with probability ?, from their true distribution D. The noise is called instance-dependent if it depends on the input, asymmetric if it dependents on the true label, and symmetric if it is independent of both x and y. Let f * ? be the optimizer of the noisy distribution risk R ? L (f ). A loss function L is then called robust if f * ? also minimizes R L . The MAE loss (L M AE (e (y) , f (x)) := e (y) ? f (x) 1 ) is robust but not CE <ref type="bibr" target="#b1">[2]</ref>.</p><p>Issue of Underfitting. Several works propose such robust loss functions and demonstrate their efficacy in preventing noise fitting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. However, all those works have observed slow convergence of such robust loss functions leading to underfitting. This can be contrasted with CE that has fast convergence but overfits to noise. Ghosh et al. <ref type="bibr" target="#b1">[2]</ref> mentions slow convergence of MAE and GCE <ref type="bibr" target="#b2">[3]</ref> extensively analyzes the undefitting thereof. SCE <ref type="bibr" target="#b3">[4]</ref> reports similar problems for the reverse cross entropy and proposes a linear combination with CE. Finally, Ma et al. <ref type="bibr" target="#b4">[5]</ref> observe the same problem and consider a combination of "active" and "passive" loss functions.</p><p>Consistency Regularization. This encourages a network to have consistent predictions for different perturbations of the same image, which has mainly been used for semi-supervised learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>Motivation. In <ref type="figure" target="#fig_8">Figure 1</ref>, we show the validation accuracy and a measure of consistency during training with the CE loss for varying amounts of noise. First, we note that training with CE loss eventually overfits to noisy labels. <ref type="figure" target="#fig_8">Figure 1a</ref>, indicates that the higher the noise rate, the more accuracy drop when it starts to overfit to noise. <ref type="figure" target="#fig_8">Figure 1</ref> For low values of ? 1 , L JS behaves like CE and for increasing values of ? 1 it behaves more like the noise robust MAE loss. <ref type="figure" target="#fig_11">Figure 3</ref>: GJS Dissection for M=K=3: The decomposition of L GJS (left) into a JS term (middle) and a consistency term (right) from Proposition 2. Each point in the simplex correspond to a p (3) ? ? 2 , where the color represents the value of the loss at that point. It can be seen that there are two ways to minimize L GJS , either by making the predictions similar to the label (middle) or similar to the other predictions (right) to increase consistency. To better highlight the variations of the losses, each loss has its own range of values. Appendix B.6 for more details. A clear correlation is observed between the accuracy and consistency of the noisy examples. This suggests that maximizing consistency of predictions may improve the robustness to noise. Next, we define simple loss functions that (i) encourage consistency around data points and (ii) alleviate the "issue of underfitting" by interpolating between CE and MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definitions</head><p>D JS . Let p <ref type="bibr" target="#b0">(1)</ref> , p (2) ? ? K?1 have corresponding weights ? = [? 1 , ? 2 ] T ? ?. Then, the Jensen-Shannon divergence between p <ref type="bibr" target="#b0">(1)</ref> and p <ref type="bibr" target="#b1">(2)</ref> is D JS? (p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) := H(m) ? ? 1 H(p <ref type="bibr" target="#b0">(1)</ref> ) ? ? 2 H(p (2) ) = ? 1 D KL (p (1) m) + ? 2 D KL (p (2) m) <ref type="bibr" target="#b0">(1)</ref> with H the Shannon entropy, and m = ? 1 p (1) + ? 2 p <ref type="bibr" target="#b1">(2)</ref> . Unlike Kullback-Leibler divergence (D KL (p (1) p (2) )) or cross entropy (CE), JS is symmetric, bounded, does not require absolute continuity, and has a crucial weighting mechanism (?), as we will see later.</p><p>D GJS . Similar to D KL , D JS satisfies D JS? (p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) ? 0, with equality iff p (1) = p <ref type="bibr" target="#b1">(2)</ref> . For D JS , this is derived from Jensen's inequality for the concave Shannon entropy. This property holds for finite number of distributions and motivates a generalization of D JS to multiple distributions <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_0">D GJS? (p (1) , . . . , p (M ) ) := H M i=1 ? i p (i) ? M i=1 ? i H(p (i) ) = M i=1 ? i D KL p (i) M j=1 ? j p (j) (2)</formula><p>where M is the number of distributions, and ? = [? 1 , . . . , ? M ] T ? ? M ?1 .</p><p>Loss functions. We aim to use D JS and D GJS divergences, to measure deviation of the predictive distribution(s), f (x), from the target distribution, e (y) . Without loss of generality, hereafter, we dedicate p <ref type="bibr" target="#b0">(1)</ref> to denote the target distribution. JS loss, therefore, can take the form of D JS? (e (y) , f (x)). Generalized JS loss is a less straight-forward construction since D GJS can accommodate more predictive distributions. While various choices can be made for these distributions, in this work, we consider predictions associated with different random perturbations of a sample, denoted by A(x). This choice, as shown later, implies an interesting analogy to consistency regularization. The choice, also entails no distinction between the M ? 1 predictive distributions. Therefore, we consider ? 2 = ? ? ? = ? M = 1??1 M ?1 in all our experiments. Finally, we scale the loss functions by a constant factor Z = ?(1 ? ? 1 ) log(1 ? ? 1 ). As we will see later, the role of this scaling is merely to strengthen the already existing and desirable behaviors of these losses as ? 1 approaches zero and one. Formally, we have JS and GJS losses:</p><formula xml:id="formula_1">L JS (y, f, x) := D JS? (e (y) , f (x)) Z , L GJS (y, f, x) := D GJS? (e (y) , f (x (2) ), . . . , f (x (M ) )) Z<label>(3)</label></formula><p>withx (i) ? A(x). Next, we study the connection between JS and losses which are based on the robustness theory of Ghosh et al. <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">JS's Connection to Robust Losses</head><p>Cross Entropy (CE) is the prevalent loss function for deep classifiers with remarkable successes. However, CE is prone to fitting noise <ref type="bibr" target="#b8">[9]</ref>. On the other hand, Mean Absolute Error (MAE) is theoretically noise-robust <ref type="bibr" target="#b1">[2]</ref>. Evidently, standard optimization algorithms struggle to minimize MAE, especially for more challenging datasets e.g. CIFAR-100 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Therefore, there have been several proposals that combine CE and MAE, such as Generalized CE (GCE) <ref type="bibr" target="#b2">[3]</ref>, Symmetric CE (SCE) <ref type="bibr" target="#b3">[4]</ref>, and Normalized CE (NCE+MAE) <ref type="bibr" target="#b4">[5]</ref>. The rationale is for CE to help with the learning dynamics of MAE. Next, we show JS has CE and MAE as its asymptotes w.r.t. ? 1 .</p><formula xml:id="formula_2">Proposition 1. Let p ? ? K?1 , then lim ?1?0 L JS (e (y) , p) = H(e (y) , p), lim ?1?1 L JS (e (y) , p) = 1 2 e (y) ? p 1</formula><p>where H(e (y) , p) is the cross entropy of e (y) relative to p. <ref type="figure" target="#fig_1">Figure 2</ref> depicts how JS interpolates between CE and MAE for ? 1 ? (0, 1). The proposition reveals an interesting connection to state-of-the-art robust loss functions, however, there are important differences. SCE is not bounded (so it cannot be used in Theorem 1), and GCE is not symmetric, while JS and MAE are both symmetric and bounded. In Appendix B.3, we perform a dissection to better understand how these properties affect learning with noisy labels. GCE is most similar to JS and is compared further in Appendix B.4.</p><p>A crucial difference to these other losses is that JS naturally extends to multiple predictive distributions (GJS). Next, we show how GJS generalizes JS by incorporating consistency regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">GJS's Connection to Consistency Regularization</head><p>In <ref type="figure" target="#fig_8">Figure 1</ref>, it was shown how the consistency of the noisy labeled examples was reduced when the network overfitted to noise. The following proposition shows how GJS naturally encourages consistency in a single principled loss function.</p><formula xml:id="formula_3">Proposition 2. Let p (2) , . . . , p (M ) ? ? K?1 with M ? 3 andp &gt;1 = M j=2 ?j p (j) 1??1</formula><p>, then L GJS (e (y) , p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) = L JS ? (e (y) ,p &gt;1 ) + (1 ? ? 1 )L GJS ? (p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) )</p><p>where ? = [? 1 , 1 ? ? 1 ] T and ? = [?2,...,? M ] T (1??1) .</p><p>Importantly, Proposition 2 shows that GJS can be decomposed into two terms: 1) a JS term between the label and the mean predictionp &gt;1 , and 2) a GJS term, but without the label. <ref type="figure" target="#fig_11">Figure 3</ref> illustrates the effect of this decomposition. The first term, similarly to the standard JS loss, encourages the predictions' mean to be closer to the label <ref type="figure" target="#fig_11">(Figure 3</ref> middle). However, the second term encourages all predictions to be similar, that is, consistency regularization (Figure 3 right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Noise Robustness</head><p>Here, the robustness properties of JS and GJS are analyzed in terms of lower (B L ) and upper bounds (B U ) for the following theorem, which generalizes the results by Zhang et al. <ref type="bibr" target="#b2">[3]</ref> to any bounded loss function, even with multiple predictive distributions.</p><formula xml:id="formula_4">Theorem 1. Under symmetric noise with ? &lt; K?1 K , if B L ? K i=1 L(e (i) , x, f ) ? B U , ?x, f is satisfied for a loss L, then 0 ? R ? L (f * ) ? R ? L (f * ? ) ? ? B U ? B L K ? 1 , and ? ?(B U ? B L ) K ? 1 ? ?K ? R L (f * ) ? R L (f * ? ) ? 0,</formula><p>A tighter bound B U ?B L , implies a smaller worst case risk difference of the optimal classifiers (robust when B U = B L ). Importantly, while L(e (i) , x, f ) = L(e (i) , f (x)) usually, this subtle distinction is useful for losses with multiple predictive distributions, see Equation <ref type="bibr" target="#b2">3</ref>. In Theorem 2 in Appendix C.3, we further prove the robustness of the proposed losses to asymmetric noise.</p><p>For losses with multiple predictive distributions, the bounds in Theorem 1 and 2 must hold for any x and f , i.e., for any combination of M ? 1 categorical distributions on K classes. Proposition 3 provides such bounds for GJS.</p><formula xml:id="formula_5">Proposition 3. GJS loss with M ? K + 1 satisfies B L ? K k=1 L GJS (e (k)</formula><p>, p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) ? B U for all p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ? ? K?1 , with the following bounds</p><formula xml:id="formula_6">B L = K k=1 L GJS (e (k) , u, . . . , u), B U = K k=1 L GJS (e (k) , e (1) , . . . , e (M ?1) ) where u ? ? K?1 is the uniform distribution.</formula><p>Note the bounds for the JS loss is a special case of Proposition 3 for M = 2. Remark 1. L JS and L GJS are robust (B L = B U ) in the limit of ? 1 ? 1.</p><p>Remark 1 is intuitive from Section 2.3 which showed that L JS is equivalent to the robust MAE in this limit and that the consistency term in Proposition 2 vanishes.</p><p>In Proposition 3, the lower bound (B L ) is the same for JS and GJS. However, the upper bound (B U ) increases for more distributions, which makes JS have a tighter bound than GJS in Theorem 1 and 2. In Proposition 4, we show that JS and GJS have the same bound for the risk difference, given an assumption based on <ref type="figure" target="#fig_8">Figure 1</ref> that the optimal classifier on clean data (f * ) is at least as consistent as the optimal classifier on noisy data (f * ? ). Proposition 4. L JS and L GJS have the same risk bounds in Theorem 1 and 2 if</p><formula xml:id="formula_7">E x [L f * GJS ? (p (2) , . . . , p (M ) )] ? E x [L f * ? GJS ? (p (2) , . . . , p (M ) )], where L f GJS ? (p (2) , . . . , p (M ) )</formula><p>is the consistency term from Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Works</head><p>Interleaved in the previous sections, we covered most-related works to us, i.e. the avenue of identification or construction of theoretically-motivated robust loss functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. These works, similar to this paper, follow the theoretical construction of Ghosh et al. <ref type="bibr" target="#b1">[2]</ref>. Furthermore, Liu&amp;Guo <ref type="bibr" target="#b9">[10]</ref> use "peer prediction" to propose a new family of robust loss functions. Different to these works, here, we propose loss functions based on D JS which holds various desirable properties of those prior works while exhibiting novel ties to consistency regularization; a recent important regularization technique.</p><p>Next, we briefly cover other lines of work. A more thorough version can be found in Appendix D.</p><p>A direction, that similar to us does not alter training, reweights a loss function by confusion matrix <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Assuming a class-conditional noise model, loss correction is theoretically motivated and perfectly orthogonal to noise-robust losses.</p><p>Consistency regularization is a recent technique that imposes smoothness in the learnt function for semi-supervised learning <ref type="bibr" target="#b6">[7]</ref> and recently for noisy data <ref type="bibr" target="#b15">[16]</ref>. These works use different complex pipelines for such regularization. GJS encourages consistency in a simple way that exhibits other desirable properties for learning with noisy labels. Importantly, Jensen-Shannon-based consistency loss functions have been used to improve test-time robustness to image corruptions <ref type="bibr" target="#b5">[6]</ref> and adversarial examples <ref type="bibr" target="#b16">[17]</ref>, which further verifies the general usefulness of GJS. In this work, we study such loss functions for a different goal: training-time label-noise robustness. In this context, our thorough analytical and empirical results are, to the best of our knowledge, novel.</p><p>Recently, loss functions with information-theoretic motivations have been proposed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. JS, with an apparent information-theoretic interpretation, has a strong connection to those. Especially, the latter is a close concurrent work studying JS and other divergences from the family of f-divergences <ref type="bibr" target="#b19">[20]</ref>. However, in this work, we consider a generalization to more than two distributions and study the role of ? 1 , which they treat as a constant (? 1 = 1 2 ). These differences lead to improved performance and novel theoretical results, e.g., Proposition 1 and 2. Lastly, another generalization of JS was recently presented by Nielsen <ref type="bibr" target="#b20">[21]</ref>, where the arithmetic mean is generalized to abstract means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section, first, empirically investigates the effectiveness of the proposed losses for learning with noisy labels on synthetic (Section 4.1) and real-world noise (Section 4.2). This is followed by several experiments and ablation studies (Section 4.3) to shed light on the properties of JS and GJS through empirical substantiation of the theories and claims provided in Section 2. All these additional experiments are done on the more challenging CIFAR-100 dataset. <ref type="table">Table 1</ref>: Synthetic Noise Benchmark on CIFAR. We reimplement other noise-robust loss functions into the same learning setup and ResNet-34, including label smoothing (LS), Bootstrap (BS), Symmetric CE (SCE), Generalized CE (GCE), and Normalized CE (NCE+RCE). We used same hyperparameter optimization budget and mechanism for all the prior works and ours. Mean test accuracy and standard deviation are reported from five runs and the statistically-significant top performers are boldfaced. The thorough analysis is evident from the higher performance of CE in our setup compared to prior works. GJS achieves state-of-the-art results for different noise rates, types, and datasets. Generally, GJS's efficacy is more evident for the more challenging CIFAR-100 dataset. Experimental Setup. We use ResNet 34 and 50 for experiments on CIFAR and WebVision datasets respectively and optimize them using SGD with momentum. The complete details of the training setup can be found in Appendix A. Most importantly, we take three main measures to ensure a fair and reliable comparison throughout the experiments: 1) we reimplement all the loss functions we compare with in a single shared learning setup, 2) we use the same hyperparameter optimization budget and mechanism for all the prior works and ours, and 3) we train and evaluate five networks for individual results, where in each run the synthetic noise, network initialization, and data-order are differently randomized. The thorough analysis is evident from the higher performance of CE in our setup compared to prior works. Where possible, we report mean and standard deviation and denote the statistically-significant top performers with student t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Noise Benchmarks: CIFAR</head><p>Here, we evaluate the proposed loss functions on the CIFAR datasets with two types of synthetic noise: symmetric and asymmetric. For symmetric noise, the labels are, with probability ?, resampled from a uniform distribution over all labels. For asymmetric noise, we follow the standard setup of Patrini et al. <ref type="bibr" target="#b21">[22]</ref>. For CIFAR-10, the labels are modified, with probability ?, as follows: truck ? automobile, bird ? airplane, cat ? dog, and deer ? horse. For CIFAR-100, labels are, with probability ?, cycled to the next sub-class of the same "super-class", e.g. the labels of super-class "vehicles 1" are modified as follows: bicycle ? bus ? motorcycle ? pickup truck ? train ? bicycle.</p><p>We compare with other noise-robust loss functions such as label smoothing (LS) <ref type="bibr" target="#b22">[23]</ref>, Bootstrap (BS) <ref type="bibr" target="#b23">[24]</ref>, Symmetric Cross-Entropy (SCE) <ref type="bibr" target="#b3">[4]</ref>, Generalized Cross-Entropy (GCE) <ref type="bibr" target="#b2">[3]</ref>, and the NCE+RCE loss of Ma et al. <ref type="bibr" target="#b4">[5]</ref>. Here, we do not compare to methods that propose a full pipeline since, first, a conclusive comparison would require re-implementation and individual evaluation of several components and second, robust loss functions can be considered orthogonal to them.</p><p>Results. <ref type="table">Table 1</ref> shows the results for symmetric and asymmetric noise on CIFAR-10 and CIFAR-100. GJS performs similarly or better than other methods for different noise rates, noise types, and data sets. Generally, GJS's efficacy is more evident for the more challenging CIFAR-100 dataset. For example, on 60% uniform noise on CIFAR-100, the difference between GJS and the second best (GCE) is 4.94 percentage points, while our results on 80% noise is lower than GCE. We attribute this to the high sensitivity of the results to the hyperparameter settings in such a high-noise rate which are also generally unrealistic (WebVision has ?20%). The performance of JS is consistently similar to the top performance of the prior works across different noise rates, types and datasets. In Section 4.3, we substantiate the importance of the consistency term, identified in Proposition 2, when going from JS to GJS that helps with the learning dynamics and reduce the susceptibility to noise. In Appendix B.1, we provide results for GJS on instance-dependent synthetic noise <ref type="bibr" target="#b24">[25]</ref>. Next, we test the proposed losses on a naturally-noisy dataset to see their efficacy in a real-world scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-World Noise Benchmark: WebVision</head><p>WebVision v1 is a large-scale image dataset collected by crawling Flickr and Google, which resulted in an estimated 20% of noisy labels <ref type="bibr" target="#b27">[28]</ref>. There are 2.4 million images of the same thousand classes as ILSVRC12. Here, we use a smaller version called mini WebVision <ref type="bibr" target="#b28">[29]</ref> consisting of the first 50 classes of the Google subset. We compare CE, JS, and GJS on WebVision following the same rigorous procedure as for the synthetic noise. However, upon request by the reviewers, we also compare with the reported results of some state-of-the-art elaborate techniques. This comparison deviates from our otherwise systematic analysis.</p><p>Results. <ref type="table" target="#tab_1">Table 2</ref>, as the common practice, reports the performances on the validation sets of WebVision and ILSVRC12 (first 50 classes). Both JS and GJS exhibit large margins with standard CE, especially for top-1 accuracy. Top-5 accuracy, due to its admissibility of wrong top predictions, can obscure the susceptibility to noise-fitting and thus indicates smaller but still significant improvements.</p><p>The two state-of-the-art methods on this dataset were DivideMix <ref type="bibr" target="#b15">[16]</ref> and ELR+ <ref type="bibr" target="#b26">[27]</ref>. Compared to our setup, both these methods use a stronger network (Inception-ResNet-V2 vs ResNet-50), stronger augmentations (Mixup vs color jittering) and co-train two networks. Furthermore, ELR+ uses an exponential moving average of weights and DivideMix treats clean and noisy labeled examples differently after separating them using Gaussian mixture models. Despite these differences, GJS performs as good or better in terms of top-1 accuracy on WebVision and significantly outperforms ELR+ on ILSVRC12 (70.29 vs 74.33). The importance of these differences becomes apparent as 1) the top-1 accuracy for DivideMix degrades when using ResNet-50, and 2) the performance of GJS improves by adding one of their components, i.e. the use of two networks. We train an ensemble of two independent networks with the GJS loss and average their predictions (last row of <ref type="table" target="#tab_1">Table 2</ref>). This simple extension, which requires no change in the training code, gives significant improvements. To the best of our knowledge, this is the highest reported top-1 accuracy on WebVision and ILSVRC12 when no pre-training is used.</p><p>In Appendix B.2, we show state-of-the-art results when using GJS on two other real-world noisy datasets: ANIMAL-10N <ref type="bibr" target="#b29">[30]</ref> and Food-101N <ref type="bibr" target="#b30">[31]</ref>.</p><p>So far, the experiments demonstrated the robustness of the proposed loss function (regarding Proposition 3) via the significant improvement of the final accuracy on noisy datasets. While this was central and informative, it is also important to investigate whether this improvement comes from the theoretical properties that were argued for JS and GJS. In what follows, we devise several such experiments, in an effort to substantiate the theoretical claims and conjectures.  Validation Accuracy(%) 0% 20% 40% 60% <ref type="figure">Figure 5</ref>: Effect of M. Validation accuracy for increasing number of distributions (M ) and different symmetric noise rates on CIFAR-100 with ? 1 = 1 2 . For all noise rates, using three instead of two distributions results in a higher accuracy. Going beyond three distributions is only helpful for lower noise rates. For simplicity we use M = 3 (corresponding to two augmentations) for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Towards a Better Understanding of the Jensen-Shannon-based Loss Functions</head><p>Here, we study the behavior of the losses for different distribution weights ? 1 , number of distributions M , and epochs. We also provide insights on why GJS performs better than JS.</p><p>How does ? 1 control the trade-off of robustness and learnability? In <ref type="figure" target="#fig_2">Figure 4</ref>, we plot the validation accuracy during training for both JS and GJS at different values of ? 1 and noise rates ?. From Proposition 1, we expect JS to behave as CE for low values of ? 1 and as MAE for larger values of ? 1 . <ref type="figure" target="#fig_2">Figure 4</ref> (a-b) confirms this. Specifically, ? 1 = 0.1 learns quickly and performs well for low noise but overfits for ? = 0.6 (characteristic of non-robust CE), on the other hand, ? 1 = 0.9 learns slowly but is robust to high noise rates (characteristic of noise-robust MAE).</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> (c-d), we observe three qualitative improvements of GJS over JS: 1) no signs of overfitting to noise for large noise rates with low values of ? 1 , 2) better learning dynamics for large values of ? 1 that otherwise learns slowly, and 3) converges to a higher validation accuracy.</p><p>How many distributions to use? <ref type="figure">Figure 5</ref> depicts validation accuracy for varying number of distributions M . For all noise rates, we observe a performance increase going from M = 2 to M = 3. However, the performance of M &gt; 3 depends on the noise rate. For lower noise rates, having more than three distributions can improve the performance. For higher noise rates e.g. 60%, having M &gt; 3 degrades the performance. We hypothesise this is due to: 1) at high noise rates, there are only a few correctly labeled examples that can help guide the learning, and 2) going from M = 2 to M = 3 adds a consistency term, while M &gt; 3 increases the importance of the consistency term in Proposition 2. Therefore, for a large enough M, the loss will find it easier to keep the consistency term low (keep predictions close to uniform as at the initialization), instead of generalizing based on the few clean examples. For simplicity, we have used M = 3 for all experiments with GJS.</p><p>Is the improvements of GJS over JS due to mean prediction or consistency? Proposition 2 decomposed GJS into a JS term with a mean prediction (p &gt;1 ) and a consistency term operating on all distributions but the target. In <ref type="table">Table 3</ref>, we compare the performance of JS and GJS to GJS without the consistency term, i.e., L JS ? (e (y) ,p &gt;1 ). The results suggest that the improvement of GJS over JS can be attributed to the consistency term. <ref type="figure" target="#fig_2">Figure 4</ref> (a-b) showed that JS improves the learning dynamics of MAE by blending it with CE, controlled by ? 1 . Similarly, we see here that the consistency term also improves the learning dynamics (underfitting and convergence speed) of MAE. Interestingly, <ref type="figure" target="#fig_2">Figure 4</ref> (c-d), shows the higher values of ? 1 (closer to MAE) work best for GJS, hinting that, the consistency term improves the learning dynamics of MAE so much so that the role of CE becomes less important. <ref type="table">Table 3</ref>: Effect of Consistency. Validation accuracy for JS, GJS w/o the consistency term in Proposition 2, and GJS for 40% noise on the CIFAR-100 dataset. Using the mean of two predictions in the JS loss does not improve performance. On the other hand, adding the consistency term significantly helps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy How is different choices of perturbations affecting GJS? In this work, we use stochastic augmentations for A, see Appendix A.1 for details. <ref type="table" target="#tab_3">Table 5</ref> reports validation results on 40% symmetric and asymmetric noise on CIFAR-100 for varying types of augmentation. We observe that all methods improve their performance with stronger augmentation and that GJS achieves the best results in all cases. Also, note that we use weak augmentation for all naturally-noisy datasets (WebVision, ANIMAL-10N, and Food-101N) and still get state-of-the-art results.</p><formula xml:id="formula_8">LJS(e (y) , p (2) ) 71.0 LJS ? (e (y) ,p&gt;1) 68.7 LGJS(e (y) , p (2) , p (3) ) 74.3</formula><p>How fast is the convergence? We found that some baselines (especially the robust NCE+RCE) had slow convergence. Therefore, we used 400 epochs for all methods to make sure all had time to converge properly. <ref type="table" target="#tab_4">Table 6</ref> shows results on 40% symmetric and asymmetric noise on CIFAR-100 when the number of epochs has been reduced by half.</p><p>Is training with the proposed losses leading to more consistent networks? Our motivation for investigating losses based on Jensen-Shannon divergence was partly due to the observation in <ref type="figure" target="#fig_8">Figure 1</ref> that consistency and accuracy correlate when learning with CE loss. In <ref type="figure" target="#fig_4">Figure 6</ref>, we compare CE, JS, and GJS losses in terms of validation accuracy and consistency during training on CIFAR-100 with 40% symmetric noise. We find that the networks trained with JS and GJS losses are more consistent and has higher accuracy. In Appendix B.7, we report the consistency of the networks in <ref type="table">Table 1</ref>.</p><p>Summary of experiments in the appendix. Due to space limitations, we report several important experiments in the appendix. We evaluate the effectiveness of GJS on 1) instance-dependent synthetic noise (Section B.1), and 2) real-world noisy datasets ANIMAL-10N and Food-101N (Section B.2). We also investigate the importance of 1) losses being symmetric and bounded for learning with noisy labels (Section B.3), and 2) a clean vs noisy validation set for hyperparameter selection and the effect of a single set of parameters for all noise rates (Section B.5).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations &amp; Future Directions</head><p>We empirically showed that the consistency of the network around noisy data degrades as it fits noise and accordingly proposed a loss based on generalized Jensen-Shannon divergence (GJS). While we empirically verified the significant role of consistency regularization in robustness to noise, we only theoretically showed the robustness (B L = B U ) of GJS at its limit (? 1 ? 1) where the consistency term gradually vanishes. Therefore, the main limitation is the lack of a theoretical proof of the robustness of the consistency term in Proposition 2. This is, in general, an important but understudied area, also for the literature of self-or semi-supervised learning and thus is of utmost importance for future works.</p><p>Secondly, we had an important observation that GJS with M &gt; 3 might not perform well under high noise rates. While we have some initial conjectures, this phenomenon deserves a systematic analysis both empirically and theoretically.</p><p>Finally, a minor practical limitation is the added computations for GJS forward passes, however this applies to training time only and in all our experiments, we only use one extra prediction (M = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Final Remarks</head><p>We first made two central observations that (i) robust loss functions have an underfitting issue and (ii) consistency of noise-fitting networks is significantly lower around noisy data points. Correspondingly, we proposed two loss functions, JS and GJS, based on Jensen-Shannon divergence that (i) interpolates between noise-robust MAE and fast-converging CE, and (ii) encourages consistency around training data points. This simple proposal led to state-of-the-art performance on both synthetic and real-world noise datasets even when compared to the more elaborate pipelines such as DivideMix or ELR+. Furthermore, we discussed their robustness within the theoretical construction of Ghosh et al. <ref type="bibr" target="#b1">[2]</ref>. By drawing further connections to other seminal loss functions such as CE, MAE, GCE, and consistency regularization, we uncovered other desirable or informative properties. We further empirically studied different aspects of the losses that corroborate various theoretical properties.</p><p>Overall, we believe the paper provides informative theoretical and empirical evidence for the usefulness of two simple and novel JS divergence-based loss functions for learning under noisy data that achieve state-of-the-art results. At the same time, it opens interesting future directions.</p><p>Ethical Considerations. Considerable resources are needed to create labeled data sets due to the burden of manual labeling process. Thus, the creators of large annotated datasets are mostly limited to well-funded companies and academic institutions. In that sense, developing robust methods against label noise enables less affluent organizations or individuals to benefit from labeled datasets since imperfect or automatic labeling can be used instead. On the other hand, proliferation of such harvested datasets can increase privacy concerns arising from redistribution and malicious use. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>All proposed losses and baselines use the same training settings, which are described in detail here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 CIFAR</head><p>General training details. For all the results on the CIFAR datasets, we use a PreActResNet-34 with a standard SGD optimizer with Nesterov momentum, and a batch size of 128. For the network, we use three stacks of five residual blocks with 32, 64, and 128 filters for the layers in these stacks, respectively. The learning rate is reduced by a factor of 10 at 50% and 75% of the total 400 epochs. For data augmentation, we use RandAugment <ref type="bibr" target="#b31">[32]</ref> with N = 1 and M = 3 using random cropping (size 32 with 4 pixels as padding), random horizontal flipping, normalization and lastly Cutout <ref type="bibr" target="#b32">[33]</ref> with length 16. We set random seeds for all methods to have the same network weight initialization, order of data for the data loader, train-validation split, and noisy labels in the training set. We use a clean validation set corresponding to 10% of the training data. A clean validation set is commonly provided with real-world noisy datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. Any potential gain from using a clean instead of a noisy validation set is the same for all methods since all share the same setup.</p><p>JS and GJS implementation. We implement the Jensen-Shannon-based losses using the definitions based on KL divergence, see Equation 2. To make sure the gradients are propagated through the target argument, we do not use the KL divergence in PyTorch. Instead, we write our own based on the official implementation.</p><p>Search for learning rate and weight decay. We do a separate hyperparameter search for learning rate and weight decay on 40% noise using both asymmetric and symmetric noises on CIFAR datasets. For CIFAR-10, we search for learning rates in , JS(? 1 ) and GJS(? 1 ), respectively. Note that, these fixed method-specific hyperparameters for both CIFAR-10 and CIFAR-100 are taken from their corresponding papers for this initial search of learning rate and weight decay but they will be further optimized systematically in the next steps.</p><p>Search for method-specific parameters. We fix the obtained best learning rate and weight decay for all other noise rates, but then for each noise rate/type, we search for method-specific parameters.</p><p>For the methods with a single hyperparameter, BS (?), LS ( ), GCE (q), JS (? 1 ), GJS (? 1 ), we try values in [0.1, 0.3, 0.5, 0.7, 0.9]. On the other hand, NCE+RCE and SCE have three hyperparameters, i.e. ? and ? that scale the two loss terms, and A := log(0) for the RCE term. We set A = log (1e ? 4) and do a grid search for three values of ? and two of beta ? (six in total) around the best reported parameters from each paper. <ref type="bibr" target="#b3">4</ref> Test evaluation. The best parameters are then used to train on the full training set with five different seeds. The final parameters that were used to get the results in <ref type="table">Table 1</ref> are shown in <ref type="table" target="#tab_7">Table 7</ref>.</p><p>For completeness, in Appendix B.5, we provide results for a less thorough hyperparameter search(more similar to related work) which also use a noisy validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 WebVision</head><p>All The learning rate is reduced by a multiplicative factor of 0.97 every epoch, and we train for a total of 300 epochs. The best starting learning rates were 0.4, 0.2, 0.1 for CE, JS and GJS, respectively. Both JS and GJS used ? 1 = 0.1. With the best learning rate and ? 1 , we ran four more runs with new seeds for the network initialization and data loader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments and Insights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Instance-Dependent Synthetic Noise</head><p>In Section 4.1, we showed results on two types of synthetic noise: symmetric (?) and asymmetric (?(y)). Although these noise types are simple to empirically and theoretically analyze, they might be different from noise observed in real-world datasets. Recently, a new type of synthetic noise has been proposed by Zhang et al. <ref type="bibr" target="#b24">[25]</ref>, where the risks of mislabeling an example of class i to class j vary per example (? ij (x)). This type of noise is called instance-dependent and is more similar the noise in real-world datasets.</p><p>In <ref type="table">Table 8</ref>, we compare CE, Generalized CE (GCE) and GJS on three different types of 35% instancedependent noise on the CIFAR datasets. The training setup is the same as for the results in <ref type="table">Table  1</ref>, described in detail in Section A.1. For all methods, we search for the best hyperparameters on the Type-I noise and use the same settings for the other two types. For CIFAR-10, the optimal hyperparameters (learning rate, weight decay, method-specific) were: (0.1, 1e-3, -), (0.005, 1e-3, 0.9), (0.001, 5e-4, 0.5) for CE, GCE, and GJS, respectively. For CIFAR-100, they were: (0.1, 5e-4, -), (0.4, 5e-5, 0.7), (0.1, 5e-4, 0.3) for CE, GCE, and GJS, respectively.</p><p>On the simpler CIFAR-10, GCE and GJS perform similarly, but on the more challenging CIFAR-100, GJS significantly outperform GCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Real-World Noise: ANIMAL-10N &amp; Food-101N</head><p>Here, we evaluate GJS on two naturally-noisy datasets: ANIMAL-10N <ref type="bibr" target="#b29">[30]</ref> and Food-101N <ref type="bibr" target="#b30">[31]</ref>. <ref type="table">Table 8</ref>: Instance-Dependent Synthetic Noise Benchmark on CIFAR. We reimplement the Generalized CE (GCE) loss function into the same learning setup and a ResNet-34 network. We used same hyperparameter optimization budget and mechanism for all methods. We evaluate on 35% noise for the three types of instance-dependent synthetic noise proposed by Zhang et al. <ref type="bibr" target="#b24">[25]</ref>. Mean test accuracy and standard deviation are reported from five runs and the statistically-significant top performers are boldfaced. As for the symmetric and asymmetric synthetic noise, the efficacy of GJS is more evident on the more challenging CIFAR-100 dataset, where GJS significantly outperforms the baselines. Food-101N. The dataset contains 301k images classified as 101 different food recipes. The images were collected using Google, Bing, Yelp, and TripAdvisor. The noise rate is estimated to be 20%.</p><p>We follow the same training setup as the recent label correction method called Progressive Label Correction (PLC) <ref type="bibr" target="#b24">[25]</ref>, i.e. we use the same network architecture, augmentation strategy, optimizer, batch size, number of epochs, and learning rate scheduling. We use an initial learning rate and weight decay of 0.001, and ? 1 = 0.3.</p><p>ANIMAL-10N. The dataset contains 55k images of 10 classes. The 10 classes can be grouped into 5 pairs of similar classes that are more likely to be confused: (cat, lynx), (jaguar, cheetah), (wolf, coyote), (chimpanzee, orangutan), (hamster, guinea pig). The images were collected using Google and Bing. The noise rate is estimated to be 8%.</p><p>We use the same training setup(network, optimizer, number of epochs, learning rate scheduling, etc) as PLC, but use cropping instead of random horizontal flipping as augmentation to reduce the risk of both augmentations being equal for GJS. We use an initial learning rate of 0.05, a weight decay of 5e-4, and ? 1 = 0.5.</p><p>Results. The mean test accuracy and standard deviation from three runs for ANIMAL-10N and Food-101N are in <ref type="table" target="#tab_9">Table 9</ref> and 10, respectively. The results for all baselines are from Zhang et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>Our GJS loss outperforms all other methods on both datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Towards a better understanding of JS</head><p>In Proposition 2, we showed that JS is an important part of GJS, and therefore deserves attention.</p><p>Here, we make a systematic ablation study to empirically examine the contribution of the difference(s) between JS loss and CE. We decompose the JS loss following the gradual construction of the Jensen-Shannon divergence in the work of Lin <ref type="bibr" target="#b7">[8]</ref>. This construction, interestingly, lends significant empirical evidence to bounded losses' robustness to noise, in connection to Theorem 1 and 2 and Proposition 3.</p><p>Let KL(p, q) denote the KL-divergence of a predictive distribution q ? ? K?1 from a target distribution p ? ? K?1 . KL divergence is neither symmetric nor bounded. K divergence, proposed by Lin et al. <ref type="bibr" target="#b7">[8]</ref>, is a bounded version defined as K(p, q) := KL(p, (p + q)/2) = KL(p, m). However, this divergence is not symmetric. A simple way to achieve symmetry is to take the average   <ref type="table">Table 11</ref> are plotted during training with 40% symmetric noise on the CIFAR-100 dataset. Notably, the only two losses that show signs of overfitting (KL and Jeffrey's) are unbounded. Interestingly, K (bounded KL) makes the learning slower, while K (bounded KL ) considerably improves the learning dynamics. Finally, it can be seen that, JS, in contrast to its unbounded version (Jeffrey's), does not overfit to noise.</p><p>of forward and reverse versions of a divergence. For KL and K, this gives rise to Jeffrey's divergence and JS with ? = [ 1 2 , 1 2 ] T , respectively. <ref type="table">Table 11</ref> provides an overview of these divergences and <ref type="figure">Figure 7</ref> shows their validation accuracy during training on CIFAR-100 with 40% symmetric noise.</p><p>Bounded. Notably, the only two losses that show signs of overfitting (KL and Jeffrey's) are unbounded. Interestingly, K (bounded KL) makes the learning much slower, while K (bounded KL ) considerably improves the learning dynamics. Finally, it can be seen that, JS, in contrast to its unbounded version (Jeffrey's), does not overfit to noise.</p><p>Symmetry. The Jeffrey's divergence performs better than either of its two constituent KL terms. This is not as clear for JS, where K is performing surprisingly well on its own. In the proof of Proposition 1, we show that K ? MAE as ? 1 ? 1, while K goes to zero, which could explain why K seems to be robust to noise. Furthermore, K , which is a component of JS, is reminiscent of label smoothing.</p><p>Beside the bound and symmetry, other notable properties of JS and GJS are the connections to MAE and consistency losses. Next section investigates the effect of hyperparameters that substantiates the connection to MAE (Proposition 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Comparison between JS and GCE</head><p>We were pleasantly surprised by the finding in Proposition 1 that JS generalizes CE and MAE, similarly to GCE. Here, we highlight differences between JS and GCE.</p><p>Theoretical properties. Our inspiration to study JS came from the symmetric loss function of SCE, and the bounded loss of GCE. JS has both properties and a rich history in the field of information theory. This is also one of the reasons we studied these properties in Section B.3. Finally, JS generalizes naturally to more than two distributions.</p><p>Gradients. The gradients of CE/KL, GCE, JS and MAE with respect to logit z i of prediction p = [p 1 , p 2 , . . . , p K ], given a label e (y) , are of the form ? ?py ?zi g(p y ) with g(p y ) being 1 py , 1</p><formula xml:id="formula_9">p 1?q y , (1 ? ? 1 ) log ?1</formula><p>(1??1)py + 1 /Z, and 1, for each of these losses respectively. Note that, q is the hyperparameter of GCE and p y denotes the yth component of p. In <ref type="figure" target="#fig_7">Figure 8</ref>, these gradients are compared by varying the hyperparameter of GCE, q ? [0.1, 0.3, 0.5, 0.7, 0.9], and finding the corresponding ? for JS ? such that the two gradients are equal at p y = 1 2 . Looking at the behaviour of the different losses at low-p y regime, intuitively, a high gradient scale for low p y means a large parameter update for deviating from the given class. This can make noise free learning faster by pushing the probability to the correct class, which is what CE does. However, if the given class is incorrect (noisy) this can cause overfitting. The gradient scale of MAE induces same update magnitude for p y , which can give the network more freedom to deviate from noisy classes, at the cost of slower learning for the correctly labeled examples.</p><p>Comparing GCE and JS ? in <ref type="figure" target="#fig_7">Figure 8</ref>, it can be seen that JS ? generally penalize lower probability in the given class less than what GCE does. In this sense, JS ? behaves more like MAE.</p><p>For a derivation of the gradients of D JS , see Section C.6.</p><p>Label distributions. GCE requires the label distribution to be onehot which makes it harder to incorporate GCE in many of the elaborate state-of-the-art methods that use "soft labels" e.g., Mixup, co-training, or knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Noisy Validation Set &amp; Single Set of Parameters</head><p>Our systematic procedure to search for hyperparameters (A.1) is done to have a more conclusive comparison to other methods. The most common procedure in related works is for each dataset, all methods use the same learning rate and weight decay(chosen seemingly arbitrary), and each method uses a single set of method-specific parameters for all noise rates and types. Baselines typically use the same method-specific parameters as reported in their respective papers. First, using the same learning rate and weight decay is problematic when comparing loss functions that have different gradient magnitudes. Second, directly using the parameters reported for the baselines is also problematic since the optimal hyperparameters depend on the training setup, which could be different, e.g., network architecture, augmentation, learning rate schedule, etc. Third, using a fixed method-specific parameter for all noise rates makes the results highly dependent on this choice. Lastly, it is not possible to know if other methods would have performed better if a proper hyperparameter search was done.</p><p>Here, for completeness, we use the same setup as in Section A.1, except we use the same learning rate and weight decay for all methods and search for hyperparameters based on a noisy validation set (more similar to related work).</p><p>The learning rate and weight decay for all methods are chosen based on noisy validation accuracy for CE on 40% symmetric noise for each dataset. The optimal learning rates and weight decays([lr,wd]) were [0.05, 1e-3] and [0.4, 1e-4] for CIFAR-10 and CIFAR-100, respectively. The method-specific parameters are found by a similar search as in Section A.1, except it is only done for 40% symmetric noise and the optimal parameters are used for all other noise rates and types. For CIFAR-10, the optimal method-specific hyperparameters were 0.5, 0.5, (0.1,0.1), 0.5, (10, 0.1), 0.5, 0.3 for BS(?), LS( ), SCE(?, ?), GCE(q), NCE+RCE(?, ?), JS(? 1 ) and GJS(? 1 ), respectively. For CIFAR-100, the <ref type="table" target="#tab_1">Table 12</ref>: Synthetic Noise Benchmark on CIFAR. We reimplement other noise-robust loss functions into the same learning setup and ResNet-34, including label smoothing (LS), Bootstrap (BS), Symmetric CE (SCE), Generalized CE (GCE), and Normalized CE (NCE+RCE). We used same hyperparameter optimization budget and mechanism for all the prior works and ours. All methods use the same learning rate and weight decay and use the optimal method-specific parameters from a search on 40% symmetric noise based on noisy validation accuracy. Mean test accuracy and standard deviation are reported from five runs and the statistically-significant top performers are boldfaced. optimal method-specific hyperparameters were 0.5, 0.7, (0.1, 0.1), 0.5, (20, 0.1), 0.1, 0.5 for BS(?), LS( ), SCE(?, ?), GCE(q), NCE+RCE(?, ?), JS(? 1 ) and GJS(? 1 ), respectively. The results with this setup can be seen in <ref type="table" target="#tab_1">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Consistency Measure</head><p>In this section, we provide more details about the consistency measure used in <ref type="figure" target="#fig_8">Figure 1</ref>. To be independent of any particular loss function, we considered a measure similar to standard Top-1 accuracy. We measure the ratio of samples that predict the same class on both the original image and an augmented version of it</p><formula xml:id="formula_10">1 N N i=1 1 arg max y f (x i ) = arg max y f (x i )<label>(4)</label></formula><p>where the sum is over all the training examples, and 1 is the indicator function, the argmax is over the predicted probability of K classes, andx i ? A(x i ) is an augmented version of x i . Notably, this measure does not depend on the labels.</p><p>In the experiment in <ref type="figure" target="#fig_8">Figure 1</ref>, the original images are only normalized, while the augmented images use the same augmentation strategy as the benchmark experiments, see Section A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Consistency of Trained Networks on CIFAR</head><p>In <ref type="table" target="#tab_13">Table 13</ref>, we report the training consistency of the networks used for the main CIFAR results in <ref type="table">Table 1</ref>. We use the same consistency measure (Section B.6) as was used in <ref type="figure" target="#fig_8">Figure 1</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>. When learning with noisy labels, the networks trained with GJS is significantly more consistent than all the other methods. This is directly in line with Proposition 2, that shows how L GJS encourages consistency.</p><p>In <ref type="table">Table 1</ref>, we noticed better performance for CE compared to reported results in related work, which we mainly attribute to our thorough hyperparameter search. In <ref type="table" target="#tab_13">Table 13</ref>, we observe better consistency for CE than in <ref type="figure" target="#fig_8">Figure 1</ref>, which we believe is for the same reason. Compared to <ref type="figure" target="#fig_8">Figure 1</ref>, the networks trained with the CE loss in <ref type="table" target="#tab_13">Table 13</ref> use a higher learning rate and weight decay, both of which have a regularizing effect, which could help against overfitting to noise.  <ref type="figure" target="#fig_8">Figure 1</ref>, the consistency is reduced for all methods for increasing noise rates. When learning with noisy labels, the networks trained with GJS are the most consistent for all noise rates and datasets.  </p><p>More specifically, we have JS ? (e (y) , p) = ? 1 D KL (e (y) m)+? 2 D KL (p m), where m = ? 1 e (y) + ? 2 p, and</p><formula xml:id="formula_12">lim ?1?0 ? 1 D KL (e (y) m) H(1 ? ? 1 ) = H(e (y) , p) (7) lim ?1?1 ? 2 D KL (p m) H(1 ? ? 1 ) = 1 2 e (y) ? p 1<label>(8)</label></formula><p>First, the we prove Equations 7 and 8, then show that the other two limits are zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Equation 7</head><p>.</p><formula xml:id="formula_13">lim ?1?0 ? 1 D KL (e (y) m) H(1 ? ? 1 ) = lim ?1?0 ?? 1 log (m y ) ?(1 ? ? 1 ) log (1 ? ? 1 )<label>(9)</label></formula><p>= lim ?1?0 log (m y )</p><formula xml:id="formula_14">1 1 ? ? 1 ? 1 log (1 ? ? 1 )<label>(10)</label></formula><p>= lim ?1?0 log (m y )</p><formula xml:id="formula_15">1 1 ? ? 1 ? ?(1 ? ? 1 )<label>(11)</label></formula><p>= log p y ? 1 ? ?1 = H(e (y) , p (2) )</p><p>where we used L'H?pital's rule for lim ?1?0 ?1 log (1??1) which is indeterminate of the form 0 0 . Proof of Equation 8. Before taking the limit, we first rewrite the equation</p><formula xml:id="formula_17">? 2 D KL (p m) H(1 ? ? 1 ) = ? 1 log (1 ? ? 1 ) K k=1 p k log p k m k (13) = ? 1 log (1 ? ? 1 ) p y log p y m y + K k =y p k log p k (1 ? ? 1 )p k (14) = ? 1 log (1 ? ? 1 ) p y log p y m y ? log (1 ? ? 1 ) K k =y p k (15) = ? 1 log (1 ? ? 1 ) p y log p y m y ? log (1 ? ? 1 )(1 ? p y )<label>(16)</label></formula><formula xml:id="formula_18">= ?p y log p y m y 1 log (1 ? ? 1 ) + 1 ? p y<label>(17)</label></formula><p>Now, we take the limit</p><formula xml:id="formula_19">lim ?1?1 ? 2 D KL (p m) H(1 ? ? 1 ) = lim ?1?1 ?p y log p y m y 1 log (1 ? ? 1 ) + 1 ? p y (18) = 0 ? 0 + 1 ? p y (19) = 1 2 (1 ? p y + 1 ? p y )<label>(20)</label></formula><formula xml:id="formula_20">= 1 2 (1 ? p y + K k =y p k )<label>(21)</label></formula><formula xml:id="formula_21">= 1 2 K k=1 e (y) k ? p k = 1 2 e (y) ? p 1<label>(22)</label></formula><p>What is left to show is that the last two terms goes to zero in their respective limits.</p><formula xml:id="formula_22">lim ?1?1 ? 1 D KL (e (y) m) H(1 ? ? 1 ) = lim ?1?1 ?? 1 log (m y ) ?(1 ? ? 1 ) log (1 ? ? 1 ) (23) = lim ?1?1 ?? 1 log (? 1 + (1 ? ? 1 )p y ) ?(1 ? ? 1 ) log (1 ? ? 1 )<label>(24)</label></formula><p>= lim ?1?1 ? 1 log (1 ? ? 1 ) log (? 1 + (1 ? ? 1 )p y ) 1 ? ? 1 (25)</p><formula xml:id="formula_23">= 0 ? (p y ? 1) = 0<label>(26)</label></formula><p>Finally, the last term. Starting from Equation 17, we get</p><formula xml:id="formula_24">lim ?1?0 ? 2 D KL (p (2) m) H(1 ? ? 1 ) = lim ?1?0 ?p y log py my log (1 ? ? 1 ) + 1 ? p y (27) = lim ?1?0 ?p y ? 1 ? p y ? 1 + (1 ? ? 1 )p y ? ?(1 ? ? 1 ) + 1 ? p y (28) = lim ?1?0 ?p y (1 ? p y )(1 ? ? 1 ) ? 1 + (1 ? ? 1 )p y + 1 ? p y<label>(29)</label></formula><p>= lim ?1?0 p y ?1 + ? 1 + (1 ? ? 1 )p y ? 1 + (1 ? ? 1 )p y + 1 ? p y</p><p>= lim ?1?0 p y ?1 ? 1 + (1 ? ? 1 )p y + 1 + 1 ? p y (31)</p><formula xml:id="formula_26">= ?1 + p y + 1 ? p y = 0<label>(32)</label></formula><p>where L'H?pital's rule was used for lim ?1?0 ?p y log py my log (1??1) which is indeterminate of the form 0 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 GJS's Connection to Consistency Regularization</head><formula xml:id="formula_27">Proposition 2. Let p (2) , . . . , p (M ) ? ? K?1 with M ? 3 andp &gt;1 = M j=2 ?j p (j) 1??1</formula><p>, then</p><formula xml:id="formula_28">L GJS (e (y) , p (2) , . . . , p (M ) ) = L JS ? (e (y) ,p &gt;1 ) + (1 ? ? 1 )L GJS ? (p (2) , . . . , p (M ) )</formula><p>where ? = [? 1 , 1 ? ? 1 ] T and ? = [?2,...,? M ] T (1??1) .</p><p>Proof of Proposition 2. The Generalized Jensen-Shannon divergence can be simplified as below GJS ? (e (y) , p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) = H(? 1 e (y)</p><formula xml:id="formula_29">+ (1 ? ? 1 )p &gt;1 ) ? M j=2 ? j H(p (j) )<label>(33)</label></formula><formula xml:id="formula_30">= H(? 1 + (1 ? ? 1 )m y ) + K i =y H((1 ? ? 1 )m i ) ? M j=2 ? j H(p (j) )<label>(34)</label></formula><p>= /H(? 2 p i ) = p i H(? 2 ) + ? 2 H(p i )/</p><formula xml:id="formula_31">= H(? 1 + (1 ? ? 1 )m y ) + K i =y [m i H(1 ? ? 1 ) + (1 ? ? 1 )H(m i )] ? M j=2 ? j H(p (j) )<label>(35)</label></formula><formula xml:id="formula_32">= H(? 1 + (1 ? ? 1 )m y ) + K i =y [m i H(1 ? ? 1 )] ? (1 ? ? 1 )H(m y )<label>(36)</label></formula><formula xml:id="formula_33">+ (1 ? ? 1 ) H(p &gt;1 ) ? 1 1 ? ? 1 M j=2 ? j H(p (j) )<label>(37)</label></formula><p>= H(? 1 + (1 ? ? 1 )m y ) + K i =y</p><formula xml:id="formula_35">[m i H(1 ? ? 1 ) + (1 ? ? 1 )(H(m i ) ? H(m i ))]<label>(39)</label></formula><p>? (1 ? ? 1 )H(m y ) + (1 ? ? 1 )D GJS ? (p (2) , . . . , p (M ) )</p><formula xml:id="formula_36">(40) = / Equation 35/ (41) = H(? 1 + (1 ? ? 1 )m y ) + K i =y H((1 ? ? 1 )m i ) ? (1 ? ? 1 )H(p &gt;1 )<label>(42)</label></formula><formula xml:id="formula_37">+ (1 ? ? 1 )D GJS ? (p (2) , . . . , p (M ) ) (43) = H(? 1 e (y) + (1 ? ? 1 )p &gt;1 ) ? (1 ? ? 1 )H(p &gt;1 ) + (1 ? ? 1 )D GJS ? (p (2) , . . . , p (M ) ) (44) = D JS ? (e (y) ,p &gt;1 ) + (1 ? ? 1 )D GJS ? (p (2) , . . . , p (M ) )<label>(45)</label></formula><p>where ? = [? 1 , 1 ? ? 1 ] and ? = [? 2 , . . . , ? M ]/(1 ? ? 1 ).</p><p>That is, when using onehot labels, the generalized Jensen-Shannon divergence is a combination of two terms, one term encourages the mean prediction to be similar to the label and another term that encourages consistency between the predictions. For M = 2, the consistency term is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Noise Robustness</head><p>The proofs of the theorems in this sections are generalizations of the proofs in by Zhang et al. <ref type="bibr" target="#b2">[3]</ref>. The original theorems are specific to their particular GCE loss and cannot directly be used for other loss functions. We generalize the theorems to be useful for any loss function satisfying certain conditions(bounded and conditions in Lemma 1). To be able to use the theorems for GJS, we also generalize them to work for more than a single predictive distribution. Here, we use (x, y) to denote a sample from D and (x,?) to denote a sample from D ? . Let ? ij denote the probability that a sample of class i was changed to class j due to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Symmetric Noise</head><formula xml:id="formula_38">Theorem 1. Under symmetric noise with ? &lt; K?1 K , if B L ? K i=1 L(e (i) , x, f ) ? B U , ?x, f is satisfied for a loss L, then 0 ? R ? L (f * ) ? R ? L (f * ? ) ? ? B U ? B L K ? 1 , and ? ?(B U ? B L ) K ? 1 ? ?K ? R L (f * ) ? R L (f * ? ) ? 0,</formula><p>Proof of Theorem 1. For any function, f , mapping an input x ? X to ? K?1 , we have</p><formula xml:id="formula_39">R L (f ) = E D [L(e (y) , x, f )] = E x,y [L(e (y) , x, f )]</formula><p>and for uniform noise with noise rate ?, the probability of a class not changing label due to noise is ? ii = 1 ? ?, while the probability of changing from one class to any other is ? ij = ? K?1 . Therefore,</p><formula xml:id="formula_40">R ? L (f ) = E D? [L(e (?) , x, f )] = E x,? [L(e (?) , x, f )] = E x E y|x E? |y,x [L(e (?) , x, f )] = E x E y|x [(1 ? ?)L(e (y) , x, f ) + ? K ? 1 K i =y L(e (i) , x, f )] = E x E y|x (1 ? ?)L(e (y) , x, f ) + ? K ? 1 K i=1 L(e (i) , x, f ) ? L(e (y) , x, f ) = 1 ? ? ? ? K ? 1 R L (f ) + ? K ? 1 E x E y|x K i=1 L(e (i) , x, f ) = 1 ? ?K K ? 1 R L (f ) + ? K ? 1 E x E y|x K i=1 L(e (i) , x, f )</formula><p>Using the bounds B L ? K k=1 L(e (k) , x, f ) ? B U , we get:</p><formula xml:id="formula_41">1 ? ?K K ? 1 R L (f ) + ?B L K ? 1 ? R ? L (f ) ? 1 ? ?K K ? 1 R L (f ) + ?B U K ? 1</formula><p>With these bounds, the difference between R ? L (f * ) and R ? L (f * ? ) can be bounded as follows</p><formula xml:id="formula_42">R ? L (f * ) ? R ? L (f * ? ) ? 1 ? ?K K ? 1 R L (f * ) + ?B U K ? 1 ? 1 ? ?K K ? 1 R L (f * ? ) + ?B L K ? 1 = = 1 ? ?K K ? 1 (R L (f * ) ? R L (f * ? )) + ?(B U ? B L ) K ? 1 ? ?(B U ? B L ) K ? 1</formula><p>where the last inequality follows from the assumption on the noise rate, i) L(e (i) , p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) = 0 ?? p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) = e (i) ,</p><formula xml:id="formula_43">(1 ? ?K K?1 ) &gt; 0, and that f * is the minimizer of R L (f ) so R L (f * ) ? R L (f * ? ) ? 0. Similarly, since f * ? is the minimizer of R ? L (f ), we have R ? L (f * ) ? R ? L (f * ? ) ? 0, which is the lower bound.</formula><p>ii) 0 ? L(e (i) , p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) ? C 1 , iii) L(e (i) , e (j) , . . . , e (j) ) = C 2 ? C 1 , with i = j.</p><p>where C 1 , C 2 are constants.</p><p>Theorem 2. Let L be any loss function satisfying the conditions in Lemma 1. Under class dependent noise, when the probability of the noise not changing label is larger than changing it to any other class(? yi &lt; ? yy , for all i = y, with y being the true label), and if R ? L (f * ) = 0, then</p><formula xml:id="formula_44">0 ? R ? L (f * ) ? R ? L (f * ? ) ? (B U ? B L )E D [? yy ] + (C 1 ? C 2 )E D [ K i =y (? yy ? ? yi )],<label>(46)</label></formula><p>where</p><formula xml:id="formula_45">B L ? K i=1 L(e (i) , x, f ) ? B U for all x and f , f * is the global minimizer of R L (f ), and f * ? is the global minimizer of R ? L (f ).</formula><p>Proof of Theorem 2. For class dependent noisy(asymmetric) and any function, f , mapping an input x ? X to ? K?1 , we have</p><formula xml:id="formula_46">R ? L (f ) = E D [? yy L(e (y) , x, f )] + E D [ K i =y ? yi L(e (i) , x, f )] = E D [? yy K i=1 L(e (i) , x, f ) ? K i =y L(e (i) , x, f ) ] + E D [ K i =y ? yi L(e (i) , x, f )] = E D [? yy K i=1 L(e (i) , x, f )] ? E D [ K i =y (? yy ? ? yi )L(e (i) , x, f )]</formula><p>By using the bounds B L , B U we get</p><formula xml:id="formula_47">R ? L (f ) ? B U E D [? yy ] ? E D [ K i =y (? yy ? ? yi )L(e (i) , x, f )] R ? L (f ) ? B L E D [? yy ] ? E D [ K i =y (? yy ? ? yi )L(e (i) , x, f )]</formula><p>Hence,</p><formula xml:id="formula_48">R ? L (f * ) ? R ? L (f * ? ) ? (B U ? B L )E D [? yy ]+ (47) + E D [ K i =y (? yy ? ? yi ) L(e (i) , x, f * ? ) ? L(e (i) , x, f * ) ]</formula><p>From the assumption that R L (f * ) = 0, we have L(e (y) , x, f * ) = 0. Using the conditions on the loss function from Lemma 1, for all i = y, we get</p><formula xml:id="formula_49">L(e (i) , x, f * ? ) ? L(e (i) , x, f * ) = / L(e (y) , x, f * ) = 0 and i) / = L(e (i) , x, f * ? ) ? L(e (i) , e (y) ) = / iii) / = L(e (i) , x, f * ? ) ? C 2 = / ii) / ? C 1 ? C 2</formula><p>By our assumption on the noise rates, we have ? yy ? ? yi &gt; 0. We have</p><formula xml:id="formula_50">R ? L (f * ) ? R ? L (f * ? ) ? (B U ? B L )E D [? yy ] + (C 1 ? C 2 )E D [ K i =y (? yy ? ? yi )] Since f * ? is the global minimizer of R ? L (f ) we have R ? L (f * ) ? R ? L (f * ? ) ? 0, which is the lower bound.</formula><p>Remark 2. The generalized Jensen-Shannon Divergence satisfies the conditions in <ref type="bibr">Lemma 1,</ref><ref type="bibr">with</ref> Proof of Proposition 4. Symmetric Noise From the proof of Theorem 1, we have for any function, f , mapping an input</p><formula xml:id="formula_51">x ? X to ? K?1 R ? L (f ) = 1 ? ?K K ? 1 R L (f ) + ? K ? 1 E x E y|x K i=1 L(e (i) , x, f )</formula><p>Using Proposition 2 for GJS, we get</p><formula xml:id="formula_52">R ? LGJS (f ) = 1 ? ?K K ? 1 R LGJS (f ) + ? K ? 1 E x E y|x K i=1 L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 ) ?K K ? 1 E x E y|x L f GJS ? (p (2) , . . . , p (M ) )</formula><p>Let B JS L , B JS U be the lower and upper bound for JS (M=2) in Proposition 5. These bounds 5 holds for any p (2) ? ? K?1 and therefore also holds forp &gt;1 . Hence, we have</p><formula xml:id="formula_53">R ? LGJS (f ) ? 1 ? ?K K ? 1 R LGJS (f ) + ?B JS L K ? 1 + (1 ? ? 1 ) ?K K ? 1 E x E y|x L f GJS ? (p (2) , . . . , p (M ) ) R ? LGJS (f ) ? 1 ? ?K K ? 1 R LGJS (f ) + ?B JS U K ? 1 + (1 ? ? 1 ) ?K K ? 1 E x E y|x L f GJS ? (p (2) , . . . , p (M ) )</formula><p>With these bounds, the difference between R ? L (f * ) and R ? L (f * ? ) can be bounded as follows</p><formula xml:id="formula_54">R ? LGJS (f * ) ? R ? LGJS (f * ? ) ? 1 ? ?K K ? 1 (R LGJS (f * ) ? R LGJS (f * ? )) + ?(B JS U ? B JS L ) K ? 1 + (1 ? ? 1 )?K K ? 1 E x E y|x L f * GJS ? (p (2) , . . . , p (M ) ) ? L f * ? GJS ? (p (2) , . . . , p (M ) ) ? ?(B JS U ? B JS L ) K ? 1</formula><p>where the last inequality follows from the assumption on the noise rate, </p><formula xml:id="formula_55">(1 ? ?K K?1 ) &gt; 0, that f * is the minimizer of R L (f ) so R L (f * ) ? R L (f * ? ) ? 0,</formula><formula xml:id="formula_56">R ? L (f ), we have R ? L (f * ) ? R ? L (f * ? ) ? 0,</formula><p>which is the lower bound. Hence, we have shown that L JS and L GJS have the same bounds for the risk difference for symmetric noise.</p><p>Asymmetric Noise For class dependent noisy(asymmetric) and any function, f , mapping an input x ? X to ? K?1 , we have</p><formula xml:id="formula_57">R ? LGJS (f ) = E D [ K i=1 ? yi L GJS (e (i) , x, f )] = E D [? yy L f JS ? (e (y) ,p &gt;1 ) + K i =y ? yi L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 )L f GJS ? (p (2) , . . . , p (M ) )] = E D [? yy K i=1 L f JS ? (e (i) ,p &gt;1 ) ? K i =y L f JS ? (e (i) ,p &gt;1 ) + K i =y ? yi L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 )L f GJS ? (p (2) , . . . , p (M ) )] = E D [? yy K i=1 L f JS ? (e (i) ,p &gt;1 ) ? K i =y (? yy ? ? yi )L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 )L f GJS ? (p (2) , . . . , p (M ) )]</formula><p>where Proposition 2 was used to separate GJS into a JS and a consistency term. By using the bounds</p><formula xml:id="formula_58">B JS L , B JS U we get R ? LGJS (f ) ? E D [? yy B JS U ? K i =y (? yy ? ? yi )L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 )L f GJS ? (p (2) , . . . , p (M ) )] R ? LGJS (f ) ? E D [? yy B JS L ? K i =y (? yy ? ? yi )L f JS ? (e (i) ,p &gt;1 ) + (1 ? ? 1 )L f GJS ? (p (2) , . . . , p (M ) )]</formula><p>Hence,</p><formula xml:id="formula_59">R ? L (f * ) ? R ? L (f * ? ) ? (B JS U ? B JS L )E D [? yy ] + E D [ K i =y (? yy ? ? yi ) L f * ? JS ? (e (i) ,p &gt;1 ) ? L f * JS ? (e (i) ,p &gt;1 ) ]<label>(49)</label></formula><formula xml:id="formula_60">+ (1 ? ? 1 ) E D [L f * GJS ? (p (2) , . . . , p (M ) )] ? E D [L f * ? GJS ? (p (2) , . . . , p (M ) )]<label>(50)</label></formula><p>From the assumption that R LGJS (f * ) = 0, we have L GJS (e (y) , x, f * ) = 0. Using the conditions on the loss function from Lemma 1, for all i = y, we get</p><formula xml:id="formula_61">L f * ? JS ? (e (i) ,p &gt;1 ) ? L f * JS ? (e (i) ,p &gt;1 ) = / L GJS (e (y) , x, f * ) = 0 and i) / = L f * ? JS ? (e (i) ,p &gt;1 ) ? L f * JS ? (e (i) , e (y) ) = / iii) and Remark 2 / = L f * ? JS ? (e (i) ,p &gt;1 ) ? C 1 ? 0</formula><p>From above and our assumption on the noise rates (? yy ? ? yi &gt; 0), we have that the term in Equation 49 is less or equal to zero. Due to the assumption on the consistency of f * and f * ? in Proposition 4, this is also the case for the term in Equation 50. We have</p><formula xml:id="formula_62">R ? LGJS (f * ) ? R ? LGJS (f * ? ) ? (B JS U ? B JS L )E D [? yy ] Since f * ? is the global minimizer of R ? LGJS (f ) we have R ? LGJS (f * ) ? R ?</formula><p>LGJS (f * ? ) ? 0, which is the lower bound. Hence, we have shown that L JS and L GJS have the same bounds for the risk difference for asymmetric noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Bounds</head><p>In this section, we first introduce some useful definitions and relate them to JS. Then, the bounds for JS and GJS are proven.  where u is the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Another Definition of Jensen-Shannon divergence</head><formula xml:id="formula_63">f ?1 (t) := H(? 1 t + 1 ? ? 1 ) ? ? 1 H(t) , t &gt; 0 (51) f ?1 (0) := lim t?0 f ?1 (t) (52) 0f ?1 0 0 := 0,<label>(53)</label></formula><formula xml:id="formula_64">+ 1 ? ?) (56) = K k=1 ?p (1) k log( p (1) k p (2) k ) ? (?p (1) k + (1 ? ?)p (2) k ) log( ?p (1) k + (1 ? ?)p (2) k p (2) k ) (57) = K k=1 ?p (1) k log( p (1) k p (2) k ) ? ?p (1) k log( ?p (1) k + (1 ? ?)p (2) k p (2) k ) ? (1 ? ?)p (2) k log( ?p (1) k + (1 ? ?)p (2) k p (2) k ) (58) = K k=1 ?p (1) k log( p (1) k ?p (1) k + (1 ? ?)p (2) k ) + (1 ? ?)p (2) k log( p (2) k ?p (1) k + (1 ? ?)p (2) k ) (59) = K k=1 ?D KL p (1) k , ?p (1) k + (1 ? ?)p (2) k + (1 ? ?)D KL p (2) k , ?p (1) k + (1 ? ?)p (2) k (60) = D JS? (p (1) , p (2) )<label>(55)</label></formula><p>Proof of Proposition 5.</p><p>First we start with two observations: 1)</p><formula xml:id="formula_66">K k=1 L JS (e (k) , p) is strictly convex. 2) K k=1 L JS (e (k)</formula><p>, p) is invariant to permutations of the components of p.</p><p>First, we show Observation 1). This is done by using Remark 3 and showing that the second derivatives are larger than zero</p><formula xml:id="formula_67">f ?1 (t) := H(? 1 t + 1 ? ? 1 ) ? ? 1 H(t) , t &gt; 0 (62) f ?1 (t) = ? 1 (? log(? 1 t + 1 ? ? 1 ) + log(t)) (63) f ?1 (t) = ? 1 (1 ? ? 1 ) ? 1 t 2 + t(1 ? ? 1 )<label>(64)</label></formula><p>Hence, f ?1 (t) is strictly convex, since ? 1 &gt; 0 and t &gt; 0, then f ?1 (t) &gt; 0. With Remark 3, and that the sum of strictly convex functions is also strictly convex, it follows that K k=1 L JS (e (k) , p) is strictly convex.</p><p>Next, we show Observation 2), i.e. that H(? 1 e (k) + ? 2 p) ? ? 2 H(p) (65)</p><formula xml:id="formula_68">= K k=1 H(? 1 + ? 2 p k ) + K i =k H(? 2 p i ) ? ? 2 H(p) (66) = K k=1 H(? 1 + ? 2 p k ) + K k=1 K i =k H(? 2 p i ) ? ? 2 KH(p) (67) = K k=1 H(? 1 + ? 2 p k ) + K k=1 H(? 2 p) ? H(? 2 p k ) ? ? 2 KH(p) (68) = K k=1 H(? 1 + ? 2 p k ) + (K ? 1)H(? 2 p) ? ? 2 KH(p) (69) = K k=1 H(? 1 + ? 2 p k ) + (K ? 1)(H(? 2 ) + ? 2 H(p)) ? ? 2 KH(p) (70) = K k=1 H(? 1 + ? 2 p k ) + (K ? 1)H(? 2 ) ? ? 2 H(p)<label>(71)</label></formula><p>Clearly, a permutation of the components of p does not change the first sum or H(p), since it would simply reorder the summands. Hence, K k=1 L JS (e (k) , p) is invariant to permutations of p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower bound:</head><p>The minimizer of a strictly convex function K k=1 L JS (e (k) , p) over a compact convex set ? K?1 is unique. Since u is the only element of ? K?1 that is the same under permutation, it is the unique minimum of K k=1 L JS (e (k) , p) for p ? ? K?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper bound:</head><p>The maximizer of a strictly convex function K k=1 L JS (e (k) , p) over a compact convex set ? K?1 is at its extreme points e (i) for i ? {1, 2, . . . , K} . All extreme points have the same value according to Observation 2). where u ? ? K?1 is the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.3 Bounds for GJS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 3.</head><p>Lower bound: Using Proposition 2 to rewrite GJS into a JS and a consistency term, we get K k=1 D GJS? (e (k) , p <ref type="bibr" target="#b1">(2)</ref> , . . . ,p (M ) ) = K k=1 D JS ? (e (k) ,p &gt;1 ) + (1 ? ? 1 )D GJS ? (p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) )</p><formula xml:id="formula_69">(72) = K k=1 D JS ? (e (k) ,p &gt;1 ) + (1 ? ? 1 )KD GJS ? (p (2) , . . . , p (M ) ) (73) ? K k=1 D JS ? (e (k) , u) + (1 ? ? 1 )KD GJS ? (p (2) , . . . , p (M ) ) (74) ? K k=1 D JS ? (e (k) , u)<label>(75)</label></formula><p>where the first inequality comes from the lower bound of Proposition 5, and the second inequality comes from (1 ? ? 1 )KD GJS ? (p (2) , . . . , p (M ) ) being non-negative. The inequalities holds with equality if and only if p (2) = ? ? ? = p (M ) = u. Notably, the lower bound of JS is the same as that of GJS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper bound:</head><p>Let's denote A(p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) = K k=1 L GJS (e (k) , p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ). First we start by making 5 observations: Unlike for JS, the extreme points of ? K?1 M ?1 do not necessarily map to the same value of A. Hence, what is left to show is that the set of extreme points with all predictive distributions being distinct unit vectors maps to the maximum value of A.</p><formula xml:id="formula_70">Observation 1: ? K?1 M ?1 = ? K?1 ? ? K?1 ? ? ? ? ? ? K?1 is a compact convex set. Observation 2: A is strictly convex over ? K?1 M ?1 .</formula><p>Given Observation 3, all the M distributions are unit vectors, therefore the maximum is of the form A(p <ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ) = K k=1 H(? 1 e (k) + (1 ? ? 1 )p &gt;1 ), wherep &gt;1 := M j=2 ? j p (j) /(1 ? ? 1 ). Furthermore, at most M ? 1 components ofp &gt;1 are non-zero (if all predictions are distinct). From Observation 4, we can WLOG permutep &gt;1 such that the first M ? 1 components are the largest ones. Letp ? &gt;1 ? ? M ?2 denote the subset of these first M ? 1 components ofp &gt;1 ? ? K?1 . Then, for all predictive distributions being unit vectors, we have</p><formula xml:id="formula_71">A(p (2) , . . . , p (M ) ) = K k=1 H(?1e (k) + (1 ? ?1)p&gt;1) (76) = M ?1 k=1 H(?1 + (1 ? ?1)m &gt;1,k ) + (K ? 1)H((1 ? ?1)m &gt;1,k ) + K k=M H(?1) (77) = M ?1 k=1 H(?1 + (1 ? ?1)m &gt;1,k ) + (K ? 1)H((1 ? ?1)p ? &gt;1 ) + K k=M H(?1) (78) ? (M ? 1)H( 1 M ? 1 M ?1 k=1 ?1 + (1 ? ?1)m &gt;1,k ) + (K ? 1)H((1 ? ?1))p ? &gt;1 ) + K k=M H(?1) (79) = (M ? 1)H(?1 + 1 ? ?1 M ? 1 ) + (K ? 1)H((1 ? ?1)p ? &gt;1 ) + K k=M H(?1)<label>(80)</label></formula><formula xml:id="formula_72">? (M ? 1)H(?1 + 1 ? ?1 M ? 1 ) + (K ? 1)H((1 ? ?1)u) + K k=M H(?1)<label>(81)</label></formula><p>The first inequality follows from Jensen's inequality and the second from the uniform distribution maximizes entropy. Both inequalities hold with equality iff m &gt;1,1 = ? ? ? = m &gt;1,M ?1 . Hence, the maximum is achieved ifp ? &gt;1 = u ? ? M ?2 , which is only possible if all M ? 1 predictive distributions are distinct unit vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Robustness of Jensen-Shannon losses</head><p>In this section, we prove that the lower (B L ) and upper (B U ) bounds become the same for JS and GJS as ? 1 ? 1 as stated in Remark 1. Remark 1. L JS and L GJS are robust (B L = B U ) in the limit of ? 1 ? 1.</p><p>Proof of Remark 1 for JS. Lower bound:</p><formula xml:id="formula_73">K k=1 D JS? (e (y) , u) = K k=1 H(? 1 e (k) + ? 2 u) ? ? 2 H(u) (82) = K[H(? 1 e (1) + ? 2 u) ? ? 2 H(u)] (83) = K[H(? 1 + ? 2 /K) + (K ? 1)H(? 2 /K) ? K? 2 H( 1 K )]<label>(84)</label></formula><p>= /H(? 2 /K) = ?? 2 /K(log ? 2 + log 1/K) = 1 K H(? 2 ) + ? 2 H(1/K)/ (85)</p><formula xml:id="formula_74">= K[H(? 1 + ? 2 /K) + (K ? 1)( 1 K H(? 2 ) + ? 2 H( 1 K )) ? K? 2 H( 1 K )] (86) = K[H(? 1 + ? 2 /K) + (K ? 1) 1 K H(? 2 ) ? ? 2 H( 1 K )]<label>(87)</label></formula><p>If one now normalize(Z = H(? 2 ) = H(1 ? ? 1 )) and take the limit as ? 1 ? 1 we get:</p><formula xml:id="formula_75">lim ?1?1 K k=1 L JS (e (y) , u) = lim ?1?1 (K ? 1) + K H(? 1 + ? 2 /K) ? ? 2 H( 1 K ) H(? 2 )<label>(88)</label></formula><p>= lim ?1?1 (K ? 1) + K ?(K ? 1)(1 + log (? 1 + ? 2 /K))/K ? log (1/K)/K log (1 ? ? 1 ) + 1 (89) = lim ?1?1 (K ? 1) ? (K ? 1)(1 + log (? 1 + ? 2 /K)) ? log (1/K) log (1 ? ? 1 ) + 1 (90) = lim ?1?1 (K ? 1) ? ((K ? 1)(1 + log (? 1 + ? 2 /K)) ? log (1/K)) 1 log (1 ? ? 1 ) + 1 (91)</p><formula xml:id="formula_76">= (K ? 1) ? (K ? 1 ? log (1/K)) ? 0 (92) = K ? 1<label>(93)</label></formula><p>where L'H?pital's rule was used for the fraction in Equation 88 which is indeterminate of the form 0 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper bound:</head><p>K k=1 L JS (e (k) , e (1) ) = 1 H(? 2 ) K k=1 H(? 1 e (k) + ? 2 e (1) ) (94)</p><formula xml:id="formula_77">= 1 H(? 2</formula><p>[(K ? 1)H(? 2 ) + (K ? 1)H(? 1 ) + H(? 1 + ? 2 )] (95)</p><formula xml:id="formula_78">= (K ? 1)[1 + H(? 1 ) H(? 2 ) ]<label>(96)</label></formula><formula xml:id="formula_79">= (K ? 1) 1 + ? 1 log ? 1 (1 ? ? 1 ) log (1 ? ? 1 )<label>(97)</label></formula><p>Taking the limit as ? 1 ? 1 gives</p><formula xml:id="formula_80">lim ?1?1 K k=1 L JS (e (k) , e (1) ) = lim ?1?1 (K ? 1) 1 + ? 1 1 log (1 ? ? 1 ) log ? 1 (1 ? ? 1 ) (98) = lim ?1?1 (K ? 1) 1 + ? 1 1 log (1 ? ? 1 ) 1 ? 1 1 ?1 (99) = (K ? 1)[1 + 1 ? 0 ? 1 ? ?1] (100) = K ? 1<label>(101)</label></formula><p>where L'H?pital's rule was used for lim ?1?1 log ?1</p><formula xml:id="formula_81">(1??1) which is indeterminate of the form 0 0 . Hence, B L = B U = K ? 1.</formula><p>Next, we look at the robustness of the generalized Jensen-Shannon loss.</p><p>Proof of Remark 1 for GJS. Proposition 2, shows that GJS can be rewritten as a JS term and a consistency term. From the proof of Remark 1 for JS above, it follows that the JS term satisfies B L = B U as ? 1 approaches 1. Hence, it is enough to show that the consistency term of GJS also becomes a constant in this limit. The consistency term is the generalized Jensen-Shannon divergence</p><formula xml:id="formula_82">lim ?1?1 (1 ? ? 1 )L GJS ? (p (2) , . . . , p (M ) ) = lim ?1?1 (1 ? ? 1 ) H(1 ? ? 1 ) D GJS ? (p (2) , . . . , p (M ) ) (102) = lim ?1?1 ? 1 log (1 ? ? 1 ) D GJS ? (p (2) , . . . , p (M ) ) (103) = 0<label>(104)</label></formula><p>where ? = [? 2 , . . . , ? M ]/(1 ? ? 1 ). D GJS ? (p (2) , . . . , p (M ) ) is bounded and ? 1 log (1??1) goes to zero as ? 1 ? 1, hence the limit of the product goes to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Gradients of Jensen-Shannon Divergence</head><p>The partial derivative of the Jensen-Shannon divergence is</p><formula xml:id="formula_83">?{H(m) ? ? 1 H(e (y) ) ? (1 ? ? 1 )H(p)} ?z i</formula><p>where m = ? 1 e (y) + ? 2 p = ? 1 e (y) + (1 ? ? 1 )p, and p j = e zj / K k=1 e z k . Note the difference between e z which is the exponential function while e (y) is a onehot label. We take the partial derivative of each term separately, but first the partial derivative of the jth component of a softmax output with respect to the ith component of the corresponding logit (1 ? ? 1 ) ?p j ?z i + (1 ? ? 1 ) ?p j ?z i log (m j ) (123)</p><formula xml:id="formula_84">= ? K j=1</formula><p>(1 ? ? 1 ) ?p j ?z i 1 + log (m j ) = / Equation 112 / (124)</p><formula xml:id="formula_85">= ?(1 ? ? 1 ) K j=1 ?p j ?z i log (m j )<label>(125)</label></formula><p>The partial derivative of the Jensen-Shannon divergence with respect to logit z i is ?{H(m) ? ? 1 H(e (y) ) ? (1 ? ? 1 )H(p)} ?z i = ?{H(m) ? (1 ? ? 1 )H(p)} ?z i (126)</p><p>= ?(1 ? ? 1 ) K j=1 ?p j ?z i log (m j ) ? log p j (127)</p><formula xml:id="formula_86">= ?(1 ? ? 1 ) K j=1 ?p j ?z i log m j p j<label>(128)</label></formula><p>If we now make use of the fact that the label is e (y) , we can write the partial derivative wrt to z i as ?{H(m) ? ? 1 H(e (y) ) ? (1 ? ? 1 )H(p)} ?z i = (129) = ?(1 ? ? 1 ) K j=1 ?p j ?z i log ? 1 e (y) j p j + (1 ? ? 1 ) (130) = ?(1 ? ? 1 ) ?p y ?z i log ? 1 p y + (1 ? ? 1 ) + K j =y ?p j ?z i log 1 ? ? 1 (131) = ?(1 ? ? 1 ) ?p y ?z i log ? 1 p y + (1 ? ? 1 ) + log 1 ? ? 1 = ?(1 ? ? 1 ) ?p y ?z i log ? 1 p y + (1 ? ? 1 ) ? log 1 ? ? 1 (134)</p><p>= ?(1 ? ? 1 ) ?p y ?z i log ? 1 (1 ? ? 1 )p y + 1 (135)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extended Related Works</head><p>Most related to us is the avenue of handling noisy labels in deep learning via the identification and construction of noise-robust loss functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Ghosh et al. <ref type="bibr" target="#b1">[2]</ref> derived sufficient conditions for a loss function, in empirical risk minimization (ERM) settings, to be robust to various kinds of sample-independent noise, including symmetric, symmetric non-uniform, and class-conditional. They further argued that, while CE is not a robust loss function, mean absolute error (MAE) is a loss that satisfies the robustness conditions and empirically demonstrated its effectiveness. On the other hand, Zhang et al. <ref type="bibr" target="#b2">[3]</ref> pointed out the challenges of training with MAE and proposed GCE which generalizes both MAE and CE losses. Tuning for this trade-off, GCE alleviates MAE's training difficulties while retaining some desirable noise-robustness properties. In a similar fashion, symmetric cross entropy (SCE) <ref type="bibr" target="#b3">[4]</ref> spans the spectrum of reverse CE as a noise-robust loss function and the standard CE. Recently, Ma et al. <ref type="bibr" target="#b4">[5]</ref> proposed a normalization mechanism to make arbitrary loss functions robust to noise. They, too, further combine two complementary loss functions to improve the data fitting while keeping robust to noise. The current work extends on this line of works.</p><p>Several other directions are pursued to improve training of deep networks under noisy labeled datasets. This includes methods to identify and remove noisy labels <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> or identify and correct noisy labels in a joint label-parameter optimization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> and those works that design an elaborate training pipeline for dealing with noise <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. In contrast to these directions, this work proposes a robust loss function based on Jensen-Shannon divergence (JS) without altering other aspects of training. In the following, we review the directions that are most related to this paper.</p><p>A close line of works to ours reweight a loss function by a known or estimated class-conditional noise model <ref type="bibr" target="#b10">[11]</ref>. This direction has been commonly studied for deep networks with a standard cross entropy (CE) loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Assuming a class-conditional noise model, loss correction is theoretically well motivated.</p><p>A common regularization technique called label smoothing <ref type="bibr" target="#b40">[41]</ref> has been recently proposed that operates similarly to the loss correction methods. While its initial purpose was for deep networks to avoid overfitting, label smoothing has been shown to have a noticeable effect when training with noisy sets by alleviating the fit to the noise <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Consistency regularization is a recently-developed technique that encourages smoothness in the learnt decision boundary by requiring minimal shifts in the learnt function when small perturbations are applied to an input sample. This technique has become increasingly common in the state-of-the-art semi-supervised learning <ref type="bibr">[42,</ref><ref type="bibr">43,</ref><ref type="bibr">44]</ref> and recently for dealing with noisy data <ref type="bibr" target="#b15">[16]</ref>. These methods use various complicated pipelines to integrate consistency regularization in training. This work shows that a multi-distribution generalization of JS can neatly incorporate such regularization. Hendrycks et al. <ref type="bibr" target="#b5">[6]</ref> recently proposed AugMix, a novel data augmentation strategy in combination with a GJS consistency loss to improve uncertainty estimation and robustness to image corruptions at test-time. Our work is orthogonal since we consider the task of learning under noisy labels at training time and conduct the corresponding experiments. We also investigate and derive the theoretical properties of the proposed loss functions. Finally, our losses are solely implemented based on JS/GJS instead of a combination of CE and GJS in case of AugMix. However, we find it promising that GJS improves robustness to both training-time label noise and test-time image corruption, which further strengthens the significance of the JS-based loss functions.</p><p>Finally, recently, Xu et al. <ref type="bibr" target="#b17">[18]</ref>; Wei &amp; Liu <ref type="bibr" target="#b18">[19]</ref> propose loss functions with information theory motivations. Jensen-Shannon divergence, with inherent information theoretic interpretations, naturally posits a strong connection of our work to those. Especially, the latter is a close concurrent work that studies the general family of f -divergences but takes a different and complementary angle. In this work, we analyze the role of ? 1 , which they treat as a constant. Varying ? 1 is important because it leads to:</p><p>? Better empirical performance. For our experiments on CIFAR, we provide the hyperparameters used in <ref type="table" target="#tab_7">Table 7</ref>, from which we can see that the optimal is equal to their setting (? 1 = 0.5) in only 3/14 cases. ? Interesting theoretical connections to related work. In Proposition 1, we show that the JS loss has CE and MAE as asymptotes when ? 1 goes to zero and one, respectively. This causes an interesting trade-off between learnability and robustness as discussed in Section 4.3.</p><p>Furthermore, we consider the generalization to more than two distributions which have proved helpful while Wei &amp; Liu <ref type="bibr" target="#b18">[19]</ref> only study two distributions.</p><p>In this work, we use a generalization of the Jensen-Shannon divergence to more than two distributions, which was introduced by Lin <ref type="bibr" target="#b7">[8]</ref>. Recently, another generalization of JS was presented by Nielsen <ref type="bibr" target="#b20">[21]</ref>, where the arithmetic mean is generalized to abstract means. JS is also a special case of a general family of divergences, the f-divergences <ref type="bibr" target="#b19">[20]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(b-c) shows the consistency of predictions for correct and noisy labeled examples of the training set, with the consistency measured as the ratio of examples that have the same class prediction for two perturbations of the same image, see JS loss generalizes CE and MAE. The Jensen-Shannon loss (L JS ) for different values of the hyperparameter ? 1 . The JS loss interpolates between CE and MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Effect of ? 1 . Validation accuracy of JS and GJS during training with symmetric noise on CIFAR100. From Proposition 1, JS behaves like CE and MAE for low and high values of ? 1 , respectively. The signs of noise-fitting for ? 1 = 0.1 on 60% noise (b), and slow learning of ? 1 = 0.9 (a-b), show this in practice. The GJS loss does not exhibit overfitting for low values of ? 1 and learns quickly for large values of ? 1 (c-d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Evolution of a trained network's consistency for the CE, JS, and GJS losses. We plot the evolution of the validation accuracy (a) and network's consistency on clean (b) and noisy (c) examples of the training set of CIFAR-100 when learning with 40% symmetric noise. All losses use the same learning rate and weight decay and both JS and GJS use ? 1 = 0.5. The consistency of the learnt function and the accuracy closely correlate. The accuracy and consistency of JS and GJS improve during training, while both degrade when learning with CE loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[0.001, 0.005, 0.01, 0.05, 0.1] and weight decays in [1e ? 4, 5e ? 4, 1e ? 3]. The method-specific hyperparameters used for this search were 0.9, 0.7, (0.1,1.0), 0.7, (1.0,1.0), 0.5, 0.5 for BS(?), LS( ), SCE(?, ?), GCE(q), NCE+RCE(?, ?), JS(? 1 ) and GJS(? 1 ), respectively. For CIFAR-100, we search for learning rates in [0.01, 0.05, 0.1, 0.2, 0.4] and weight decays in [1e?5, 5e?5, 1e?4]. The method-specific hyperparameters used for this search were 0.9, 0.7, (6.0,0.1), 0.7, (10.0,0.1), 0.5, 0.5 for BS(?), LS( ), SCE(?, ?), GCE(q), NCE+RCE(?, ?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>11 :Figure 7 :</head><label>117</label><figDesc>Ablation Study of JS. A comparison of JS and other KL-based divergences and their relationship to symmetry and boundedness. The distribution m is the mean of p and q. p) Jeffrey's (KL(p, q) + KL(q, p))/2 K KL(p, m) K' KL(q, m) JS (KL(p, m)+KL(q, m)Ablation Study of JS. Validation accuracy of the divergences in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Comparison between JS and GCE. A comparison of gradients scales between JS and GCE. For each q of GCE, a corresponding ? 1 of JS is chosen such that the gradient scales are equal at p y = 1 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 .LLL</head><label>1</label><figDesc>'s Connection to CE and MAE Proposition Let p ? ? K?1 , then lim ?1?0 JS (e (y) , p) = H(e (y) , p), lim ?1?1 L JS (e (y) , p) = 1 2 e (y) ? p 1 where H(e (y) , p) is the cross entropy of e (y) relative to p. Proof of Proposition 1. We want to show lim ?1?0 JS (e (y) , p) = lim ?1?0 JS ? (e (y) , p) H(1 ? ? 1 ) = H(e (y) , p) (5) lim ?1?1 JS (e (y) , p) = lim ?1?1 JS ? (e (y) , p) H(1 ? ? 1 ) = 1 2 e (y) ? p 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C. 3 . 2 1 .</head><label>321</label><figDesc>Asymmetric NoiseLemma Consider the following conditions for a loss with label e (i) , for any i ? {1, 2, . . . , K} and M-1 distributions p<ref type="bibr" target="#b1">(2)</ref> , . . . , p (M ) ? ? K?1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>and the assumption on the consistency of f * and f * ? . Similarly, since f * ? is the minimizer of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Remark 3 .</head><label>3</label><figDesc>The Jensen-Shannon divergence can be rewritten using Equation 51 as follows D JS? (p<ref type="bibr" target="#b0">(1)</ref> , p (2) ) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>) C. 4 . 2</head><label>42</label><figDesc>Bounds for JS Proposition 5. L JS has BL ? K k=1 LJS(e (k) , f (x)) ? BU with BL = K k=1 LJS(e (k) , u), BU = K k=1 LJS(e (k) , e (1) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>K k=1 L</head><label>k=1</label><figDesc>JS (e (k) , p) is invariant to permutations of p K k=1 D JS (e (k) , p) = K k=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Proposition 3 .</head><label>3</label><figDesc>GJS loss with M ? K + 1 satisfies B L ? K k=1 L GJS (e (k) , p (2) , . . . , p (M ) ) ? B U for all p (2) , . . . , p (M ) ? ? K?1 ,with the following bounds B L = K k=1 L GJS (e (k) , u, . . . , u), B U = K k=1 L GJS (e (k) , e (1) , . . . , e (M ?1) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Observation 3 :</head><label>3</label><figDesc>From Observations 1 and 2 we have that the maximizer of A should be at extreme points of ? K?1 M ?1 , i.e., a unit vector in every M ? 1 individual ? K?1 subspaces of ? K?1 M ?1 . Observation 4: A is symmetric w.r.t. permutations of the components of predictive distributions p (i) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>k=1 e z k 2 (?Hj?</head><label>2</label><figDesc>= j)e zj K k=1 e z k ? e zj e zi K = j)e zj ? p j e zi K k=1 e z k (108)= 1(i = j)p j ? p j p i (109) = p j (1(i = j) ? p i )(110)= p i (1(i = j) ? p j ) = ?p i ?z j(111)where 1(i = j) is the indicator function, i.e. 1 when i = j and zero otherwise. Using the above, weget i = j) ? p j ) = p i (1 ? 1) = 0 (112) First, the partial derivative of H(p) wrt z i partial derivative of H(m) wrt z i ?{H(m)} ?z i = ?{? 1 H(e (y) , m) + (1 ? ? 1 )H(p, m)} ?z i ? log (m j ) ?z i + (1 ? ? 1 ) ?{p j log (m j )} ?z i log (m j ) ?z i + (1 ? ? 1 ) ?p j ?z i log (m j ) + p j ? log (m j ) ?z i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Real-world Noise Benchmark on WebVision. Mean test accuracy and standard deviation from five runs are reported for the validation sets of (mini) WebVision and ILSVRC12. GJS with two networks correspond to the mean prediction of two independently trained GJS networks with different seeds for data augmentation and weight initialization. Here, GJS uses Z = 1. Results marked with ? are from Zheltonozhskii et al.<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell cols="2">Augmentation Networks</cell><cell>WebVision</cell><cell></cell><cell cols="2">ILSVRC12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top 1</cell><cell>Top 5</cell><cell>Top 1</cell><cell>Top 5</cell></row><row><cell>ELR+ [27] ?</cell><cell>Inception-ResNet-V2</cell><cell>Mixup</cell><cell>2</cell><cell>77.78</cell><cell>91.68</cell><cell>70.29</cell><cell>89.76</cell></row><row><cell cols="2">DivideMix [16] ? Inception-ResNet-V2</cell><cell>Mixup</cell><cell>2</cell><cell>77.32</cell><cell>91.64</cell><cell>75.20</cell><cell>90.84</cell></row><row><cell>DivideMix [16] ?</cell><cell>ResNet-50</cell><cell>Mixup</cell><cell>2</cell><cell cols="4">76.32 ? 0.36 90.65 ? 0.16 74.42 ? 0.29 91.21 ? 0.12</cell></row><row><cell>CE</cell><cell>ResNet-50</cell><cell>ColorJitter</cell><cell>1</cell><cell cols="3">70.69 ? 0.66 88.64 ? 0.17 67.32 ? 0.57</cell><cell>88.00 ? 0.49</cell></row><row><cell>JS</cell><cell>ResNet-50</cell><cell>ColorJitter</cell><cell>1</cell><cell cols="3">74.56 ? 0.32 91.09 ? 0.08 70.36 ? 0.12</cell><cell>90.60 ? 0.09</cell></row><row><cell>GJS</cell><cell>ResNet-50</cell><cell>ColorJitter</cell><cell>1</cell><cell cols="3">77.99 ? 0.35 90.62 ? 0.28 74.33 ? 0.46</cell><cell>90.33 ? 0.20</cell></row><row><cell>GJS</cell><cell>ResNet-50</cell><cell>ColorJitter</cell><cell>2</cell><cell cols="4">79.28 ? 0.24 91.22 ? 0.30 75.50 ? 0.17 91.27 ? 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Effect of GJS. Validation accuracy when using different loss functions for clean and noisy examples of the CIFAR-100 training set with 40% symmetric noise. Noisy examples benefit significantly more from GJS than clean examples (74.1 vs 72.9).Is GJS mostly helping the clean or noisy examples? To better understand the improvements of GJS over JS, we perform an ablation with different losses for clean and noisy examples, seeTable 4. We observe that using GJS instead of JS improves performance in all cases. Importantly, using GJS only for the noisy examples performs significantly better than only using it for the clean examples (74.1 vs 72.9). The best result is achieved when using GJS for both clean and noisy examples but still close to the noisy-only case (74.7 vs 74.1).</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell>?1</cell></row><row><cell cols="2">Clean Noisy</cell><cell>0.1</cell><cell>0.5</cell><cell>0.9</cell></row><row><cell>JS</cell><cell>JS</cell><cell cols="3">70.0 71.5 55.3</cell></row><row><cell>GJS</cell><cell>JS</cell><cell cols="3">72.6 72.9 70.2</cell></row><row><cell>JS</cell><cell>GJS</cell><cell cols="3">71.0 74.1 68.0</cell></row><row><cell>GJS</cell><cell>GJS</cell><cell cols="3">71.3 74.7 73.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Effect of Augmentation Strategy.</figDesc><table><row><cell>Val-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Effect of Number of Epochs. Validation accuracy for training with 200 and 400 epochs for 40% symmetric and asymmetric noise on CIFAR-100. GJS still outperforms the baselines and NCE+RCE's performance is reduced heavily by the decrease in epochs.</figDesc><table><row><cell>Method</cell><cell cols="2">Symmetric</cell><cell cols="2">Asymmetric</cell></row><row><cell></cell><cell>200</cell><cell>400</cell><cell>200</cell><cell>400</cell></row><row><cell>GCE</cell><cell>70.3</cell><cell>70.8</cell><cell>39.1</cell><cell>51.7</cell></row><row><cell>NCE+RCE</cell><cell>60.0</cell><cell>68.5</cell><cell>35.0</cell><cell>57.5</cell></row><row><cell>GJS</cell><cell>72.9</cell><cell>74.8</cell><cell>43.2</cell><cell>62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>[42] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979-1993, 2018.[43] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pages 5049-5059, 2019.[44] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195-1204, 2017.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>methods train a randomly initialized ResNet-50 model from PyTorch using the SGD optimizer with Nesterov momentum, and a batch size of 32 for GJS and 64 for CE and JS. For data augmentation, we do a random resize crop of size 224, random horizontal flips, and color jitter (torchvision ColorJitter transform with brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2). We use a fixed weight decay of 1e ? 4 and do a grid search for the best learning rate in [0.1, 0.2, 0.4] and ? 1 ? [0.1, 0.3, 0.5, 0.7, 0.9].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for CIFAR. A hyperparameter search over learning rates and weight decays, was done for 40% noise on both symmetric and asymmetric noise for the CIFAR datasets. The best parameters for each method are shown in this table, where the format is [learning rate, weight decay]. The hyperparameters for zero percent noise uses the same settings as for the symmetric noise. For the best learning rate and weight decay, another search is done for method-specific hyperparameters, and the best values are shown here. For methods with a single hyperparameter, the value correspond to their respective hyperparameter, i.e. BS (?), LS ( ), GCE (q), JS (? 1 ), GJS (? 1 ). For NCE+RCE and SCE the value correspond to [?, ?].</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Learning Rate &amp; Weight Decay</cell><cell></cell><cell></cell><cell cols="3">Method-specific Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>Sym Noise</cell><cell>Asym Noise</cell><cell>No Noise</cell><cell></cell><cell cols="2">Sym Noise</cell><cell></cell><cell cols="2">Asym Noise</cell><cell></cell></row><row><cell></cell><cell></cell><cell>20-80%</cell><cell>20-40%</cell><cell>0%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>20%</cell><cell>40%</cell><cell></cell></row><row><cell></cell><cell>CE</cell><cell>[0.05, 1e-3]</cell><cell>[0.1, 1e-3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>BS</cell><cell>[0.1, 1e-3]</cell><cell>[0.1, 1e-3]</cell><cell>0.5</cell><cell>0.5</cell><cell>0.7</cell><cell>0.7</cell><cell>0.9</cell><cell>0.7</cell><cell>0.5</cell><cell></cell></row><row><cell></cell><cell>LS</cell><cell>[0.1, 5e-4]</cell><cell>[0.1, 1e-3]</cell><cell>0.1</cell><cell>0.5</cell><cell>0.9</cell><cell>0.7</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell></cell></row><row><cell>CIFAR-10</cell><cell>SCE GCE</cell><cell>[0.01, 5e-4] [0.01, 5e-4]</cell><cell>[0.05, 1e-3] [0.1, 1e-3]</cell><cell cols="8">[0.2, 0.1] [0.05, 0.1] [0.1, 0.1] [0.2, 1.0] [0.1,1.0] [0.1, 0.1] [0.2, 1.0] 0.5 0.7 0.7 0.7 0.9 0.1 0.1</cell></row><row><cell></cell><cell cols="2">NCE+RCE [0.005, 1e-3]</cell><cell>[0.05, 1e-4]</cell><cell>[10, 0.1]</cell><cell cols="7">[10, 0.1] [10, 0.1] [1.0, 0.1] [10,1.0] [10, 0.1] [1.0, 0.1]</cell></row><row><cell></cell><cell>JS</cell><cell>[0.01, 5e-4]</cell><cell>[0.1, 1e-3]</cell><cell>0.1</cell><cell>0.7</cell><cell>0.7</cell><cell>0.9</cell><cell>0.9</cell><cell>0.3</cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell>GJS</cell><cell>[0.1, 5e-4]</cell><cell>[0.1, 1e-3]</cell><cell>0.5</cell><cell>0.3</cell><cell>0.9</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell>CE</cell><cell>[0.4, 1e-4]</cell><cell>[0.2, 1e-4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>BS</cell><cell>[0.4, 1e-4]</cell><cell>[0.4, 1e-4]</cell><cell>0.7</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.9</cell><cell>0.3</cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell>LS</cell><cell>[0.2, 5e-5]</cell><cell>[0.4, 1e-4]</cell><cell>0.1</cell><cell>0.7</cell><cell>0.7</cell><cell>0.7</cell><cell>0.9</cell><cell>0.5</cell><cell>0.7</cell><cell></cell></row><row><cell>CIFAR-100</cell><cell>SCE GCE</cell><cell>[0.2, 1e-4] [0.4, 1e-5]</cell><cell>[0.4, 5e-5] [0.2, 1e-4]</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.7</cell><cell>0.7</cell><cell>0.7</cell><cell>0.7</cell><cell>0]</cell></row><row><cell></cell><cell cols="2">NCE+RCE [0.2, 5e-5]</cell><cell>[0.2, 5e-5]</cell><cell>[20, 0.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1]</cell></row><row><cell></cell><cell>JS</cell><cell>[0.2, 1e-4]</cell><cell>[0.1, 1e-4]</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.3</cell><cell>0.5</cell><cell>0.5</cell><cell></cell></row><row><cell></cell><cell>GJS</cell><cell>[0.2, 5e-5]</cell><cell>[0.4, 1e-4]</cell><cell>0.3</cell><cell>0.3</cell><cell>0.5</cell><cell>0.9</cell><cell>0.1</cell><cell>0.5</cell><cell>0.1</cell><cell></cell></row></table><note>[0.1, 0.1] [0.1, 0.1] [0.1, 0.1] [0.1, 1.0] [0.1,0.1] [0.1, 1.0] [0.1, 1.[20, 0.1] [20, 0.1] [20, 0.1] [20,0.1] [20, 0.1] [10, 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Real-world Noise: ANIMAL-10N.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>CE</cell><cell>79.4 ? 0.14</cell></row><row><cell>SELFIE</cell><cell>81.8 ? 0.09</cell></row><row><cell>PLC</cell><cell>83.4 ? 0.43</cell></row><row><cell>GJS</cell><cell>84.2 ? 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Real-world Noise: Food-101N.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>CE</cell><cell>81.67</cell></row><row><cell>CleanNet</cell><cell>83.95</cell></row><row><cell>PLC</cell><cell>85.28 ? 0.04</cell></row><row><cell>GJS</cell><cell>86.56 ? 0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Consistency of Trained Networks on CIFAR. The training consistency of the networks fromTable 1. Mean train consistency and standard deviation are reported from five runs and the networks with significantly higher consistency are boldfaced. As observed in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">e.g. softmax neural network classifiers in this work</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We also tried using ? = 1 ? ?, and mapping the best parameters from the papers to this range, combined with a similar search as for the single parameter methods, but this resulted in worse performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>C 1 = H(?), C 2 = H(? 1 ) + H(1 ? ? 1 ). Proof of Remark 2. i). Follows directly from Jensen's inequality for the Shannon entropy. ii). The lower bound follows directly from Jensen's inequality for the non-negative Shannon entropy. The upper bound is shown below</p><p>where the inequality holds with equality iff 1</p><p>&gt; 0 for all j ? {1, 2, . . . , M } and l ? {1, 2, . . . , K}. Hence, GJS is bounded above by H(?). iii). Let the label be e (i) and the other M-1 distributions be e (j) with i = j then</p><p>? l e (j) ) ? ? 1 H(e (i) ) ? M l=2 ? l H(e (j) ) = H(? 1 e (i) + (1 ? ? 1 )e (j) ) (48) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.3 Improving GJS Risk Difference Bounds</head><p>, where L f GJS ? (p (2) , . . . , p (M ) ) is the consistency term from Proposition 2.</p><p>Proof of Remark 3. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09170</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum?? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on learning representation</title>
		<meeting>the international conference on learning representation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5836" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6838" to="6849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Sung Ju Hwang, and Jinwoo Shin. Consistency regularization for adversarial robustness</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6225" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When optimizing f-divergence is robust with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Information-type measures of difference of probability distributions and indirect observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studia Scientiarum Mathematicarum Hungarica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="318" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the jensen-shannon symmetrization of distances relying on abstract means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Making deep neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Contrast to divide: Self-supervised pre-training for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Earlylearning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SELFIE: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self: Learning to filter noisy labels with selfensembling</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tailin</forename><surname>Curtis G Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01936</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for learning with few clean and many noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combinatorial inference against label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geeho</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1173" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
